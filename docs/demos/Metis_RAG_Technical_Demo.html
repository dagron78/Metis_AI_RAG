<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG Technical Demonstration</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        h2 {
            color: #2980b9;
            border-left: 4px solid #3498db;
            padding-left: 10px;
            margin-top: 30px;
        }
        ul {
            padding-left: 20px;
        }
        li {
            margin-bottom: 8px;
        }
        .feature {
            font-weight: bold;
            color: #2c3e50;
        }
        .section {
            background-color: white;
            border-radius: 5px;
            padding: 15px 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .highlight {
            background-color: #f1f8ff;
            border-left: 4px solid #3498db;
            padding: 10px 15px;
            margin: 15px 0;
        }
        ol {
            padding-left: 20px;
        }
        ol li {
            margin-bottom: 10px;
        }
        .strategy {
            margin-bottom: 20px;
        }
        .strategy-name {
            font-weight: bold;
            color: #2980b9;
        }
        .future-feature {
            background-color: #e8f4fc;
            border-radius: 5px;
            padding: 10px 15px;
            margin-bottom: 15px;
        }
        .future-feature h3 {
            color: #2c3e50;
            margin-top: 0;
        }
    </style>
</head>
<body>
    <h1>Metis RAG Technical Demonstration</h1>
    
    <div class="section">
        <h2>What is Metis RAG?</h2>
        <p>
            Metis RAG is an advanced Retrieval-Augmented Generation platform that combines conversational AI with document-based knowledge retrieval. It enables users to chat with large language models while providing relevant context from uploaded documents, enhancing response accuracy and factual grounding.
        </p>
    </div>

    <div class="section">
        <h2>Key Features</h2>
        <ul>
            <li><span class="feature">Document Intelligence:</span> Supports multiple file formats (PDF, TXT, CSV, MD) with advanced processing capabilities</li>
            <li><span class="feature">Advanced RAG Engine:</span> Implements multiple chunking strategies (recursive, token-based, markdown header) for optimal document representation</li>
            <li><span class="feature">Vector Database Integration:</span> Uses ChromaDB with optimized caching for efficient similarity search</li>
            <li><span class="feature">LLM Integration:</span> Connects to Ollama for local LLM inference with enhanced prompt engineering</li>
            <li><span class="feature">Document Management:</span> Provides tagging, organization, and folder hierarchy for documents</li>
            <li><span class="feature">Analytics Dashboard:</span> Tracks system usage, performance metrics, and document utilization</li>
            <li><span class="feature">Responsive UI:</span> Features modern interface with light/dark mode and mobile optimization</li>
        </ul>
    </div>

    <div class="section">
        <h2>Technical Architecture</h2>
        <ul>
            <li><span class="feature">Frontend:</span> HTML/CSS/JavaScript with responsive design and streaming response handling</li>
            <li><span class="feature">Backend:</span> FastAPI (Python) with modular component design</li>
            <li><span class="feature">RAG Engine:</span> Custom implementation with document processor, vector store, and LLM integration</li>
            <li><span class="feature">Vector Database:</span> ChromaDB with performance optimizations and caching</li>
            <li><span class="feature">Testing Framework:</span> Comprehensive test suite for quality, performance, and edge cases</li>
        </ul>
    </div>

    <div class="section">
        <h2>Advanced Chunking Strategies</h2>
        
        <div class="strategy">
            <p><span class="strategy-name">1. Recursive Chunking</span> (Default)</p>
            <ul>
                <li>Recursively splits text by characters with configurable chunk size and overlap</li>
                <li>Maintains semantic coherence by respecting natural text boundaries</li>
                <li>Optimal for general-purpose documents with varied content</li>
                <li>Configurable parameters: chunk size (default: 500), chunk overlap (default: 50)</li>
            </ul>
        </div>
        
        <div class="strategy">
            <p><span class="strategy-name">2. Token-based Chunking</span></p>
            <ul>
                <li>Splits text based on token count rather than character count</li>
                <li>Preserves semantic meaning by respecting token boundaries</li>
                <li>Better suited for technical content where token-level precision matters</li>
                <li>Avoids mid-word splits that can occur with character-based chunking</li>
                <li>Optimizes for LLM context window utilization</li>
            </ul>
        </div>
        
        <div class="strategy">
            <p><span class="strategy-name">3. Markdown Header Chunking</span></p>
            <ul>
                <li>Intelligently splits markdown documents based on header structure</li>
                <li>Preserves document hierarchy and organizational structure</li>
                <li>Creates chunks that align with logical document sections</li>
                <li>Maintains header context for improved retrieval relevance</li>
                <li>Particularly effective for technical documentation and knowledge bases</li>
            </ul>
        </div>
        
        <div class="highlight">
            The chunking strategy selection is automatically determined based on document type but can be manually overridden. This multi-strategy approach significantly improves retrieval accuracy by ensuring that document chunks maintain semantic coherence and structural integrity.
        </div>
    </div>

    <div class="section">
        <h2>Basic Operation</h2>
        
        <ol>
            <li>
                <strong>Document Upload & Processing:</strong>
                <ul>
                    <li>Upload documents through the document management interface</li>
                    <li>Documents are processed through configurable chunking strategies</li>
                    <li>Text is embedded and stored in the vector database with metadata</li>
                </ul>
            </li>
            
            <li>
                <strong>Chat Interaction:</strong>
                <ul>
                    <li>User sends a query through the chat interface</li>
                    <li>System retrieves relevant document chunks based on semantic similarity</li>
                    <li>Retrieved context is combined with the query in a prompt to the LLM</li>
                    <li>Response is generated with citations to source documents</li>
                </ul>
            </li>
            
            <li>
                <strong>System Management:</strong>
                <ul>
                    <li>Monitor system performance through the analytics dashboard</li>
                    <li>Configure models and system parameters</li>
                    <li>View document usage statistics and query patterns</li>
                </ul>
            </li>
        </ol>
    </div>

    <div class="section">
        <h2>Performance & Testing</h2>
        <ul>
            <li>Comprehensive testing framework with quality, performance, and edge case tests</li>
            <li>Current response time averaging ~9.8s (optimization target: <3s)</li>
            <li>Support for up to 10,000 documents with efficient retrieval</li>
            <li>Automated test suite for RAG functionality verification</li>
            <li>Performance benchmarking for response time, throughput, and resource utilization</li>
        </ul>
    </div>

    <div class="section">
        <h2>Future Advances: Agentic RAG</h2>
        
        <div class="future-feature">
            <h3>1. Autonomous Information Gathering</h3>
            <ul>
                <li>Self-directed exploration of document corpus beyond initial retrieval</li>
                <li>Iterative search refinement based on information gaps</li>
                <li>Multi-hop reasoning across document boundaries</li>
                <li>Autonomous query decomposition and recomposition</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>2. Tool Integration</h3>
            <ul>
                <li>Integration with external tools and APIs for data enrichment</li>
                <li>Ability to execute code for data analysis within responses</li>
                <li>Database querying capabilities for structured data integration</li>
                <li>Web search integration for supplementing internal knowledge</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>3. Adaptive Retrieval</h3>
            <ul>
                <li>Dynamic adjustment of retrieval parameters based on query complexity</li>
                <li>Automatic chunking strategy selection based on document content</li>
                <li>Self-tuning relevance thresholds based on feedback</li>
                <li>Context-aware retrieval that considers conversation history</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>4. Reasoning and Synthesis</h3>
            <ul>
                <li>Enhanced multi-document synthesis with cross-reference analysis</li>
                <li>Contradiction detection and resolution across documents</li>
                <li>Temporal awareness for handling time-sensitive information</li>
                <li>Uncertainty quantification in responses</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>5. Feedback Loop Integration</h3>
            <ul>
                <li>Learning from user interactions to improve retrieval</li>
                <li>Automated fine-tuning of embeddings based on usage patterns</li>
                <li>Continuous improvement of prompt templates</li>
                <li>Self-evaluation of response quality</li>
            </ul>
        </div>
        
        <div class="highlight">
            These Agentic RAG capabilities will transform Metis RAG from a passive retrieval system to an active knowledge assistant that can autonomously navigate complex information landscapes, reason across documents, and continuously improve its performance.
        </div>
    </div>

    <div class="section">
        <h2>Demonstration Points</h2>
        <ol>
            <li><strong>Document Upload:</strong> Show support for multiple file types and processing options</li>
            <li><strong>RAG in Action:</strong> Demonstrate how responses incorporate document knowledge</li>
            <li><strong>Source Citations:</strong> Highlight how the system cites sources for factual claims</li>
            <li><strong>Multiple Document Synthesis:</strong> Show how the system combines information across documents</li>
            <li><strong>Analytics:</strong> Display system performance metrics and document utilization</li>
            <li><strong>Testing:</strong> Run the test script to demonstrate RAG retrieval verification</li>
        </ol>
        
        <div class="highlight">
            Metis RAG represents a significant advancement in combining local LLM capabilities with enterprise knowledge management, providing accurate, contextual responses grounded in organizational documents.
        </div>
    </div>
</body>
</html>