"""
AuditReportGenerator - Generates comprehensive audit reports for the RAG process
"""
import logging
import time
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

class AuditReportGenerator:
    """
    Generates comprehensive audit reports for the RAG process
    
    The AuditReportGenerator is responsible for creating detailed audit reports that
    track information sources, extract reasoning traces, and determine verification
    status for responses generated by the RAG system. These reports provide transparency
    and accountability for the system's outputs.
    """
    
    def __init__(
        self,
        process_logger,
        llm_provider = None
    ):
        """
        Initialize the audit report generator
        
        Args:
            process_logger: ProcessLogger instance
            llm_provider: LLM provider for additional analysis (optional)
        """
        self.process_logger = process_logger
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.rag.audit_report_generator")
    
    async def generate_report(
        self,
        query_id: str,
        include_llm_analysis: bool = True
    ) -> Dict[str, Any]:
        """
        Generate an audit report for a query
        
        Args:
            query_id: Unique query ID
            include_llm_analysis: Whether to include LLM-based analysis (optional)
            
        Returns:
            Comprehensive audit report
        """
        start_time = time.time()
        self.logger.info(f"Generating audit report for query {query_id}")
        
        # Get the process log
        process_log = self.process_logger.get_process_log(query_id)
        if not process_log:
            error_msg = f"No process log found for query {query_id}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Extract basic information
        query = process_log.get("query", "")
        timestamp = process_log.get("timestamp", datetime.now().isoformat())
        steps = process_log.get("steps", [])
        final_response = process_log.get("final_response", {})
        
        # Generate the report
        report = {
            "query_id": query_id,
            "query": query,
            "timestamp": timestamp,
            "process_summary": self._generate_process_summary(steps),
            "information_sources": self._extract_information_sources(steps),
            "reasoning_trace": self._extract_reasoning_trace(steps),
            "verification_status": self._determine_verification_status(steps, final_response),
            "execution_timeline": self._create_execution_timeline(steps),
            "response_quality": self._extract_response_quality(steps)
        }
        
        # Add LLM analysis if requested and available
        if include_llm_analysis and self.llm_provider:
            llm_analysis = await self._generate_llm_analysis(query, steps, final_response)
            report["llm_analysis"] = llm_analysis
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Audit report generation completed in {elapsed_time:.2f}s")
        
        # Log the audit report generation
        self.process_logger.log_step(
            query_id=query_id,
            step_name="audit_report_generation",
            step_data={
                "report_summary": {
                    "verification_status": report["verification_status"],
                    "source_count": len(report["information_sources"]),
                    "execution_time": elapsed_time
                }
            }
        )
        
        return report
    
    def _generate_process_summary(self, steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Generate a summary of the process
        
        Args:
            steps: List of process steps
            
        Returns:
            Process summary
        """
        # Count steps by type
        step_types = {}
        for step in steps:
            step_name = step.get("step_name", "unknown")
            step_types[step_name] = step_types.get(step_name, 0) + 1
        
        # Calculate total execution time
        total_time = 0
        for step in steps:
            step_data = step.get("data", {})
            if "execution_time" in step_data:
                total_time += step_data["execution_time"]
        
        # Identify key stages
        stages = []
        stage_mapping = {
            "query_analysis": "Query Analysis",
            "plan_query": "Query Planning",
            "execute_plan": "Plan Execution",
            "retrieve_chunks": "Information Retrieval",
            "refine_query": "Query Refinement",
            "optimize_context": "Context Optimization",
            "response_synthesis": "Response Synthesis",
            "response_evaluation": "Response Evaluation",
            "response_refinement": "Response Refinement"
        }
        
        for step in steps:
            step_name = step.get("step_name", "")
            for key, stage in stage_mapping.items():
                if key in step_name and stage not in stages:
                    stages.append(stage)
        
        return {
            "total_steps": len(steps),
            "step_types": step_types,
            "total_execution_time": total_time,
            "stages_executed": stages
        }
    
    def _extract_information_sources(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract information sources from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            List of information sources
        """
        sources = []
        source_ids = set()  # Track source IDs to avoid duplicates
        
        # Look for retrieval steps
        for step in steps:
            step_name = step.get("step_name", "")
            step_data = step.get("data", {})
            
            # Check for retrieval steps
            if "retrieve_chunks" in step_name or "retrieval" in step_name:
                if "chunks" in step_data:
                    for chunk in step_data["chunks"]:
                        if "metadata" in chunk:
                            metadata = chunk["metadata"]
                            source_id = metadata.get("document_id", "") + ":" + metadata.get("chunk_id", "")
                            
                            if source_id not in source_ids:
                                source_ids.add(source_id)
                                sources.append({
                                    "document_id": metadata.get("document_id", ""),
                                    "chunk_id": metadata.get("chunk_id", ""),
                                    "filename": metadata.get("filename", "Unknown"),
                                    "tags": metadata.get("tags", []),
                                    "folder": metadata.get("folder", "/"),
                                    "relevance_score": chunk.get("relevance_score", chunk.get("distance", 0)),
                                    "content_preview": chunk.get("content", "")[:200] + "..." if len(chunk.get("content", "")) > 200 else chunk.get("content", "")
                                })
            
            # Check for response synthesis steps that include sources
            if "response_synthesis" in step_name:
                if "sources" in step_data:
                    for source in step_data["sources"]:
                        source_id = source.get("document_id", "") + ":" + source.get("chunk_id", "")
                        
                        if source_id not in source_ids:
                            source_ids.add(source_id)
                            sources.append(source)
        
        # Sort sources by relevance score (descending)
        sources.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        return sources
    
    def _extract_reasoning_trace(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract reasoning trace from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            Reasoning trace
        """
        reasoning_trace = []
        
        # Define the key steps to include in the reasoning trace
        key_steps = [
            "query_analysis",
            "plan_query",
            "execute_plan",
            "retrieve_chunks",
            "refine_query",
            "optimize_context",
            "response_synthesis",
            "response_evaluation",
            "response_refinement"
        ]
        
        for step in steps:
            step_name = step.get("step_name", "")
            step_data = step.get("data", {})
            step_time = step.get("timestamp", "")
            
            # Check if this is a key step
            include_step = False
            for key_step in key_steps:
                if key_step in step_name:
                    include_step = True
                    break
            
            if include_step:
                # Extract the relevant information for the reasoning trace
                trace_entry = {
                    "step": step_name,
                    "timestamp": step_time,
                    "reasoning": {}
                }
                
                # Extract reasoning information based on step type
                if "query_analysis" in step_name:
                    if "analysis" in step_data:
                        trace_entry["reasoning"] = {
                            "complexity": step_data["analysis"].get("complexity", ""),
                            "justification": step_data["analysis"].get("justification", ""),
                            "requires_tools": step_data["analysis"].get("requires_tools", []),
                            "sub_queries": step_data["analysis"].get("sub_queries", [])
                        }
                elif "plan_query" in step_name:
                    if "plan" in step_data:
                        trace_entry["reasoning"] = {
                            "steps": step_data["plan"].get("steps", []),
                            "reasoning": step_data["plan"].get("reasoning", "")
                        }
                elif "execute_plan" in step_name:
                    if "results" in step_data:
                        trace_entry["reasoning"] = {
                            "execution_results": step_data["results"]
                        }
                elif "retrieve_chunks" in step_name:
                    trace_entry["reasoning"] = {
                        "chunks_retrieved": len(step_data.get("chunks", [])),
                        "relevance_threshold": step_data.get("relevance_threshold", "")
                    }
                elif "refine_query" in step_name:
                    trace_entry["reasoning"] = {
                        "original_query": step_data.get("original_query", ""),
                        "refined_query": step_data.get("refined_query", ""),
                        "refinement_reason": step_data.get("refinement_reason", "")
                    }
                elif "optimize_context" in step_name:
                    trace_entry["reasoning"] = {
                        "optimization_strategy": step_data.get("optimization_strategy", ""),
                        "chunks_before": step_data.get("chunks_before", 0),
                        "chunks_after": step_data.get("chunks_after", 0)
                    }
                elif "response_synthesis" in step_name:
                    trace_entry["reasoning"] = {
                        "sources_used": len(step_data.get("sources", [])),
                        "context_length": step_data.get("context_length", 0)
                    }
                elif "response_evaluation" in step_name:
                    if "evaluation" in step_data:
                        trace_entry["reasoning"] = {
                            "factual_accuracy": step_data["evaluation"].get("factual_accuracy", 0),
                            "completeness": step_data["evaluation"].get("completeness", 0),
                            "relevance": step_data["evaluation"].get("relevance", 0),
                            "hallucination_detected": step_data["evaluation"].get("hallucination_detected", False),
                            "overall_score": step_data["evaluation"].get("overall_score", 0)
                        }
                elif "response_refinement" in step_name:
                    trace_entry["reasoning"] = {
                        "improvement_summary": step_data.get("improvement_summary", ""),
                        "iteration": step_data.get("iteration", 1)
                    }
                
                reasoning_trace.append(trace_entry)
        
        return reasoning_trace
    
    def _determine_verification_status(
        self,
        steps: List[Dict[str, Any]],
        final_response: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Determine the verification status of the response
        
        Args:
            steps: List of process steps
            final_response: Final response data
            
        Returns:
            Verification status
        """
        # Initialize verification status
        verification_status = {
            "status": "unverified",
            "confidence": 0,
            "hallucination_detected": False,
            "factual_accuracy_score": 0,
            "verification_method": "none",
            "verification_details": ""
        }
        
        # Look for evaluation steps
        evaluation_steps = [step for step in steps if "response_evaluation" in step.get("step_name", "")]
        
        if evaluation_steps:
            # Use the most recent evaluation
            latest_evaluation = evaluation_steps[-1]
            evaluation_data = latest_evaluation.get("data", {}).get("evaluation", {})
            
            if evaluation_data:
                # Extract verification information
                factual_accuracy = evaluation_data.get("factual_accuracy", 0)
                hallucination_detected = evaluation_data.get("hallucination_detected", False)
                overall_score = evaluation_data.get("overall_score", 0)
                
                # Determine verification status
                if hallucination_detected:
                    status = "contains_hallucinations"
                    confidence = max(0, min(factual_accuracy / 10, 1))
                elif factual_accuracy >= 8:
                    status = "verified"
                    confidence = factual_accuracy / 10
                elif factual_accuracy >= 5:
                    status = "partially_verified"
                    confidence = factual_accuracy / 10
                else:
                    status = "unverified"
                    confidence = factual_accuracy / 10
                
                verification_status = {
                    "status": status,
                    "confidence": confidence,
                    "hallucination_detected": hallucination_detected,
                    "factual_accuracy_score": factual_accuracy,
                    "verification_method": "llm_evaluation",
                    "verification_details": evaluation_data.get("hallucination_details", "")
                }
        
        return verification_status
    
    def _create_execution_timeline(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Create an execution timeline from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            Execution timeline
        """
        timeline = []
        
        for step in steps:
            step_name = step.get("step_name", "")
            step_time = step.get("timestamp", "")
            step_data = step.get("data", {})
            
            # Extract execution time if available
            execution_time = step_data.get("execution_time", 0)
            
            # Create a timeline entry
            timeline_entry = {
                "step": step_name,
                "timestamp": step_time,
                "execution_time": execution_time
            }
            
            timeline.append(timeline_entry)
        
        return timeline
    
    def _extract_response_quality(self, steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Extract response quality metrics from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            Response quality metrics
        """
        # Initialize quality metrics
        quality = {
            "factual_accuracy": 0,
            "completeness": 0,
            "relevance": 0,
            "overall_score": 0,
            "strengths": [],
            "weaknesses": [],
            "improvement_suggestions": []
        }
        
        # Look for evaluation steps
        evaluation_steps = [step for step in steps if "response_evaluation" in step.get("step_name", "")]
        
        if evaluation_steps:
            # Use the most recent evaluation
            latest_evaluation = evaluation_steps[-1]
            evaluation_data = latest_evaluation.get("data", {}).get("evaluation", {})
            
            if evaluation_data:
                # Extract quality metrics
                quality = {
                    "factual_accuracy": evaluation_data.get("factual_accuracy", 0),
                    "completeness": evaluation_data.get("completeness", 0),
                    "relevance": evaluation_data.get("relevance", 0),
                    "overall_score": evaluation_data.get("overall_score", 0),
                    "strengths": evaluation_data.get("strengths", []),
                    "weaknesses": evaluation_data.get("weaknesses", []),
                    "improvement_suggestions": evaluation_data.get("improvement_suggestions", [])
                }
        
        return quality
    
    async def _generate_llm_analysis(
        self,
        query: str,
        steps: List[Dict[str, Any]],
        final_response: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate LLM-based analysis of the process
        
        Args:
            query: Original query
            steps: List of process steps
            final_response: Final response data
            
        Returns:
            LLM analysis
        """
        if not self.llm_provider:
            return {"error": "LLM provider not available"}
        
        try:
            # Create a prompt for the LLM
            prompt = self._create_llm_analysis_prompt(query, steps, final_response)
            
            # Generate the analysis
            response = await self.llm_provider.generate(
                prompt=prompt,
                system_prompt=self._create_llm_analysis_system_prompt()
            )
            
            # Parse the analysis
            analysis_text = response.get("response", "")
            analysis = self._parse_llm_analysis(analysis_text)
            
            return analysis
        except Exception as e:
            self.logger.error(f"Error generating LLM analysis: {str(e)}")
            return {"error": f"Error generating LLM analysis: {str(e)}"}
    
    def _create_llm_analysis_prompt(
        self,
        query: str,
        steps: List[Dict[str, Any]],
        final_response: Dict[str, Any]
    ) -> str:
        """
        Create a prompt for LLM analysis
        
        Args:
            query: Original query
            steps: List of process steps
            final_response: Final response data
            
        Returns:
            LLM analysis prompt
        """
        prompt = f"""
You are analyzing the execution of a RAG (Retrieval-Augmented Generation) process for the following query:

QUERY: {query}

FINAL RESPONSE:
{final_response.get("text", "")}

PROCESS STEPS:
{json.dumps(steps, indent=2)}

Please analyze the RAG process execution and provide insights on:

1. Process Efficiency:
   - Were there any unnecessary steps or redundancies?
   - Could the process have been more efficient?
   - Were appropriate tools and techniques used?

2. Information Retrieval Quality:
   - Was the retrieval effective in finding relevant information?
   - Were there any issues with the retrieval process?
   - How could retrieval be improved?

3. Response Quality:
   - Is the response accurate, complete, and relevant?
   - Are there any potential hallucinations or factual errors?
   - How well does the response address the query?

4. Overall Assessment:
   - What are the strengths of this RAG process execution?
   - What are the weaknesses or areas for improvement?
   - What specific recommendations would you make to improve the process?

FORMAT YOUR ANALYSIS AS FOLLOWS:
```json
{
  "process_efficiency": {
    "assessment": "Your assessment of process efficiency",
    "issues_identified": ["Issue 1", "Issue 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  },
  "retrieval_quality": {
    "assessment": "Your assessment of retrieval quality",
    "issues_identified": ["Issue 1", "Issue 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  },
  "response_quality": {
    "assessment": "Your assessment of response quality",
    "issues_identified": ["Issue 1", "Issue 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  },
  "overall_assessment": {
    "strengths": ["Strength 1", "Strength 2", ...],
    "weaknesses": ["Weakness 1", "Weakness 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  }
}
```

IMPORTANT: Your analysis must be objective, insightful, and based solely on the provided information. The analysis must be returned in the exact JSON format specified above.
"""
        
        return prompt
    
    def _create_llm_analysis_system_prompt(self) -> str:
        """
        Create a system prompt for LLM analysis
        
        Returns:
            System prompt
        """
        return """You are an expert RAG (Retrieval-Augmented Generation) system analyst.

Your role is to analyze RAG process executions and provide insightful feedback on efficiency, retrieval quality, response quality, and overall performance.

GUIDELINES:
1. Be objective and data-driven in your analysis.
2. Identify specific issues and provide actionable recommendations.
3. Consider the entire process flow from query analysis to response generation.
4. Evaluate the effectiveness of retrieval in finding relevant information.
5. Assess the quality of the final response in terms of accuracy, completeness, and relevance.
6. Identify potential hallucinations or factual errors in the response.
7. Provide specific recommendations for improving the process.
8. Format your analysis in the exact JSON format specified in the prompt.
9. Be thorough and detailed in your analysis.
10. Focus on practical improvements that could be implemented.
"""
    
    def _parse_llm_analysis(self, analysis_text: str) -> Dict[str, Any]:
        """
        Parse the LLM analysis response
        
        Args:
            analysis_text: Raw analysis text from the LLM
            
        Returns:
            Parsed analysis
        """
        # Extract JSON from the response
        try:
            # Look for JSON block in markdown format
            import re
            json_match = re.search(r'```(?:json)?\s*({\s*".*})\s*```', analysis_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without markdown formatting
                json_match = re.search(r'({[\s\S]*"overall_assessment"[\s\S]*})', analysis_text)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Fallback: assume the entire text might be JSON
                    json_str = analysis_text
            
            # Parse the JSON
            analysis = json.loads(json_str)
            
            # Ensure all required fields are present
            required_sections = ["process_efficiency", "retrieval_quality", "response_quality", "overall_assessment"]
            
            for section in required_sections:
                if section not in analysis:
                    analysis[section] = {
                        "assessment": "Not provided",
                        "issues_identified": [],
                        "recommendations": []
                    }
            
            return analysis
        except Exception as e:
            self.logger.error(f"Error parsing LLM analysis: {str(e)}")
            
            # Return a default analysis
            return {
                "error": f"Failed to parse LLM analysis: {str(e)}",
                "process_efficiency": {
                    "assessment": "Not available",
                    "issues_identified": [],
                    "recommendations": []
                },
                "retrieval_quality": {
                    "assessment": "Not available",
                    "issues_identified": [],
                    "recommendations": []
                },
                "response_quality": {
                    "assessment": "Not available",
                    "issues_identified": [],
                    "recommendations": []
                },
                "overall_assessment": {
                    "strengths": [],
                    "weaknesses": [],
                    "recommendations": []
                }
            }