# Response Quality Components

This directory contains components for improving the quality of responses generated by the Metis RAG system. These components work together to synthesize, evaluate, refine, and audit responses to ensure they are accurate, complete, relevant, and free from hallucinations.

## Components

### ResponseSynthesizer

The `ResponseSynthesizer` is responsible for combining retrieval results and tool outputs into coherent, well-structured responses with proper source attribution.

**Key Features:**
- Combines context from retrieved documents with execution results
- Adds proper source attribution using the [n] citation format
- Formats responses with appropriate structure
- Optimizes context assembly for better responses

### ResponseEvaluator

The `ResponseEvaluator` assesses the quality of responses based on multiple criteria to ensure they meet the required standards.

**Key Features:**
- Evaluates factual accuracy (0-10 scale)
- Assesses completeness in addressing the query (0-10 scale)
- Measures relevance to the original query (0-10 scale)
- Detects hallucinations (statements not supported by the context)
- Provides an overall quality score (0-10 scale)
- Identifies strengths and weaknesses
- Suggests improvements

### ResponseRefiner

The `ResponseRefiner` improves responses based on evaluation results to address identified issues and enhance quality.

**Key Features:**
- Iterative refinement process
- Addresses factual inaccuracies
- Improves completeness by adding missing information
- Enhances relevance by focusing on the query
- Removes hallucinations
- Improves overall structure and clarity

### AuditReportGenerator

The `AuditReportGenerator` creates comprehensive audit reports that provide transparency and accountability for the RAG process.

**Key Features:**
- Tracks information sources used in responses
- Extracts reasoning traces from the process
- Determines verification status of responses
- Creates execution timelines
- Provides quality metrics
- Generates LLM-based analysis of the process

### ResponseQualityPipeline

The `ResponseQualityPipeline` integrates all the above components into a cohesive pipeline for generating high-quality responses.

**Key Features:**
- End-to-end pipeline for response quality
- Configurable quality thresholds
- Iterative refinement until quality standards are met
- Comprehensive logging and auditing
- Integration with the existing RAG system

## Usage

### Basic Usage

```python
from app.rag.response_quality_pipeline import ResponseQualityPipeline
from app.rag.process_logger import ProcessLogger
from app.rag.ollama_client import OllamaClient

# Initialize components
ollama_client = OllamaClient()
process_logger = ProcessLogger(log_dir="data/process_logs")

# Create the pipeline
pipeline = ResponseQualityPipeline(
    llm_provider=ollama_client,
    process_logger=process_logger,
    max_refinement_iterations=2,
    quality_threshold=8.0,
    enable_audit_reports=True
)

# Process a query
result = await pipeline.process(
    query="What is the capital of France?",
    context="[1] Source: geography.txt, Tags: [geography, europe], Folder: /\n\nParis is the capital of France.",
    sources=[
        {
            "document_id": "doc1",
            "chunk_id": "chunk1",
            "relevance_score": 0.95,
            "excerpt": "Paris is the capital of France.",
            "filename": "geography.txt",
            "tags": ["geography", "europe"],
            "folder": "/"
        }
    ]
)

# Access the results
response = result["response"]
evaluation = result["evaluation"]
refinement_iterations = result["refinement_iterations"]
audit_report = result["audit_report"]
```

### Integration with RAG Engine

To integrate the response quality pipeline with the existing RAG engine, you can enhance the RAG engine's query method:

```python
from app.rag.rag_engine import RAGEngine
from app.rag.response_quality_pipeline import ResponseQualityPipeline

# Create the RAG engine and pipeline
rag_engine = RAGEngine(...)
pipeline = ResponseQualityPipeline(...)

# Enhanced query method
async def enhanced_query(query, **kwargs):
    # Get the standard RAG result
    rag_result = await rag_engine.query(query, **kwargs)
    
    # Extract components
    response = rag_result.get("answer", "")
    sources = rag_result.get("sources", [])
    context = "..." # Extract context from RAG result
    
    # Process through the quality pipeline
    quality_result = await pipeline.process(
        query=query,
        context=context,
        sources=sources,
        conversation_context=kwargs.get("conversation_history")
    )
    
    # Return enhanced result
    return {
        "query": query,
        "answer": quality_result["response"],
        "sources": sources,
        "evaluation": quality_result["evaluation"],
        "quality_score": quality_result["evaluation"]["overall_score"]
    }
```

## Configuration Options

### ResponseSynthesizer

- `llm_provider`: LLM provider for generating responses
- `process_logger`: ProcessLogger instance for logging (optional)

### ResponseEvaluator

- `llm_provider`: LLM provider for evaluation
- `process_logger`: ProcessLogger instance for logging (optional)

### ResponseRefiner

- `llm_provider`: LLM provider for refinement
- `process_logger`: ProcessLogger instance for logging (optional)
- `max_refinement_iterations`: Maximum number of refinement iterations

### AuditReportGenerator

- `process_logger`: ProcessLogger instance for accessing process logs
- `llm_provider`: LLM provider for additional analysis (optional)

### ResponseQualityPipeline

- `llm_provider`: LLM provider for all components
- `process_logger`: ProcessLogger instance for logging (optional)
- `max_refinement_iterations`: Maximum number of refinement iterations
- `quality_threshold`: Minimum quality score to accept a response (0-10)
- `enable_audit_reports`: Whether to generate audit reports

## Testing

Unit tests for the response quality components are available in `tests/unit/test_response_quality.py`.

Integration tests demonstrating how to use the components with the RAG engine are available in `tests/integration/test_response_quality_integration.py`.

To run the tests:

```bash
pytest tests/unit/test_response_quality.py -v
pytest tests/integration/test_response_quality_integration.py -v
```

## LangGraph Integration

The response quality components are designed to integrate with the LangGraph workflow. The `langgraph_states.py` file has been updated to include the following states:

- `ResponseEvaluationState`: State for response evaluation
- `ResponseRefinementState`: State for response refinement
- `AuditReportState`: State for audit report generation

These states can be used to incorporate response quality into the LangGraph workflow.