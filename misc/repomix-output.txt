This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
alembic/
  versions/
    add_background_tasks_table.py
    add_document_permissions.py
    add_memories_table.py
    add_notifications_table.py
    add_organizations_tables.py
    add_password_reset_tokens.py
    add_roles_tables.py
    add_user_id_columns.py
    add_users_table.py
    bb56459de93d_merge_heads_for_phase4.py
    enable_row_level_security.py
    ensure_doc_metadata_column.py
    f7e702fc686e_merge_heads_for_document_permissions.py
    fix_rls_current_setting.py
    initial_schema.py
    merge_heads.py
    rename_metadata_columns.py
    update_metadata_to_jsonb.py
  env.py
  script.py.mako
app/
  api/
    __init__.py
    admin.py
    analytics.py
    auth.py
    chat.py
    document_sharing.py
    documents.py
    health.py
    notifications.py
    organizations.py
    password_reset.py
    processing.py
    query_analysis.py
    roles.py
    schema.py
    system.py
    tasks.py
    test.py
  cache/
    __init__.py
    base.py
    cache_manager.py
    document_cache.py
    llm_response_cache.py
    README.md
    vector_search_cache.py
  core/
    __init__.py
    config.py
    email.py
    logging.py
    permissions.py
    rate_limit.py
    security_alerts.py
    security.py
  db/
    repositories/
      __init__.py
      analytics_repository.py
      base.py
      conversation_repository.py
      document_repository.py
      memory_repository.py
      notification_repository.py
      organization_repository.py
      password_reset_repository.py
      role_repository.py
      user_repository_updated.py
      user_repository.py
    __init__.py
    adapters.py
    connection_manager.py
    dependencies.py
    mem0_integration.py
    models.py
    README.md
    schema_inspector.py
    session.py
  middleware/
    __init__.py
    auth.py
    db_context.py
    jwt_bearer.py
  models/
    __init__.py
    chat.py
    conversation.py
    document.py
    memory.py
    notification.py
    organization.py
    password_reset.py
    role.py
    system.py
    user.py
  rag/
    agents/
      __init__.py
      chunking_judge.py
      enhanced_langgraph_rag_agent.py
      langgraph_rag_agent.py
      retrieval_judge.py
    chunkers/
      __init__.py
      semantic_chunker.py
    system_prompts/
      __init__.py
      code_generation.py
      conversation.py
      rag.py
    tools/
      __init__.py
      base.py
      calculator_tool.py
      csv_json_handler.py
      database_tool_async.py
      database_tool.py
      postgresql_tool.py
      rag_tool.py
      registry.py
    __init__.py
    audit_report_generator.py
    document_analysis_service.py
    document_processor.py
    langgraph_states.py
    mem0_client.py
    memory_buffer.py
    ollama_client.py
    plan_executor.py
    process_logger.py
    processing_job.py
    prompt_manager.py
    query_analyzer.py
    query_planner.py
    rag_engine_base.py
    rag_engine.py
    rag_generation.py
    rag_retrieval.py
    README_MEM0_INTEGRATION.md
    README_RESPONSE_QUALITY.md
    response_evaluator.py
    response_quality_pipeline.py
    response_refiner.py
    response_synthesizer.py
    system_prompts.py
    tool_initializer.py
    vector_store.py
  static/
    css/
      document-manager.css
      document-upload-enhanced.css
      fonts.css
      login.css
      register.css
      schema.css
      styles.css
      tasks.css
    js/
      chat.js
      document-manager.js
      document-upload-enhanced.js
      document-upload-fix.js
      error-feedback-enhancement.js
      login_handler.js
      login.js
      main.js
      register.js
      schema.js
      tasks.js
      test_models.js
  tasks/
    __init__.py
    example_tasks.py
    README.md
    resource_monitor.py
    scheduler.py
    task_manager.py
    task_models.py
    task_repository.py
  templates/
    admin.html
    analytics.html
    base.html
    chat.html
    documents_enhanced.html
    documents.html
    forgot_password.html
    login.html
    register.html
    reset_password.html
    schema.html
    system.html
    tasks.html
    test_models.html
  utils/
    __init__.py
    email.py
    file_utils.py
    text_processor.py
    text_utils.py
  main.py
config/
  .env.docker
  .env.example
  .env.test
  docker-compose.mem0.yml
  docker-compose.yml
  Dockerfile
  requirements.txt
data/
  cache/
    llm_response/
      stats.json
    vector_search/
      stats.json
  demo_cache/
    perf_cache/
      stats.json
  test_docs/
    create_sample_files.py
    document_metrics.csv
    implementation_guide.txt
    performance_benchmarks.txt
    quarterly_report.txt
    rag_system_overview.txt
    rag_test_results.json
    sample_analysis.py
    smart_home_developer_reference.md
    smart_home_device_comparison.csv
    smart_home_technical_specs.txt
    smart_home_user_guide.txt
    technical_documentation.md
  test_quality_docs/
    product_specifications.csv
    quarterly_report.txt
    technical_documentation.md
  cookies.txt
  metis_rag_visualization.html
docs/
  bug_fixes/
    2025-03-27-column-name-and-method-signature-fixes.md
    2025-03-27-user-id-consistency-fix.md
    2025-03-31-chat-memory-fix.md
  demos/
    Metis_RAG_Technical_Demo.html
    Metis_RAG_Technical_Demo.md
  deployment/
    docker_deployment.md
  implementation/
    chat_memory_augmentation_plan.md
    llm_enhanced_rag_implementation_plan_updated.md
    llm_enhanced_rag_implementation_plan.md
    Mem0_Docker_Integration_Plan.md
    mem0_integration_plan.md
    Metis_RAG_Access_Control_Implementation_Plan.md
    metis_rag_agentic_enhancement_plan.md
    Metis_RAG_Authentication_Implementation_Plan.md
    Metis_RAG_Database_Integration_Plan.md
    metis_rag_implementation_checklist.md
    Metis_RAG_Implementation_Plan_Part1.md
    Metis_RAG_Implementation_Plan_Part2.md
    Metis_RAG_Implementation_Plan_Part3.md
    Metis_RAG_Implementation_Plan_Part4.md
    Metis_RAG_Implementation_Progress_Update.md
    Metis_RAG_Implementation_Steps.md
    metis_rag_infrastructure_improvement_plan.md
  security/
    credentials_in_url_vulnerability.md
    login_csp_fix.md
    row_level_security_fix.md
  setup/
    Metis_RAG_Improvement_Plan.md
    Metis_RAG_Setup_Plan.md
  technical/
    AUTHENTICATION.md
    Fix_SQLAlchemy_Metadata_Conflict.md
    metis_rag_visualization.md
    query_refinement_fix.md
    reorganization_summary.md
    technical_documentation.md
    TESTING.md
  async_database_implementation.md
  authentication_demo.html
  authentication_system_improvements.md
  authentication_testing_guide.md
  document_upload_improvements_phase1.md
  document_upload_improvements_phase2_plan.md
  document_upload_improvements_phase2_prompt.md
  document_upload_improvements_phase2_readme.md
  document_upload_ui_mockup.html
  file_structure_proposal.md
  Metis_RAG_Authentication_Implementation_Detailed_Plan.md
  Metis_RAG_Authentication_Implementation_Prompt.md
  Metis_RAG_Authentication_Testing_Plan.md
  Metis_RAG_Database_Security_Implementation_Plan.md
  Metis_RAG_PG_MCP_Integration_Plan.md
  performance_optimization_guide.md
  performance_optimization.md
  prompt_manager_refactor.md
scripts/
  demo/
    demo_cache.py
    demo_presentation.py
    demo_tests.py
  generation/
    generate_pdf.py
    generate_test_data.py
  maintenance/
    clear_cache.py
    clear_database.py
    reprocess_documents.py
    run_tests.py
    test_rag_retrieval.py
  utils/
    create_test_user_simple.py
    simple_app.py
    view_visualization.py
  benchmark_database_performance.py
  clear_cache_and_test.py
  create_admin_user.py
  create_default_roles.py
  create_password_reset_table.py
  create_system_user.py
  create_tables.py
  create_test_folders.py
  create_test_user.py
  grant_permissions.sql
  implement_database_enhancements.py
  implement_postgres_enhancements.py
  initialize_test_chroma.py
  make_user_admin.py
  migrate_database.py
  migrate_to_async_database_tool.py
  optimize_chunking_strategy.py
  optimize_sqlite_for_concurrency.py
  README_BACKGROUND_TASKS.md
  README_SYSTEM_PROMPT_TESTING.md
  run_app.py
  run_background_task_tests.py
  run_document_processing_performance_tests.py
  run_grant_permissions.py
  run_memory_migration.py
  run_migrations.py
  run_phase4_migrations.py
  run_query_refinement_test.sh
  setup.sh
  test_adapter_functions.py
  test_api_directly.py
  test_api_endpoints.py
  test_authentication.py
  test_background_tasks.py
  test_chat_api.py
  test_document_processing.py
  test_memory_buffer.py
  test_schema_inspector.py
  test_system_prompts.py
  update_docker_config.py
  update_docker_for_postgres.py
test_results/
  api_test_query_results.json
  api_test_upload_results.json
  test_e2e_demo_comprehensive_report.json
  test_e2e_demo_query_results.json
  test_e2e_demo_upload_results.json
tests/
  data/
    test_data.csv
    test_document.txt
    test_models.html
    test_upload_document.txt
  e2e/
    test_auth_flows.py
    test_permission_scenarios.py
  integration/
    test_api.py
    test_auth_endpoints.py
    test_chunking_judge_integration.py
    test_enhanced_langgraph_rag_integration.py
    test_langgraph_rag_integration.py
    test_permissions_db.py
    test_permissions_vector.py
    test_response_quality_integration.py
    test_retrieval_judge_integration.py
    test_semantic_chunker_integration.py
  results/
    chunking_judge_results/
      chunking_judge_real_results.json
      chunking_judge_test_results.json
    quality_results/
      metis_rag_test_report.json
      test_quality_results.json
    query_basic_entity/
      report.md
      results.json
    query_context_synthesis/
      report.md
      results.json
    query_multi_entity/
      report.md
      results.json
    query_potential_ambiguity/
      report.md
      results.json
    query_specialized_terminology/
      report.md
      results.json
    retrieval_results/
      test_citation_results.json
      test_multi_doc_results.json
      test_response_time_results.json
    chunking_judge_real_results.json
    db_benchmark_postgresql_20250320_110230.json
    db_benchmark_sqlite_20250320_110220.json
    db_comparison_report_20250320_110308.html
    metis_rag_test_report.html
    summary_report.md
  retrieval_judge/
    data/
      performance_test_document.md
      product_specifications.md
      quarterly_report.txt
      technical_documentation.md
      test_document.md
      timing_test_document.md
    results/
      chunk_evaluation_results.json
      context_optimization_results.json
      judge_edge_case_analysis.json
      performance_analysis_results.json
      performance_analysis.json
      query_analysis_results.json
      query_refinement_results.json
      single_query_result.json
      timing_analysis_results.json
      timing_analysis.json
    analyze_retrieval_judge_results.py
    IMPLEMENTATION_NOTES.md
    README.md
    run_tests.py
    run_tests.sh
    test_judge_edge_cases.py
    test_performance_analysis.py
    test_retrieval_judge_comparison.py
    test_single_query.py
    test_timing_analysis.py
  unit/
    test_cache.py
    test_chunking_judge.py
    test_connection_manager.py
    test_csv_json_handler.py
    test_database_tool_async_concurrent.py
    test_database_tool_async.py
    test_database_tool_simple.py
    test_document_analysis_service.py
    test_mem0_client.py
    test_plan_executor.py
    test_process_logger.py
    test_processing_job.py
    test_prompt_manager.py
    test_query_analyzer.py
    test_query_planner.py
    test_rag_engine.py
    test_response_quality.py
    test_retrieval_judge.py
    test_schema_inspector.py
    test_security_utils.py
    test_semantic_chunker.py
    test_tools.py
  utils/
    create_test_pdf.py
    test_auth_helper.py
  authentication_setup_guide.md
  authentication_testing_guide.md
  conftest.py
  e2e_test_README.md
  metis_rag_e2e_test_plan.md
  README.md
  run_api_test.py
  run_authentication_test.py
  run_metis_rag_e2e_demo.py
  run_metis_rag_e2e_test.py
  simple_auth_test.py
  test_auth_simple.py
  test_authentication.py
  test_background_tasks.py
  test_chat_api.py
  test_chunking_judge_phase1.py
  test_chunking_judge_real.py
  test_db_connection_simple.py
  test_db_connection.py
  test_db_simple.py
  test_document_processing_performance.py
  test_document_repository.py
  test_document_upload_enhanced.html
  test_document_upload.py
  test_edge_cases.py
  test_entity_preservation.py
  test_file_handling.py
  test_fixes.py
  test_metis_rag_e2e_demo.py
  test_metis_rag_e2e.py
  test_performance.py
  test_query_performance.py
  test_query_refinement_fix.py
  test_rag_entity_preservation.py
  test_rag_quality.py
  test_rag_retrieval.py
  test_simplified_document_processing_with_db.py
  test_simplified_document_processing.py
  test_simplified_performance.py
  try_rag_query.py
unused_code/
  app/
    rag/
      system_prompts/
        __init__.py
        conversation.py
        rag.py
venv_py310/
  bin/
    activate
    activate.csh
    activate.fish
    Activate.ps1
    chroma
    coloredlogs
    distro
    dotenv
    email_validator
    f2py
    fastapi
    httpx
    huggingface-cli
    humanfriendly
    isympy
    jsondiff
    jsonpatch
    jsonpointer
    langchain-server
    langsmith
    markdown-it
    normalizer
    onnxruntime_test
    pip
    pip3
    pip3.10
    py.test
    pygmentize
    pyrsa-decrypt
    pyrsa-encrypt
    pyrsa-keygen
    pyrsa-priv2pub
    pyrsa-sign
    pyrsa-verify
    pytest
    tqdm
    typer
    uvicorn
    watchfiles
    websockets
  include/
    site/
      python3.10/
        greenlet/
          greenlet.h
  share/
    man/
      man1/
        isympy.1
  pyvenv.cfg
.dockerignore
.gitignore
alembic.ini
Fixed_SQLAlchemy_Models_Code.md
metis_improved_prompt.md
Metis_RAG_Critical_Analysis_Prompt.md
Metis_RAG_Fix_Plan_Checklist.md
Metis_RAG_Fix_Plan.md
Metis_RAG_Improved_System_Prompt.md
Metis_RAG_Memory_Implementation_Plan.md
Metis_RAG_Response_Format_Analysis.md
Metis_RAG_Response_Format_Simplified_Plan.md
Metis_RAG_Simplified_System_Prompt.md
metis_rag_testing_prompt.md
pull_request_template.md
pyproject.toml
README.md
requirements.txt

================================================================
Files
================================================================

================
File: alembic/versions/add_background_tasks_table.py
================
"""add background tasks table

Revision ID: add_background_tasks
Revises: initial_schema
Create Date: 2025-03-18 19:31:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB


# revision identifiers, used by Alembic.
revision = 'add_background_tasks'
down_revision = 'initial_schema'
branch_labels = None
depends_on = None


def upgrade():
    # Create background_tasks table
    op.create_table(
        'background_tasks',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('name', sa.String(), nullable=False),
        sa.Column('task_type', sa.String(), nullable=False),
        sa.Column('params', JSONB(), nullable=True),
        sa.Column('priority', sa.Integer(), nullable=True, default=50),
        sa.Column('dependencies', sa.Text(), nullable=True),
        sa.Column('schedule_time', sa.DateTime(), nullable=True),
        sa.Column('timeout_seconds', sa.Integer(), nullable=True),
        sa.Column('max_retries', sa.Integer(), nullable=True, default=0),
        sa.Column('metadata', JSONB(), nullable=True),
        sa.Column('status', sa.String(), nullable=False, default='pending'),
        sa.Column('created_at', sa.DateTime(), nullable=True),
        sa.Column('scheduled_at', sa.DateTime(), nullable=True),
        sa.Column('started_at', sa.DateTime(), nullable=True),
        sa.Column('completed_at', sa.DateTime(), nullable=True),
        sa.Column('retry_count', sa.Integer(), nullable=True, default=0),
        sa.Column('result', sa.Text(), nullable=True),
        sa.Column('error', sa.Text(), nullable=True),
        sa.Column('progress', sa.Float(), nullable=True, default=0.0),
        sa.Column('resource_usage', JSONB(), nullable=True),
        sa.Column('execution_time_ms', sa.Float(), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Create indexes
    op.create_index('ix_background_tasks_status', 'background_tasks', ['status'])
    op.create_index('ix_background_tasks_task_type', 'background_tasks', ['task_type'])
    op.create_index('ix_background_tasks_created_at', 'background_tasks', ['created_at'])
    op.create_index('ix_background_tasks_schedule_time', 'background_tasks', ['schedule_time'])


def downgrade():
    # Drop indexes
    op.drop_index('ix_background_tasks_schedule_time')
    op.drop_index('ix_background_tasks_created_at')
    op.drop_index('ix_background_tasks_task_type')
    op.drop_index('ix_background_tasks_status')
    
    # Drop table
    op.drop_table('background_tasks')

================
File: alembic/versions/add_document_permissions.py
================
"""Add document_permissions table and is_public flag to documents

Revision ID: add_document_permissions
Revises: add_password_reset_tokens
Create Date: 2025-03-27 12:45:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID


# revision identifiers, used by Alembic.
revision = 'add_document_permissions'
down_revision = 'add_password_reset_tokens'
branch_labels = None
depends_on = None


def upgrade():
    # Check if columns already exist
    from sqlalchemy import inspect
    
    # Get inspector
    conn = op.get_bind()
    inspector = inspect(conn)
    
    # Check if document_permissions table exists
    tables = inspector.get_table_names()
    if 'document_permissions' not in tables:
        # Create document_permissions table
        op.create_table(
            'document_permissions',
            sa.Column('id', UUID(as_uuid=True), nullable=False, server_default=sa.text("uuid_generate_v4()")),
            sa.Column('document_id', UUID(as_uuid=True), nullable=False),
            sa.Column('user_id', UUID(as_uuid=True), nullable=False),
            sa.Column('permission_level', sa.String(), nullable=False),
            sa.Column('created_at', sa.DateTime(), nullable=True, server_default=sa.func.now()),
            sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ondelete='CASCADE'),
            sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
            sa.PrimaryKeyConstraint('id'),
            sa.UniqueConstraint('document_id', 'user_id', name='uq_document_permissions_document_user')
        )
        op.create_index('ix_document_permissions_document_id', 'document_permissions', ['document_id'])
        op.create_index('ix_document_permissions_user_id', 'document_permissions', ['user_id'])
    
    # Check if is_public column exists in documents table
    doc_columns = [col['name'] for col in inspector.get_columns('documents')]
    if 'is_public' not in doc_columns:
        # Add is_public column to documents table
        op.add_column('documents', sa.Column('is_public', sa.Boolean(), nullable=True, server_default='false'))
        op.create_index('ix_documents_is_public', 'documents', ['is_public'])


def downgrade():
    # Drop document_permissions table
    op.drop_index('ix_document_permissions_user_id', table_name='document_permissions')
    op.drop_index('ix_document_permissions_document_id', table_name='document_permissions')
    op.drop_table('document_permissions')
    
    # Drop is_public column from documents table
    op.drop_index('ix_documents_is_public', table_name='documents')
    op.drop_column('documents', 'is_public')

================
File: alembic/versions/add_memories_table.py
================
"""Add memories table for explicit memory storage

Revision ID: add_memories_table
Revises: bb56459de93d
Create Date: 2025-03-31 18:01:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID


# revision identifiers, used by Alembic.
revision = 'add_memories_table'
down_revision = 'bb56459de93d'
branch_labels = None
depends_on = None

def upgrade():
    # Create memories table
    op.create_table(
        'memories',
        sa.Column('id', UUID(as_uuid=True), primary_key=True, server_default=sa.text('uuid_generate_v4()')),
        sa.Column('conversation_id', UUID(as_uuid=True), sa.ForeignKey('conversations.id', ondelete='CASCADE'), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('label', sa.String(50), nullable=False, server_default='explicit_memory'),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
    )
    
    # Add index for faster lookups by conversation_id
    op.create_index(
        'ix_memories_conversation_id',
        'memories',
        ['conversation_id']
    )
    
    # Add index for faster lookups by label
    op.create_index(
        'ix_memories_label',
        'memories',
        ['label']
    )

def downgrade():
    # Drop indexes
    op.drop_index('ix_memories_label')
    op.drop_index('ix_memories_conversation_id')
    
    # Drop memories table
    op.drop_table('memories')

================
File: alembic/versions/add_notifications_table.py
================
"""add_notifications_table

Revision ID: add_notifications_table
Revises: add_roles_tables
Create Date: 2025-03-27 16:12:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB


# revision identifiers, used by Alembic.
revision = 'add_notifications_table'
down_revision = 'add_roles_tables'
branch_labels = None
depends_on = None


def upgrade():
    # Create notifications table
    op.create_table(
        'notifications',
        sa.Column('id', UUID(as_uuid=True), nullable=False),
        sa.Column('user_id', UUID(as_uuid=True), nullable=False),
        sa.Column('type', sa.String(), nullable=False),
        sa.Column('title', sa.String(), nullable=False),
        sa.Column('message', sa.Text(), nullable=False),
        sa.Column('data', JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb")),
        sa.Column('is_read', sa.Boolean(), nullable=True, server_default=sa.text("false")),
        sa.Column('created_at', sa.DateTime(), nullable=True, server_default=sa.func.now()),
        sa.Column('read_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_notifications_user_id', 'notifications', ['user_id'])
    op.create_index('ix_notifications_created_at', 'notifications', ['created_at'])
    op.create_index('ix_notifications_is_read', 'notifications', ['is_read'])


def downgrade():
    op.drop_table('notifications')

================
File: alembic/versions/add_organizations_tables.py
================
"""add_organizations_tables

Revision ID: add_organizations_tables
Revises: add_notifications_table
Create Date: 2025-03-27 16:15:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB


# revision identifiers, used by Alembic.
revision = 'add_organizations_tables'
down_revision = 'add_notifications_table'
branch_labels = None
depends_on = None


def upgrade():
    # Create organizations table
    op.create_table(
        'organizations',
        sa.Column('id', UUID(as_uuid=True), nullable=False),
        sa.Column('name', sa.String(), nullable=False),
        sa.Column('description', sa.String(), nullable=True),
        sa.Column('settings', JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb")),
        sa.Column('created_at', sa.DateTime(), nullable=True, server_default=sa.func.now()),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_organizations_name', 'organizations', ['name'])

    # Create organization_members table
    op.create_table(
        'organization_members',
        sa.Column('organization_id', UUID(as_uuid=True), nullable=False),
        sa.Column('user_id', UUID(as_uuid=True), nullable=False),
        sa.Column('role', sa.String(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=True, server_default=sa.func.now()),
        sa.ForeignKeyConstraint(['organization_id'], ['organizations.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('organization_id', 'user_id')
    )
    op.create_index('ix_organization_members_organization_id', 'organization_members', ['organization_id'])
    op.create_index('ix_organization_members_user_id', 'organization_members', ['user_id'])

    # Add organization_id to documents table
    op.add_column('documents', sa.Column('organization_id', UUID(as_uuid=True), nullable=True))
    op.create_foreign_key(
        'fk_documents_organization_id', 'documents', 'organizations',
        ['organization_id'], ['id']
    )
    op.create_index('ix_documents_organization_id', 'documents', ['organization_id'])


def downgrade():
    # Remove organization_id from documents table
    op.drop_index('ix_documents_organization_id', table_name='documents')
    op.drop_constraint('fk_documents_organization_id', 'documents', type_='foreignkey')
    op.drop_column('documents', 'organization_id')

    # Drop organization_members table
    op.drop_table('organization_members')

    # Drop organizations table
    op.drop_table('organizations')

================
File: alembic/versions/add_password_reset_tokens.py
================
"""Add password reset tokens table

Revision ID: add_password_reset_tokens
Revises: add_user_id_columns
Create Date: 2025-03-25 15:06:30.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB


# revision identifiers, used by Alembic.
revision = 'add_password_reset_tokens'
down_revision = 'add_user_id_columns'
branch_labels = None
depends_on = None


def upgrade():
    # Create password_reset_tokens table
    op.create_table(
        'password_reset_tokens',
        sa.Column('id', UUID(), nullable=False),
        sa.Column('user_id', UUID(), nullable=False),
        sa.Column('token', sa.String(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=True),
        sa.Column('expires_at', sa.DateTime(), nullable=False),
        sa.Column('is_used', sa.Boolean(), default=False),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('token'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    )
    op.create_index('ix_password_reset_tokens_token', 'password_reset_tokens', ['token'])
    op.create_index('ix_password_reset_tokens_user_id', 'password_reset_tokens', ['user_id'])
    op.create_index('ix_password_reset_tokens_expires_at', 'password_reset_tokens', ['expires_at'])


def downgrade():
    # Drop password_reset_tokens table
    op.drop_index('ix_password_reset_tokens_expires_at', table_name='password_reset_tokens')
    op.drop_index('ix_password_reset_tokens_user_id', table_name='password_reset_tokens')
    op.drop_index('ix_password_reset_tokens_token', table_name='password_reset_tokens')
    op.drop_table('password_reset_tokens')

================
File: alembic/versions/add_roles_tables.py
================
"""add_roles_tables

Revision ID: add_roles_tables
Revises: rename_metadata_columns
Create Date: 2025-03-27 16:06:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB


# revision identifiers, used by Alembic.
revision = 'add_roles_tables'
down_revision = 'rename_metadata_columns'
branch_labels = None
depends_on = None


def upgrade():
    # Create roles table
    op.create_table(
        'roles',
        sa.Column('id', UUID(as_uuid=True), nullable=False),
        sa.Column('name', sa.String(), nullable=False),
        sa.Column('description', sa.String(), nullable=True),
        sa.Column('permissions', JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb")),
        sa.Column('created_at', sa.DateTime(), nullable=True, server_default=sa.func.now()),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('name')
    )
    op.create_index('ix_roles_name', 'roles', ['name'])

    # Create user_roles association table
    op.create_table(
        'user_roles',
        sa.Column('user_id', UUID(as_uuid=True), nullable=False),
        sa.Column('role_id', UUID(as_uuid=True), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=True, server_default=sa.func.now()),
        sa.ForeignKeyConstraint(['role_id'], ['roles.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('user_id', 'role_id')
    )
    op.create_index('ix_user_roles_user_id', 'user_roles', ['user_id'])
    op.create_index('ix_user_roles_role_id', 'user_roles', ['role_id'])


def downgrade():
    op.drop_table('user_roles')
    op.drop_table('roles')

================
File: alembic/versions/add_user_id_columns.py
================
"""Add user_id columns to documents and conversations tables

Revision ID: add_user_id_columns
Revises: add_background_tasks
Create Date: 2025-03-21 11:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID


# revision identifiers, used by Alembic.
revision = 'add_user_id_columns'
down_revision = 'add_background_tasks'
branch_labels = None
depends_on = None


def upgrade():
    # Check if columns already exist
    from sqlalchemy import inspect
    
    # Get inspector
    conn = op.get_bind()
    inspector = inspect(conn)
    
    # Check if user_id column exists in documents table
    doc_columns = [col['name'] for col in inspector.get_columns('documents')]
    if 'user_id' not in doc_columns:
        # Add user_id column to documents table
        op.add_column('documents', sa.Column('user_id', UUID(), nullable=True))
        op.create_foreign_key('fk_documents_user_id', 'documents', 'users', ['user_id'], ['id'])
        op.create_index('ix_documents_user_id', 'documents', ['user_id'])
    
    # Check if user_id column exists in conversations table
    conv_columns = [col['name'] for col in inspector.get_columns('conversations')]
    if 'user_id' not in conv_columns:
        # Add user_id column to conversations table
        op.add_column('conversations', sa.Column('user_id', UUID(), nullable=True))
        op.create_foreign_key('fk_conversations_user_id', 'conversations', 'users', ['user_id'], ['id'])
        op.create_index('ix_conversations_user_id', 'conversations', ['user_id'])


def downgrade():
    # Drop foreign keys and indexes
    op.drop_constraint('fk_documents_user_id', 'documents', type_='foreignkey')
    op.drop_index('ix_documents_user_id', table_name='documents')
    op.drop_constraint('fk_conversations_user_id', 'conversations', type_='foreignkey')
    op.drop_index('ix_conversations_user_id', table_name='conversations')
    
    # Drop columns
    op.drop_column('documents', 'user_id')
    op.drop_column('conversations', 'user_id')

================
File: alembic/versions/add_users_table.py
================
"""Add users table and update documents and conversations tables

Revision ID: add_users_table
Revises: initial_schema
Create Date: 2025-03-21 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB


# revision identifiers, used by Alembic.
revision = 'add_users_table'
down_revision = 'add_background_tasks'
branch_labels = None
depends_on = None


def upgrade():
    # Check if users table already exists
    from sqlalchemy import inspect
    from sqlalchemy.engine import reflection
    
    # Get inspector
    conn = op.get_bind()
    inspector = inspect(conn)
    
    # Check if users table exists
    if 'users' not in inspector.get_table_names():
        # Create users table
        op.create_table(
            'users',
            sa.Column('id', UUID(), nullable=False),
            sa.Column('username', sa.String(), nullable=False),
            sa.Column('email', sa.String(), nullable=False),
            sa.Column('password_hash', sa.String(), nullable=False),
            sa.Column('full_name', sa.String(), nullable=True),
            sa.Column('is_active', sa.Boolean(), nullable=True, default=True),
            sa.Column('is_admin', sa.Boolean(), nullable=True, default=False),
            sa.Column('created_at', sa.DateTime(), nullable=True, default=sa.func.now()),
            sa.Column('last_login', sa.DateTime(), nullable=True),
            sa.Column('metadata', JSONB(), nullable=True),
            sa.PrimaryKeyConstraint('id'),
            sa.UniqueConstraint('username'),
            sa.UniqueConstraint('email')
        )
        op.create_index('ix_users_username', 'users', ['username'])
        op.create_index('ix_users_email', 'users', ['email'])
    
    # Check if user_id column exists in documents table
    inspector = inspect(conn)
    doc_columns = [col['name'] for col in inspector.get_columns('documents')]
    if 'user_id' not in doc_columns:
        # Add user_id column to documents table
        op.add_column('documents', sa.Column('user_id', UUID(), nullable=True))
        op.create_foreign_key('fk_documents_user_id', 'documents', 'users', ['user_id'], ['id'])
        op.create_index('ix_documents_user_id', 'documents', ['user_id'])
    
    # Check if user_id column exists in conversations table
    conv_columns = [col['name'] for col in inspector.get_columns('conversations')]
    if 'user_id' not in conv_columns:
        # Add user_id column to conversations table
        op.add_column('conversations', sa.Column('user_id', UUID(), nullable=True))
        op.create_foreign_key('fk_conversations_user_id', 'conversations', 'users', ['user_id'], ['id'])
        op.create_index('ix_conversations_user_id', 'conversations', ['user_id'])


def downgrade():
    # Drop foreign keys and indexes
    op.drop_constraint('fk_documents_user_id', 'documents', type_='foreignkey')
    op.drop_index('ix_documents_user_id', table_name='documents')
    op.drop_constraint('fk_conversations_user_id', 'conversations', type_='foreignkey')
    op.drop_index('ix_conversations_user_id', table_name='conversations')
    
    # Drop columns
    op.drop_column('documents', 'user_id')
    op.drop_column('conversations', 'user_id')
    
    # Drop users table
    op.drop_index('ix_users_email', table_name='users')
    op.drop_index('ix_users_username', table_name='users')
    op.drop_table('users')

================
File: alembic/versions/bb56459de93d_merge_heads_for_phase4.py
================
"""merge_heads_for_phase4

Revision ID: bb56459de93d
Revises: add_organizations_tables, f7e702fc686e
Create Date: 2025-03-27 16:22:33.920973

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'bb56459de93d'
down_revision = ('add_organizations_tables', 'f7e702fc686e')
branch_labels = None
depends_on = None


def upgrade():
    pass


def downgrade():
    pass

================
File: alembic/versions/enable_row_level_security.py
================
"""Enable Row Level Security on document tables

Revision ID: enable_row_level_security
Revises: add_document_permissions
Create Date: 2025-03-27 12:48:00.000000

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'enable_row_level_security'
down_revision = 'add_document_permissions'
branch_labels = None
depends_on = None


def upgrade():
    # Enable RLS on documents table
    op.execute("ALTER TABLE documents ENABLE ROW LEVEL SECURITY;")
    
    # Enable RLS on chunks table (document sections)
    op.execute("ALTER TABLE chunks ENABLE ROW LEVEL SECURITY;")
    
    # Create ownership RLS policy for documents (SELECT)
    op.execute("""
    CREATE POLICY "Users can view their own documents" 
    ON documents FOR SELECT 
    USING (user_id = current_setting('app.current_user_id')::uuid OR is_public = true);
    """)
    
    # Create ownership RLS policy for documents (UPDATE)
    op.execute("""
    CREATE POLICY "Users can update their own documents" 
    ON documents FOR UPDATE
    USING (user_id = current_setting('app.current_user_id')::uuid);
    """)
    
    # Create ownership RLS policy for documents (DELETE)
    op.execute("""
    CREATE POLICY "Users can delete their own documents" 
    ON documents FOR DELETE
    USING (user_id = current_setting('app.current_user_id')::uuid);
    """)
    
    # Create document sharing RLS policy (SELECT)
    op.execute("""
    CREATE POLICY "Users can view documents shared with them" 
    ON documents FOR SELECT 
    USING (id IN (
      SELECT document_id FROM document_permissions WHERE user_id = current_setting('app.current_user_id')::uuid
    ));
    """)
    
    # Create document sharing RLS policy (UPDATE)
    op.execute("""
    CREATE POLICY "Users can update documents shared with write permission" 
    ON documents FOR UPDATE
    USING (id IN (
      SELECT document_id FROM document_permissions 
      WHERE user_id = current_setting('app.current_user_id')::uuid
      AND permission_level IN ('write', 'admin')
    ));
    """)
    
    # Create policy for document sections (chunks) - SELECT
    op.execute("""
    CREATE POLICY "Users can view their own document sections" 
    ON chunks FOR SELECT 
    USING (document_id IN (
      SELECT id FROM documents WHERE user_id = current_setting('app.current_user_id')::uuid OR is_public = true
    ));
    """)
    
    # Create policy for document sections shared with users - SELECT
    op.execute("""
    CREATE POLICY "Users can view document sections shared with them" 
    ON chunks FOR SELECT 
    USING (document_id IN (
      SELECT document_id FROM document_permissions WHERE user_id = current_setting('app.current_user_id')::uuid
    ));
    """)
    
    # Create policy for document sections - UPDATE
    op.execute("""
    CREATE POLICY "Users can update their own document sections" 
    ON chunks FOR UPDATE
    USING (document_id IN (
      SELECT id FROM documents WHERE user_id = current_setting('app.current_user_id')::uuid
    ));
    """)
    
    # Create policy for document sections shared with write permission - UPDATE
    op.execute("""
    CREATE POLICY "Users can update document sections shared with write permission" 
    ON chunks FOR UPDATE
    USING (document_id IN (
      SELECT document_id FROM document_permissions 
      WHERE user_id = current_setting('app.current_user_id')::uuid
      AND permission_level IN ('write', 'admin')
    ));
    """)


def downgrade():
    # Drop RLS policies for documents
    op.execute('DROP POLICY IF EXISTS "Users can view their own documents" ON documents;')
    op.execute('DROP POLICY IF EXISTS "Users can update their own documents" ON documents;')
    op.execute('DROP POLICY IF EXISTS "Users can delete their own documents" ON documents;')
    op.execute('DROP POLICY IF EXISTS "Users can view documents shared with them" ON documents;')
    op.execute('DROP POLICY IF EXISTS "Users can update documents shared with write permission" ON documents;')
    
    # Drop RLS policies for chunks
    op.execute('DROP POLICY IF EXISTS "Users can view their own document sections" ON chunks;')
    op.execute('DROP POLICY IF EXISTS "Users can view document sections shared with them" ON chunks;')
    op.execute('DROP POLICY IF EXISTS "Users can update their own document sections" ON chunks;')
    op.execute('DROP POLICY IF EXISTS "Users can update document sections shared with write permission" ON chunks;')
    
    # Disable RLS
    op.execute("ALTER TABLE documents DISABLE ROW LEVEL SECURITY;")
    op.execute("ALTER TABLE chunks DISABLE ROW LEVEL SECURITY;")

================
File: alembic/versions/ensure_doc_metadata_column.py
================
"""ensure_doc_metadata_column

Revision ID: ensure_doc_metadata_column
Revises: rename_metadata_columns
Create Date: 2025-03-25 15:42:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.sql import text

# revision identifiers, used by Alembic.
revision = 'ensure_doc_metadata_column'
down_revision = 'rename_metadata_columns'
branch_labels = None
depends_on = None


def upgrade():
    # Check if doc_metadata column exists in documents table
    conn = op.get_bind()
    inspector = sa.inspect(conn)
    columns = [col['name'] for col in inspector.get_columns('documents')]
    
    # Add doc_metadata column if it doesn't exist
    if 'doc_metadata' not in columns:
        op.add_column('documents', 
                     sa.Column('doc_metadata', JSONB(), 
                              nullable=True, 
                              server_default=text("'{}'::jsonb")))
        
        # If metadata column exists, migrate data from metadata to doc_metadata
        if 'metadata' in columns:
            op.execute(text(
                "UPDATE documents SET doc_metadata = metadata::jsonb WHERE metadata IS NOT NULL"
            ))
            # Drop the old metadata column
            op.drop_column('documents', 'metadata')


def downgrade():
    # This is a safety migration, so downgrade does nothing
    pass

================
File: alembic/versions/f7e702fc686e_merge_heads_for_document_permissions.py
================
"""merge_heads_for_document_permissions

Revision ID: f7e702fc686e
Revises: enable_row_level_security, ensure_doc_metadata_column, merge_heads
Create Date: 2025-03-27 12:53:36.110083

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'f7e702fc686e'
down_revision = ('enable_row_level_security', 'ensure_doc_metadata_column', 'merge_heads')
branch_labels = None
depends_on = None


def upgrade():
    pass


def downgrade():
    pass

================
File: alembic/versions/fix_rls_current_setting.py
================
"""Fix RLS current_setting function

Revision ID: fix_rls_current_setting
Revises: enable_row_level_security
Create Date: 2025-03-27 17:40:00.000000

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'fix_rls_current_setting'
down_revision = 'enable_row_level_security'
branch_labels = None
depends_on = None


def upgrade():
    # Create a function to safely get the app.current_user_id setting
    # This function will return NULL if the setting doesn't exist
    op.execute("""
    CREATE OR REPLACE FUNCTION safe_get_current_user_id() RETURNS uuid AS $$
    DECLARE
        user_id uuid;
    BEGIN
        BEGIN
            -- Try to get the current user ID
            user_id := current_setting('app.current_user_id')::uuid;
        EXCEPTION WHEN OTHERS THEN
            -- If it fails, return NULL
            user_id := NULL;
        END;
        RETURN user_id;
    END;
    $$ LANGUAGE plpgsql;
    """)
    
    # Update the RLS policies to use the safe function
    # Update ownership RLS policy for documents (SELECT)
    op.execute('DROP POLICY IF EXISTS "Users can view their own documents" ON documents;')
    op.execute("""
    CREATE POLICY "Users can view their own documents"
    ON documents FOR SELECT
    USING (user_id = safe_get_current_user_id() OR is_public = true);
    """)
    
    # Update ownership RLS policy for documents (UPDATE)
    op.execute('DROP POLICY IF EXISTS "Users can update their own documents" ON documents;')
    op.execute("""
    CREATE POLICY "Users can update their own documents"
    ON documents FOR UPDATE
    USING (user_id = safe_get_current_user_id());
    """)
    
    # Update ownership RLS policy for documents (DELETE)
    op.execute('DROP POLICY IF EXISTS "Users can delete their own documents" ON documents;')
    op.execute("""
    CREATE POLICY "Users can delete their own documents"
    ON documents FOR DELETE
    USING (user_id = safe_get_current_user_id());
    """)
    
    # Update document sharing RLS policy (SELECT)
    op.execute('DROP POLICY IF EXISTS "Users can view documents shared with them" ON documents;')
    op.execute("""
    CREATE POLICY "Users can view documents shared with them"
    ON documents FOR SELECT
    USING (id IN (
      SELECT document_id FROM document_permissions WHERE user_id = safe_get_current_user_id()
    ));
    """)
    
    # Update document sharing RLS policy (UPDATE)
    op.execute('DROP POLICY IF EXISTS "Users can update documents shared with write permission" ON documents;')
    op.execute("""
    CREATE POLICY "Users can update documents shared with write permission"
    ON documents FOR UPDATE
    USING (id IN (
      SELECT document_id FROM document_permissions
      WHERE user_id = safe_get_current_user_id()
      AND permission_level IN ('write', 'admin')
    ));
    """)
    
    # Update policy for document sections (chunks) - SELECT
    op.execute('DROP POLICY IF EXISTS "Users can view their own document sections" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can view their own document sections"
    ON chunks FOR SELECT
    USING (document_id IN (
      SELECT id FROM documents WHERE user_id = safe_get_current_user_id() OR is_public = true
    ));
    """)
    
    # Update policy for document sections shared with users - SELECT
    op.execute('DROP POLICY IF EXISTS "Users can view document sections shared with them" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can view document sections shared with them"
    ON chunks FOR SELECT
    USING (document_id IN (
      SELECT document_id FROM document_permissions WHERE user_id = safe_get_current_user_id()
    ));
    """)
    
    # Update policy for document sections - UPDATE
    op.execute('DROP POLICY IF EXISTS "Users can update their own document sections" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can update their own document sections"
    ON chunks FOR UPDATE
    USING (document_id IN (
      SELECT id FROM documents WHERE user_id = safe_get_current_user_id()
    ));
    """)
    
    # Update policy for document sections shared with write permission - UPDATE
    op.execute('DROP POLICY IF EXISTS "Users can update document sections shared with write permission" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can update document sections shared with write permission"
    ON chunks FOR UPDATE
    USING (document_id IN (
      SELECT document_id FROM document_permissions
      WHERE user_id = safe_get_current_user_id()
      AND permission_level IN ('write', 'admin')
    ));
    """)


def downgrade():
    # Revert to the original RLS policies
    # Revert ownership RLS policy for documents (SELECT)
    op.execute('DROP POLICY IF EXISTS "Users can view their own documents" ON documents;')
    op.execute("""
    CREATE POLICY "Users can view their own documents"
    ON documents FOR SELECT
    USING (user_id = current_setting('app.current_user_id')::uuid OR is_public = true);
    """)
    
    # Revert ownership RLS policy for documents (UPDATE)
    op.execute('DROP POLICY IF EXISTS "Users can update their own documents" ON documents;')
    op.execute("""
    CREATE POLICY "Users can update their own documents"
    ON documents FOR UPDATE
    USING (user_id = current_setting('app.current_user_id')::uuid);
    """)
    
    # Revert ownership RLS policy for documents (DELETE)
    op.execute('DROP POLICY IF EXISTS "Users can delete their own documents" ON documents;')
    op.execute("""
    CREATE POLICY "Users can delete their own documents"
    ON documents FOR DELETE
    USING (user_id = current_setting('app.current_user_id')::uuid);
    """)
    
    # Revert document sharing RLS policy (SELECT)
    op.execute('DROP POLICY IF EXISTS "Users can view documents shared with them" ON documents;')
    op.execute("""
    CREATE POLICY "Users can view documents shared with them"
    ON documents FOR SELECT
    USING (id IN (
      SELECT document_id FROM document_permissions WHERE user_id = current_setting('app.current_user_id')::uuid
    ));
    """)
    
    # Revert document sharing RLS policy (UPDATE)
    op.execute('DROP POLICY IF EXISTS "Users can update documents shared with write permission" ON documents;')
    op.execute("""
    CREATE POLICY "Users can update documents shared with write permission"
    ON documents FOR UPDATE
    USING (id IN (
      SELECT document_id FROM document_permissions
      WHERE user_id = current_setting('app.current_user_id')::uuid
      AND permission_level IN ('write', 'admin')
    ));
    """)
    
    # Revert policy for document sections (chunks) - SELECT
    op.execute('DROP POLICY IF EXISTS "Users can view their own document sections" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can view their own document sections"
    ON chunks FOR SELECT
    USING (document_id IN (
      SELECT id FROM documents WHERE user_id = current_setting('app.current_user_id')::uuid OR is_public = true
    ));
    """)
    
    # Revert policy for document sections shared with users - SELECT
    op.execute('DROP POLICY IF EXISTS "Users can view document sections shared with them" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can view document sections shared with them"
    ON chunks FOR SELECT
    USING (document_id IN (
      SELECT document_id FROM document_permissions WHERE user_id = current_setting('app.current_user_id')::uuid
    ));
    """)
    
    # Revert policy for document sections - UPDATE
    op.execute('DROP POLICY IF EXISTS "Users can update their own document sections" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can update their own document sections"
    ON chunks FOR UPDATE
    USING (document_id IN (
      SELECT id FROM documents WHERE user_id = current_setting('app.current_user_id')::uuid
    ));
    """)
    
    # Revert policy for document sections shared with write permission - UPDATE
    op.execute('DROP POLICY IF EXISTS "Users can update document sections shared with write permission" ON chunks;')
    op.execute("""
    CREATE POLICY "Users can update document sections shared with write permission"
    ON chunks FOR UPDATE
    USING (document_id IN (
      SELECT document_id FROM document_permissions
      WHERE user_id = current_setting('app.current_user_id')::uuid
      AND permission_level IN ('write', 'admin')
    ));
    """)
    
    # Drop the safe function
    op.execute("DROP FUNCTION IF EXISTS safe_get_current_user_id();")

================
File: alembic/versions/initial_schema.py
================
"""Initial database schema

Revision ID: initial_schema
Revises: 
Create Date: 2025-03-18 16:27:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB


# revision identifiers, used by Alembic.
revision = 'initial_schema'
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    # Create folders table
    op.create_table(
        'folders',
        sa.Column('path', sa.String(), nullable=False),
        sa.Column('name', sa.String(), nullable=False),
        sa.Column('parent_path', sa.String(), nullable=True),
        sa.Column('document_count', sa.Integer(), nullable=True, default=0),
        sa.Column('created_at', sa.DateTime(), nullable=True, default=sa.func.now()),
        sa.ForeignKeyConstraint(['parent_path'], ['folders.path'], ),
        sa.PrimaryKeyConstraint('path')
    )
    op.create_index('ix_folders_parent_path', 'folders', ['parent_path'])

    # Create documents table
    op.create_table(
        'documents',
        sa.Column('id', UUID(), nullable=False),
        sa.Column('filename', sa.String(), nullable=False),
        sa.Column('content', sa.Text(), nullable=True),
        sa.Column('metadata', JSONB(), nullable=True),
        sa.Column('folder', sa.String(), nullable=True),
        sa.Column('uploaded', sa.DateTime(), nullable=True),
        sa.Column('processing_status', sa.String(), nullable=True),
        sa.Column('processing_strategy', sa.String(), nullable=True),
        sa.Column('file_size', sa.Integer(), nullable=True),
        sa.Column('file_type', sa.String(), nullable=True),
        sa.Column('last_accessed', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['folder'], ['folders.path'], ),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_documents_filename', 'documents', ['filename'])
    op.create_index('ix_documents_folder', 'documents', ['folder'])
    op.create_index('ix_documents_processing_status', 'documents', ['processing_status'])

    # Create chunks table
    op.create_table(
        'chunks',
        sa.Column('id', UUID(), nullable=False),
        sa.Column('document_id', UUID(), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('metadata', JSONB(), nullable=True),
        sa.Column('index', sa.Integer(), nullable=False),
        sa.Column('embedding_quality', sa.Float(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_chunks_document_id', 'chunks', ['document_id'])
    op.create_index('ix_chunks_document_id_index', 'chunks', ['document_id', 'index'])

    # Create tags table
    op.create_table(
        'tags',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('name', sa.String(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=True),
        sa.Column('usage_count', sa.Integer(), nullable=True),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('name')
    )
    op.create_index('ix_tags_name', 'tags', ['name'])

    # Create document_tags association table
    op.create_table(
        'document_tags',
        sa.Column('document_id', UUID(), nullable=False),
        sa.Column('tag_id', sa.Integer(), nullable=False),
        sa.Column('added_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ),
        sa.ForeignKeyConstraint(['tag_id'], ['tags.id'], ),
        sa.PrimaryKeyConstraint('document_id', 'tag_id')
    )

    # Create conversations table
    op.create_table(
        'conversations',
        sa.Column('id', UUID(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=True),
        sa.Column('updated_at', sa.DateTime(), nullable=True),
        sa.Column('metadata', JSONB(), nullable=True),
        sa.Column('message_count', sa.Integer(), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_conversations_created_at', 'conversations', ['created_at'])
    op.create_index('ix_conversations_updated_at', 'conversations', ['updated_at'])

    # Create messages table
    op.create_table(
        'messages',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('conversation_id', UUID(), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('role', sa.String(), nullable=False),
        sa.Column('timestamp', sa.DateTime(), nullable=True),
        sa.Column('token_count', sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(['conversation_id'], ['conversations.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_messages_conversation_id', 'messages', ['conversation_id'])
    op.create_index('ix_messages_timestamp', 'messages', ['timestamp'])

    # Create citations table
    op.create_table(
        'citations',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('message_id', sa.Integer(), nullable=False),
        sa.Column('document_id', UUID(), nullable=True),
        sa.Column('chunk_id', UUID(), nullable=True),
        sa.Column('relevance_score', sa.Float(), nullable=True),
        sa.Column('excerpt', sa.Text(), nullable=True),
        sa.Column('character_range_start', sa.Integer(), nullable=True),
        sa.Column('character_range_end', sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(['chunk_id'], ['chunks.id'], ),
        sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ),
        sa.ForeignKeyConstraint(['message_id'], ['messages.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_citations_chunk_id', 'citations', ['chunk_id'])
    op.create_index('ix_citations_document_id', 'citations', ['document_id'])
    op.create_index('ix_citations_message_id', 'citations', ['message_id'])

    # Create processing_jobs table
    op.create_table(
        'processing_jobs',
        sa.Column('id', UUID(), nullable=False),
        sa.Column('status', sa.String(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=True),
        sa.Column('completed_at', sa.DateTime(), nullable=True),
        sa.Column('document_count', sa.Integer(), nullable=True),
        sa.Column('processed_count', sa.Integer(), nullable=True),
        sa.Column('strategy', sa.String(), nullable=True),
        sa.Column('metadata', JSONB(), nullable=True),
        sa.Column('progress_percentage', sa.Float(), nullable=True),
        sa.Column('error_message', sa.Text(), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_processing_jobs_created_at', 'processing_jobs', ['created_at'])
    op.create_index('ix_processing_jobs_status', 'processing_jobs', ['status'])

    # Create analytics_queries table
    op.create_table(
        'analytics_queries',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('query', sa.Text(), nullable=False),
        sa.Column('model', sa.String(), nullable=True),
        sa.Column('use_rag', sa.Boolean(), nullable=True),
        sa.Column('timestamp', sa.DateTime(), nullable=True),
        sa.Column('response_time_ms', sa.Float(), nullable=True),
        sa.Column('token_count', sa.Integer(), nullable=True),
        sa.Column('document_ids', JSONB(), nullable=True),
        sa.Column('query_type', sa.String(), nullable=True),
        sa.Column('successful', sa.Boolean(), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_analytics_queries_model', 'analytics_queries', ['model'])
    op.create_index('ix_analytics_queries_query_type', 'analytics_queries', ['query_type'])
    op.create_index('ix_analytics_queries_timestamp', 'analytics_queries', ['timestamp'])


def downgrade():
    op.drop_table('analytics_queries')
    op.drop_table('processing_jobs')
    op.drop_table('citations')
    op.drop_table('messages')
    op.drop_table('conversations')
    op.drop_table('document_tags')
    op.drop_table('tags')
    op.drop_table('chunks')
    op.drop_table('documents')
    op.drop_table('folders')

================
File: alembic/versions/merge_heads.py
================
"""merge heads

Revision ID: merge_heads
Revises: add_user_id_columns, add_users_table
Create Date: 2025-03-21 11:20:00.000000

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'merge_heads'
down_revision = ('add_user_id_columns', 'add_users_table')
branch_labels = None
depends_on = None


def upgrade():
    # This is a merge migration, no schema changes needed
    pass


def downgrade():
    # This is a merge migration, no schema changes needed
    pass

================
File: alembic/versions/rename_metadata_columns.py
================
"""rename_metadata_columns

Revision ID: rename_metadata_columns
Revises: update_metadata_to_jsonb
Create Date: 2025-03-25 14:34:44.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB

# revision identifiers, used by Alembic.
revision = 'rename_metadata_columns'
down_revision = 'update_metadata_to_jsonb'
branch_labels = None
depends_on = None


def upgrade():
    # Rename metadata column in documents table
    op.alter_column('documents', 'metadata', new_column_name='doc_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename metadata column in chunks table
    op.alter_column('chunks', 'metadata', new_column_name='chunk_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename metadata column in conversations table
    op.alter_column('conversations', 'metadata', new_column_name='conv_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename metadata column in processing_jobs table
    op.alter_column('processing_jobs', 'metadata', new_column_name='job_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename document_ids column in analytics_queries table
    op.alter_column('analytics_queries', 'document_ids', new_column_name='document_id_list', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'[]'::jsonb"))


def downgrade():
    # Revert column name changes
    op.alter_column('documents', 'doc_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('chunks', 'chunk_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('conversations', 'conv_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('processing_jobs', 'job_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('analytics_queries', 'document_id_list', new_column_name='document_ids', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'[]'::jsonb"))

================
File: alembic/versions/update_metadata_to_jsonb.py
================
"""Update metadata columns to use JSONB

Revision ID: update_metadata_to_jsonb
Revises: initial_schema
Create Date: 2025-03-20

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB

# revision identifiers, used by Alembic.
revision = 'update_metadata_to_jsonb'
down_revision = 'initial_schema'
branch_labels = None
depends_on = None

def upgrade():
    # For PostgreSQL, convert JSON to JSONB
    if op.get_bind().dialect.name == 'postgresql':
        # Update documents table
        op.alter_column('documents', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        # Update chunks table
        op.alter_column('chunks', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        # Update conversations table
        op.alter_column('conversations', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        # Update processing_jobs table
        op.alter_column('processing_jobs', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        # Update background_tasks table
        op.alter_column('background_tasks', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        # Update params column in background_tasks
        op.alter_column('background_tasks', 'params', 
                        type_=JSONB, 
                        postgresql_using='params::jsonb')
        
        # Update resource_usage column in background_tasks
        op.alter_column('background_tasks', 'resource_usage', 
                        type_=JSONB, 
                        postgresql_using='resource_usage::jsonb')
        
        # Update document_ids column in analytics_queries
        op.alter_column('analytics_queries', 'document_ids', 
                        type_=JSONB, 
                        postgresql_using='document_ids::jsonb')

def downgrade():
    # For PostgreSQL, convert JSONB back to JSON
    if op.get_bind().dialect.name == 'postgresql':
        # Update documents table
        op.alter_column('documents', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        # Update chunks table
        op.alter_column('chunks', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        # Update conversations table
        op.alter_column('conversations', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        # Update processing_jobs table
        op.alter_column('processing_jobs', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        # Update background_tasks table
        op.alter_column('background_tasks', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        # Update params column in background_tasks
        op.alter_column('background_tasks', 'params', 
                        type_=sa.JSON, 
                        postgresql_using='params::json')
        
        # Update resource_usage column in background_tasks
        op.alter_column('background_tasks', 'resource_usage', 
                        type_=sa.JSON, 
                        postgresql_using='resource_usage::json')
        
        # Update document_ids column in analytics_queries
        op.alter_column('analytics_queries', 'document_ids', 
                        type_=sa.JSON, 
                        postgresql_using='document_ids::json')

================
File: alembic/env.py
================
import os
import sys
import asyncio
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool
from sqlalchemy.ext.asyncio import create_async_engine

from alembic import context

# Add the app directory to the Python path
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# Import the SQLAlchemy Base and models
from app.db.session import Base
import app.db.models  # Import models to register them with Base

# Get database URL from environment or use default
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://postgres:postgres@localhost:5432/metis_rag"
)

print(f"DATABASE_URL from environment: {DATABASE_URL}")

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)

# Set the SQLAlchemy URL in the Alembic config
config.set_main_option("sqlalchemy.url", DATABASE_URL)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline():
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    # Use the DATABASE_URL directly
    context.configure(
        url=DATABASE_URL,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Create an async engine
    connectable = create_async_engine(DATABASE_URL)

    # Create a function that will run in a new event loop
    async def run_async_migrations():
        async with connectable.connect() as connection:
            await connection.run_sync(do_run_migrations)

    # Function to run migrations with a synchronous connection
    def do_run_migrations(connection):
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

    # Run the async function
    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

================
File: alembic/script.py.mako
================
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision = ${repr(up_revision)}
down_revision = ${repr(down_revision)}
branch_labels = ${repr(branch_labels)}
depends_on = ${repr(depends_on)}


def upgrade():
    ${upgrades if upgrades else "pass"}


def downgrade():
    ${downgrades if downgrades else "pass"}

================
File: app/api/__init__.py
================
from app.api.chat import router as chat_router
from app.api.documents import router as documents_router
from app.api.system import router as system_router

================
File: app/api/admin.py
================
from typing import List, Optional
from fastapi import APIRouter, HTTPException, Depends, status, Query
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.user import User, UserCreate, UserUpdate
from app.core.security import get_current_admin_user
from app.db.dependencies import get_db, get_user_repository
from app.db.repositories.user_repository import UserRepository

# Create router
router = APIRouter()

@router.get("/users", response_model=List[User])
async def get_users(
    skip: int = 0,
    limit: int = 100,
    search: Optional[str] = None,
    current_user: User = Depends(get_current_admin_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all users (admin only)
    
    Args:
        skip: Number of users to skip
        limit: Maximum number of users to return
        search: Search term for username or email
        current_user: Current user (admin only)
        db: Database session
        
    Returns:
        List of users
    """
    user_repository = await get_user_repository(db)
    
    if search:
        users = await user_repository.search_users(search, skip=skip, limit=limit)
    else:
        users = await user_repository.get_all_users(skip=skip, limit=limit)
    
    return users

@router.get("/users/{user_id}", response_model=User)
async def get_user(
    user_id: str,
    current_user: User = Depends(get_current_admin_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a user by ID (admin only)
    
    Args:
        user_id: User ID
        current_user: Current user (admin only)
        db: Database session
        
    Returns:
        User
    """
    user_repository = await get_user_repository(db)
    user = await user_repository.get_by_id(user_id)
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    return user

@router.post("/users", response_model=User)
async def create_user(
    user_data: UserCreate,
    current_user: User = Depends(get_current_admin_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Create a new user (admin only)
    
    Args:
        user_data: User creation data
        current_user: Current user (admin only)
        db: Database session
        
    Returns:
        Created user
    """
    try:
        user_repository = await get_user_repository(db)
        user = await user_repository.create_user(user_data)
        return user
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.put("/users/{user_id}", response_model=User)
async def update_user(
    user_id: str,
    user_data: UserUpdate,
    current_user: User = Depends(get_current_admin_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Update a user (admin only)
    
    Args:
        user_id: User ID
        user_data: User update data
        current_user: Current user (admin only)
        db: Database session
        
    Returns:
        Updated user
    """
    try:
        user_repository = await get_user_repository(db)
        updated_user = await user_repository.update_user(user_id, user_data)
        
        if not updated_user:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="User not found"
            )
        
        return updated_user
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.delete("/users/{user_id}", response_model=dict)
async def delete_user(
    user_id: str,
    current_user: User = Depends(get_current_admin_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Delete a user (admin only)
    
    Args:
        user_id: User ID
        current_user: Current user (admin only)
        db: Database session
        
    Returns:
        Success message
    """
    # Prevent deleting yourself
    if user_id == current_user.id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Cannot delete your own account"
        )
    
    user_repository = await get_user_repository(db)
    success = await user_repository.delete_user(user_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    return {"message": "User deleted successfully"}

================
File: app/api/analytics.py
================
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from uuid import UUID
from fastapi import APIRouter, HTTPException, Query, Depends
from sqlalchemy.orm import Session
import json

from app.db.dependencies import get_db, get_analytics_repository, get_document_repository
from app.db.repositories.analytics_repository import AnalyticsRepository
from app.db.repositories.document_repository import DocumentRepository

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.analytics")

@router.post("/record_query")
async def record_query(
    query_data: Dict[str, Any],
    db: Session = Depends(get_db),
    analytics_repository: AnalyticsRepository = Depends(get_analytics_repository)
):
    """
    Record a query for analytics
    """
    try:
        # Add timestamp if not provided
        if "timestamp" not in query_data:
            query_data["timestamp"] = datetime.now().isoformat()
        
        # Create analytics query record
        analytics_query = await analytics_repository.create_analytics_query(
            query=query_data.get("query", ""),
            model=query_data.get("model", ""),
            use_rag=query_data.get("use_rag", False),
            response_time_ms=query_data.get("response_time_ms", 0),
            token_count=query_data.get("token_count", 0),
            document_ids=query_data.get("document_ids", []),
            query_type=query_data.get("query_type", "standard"),
            successful=query_data.get("successful", True)
        )
        
        return {"success": True, "message": "Query recorded for analytics", "id": analytics_query.id}
    except Exception as e:
        logger.error(f"Error recording query analytics: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error recording query analytics: {str(e)}")

@router.get("/query_stats")
async def get_query_stats(
    time_period: Optional[str] = Query("all", description="Time period for stats (all, day, week, month)"),
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    db: Session = Depends(get_db),
    analytics_repository: AnalyticsRepository = Depends(get_analytics_repository)
):
    """
    Get query statistics with pagination
    """
    try:
        # Get cutoff date based on time period
        cutoff_date = get_cutoff_date(time_period)
        
        # Get query stats from repository
        stats = analytics_repository.get_query_stats(cutoff_date)
        
        # Get most common queries
        most_common_queries = analytics_repository.get_most_common_queries(
            cutoff_date=cutoff_date,
            limit=10
        )
        
        # Get recent queries with pagination
        recent_queries = analytics_repository.get_recent_queries(
            cutoff_date=cutoff_date,
            skip=skip,
            limit=10  # Always get the 10 most recent
        )
        
        # Format the response
        return {
            "query_count": stats.get("query_count", 0),
            "avg_response_time_ms": stats.get("avg_response_time_ms", 0),
            "avg_token_count": stats.get("avg_token_count", 0),
            "rag_usage_percent": stats.get("rag_usage_percent", 0),
            "most_common_queries": [
                {"query": q.query, "count": q.count}
                for q in most_common_queries
            ],
            "recent_queries": [
                {
                    "id": q.id,
                    "query": q.query,
                    "model": q.model,
                    "use_rag": q.use_rag,
                    "timestamp": q.timestamp.isoformat() if q.timestamp else None,
                    "response_time_ms": q.response_time_ms,
                    "token_count": q.token_count,
                    "document_ids": q.document_ids,
                    "query_type": q.query_type,
                    "successful": q.successful
                }
                for q in recent_queries
            ],
            "time_period": time_period,
            "pagination": {
                "skip": skip,
                "limit": limit,
                "total": stats.get("query_count", 0)
            }
        }
    except Exception as e:
        logger.error(f"Error getting query stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting query stats: {str(e)}")

@router.get("/document_usage")
async def get_document_usage(
    time_period: Optional[str] = Query("all", description="Time period for stats (all, day, week, month)"),
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100),
    db: Session = Depends(get_db),
    analytics_repository: AnalyticsRepository = Depends(get_analytics_repository),
    document_repository: DocumentRepository = Depends(get_document_repository)
):
    """
    Get document usage statistics with pagination
    """
    try:
        # Get cutoff date based on time period
        cutoff_date = get_cutoff_date(time_period)
        
        # Get document usage stats from repository
        document_usage = analytics_repository.get_document_usage_stats(
            cutoff_date=cutoff_date,
            skip=skip,
            limit=limit
        )
        
        # Get total document count
        document_count = analytics_repository.count_documents_used(cutoff_date)
        
        # Enrich with document metadata
        enriched_usage = []
        for usage in document_usage:
            try:
                # Get document info
                document = document_repository.get_by_id(UUID(usage.document_id))
                
                # Add document info to usage stats
                enriched_usage.append({
                    "id": usage.document_id,
                    "usage_count": usage.usage_count,
                    "last_used": usage.last_used.isoformat() if usage.last_used else None,
                    "filename": document.filename if document else "Unknown",
                    "folder": document.folder if document else "Unknown"
                })
            except Exception as doc_error:
                logger.error(f"Error enriching document usage: {str(doc_error)}")
                # Include basic usage stats without document info
                enriched_usage.append({
                    "id": usage.document_id,
                    "usage_count": usage.usage_count,
                    "last_used": usage.last_used.isoformat() if usage.last_used else None,
                    "filename": "Unknown",
                    "folder": "Unknown"
                })
        
        return {
            "document_count": document_count,
            "most_used": enriched_usage,
            "time_period": time_period,
            "pagination": {
                "skip": skip,
                "limit": limit,
                "total": document_count
            }
        }
    except Exception as e:
        logger.error(f"Error getting document usage stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting document usage stats: {str(e)}")

@router.get("/system_stats")
async def get_system_stats(
    db: Session = Depends(get_db),
    analytics_repository: AnalyticsRepository = Depends(get_analytics_repository),
    document_repository: DocumentRepository = Depends(get_document_repository)
):
    """
    Get system statistics
    """
    try:
        from app.rag.vector_store import VectorStore
        
        # Get vector store stats
        vector_store = VectorStore()
        vector_stats = vector_store.get_stats()
        
        # Get query stats
        query_count = analytics_repository.count_queries()
        
        # Get document stats
        document_count = document_repository.count()
        
        # Get additional stats
        rag_query_count = analytics_repository.count_rag_queries()
        avg_response_time = analytics_repository.get_avg_response_time()
        
        return {
            "vector_store": vector_stats,
            "query_count": query_count,
            "document_count": document_count,
            "rag_query_count": rag_query_count,
            "rag_usage_percent": (rag_query_count / query_count * 100) if query_count > 0 else 0,
            "avg_response_time_ms": avg_response_time,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting system stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting system stats: {str(e)}")

@router.get("/model_performance")
async def get_model_performance(
    time_period: Optional[str] = Query("all", description="Time period for stats (all, day, week, month)"),
    db: Session = Depends(get_db),
    analytics_repository: AnalyticsRepository = Depends(get_analytics_repository)
):
    """
    Get model performance statistics
    """
    try:
        # Get cutoff date based on time period
        cutoff_date = get_cutoff_date(time_period)
        
        # Get model performance stats from repository
        model_stats = analytics_repository.get_model_performance_stats(cutoff_date)
        
        return {
            "models": [
                {
                    "model": stat.model,
                    "query_count": stat.query_count,
                    "avg_response_time_ms": stat.avg_response_time_ms,
                    "avg_token_count": stat.avg_token_count,
                    "success_rate": stat.success_rate
                }
                for stat in model_stats
            ],
            "time_period": time_period
        }
    except Exception as e:
        logger.error(f"Error getting model performance stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting model performance stats: {str(e)}")

@router.get("/query_types")
async def get_query_types(
    time_period: Optional[str] = Query("all", description="Time period for stats (all, day, week, month)"),
    db: Session = Depends(get_db),
    analytics_repository: AnalyticsRepository = Depends(get_analytics_repository)
):
    """
    Get query type statistics
    """
    try:
        # Get cutoff date based on time period
        cutoff_date = get_cutoff_date(time_period)
        
        # Get query type stats from repository
        query_type_stats = analytics_repository.get_query_type_stats(cutoff_date)
        
        return {
            "query_types": [
                {
                    "query_type": stat.query_type,
                    "query_count": stat.query_count,
                    "avg_response_time_ms": stat.avg_response_time_ms,
                    "success_rate": stat.success_rate
                }
                for stat in query_type_stats
            ],
            "time_period": time_period
        }
    except Exception as e:
        logger.error(f"Error getting query type stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting query type stats: {str(e)}")

def get_cutoff_date(time_period):
    """
    Get cutoff date for time period
    """
    now = datetime.now()
    
    if time_period == "day":
        return now - timedelta(days=1)
    elif time_period == "week":
        return now - timedelta(days=7)
    elif time_period == "month":
        return now - timedelta(days=30)
    else:
        # Default to all time
        return now - timedelta(days=365 * 10)  # 10 years ago

================
File: app/api/auth.py
================
from datetime import timedelta, datetime
from typing import Optional, List
from fastapi import APIRouter, HTTPException, Depends, status, Request, Body
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.ext.asyncio import AsyncSession
import logging
import uuid

from app.models.user import User, UserCreate, UserUpdate
from app.core.security import (
    create_access_token, create_refresh_token, verify_refresh_token,
    get_current_user, get_current_active_user, get_current_admin_user,
    Token, RefreshToken
)
from app.core.config import SETTINGS
from app.core.rate_limit import login_rate_limit
from app.core.security_alerts import SecurityEvent, log_security_event
from app.db.dependencies import get_db, get_user_repository
from app.db.repositories.user_repository import UserRepository

# Setup logging
logger = logging.getLogger("app.api.auth")

# Create router
router = APIRouter()

@router.post("/token", response_model=Token)
async def login_for_access_token(
    request: Request,
    form_data: OAuth2PasswordRequestForm = Depends(),
    db: AsyncSession = Depends(get_db),
    rate_limiter: None = Depends(login_rate_limit) if SETTINGS.rate_limiting_enabled else None
):
    """
    Get an access token and refresh token for a user
    
    This endpoint authenticates a user with username and password,
    and returns JWT access and refresh tokens if successful.
    
    Args:
        request: The FastAPI request object
        form_data: The OAuth2 password request form data
        db: The database session
        rate_limiter: Optional rate limiter dependency
        
    Returns:
        A Token object containing the access token, refresh token, and expiration
        
    Raises:
        HTTPException: If authentication fails
    """
    client_ip = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("user-agent", "unknown")
    
    # Log login attempt
    logger.info(f"Login attempt for user: {form_data.username}, IP: {client_ip}, User-Agent: {user_agent}")
    
    user_repository = await get_user_repository(db)
    user = await user_repository.authenticate_user(form_data.username, form_data.password)
    if not user:
        # Log failed login attempt
        logger.warning(f"Failed login attempt for user: {form_data.username}, IP: {client_ip}, User-Agent: {user_agent}")
        
        # Create and log security event
        security_event = SecurityEvent(
            event_type="failed_login",
            severity="medium",
            source_ip=client_ip,
            username=form_data.username,
            user_agent=user_agent,
            details={"reason": "Incorrect username or password"}
        )
        log_security_event(security_event)
        
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Log successful login
    logger.info(f"Successful login for user: {form_data.username}, IP: {client_ip}")
    
    # Create and log security event for successful login
    security_event = SecurityEvent(
        event_type="successful_login",
        severity="low",
        source_ip=client_ip,
        username=form_data.username,
        user_agent=user_agent
    )
    log_security_event(security_event)
    
    # Update user's last login time
    await user_repository.update_user(user.id, {"last_login": datetime.utcnow()})
    
    # Create tokens with additional claims
    token_data = {
        "sub": user.username,
        "user_id": user.id,
        "aud": SETTINGS.jwt_audience,
        "iss": SETTINGS.jwt_issuer,
        "jti": str(uuid.uuid4())  # Unique token ID
    }
    
    # Create access token
    access_token_expires = timedelta(minutes=SETTINGS.access_token_expire_minutes)
    access_token = create_access_token(
        data=token_data,
        expires_delta=access_token_expires
    )
    
    # Create refresh token
    refresh_token_expires = timedelta(days=SETTINGS.refresh_token_expire_days)
    refresh_token = create_refresh_token(
        data=token_data,
        expires_delta=refresh_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": SETTINGS.access_token_expire_minutes * 60,  # in seconds
        "refresh_token": refresh_token
    }

@router.post("/refresh", response_model=Token)
async def refresh_access_token(
    request: Request,
    refresh_token_data: RefreshToken,
    db: AsyncSession = Depends(get_db)
):
    """
    Refresh an access token using a refresh token
    
    This endpoint validates a refresh token and issues a new access token
    if the refresh token is valid.
    
    Args:
        request: The FastAPI request object
        refresh_token_data: The refresh token data
        db: The database session
        
    Returns:
        A Token object containing the new access token and expiration
        
    Raises:
        HTTPException: If the refresh token is invalid
    """
    client_ip = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("user-agent", "unknown")
    
    # Verify refresh token
    payload = verify_refresh_token(refresh_token_data.refresh_token)
    if not payload:
        # Log failed refresh attempt
        logger.warning(f"Failed token refresh attempt, IP: {client_ip}, User-Agent: {user_agent}")
        
        # Create and log security event
        security_event = SecurityEvent(
            event_type="failed_token_refresh",
            severity="medium",
            source_ip=client_ip,
            user_agent=user_agent,
            details={"reason": "Invalid refresh token"}
        )
        log_security_event(security_event)
        
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid refresh token",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Extract user information from payload
    username = payload.get("sub")
    user_id = payload.get("user_id")
    
    if not username or not user_id:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token payload",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Verify user exists
    user_repository = await get_user_repository(db)
    user = await user_repository.get_by_username(username)
    
    if not user or not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="User not found or inactive",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Log successful token refresh
    logger.info(f"Successful token refresh for user: {username}, IP: {client_ip}")
    
    # Create and log security event for successful token refresh
    security_event = SecurityEvent(
        event_type="successful_token_refresh",
        severity="low",
        source_ip=client_ip,
        username=username,
        user_agent=user_agent
    )
    log_security_event(security_event)
    
    # Update user's last login time
    await user_repository.update_user(user.id, {"last_login": datetime.utcnow()})
    
    # Create new token with the same claims as the refresh token
    # but with a new JTI (JWT ID)
    token_data = {
        "sub": username,
        "user_id": user_id,
        "aud": payload.get("aud", SETTINGS.jwt_audience),
        "iss": payload.get("iss", SETTINGS.jwt_issuer),
        "jti": str(uuid.uuid4())  # New unique token ID
    }
    
    # Create new access token
    access_token_expires = timedelta(minutes=SETTINGS.access_token_expire_minutes)
    access_token = create_access_token(
        data=token_data,
        expires_delta=access_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": SETTINGS.access_token_expire_minutes * 60,  # in seconds
        "refresh_token": refresh_token_data.refresh_token  # Return the same refresh token
    }

@router.post("/register", response_model=User)
async def register_user(
    user_data: UserCreate,
    db: AsyncSession = Depends(get_db)
):
    """
    Register a new user
    """
    try:
        user_repository = await get_user_repository(db)
        user = await user_repository.create_user(user_data)
        return user
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.get("/me", response_model=User)
async def read_users_me(current_user: User = Depends(get_current_active_user)):
    """
    Get the current user
    """
    return current_user

@router.put("/me", response_model=User)
async def update_user_me(
    user_data: UserUpdate,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Update the current user
    """
    try:
        user_repository = await get_user_repository(db)
        updated_user = await user_repository.update_user(current_user.id, user_data)
        return updated_user
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.get("/users", response_model=List[User])
async def read_users(
    skip: int = 0,
    limit: int = 100,
    current_user: User = Depends(get_current_admin_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all users (admin only)
    """
    user_repository = await get_user_repository(db)
    users = await user_repository.get_all_users(skip=skip, limit=limit)
    return users

@router.get("/users/{user_id}", response_model=User)
async def read_user(
    user_id: str,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a user by ID (admin only or self)
    """
    if not current_user.is_admin and current_user.id != user_id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )
    
    user_repository = await get_user_repository(db)
    user = await user_repository.get_by_id(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    return user

================
File: app/api/chat.py
================
import logging
import json
from typing import Dict, List, Optional, Any
from uuid import UUID, uuid4
from fastapi import APIRouter, HTTPException, Depends, Response, Query, Request
from sse_starlette.sse import EventSourceResponse
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.chat import (
    ChatQuery,
    ChatResponse,
    Conversation,
    Message,
    Citation
)
from app.models.user import User
from app.rag.rag_engine import RAGEngine
from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent
from app.rag.agents.enhanced_langgraph_rag_agent import EnhancedLangGraphRAGAgent
from app.utils.text_utils import extract_citations
from app.core.config import DEFAULT_MODEL, USE_LANGGRAPH_RAG, USE_ENHANCED_LANGGRAPH_RAG
from app.db.dependencies import get_db, get_conversation_repository
from app.db.repositories.conversation_repository import ConversationRepository
from app.core.security import get_current_active_user

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.chat")

# RAG engine instance
rag_engine = RAGEngine()

# LangGraph RAG Agent instances (conditional on configuration)
langgraph_rag_agent = LangGraphRAGAgent() if USE_LANGGRAPH_RAG else None
enhanced_langgraph_rag_agent = EnhancedLangGraphRAGAgent() if USE_LANGGRAPH_RAG and USE_ENHANCED_LANGGRAPH_RAG else None

if USE_LANGGRAPH_RAG:
    logger.info("LangGraph RAG Agent is enabled")
    if USE_ENHANCED_LANGGRAPH_RAG:
        logger.info("Enhanced LangGraph RAG Agent is enabled")
    else:
        logger.info("Enhanced LangGraph RAG Agent is disabled")
else:
    logger.info("LangGraph RAG Agents are disabled")

# Maximum number of messages to include in conversation history
MAX_HISTORY_MESSAGES = 25

@router.post("/query", response_model=ChatResponse)
async def query_chat(
    query: ChatQuery,
    request: Request,
    db: AsyncSession = Depends(get_db),
    conversation_repository: ConversationRepository = Depends(get_conversation_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Send a chat query and get a response
    """
    try:
        # Get or create conversation
        conversation_id = query.conversation_id
        
        if conversation_id:
            # Try to get existing conversation
            try:
                conversation_uuid = UUID(conversation_id)
                conversation = await conversation_repository.get_by_id(conversation_uuid)
                
                # The repository will handle permission checks based on user_id
                # If the conversation doesn't belong to the user, it will return None
                if not conversation:
                    # Create new conversation if not found or not authorized
                    # The repository will use its user_id context
                    conversation = await conversation_repository.create_conversation()
                    conversation_id = str(conversation.id)
            except ValueError:
                # Invalid UUID format, create new conversation
                conversation = await conversation_repository.create_conversation()
                conversation_id = str(conversation.id)
        else:
            # Create new conversation
            # The repository will use its user_id context
            conversation = await conversation_repository.create_conversation()
            conversation_id = str(conversation.id)
        
        # Add user message to conversation
        user_message = await conversation_repository.add_message(
            conversation_id=UUID(conversation_id),
            content=query.message,
            role="user"
        )
        
        # Get model name
        model = query.model or DEFAULT_MODEL
        
        # Query RAG engine
        if query.stream:
            # For streaming, return an EventSourceResponse
            logger.info(f"Streaming response for conversation {conversation_id}")
            
            async def event_generator():
                full_response = ""
                
                # First, send the conversation ID as a separate event with a specific event type
                # The SSE format requires a dictionary with 'event' and 'data' keys
                yield {"event": "conversation_id", "data": conversation_id}
                
                # Get conversation history
                conversation_messages = await conversation_repository.get_conversation_messages(
                    conversation_id=UUID(conversation_id),
                    limit=MAX_HISTORY_MESSAGES
                )
                
                # Extract metadata filters if provided
                metadata_filters = query.metadata_filters if hasattr(query, 'metadata_filters') else None
                
                # Get RAG response
                rag_response = await rag_engine.query(
                    query=query.message,
                    model=model,
                    use_rag=query.use_rag,
                    stream=True,
                    model_parameters=query.model_parameters,
                    conversation_history=conversation_messages,
                    metadata_filters=metadata_filters,
                    user_id=conversation.conv_metadata.get("user_id")
                )
                
                # Get sources (with safety check)
                sources = rag_response.get("sources")
                if sources is None:
                    logger.warning("No sources returned from RAG engine")
                    sources = []
                
                # Stream the response
                async for token in rag_response["stream"]:
                    full_response += token
                    yield token
                
                # Add assistant message to conversation
                assistant_message = await conversation_repository.add_message(
                    conversation_id=UUID(conversation_id),
                    content=full_response,
                    role="assistant"
                )
                
                # Add citations if any
                if sources:
                    for source in sources:
                        await conversation_repository.add_citation(
                            message_id=assistant_message.id,
                            document_id=UUID(source.document_id) if hasattr(source, "document_id") else None,
                            chunk_id=UUID(source.chunk_id) if hasattr(source, "chunk_id") else None,
                            relevance_score=source.relevance_score if hasattr(source, "relevance_score") else None,
                            excerpt=source.excerpt if hasattr(source, "excerpt") else ""
                        )
            
            return EventSourceResponse(event_generator())
        else:
            # For non-streaming, return the complete response
            logger.info(f"Generating response for conversation {conversation_id}")
            
            # Get conversation history
            conversation_messages = await conversation_repository.get_conversation_messages(
                conversation_id=UUID(conversation_id),
                limit=MAX_HISTORY_MESSAGES
            )
            
            # Extract metadata filters if provided
            metadata_filters = query.metadata_filters if hasattr(query, 'metadata_filters') else None
            
            # Get RAG response
            rag_response = await rag_engine.query(
                query=query.message,
                model=model,
                use_rag=query.use_rag,
                stream=False,
                model_parameters=query.model_parameters,
                conversation_history=conversation_messages,
                metadata_filters=metadata_filters,
                user_id=conversation.conv_metadata.get("user_id")
            )
            
            # Get response and sources
            response_text = rag_response.get("answer", "")
            sources = rag_response.get("sources")
            if sources is None:
                logger.warning("No sources returned from RAG engine")
                sources = []
            
            # Add assistant message to conversation
            assistant_message = await conversation_repository.add_message(
                conversation_id=UUID(conversation_id),
                content=response_text,
                role="assistant"
            )
            
            # Add citations if any
            if sources:
                for source in sources:
                    await conversation_repository.add_citation(
                        message_id=assistant_message.id,
                        document_id=UUID(source.document_id) if hasattr(source, "document_id") else None,
                        chunk_id=UUID(source.chunk_id) if hasattr(source, "chunk_id") else None,
                        relevance_score=source.relevance_score if hasattr(source, "relevance_score") else None,
                        excerpt=source.excerpt if hasattr(source, "excerpt") else ""
                    )
            
            # Return response
            return ChatResponse(
                message=response_text,
                conversation_id=conversation_id,
                citations=sources
            )
    except Exception as e:
        logger.error(f"Error generating chat response: {str(e)}")
        
        # Create a user-friendly error message
        error_message = "Sorry, there was an error processing your request."
        
        # Check if it's a future date query
        if "conversation_id" in locals() and conversation_id:
            try:
                conversation_uuid = UUID(conversation_id)
                conversation = await conversation_repository.get_by_id(conversation_uuid)
                if conversation:
                    # Get the last user message
                    last_message = await conversation_repository.get_last_user_message(conversation_uuid)
                    if last_message:
                        user_query = last_message.content.lower()
                        
                        # Check for future year patterns
                        import re
                        current_year = 2025  # Hardcoded for now, could use datetime.now().year
                        year_match = re.search(r'\b(20\d\d|19\d\d)\b', user_query)
                        
                        if year_match and int(year_match.group(1)) > current_year:
                            error_message = f"I cannot provide information about events in {year_match.group(1)} as it's in the future. The current year is {current_year}."
                        elif re.search(r'what will happen|what is going to happen|predict the future|future events|in the future', user_query):
                            error_message = "I cannot predict future events or provide information about what will happen in the future."
            except (ValueError, Exception) as e:
                logger.error(f"Error checking for future date query: {str(e)}")
        
        # Return a 200 response with the error message instead of raising an exception
        # This allows the frontend to display the message properly
        return ChatResponse(
            message=error_message,
            conversation_id=conversation_id if "conversation_id" in locals() else None,
            citations=None
        )

@router.get("/history")
async def get_history(
    conversation_id: UUID,
    request: Request,
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    db: AsyncSession = Depends(get_db),
    conversation_repository: ConversationRepository = Depends(get_conversation_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Get conversation history with pagination
    """
    # The repository will handle permission checks based on user_id
    conversation = await conversation_repository.get_by_id(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail=f"Conversation {conversation_id} not found")
    
    # Note: The repository's get_by_id method already checks permissions
    # If the conversation doesn't belong to the user, it will return None
    
    # Get messages with pagination
    messages = await conversation_repository.get_conversation_messages(
        conversation_id=conversation_id,
        skip=skip,
        limit=limit
    )
    
    # Get total message count
    total_messages = conversation.message_count
    
    return {
        "id": str(conversation.id),
        "user_id": conversation.conv_metadata.get("user_id"),
        "created_at": conversation.created_at,
        "updated_at": conversation.updated_at,
        "messages": messages,
        "total_messages": total_messages,
        "pagination": {
            "skip": skip,
            "limit": limit,
            "total": total_messages
        }
    }

@router.post("/save")
async def save_conversation(
    conversation_id: UUID,
    request: Request,
    db: AsyncSession = Depends(get_db),
    conversation_repository: ConversationRepository = Depends(get_conversation_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Save a conversation (mark as saved in metadata)
    """
    # The repository will handle permission checks based on user_id
    conversation = await conversation_repository.get_by_id(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail=f"Conversation {conversation_id} not found")
    
    # Note: The repository's get_by_id method already checks permissions
    # If the conversation doesn't belong to the user, it will return None
    
    # Update metadata to mark as saved
    metadata = conversation.conv_metadata or {}
    metadata["saved"] = True
    
    # Update conversation
    updated_conversation = await conversation_repository.update_conversation(
        conversation_id=conversation_id,
        metadata=metadata
    )
    
    return {"success": True, "message": f"Conversation {conversation_id} saved"}

@router.delete("/clear")
async def clear_conversation(
    request: Request,
    conversation_id: Optional[UUID] = None,
    db: AsyncSession = Depends(get_db),
    conversation_repository: ConversationRepository = Depends(get_conversation_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Clear a conversation or all conversations for the current user
    """
    if conversation_id:
        # The repository will handle permission checks based on user_id
        conversation = await conversation_repository.get_by_id(conversation_id)
        if not conversation:
            raise HTTPException(status_code=404, detail=f"Conversation {conversation_id} not found")
        
        # Note: The repository's get_by_id method already checks permissions
        # If the conversation doesn't belong to the user, it will return None
        
        # Delete specific conversation
        await conversation_repository.delete(conversation_id)
        return {"success": True, "message": f"Conversation {conversation_id} cleared"}
    else:
        # Delete all conversations for the current user
        await conversation_repository.delete_by_user_id(current_user.id)
        return {"success": True, "message": "All your conversations cleared"}

@router.post("/langgraph_rag", response_model=ChatResponse)
async def langgraph_query_chat(
    query: ChatQuery,
    request: Request,
    db: AsyncSession = Depends(get_db),
    conversation_repository: ConversationRepository = Depends(get_conversation_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Send a chat query to the LangGraph RAG Agent and get a response
    """
    if not USE_LANGGRAPH_RAG or not langgraph_rag_agent:
        raise HTTPException(status_code=400, detail="LangGraph RAG Agent is not enabled")
    
    try:
        # Get or create conversation
        conversation_id = query.conversation_id
        
        if conversation_id:
            # Try to get existing conversation
            try:
                conversation_uuid = UUID(conversation_id)
                conversation = await conversation_repository.get_by_id(conversation_uuid)
                
                # The repository will handle permission checks based on user_id
                # If the conversation doesn't belong to the user, it will return None
                if not conversation:
                    # Create new conversation if not found or not authorized
                    # The repository will use its user_id context
                    conversation = await conversation_repository.create_conversation()
                    conversation_id = str(conversation.id)
            except ValueError:
                # Invalid UUID format, create new conversation
                conversation = await conversation_repository.create_conversation()
                conversation_id = str(conversation.id)
        else:
            # Create new conversation
            # The repository will use its user_id context
            conversation = await conversation_repository.create_conversation()
            conversation_id = str(conversation.id)
        
        # Add user message to conversation
        user_message = await conversation_repository.add_message(
            conversation_id=UUID(conversation_id),
            content=query.message,
            role="user"
        )
        
        # Get model name
        model = query.model or DEFAULT_MODEL
        
        # Format conversation history
        conversation_context = None
        messages = await conversation_repository.get_conversation_messages(
            conversation_id=UUID(conversation_id),
            limit=6  # Get 6 messages to exclude the current one
        )
        
        if len(messages) > 1:  # Only include history if there's more than just the current message
            # Get the last few messages (up to 5) to keep context manageable, but exclude the most recent user message
            recent_history = messages[:-1]
            if len(recent_history) > 5:
                recent_history = recent_history[-5:]
            
            # Format the conversation history
            history_pieces = []
            for msg in recent_history:
                role_prefix = "User" if msg.role == "user" else "Assistant"
                history_pieces.append(f"{role_prefix}: {msg.content}")
            
            conversation_context = "\n".join(history_pieces)
            logger.info(f"Including conversation history with {len(recent_history)} messages")
        else:
            logger.info("No previous conversation history to include")
        
        # Extract metadata filters if provided
        metadata_filters = query.metadata_filters if hasattr(query, 'metadata_filters') else None
        
        # Query LangGraph RAG Agent
        if query.stream:
            # For streaming, return an EventSourceResponse
            logger.info(f"Streaming response for conversation {conversation_id} using LangGraph RAG Agent")
            
            async def event_generator():
                full_response = ""
                
                # First, send the conversation ID as a separate event with a specific event type
                yield {"event": "conversation_id", "data": conversation_id}
                
                # Get LangGraph RAG response
                langgraph_response = await langgraph_rag_agent.query(
                    query=query.message,
                    model=model,
                    system_prompt=None,  # Use default system prompt
                    stream=True,
                    model_parameters=query.model_parameters,
                    conversation_context=conversation_context,
                    metadata_filters=metadata_filters,
                    user_id=user_id,
                    use_rag=query.use_rag
                )
                
                # Get sources (with safety check)
                sources = langgraph_response.get("sources")
                if sources is None:
                    logger.warning("No sources returned from LangGraph RAG Agent")
                    sources = []
                
                # Stream the response
                async for token in langgraph_response["stream"]:
                    full_response += token
                    yield token
                
                # Add assistant message to conversation
                assistant_message = await conversation_repository.add_message(
                    conversation_id=UUID(conversation_id),
                    content=full_response,
                    role="assistant"
                )
                
                # Add citations if any
                if sources:
                    for source in sources:
                        await conversation_repository.add_citation(
                            message_id=assistant_message.id,
                            document_id=UUID(source.document_id) if hasattr(source, "document_id") else None,
                            chunk_id=UUID(source.chunk_id) if hasattr(source, "chunk_id") else None,
                            relevance_score=source.relevance_score if hasattr(source, "relevance_score") else None,
                            excerpt=source.excerpt if hasattr(source, "excerpt") else ""
                        )
            
            return EventSourceResponse(event_generator())
        else:
            # For non-streaming, return the complete response
            logger.info(f"Generating response for conversation {conversation_id} using LangGraph RAG Agent")
            
            # Get LangGraph RAG response
            langgraph_response = await langgraph_rag_agent.query(
                query=query.message,
                model=model,
                system_prompt=None,  # Use default system prompt
                stream=False,
                model_parameters=query.model_parameters,
                conversation_context=conversation_context,
                metadata_filters=metadata_filters,
                user_id=user_id,
                use_rag=query.use_rag
            )
            
            # Get response and sources
            response_text = langgraph_response.get("answer", "")
            sources = langgraph_response.get("sources")
            if sources is None:
                logger.warning("No sources returned from LangGraph RAG Agent")
                sources = []
            
            # Add assistant message to conversation
            assistant_message = await conversation_repository.add_message(
                conversation_id=UUID(conversation_id),
                content=response_text,
                role="assistant"
            )
            
            # Add citations if any
            if sources:
                for source in sources:
                    await conversation_repository.add_citation(
                        message_id=assistant_message.id,
                        document_id=UUID(source.document_id) if hasattr(source, "document_id") else None,
                        chunk_id=UUID(source.chunk_id) if hasattr(source, "chunk_id") else None,
                        relevance_score=source.relevance_score if hasattr(source, "relevance_score") else None,
                        excerpt=source.excerpt if hasattr(source, "excerpt") else ""
                    )
            
            # Return response
            return ChatResponse(
                message=response_text,
                conversation_id=conversation_id,
                citations=sources
            )
    except Exception as e:
        logger.error(f"Error generating chat response with LangGraph RAG Agent: {str(e)}")
        
        # Create a user-friendly error message
        error_message = "Sorry, there was an error processing your request with the LangGraph RAG Agent."
        
        # Return a 200 response with the error message instead of raising an exception
        return ChatResponse(
            message=error_message,
            conversation_id=conversation_id if "conversation_id" in locals() else None,
            citations=None
        )

@router.post("/enhanced_langgraph_query", response_model=ChatResponse)
async def enhanced_langgraph_query_chat(
    query: ChatQuery,
    request: Request,
    db: AsyncSession = Depends(get_db),
    conversation_repository: ConversationRepository = Depends(get_conversation_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Send a chat query to the Enhanced LangGraph RAG Agent and get a response
    
    This endpoint uses the enhanced LangGraph RAG agent that integrates:
    - QueryPlanner for creating execution plans
    - PlanExecutor for executing multi-step plans
    - Retrieval Judge for query refinement and context optimization
    """
    if not USE_LANGGRAPH_RAG or not USE_ENHANCED_LANGGRAPH_RAG or not enhanced_langgraph_rag_agent:
        raise HTTPException(status_code=400, detail="Enhanced LangGraph RAG Agent is not enabled")
    
    try:
        # Get or create conversation
        conversation_id = query.conversation_id
        
        if conversation_id:
            # Try to get existing conversation
            try:
                conversation_uuid = UUID(conversation_id)
                conversation = await conversation_repository.get_by_id(conversation_uuid)
                
                # The repository will handle permission checks based on user_id
                # If the conversation doesn't belong to the user, it will return None
                if not conversation:
                    # Create new conversation if not found or not authorized
                    # The repository will use its user_id context
                    conversation = await conversation_repository.create_conversation()
                    conversation_id = str(conversation.id)
            except ValueError:
                # Invalid UUID format, create new conversation
                conversation = await conversation_repository.create_conversation()
                conversation_id = str(conversation.id)
        else:
            # Create new conversation
            # The repository will use its user_id context
            conversation = await conversation_repository.create_conversation()
            conversation_id = str(conversation.id)
        
        # Add user message to conversation
        user_message = await conversation_repository.add_message(
            conversation_id=UUID(conversation_id),
            content=query.message,
            role="user"
        )
        
        # Get model name
        model = query.model or DEFAULT_MODEL
        
        # Format conversation history
        conversation_context = None
        messages = await conversation_repository.get_conversation_messages(
            conversation_id=UUID(conversation_id),
            limit=6  # Get 6 messages to exclude the current one
        )
        
        if len(messages) > 1:  # Only include history if there's more than just the current message
            # Get the last few messages (up to 5) to keep context manageable, but exclude the most recent user message
            recent_history = messages[:-1]
            if len(recent_history) > 5:
                recent_history = recent_history[-5:]
            
            # Format the conversation history
            history_pieces = []
            for msg in recent_history:
                role_prefix = "User" if msg.role == "user" else "Assistant"
                history_pieces.append(f"{role_prefix}: {msg.content}")
            
            conversation_context = "\n".join(history_pieces)
            logger.info(f"Including conversation history with {len(recent_history)} messages")
        else:
            logger.info("No previous conversation history to include")
        
        # Extract metadata filters if provided
        metadata_filters = query.metadata_filters if hasattr(query, 'metadata_filters') else None
        
        # Query Enhanced LangGraph RAG Agent
        if query.stream:
            # For streaming, return an EventSourceResponse
            logger.info(f"Streaming response for conversation {conversation_id} using Enhanced LangGraph RAG Agent")
            
            async def event_generator():
                full_response = ""
                
                # First, send the conversation ID as a separate event with a specific event type
                yield {"event": "conversation_id", "data": conversation_id}
                
                # Get Enhanced LangGraph RAG response
                enhanced_response = await enhanced_langgraph_rag_agent.query(
                    query=query.message,
                    model=model,
                    system_prompt=None,  # Use default system prompt
                    stream=True,
                    model_parameters=query.model_parameters,
                    conversation_context=conversation_context,
                    metadata_filters=metadata_filters,
                    user_id=user_id,
                    use_rag=query.use_rag
                )
                
                # Get sources (with safety check)
                sources = enhanced_response.get("sources")
                if sources is None:
                    logger.warning("No sources returned from Enhanced LangGraph RAG Agent")
                    sources = []
                
                # Stream the response
                async for token in enhanced_response["stream"]:
                    full_response += token
                    yield token
                
                # Add assistant message to conversation
                assistant_message = await conversation_repository.add_message(
                    conversation_id=UUID(conversation_id),
                    content=full_response,
                    role="assistant"
                )
                
                # Add citations if any
                if sources:
                    for source in sources:
                        await conversation_repository.add_citation(
                            message_id=assistant_message.id,
                            document_id=UUID(source.document_id) if hasattr(source, "document_id") else None,
                            chunk_id=UUID(source.chunk_id) if hasattr(source, "chunk_id") else None,
                            relevance_score=source.relevance_score if hasattr(source, "relevance_score") else None,
                            excerpt=source.excerpt if hasattr(source, "excerpt") else ""
                        )
            
            return EventSourceResponse(event_generator())
        else:
            # For non-streaming, return the complete response
            logger.info(f"Generating response for conversation {conversation_id} using Enhanced LangGraph RAG Agent")
            
            # Get Enhanced LangGraph RAG response
            enhanced_response = await enhanced_langgraph_rag_agent.query(
                query=query.message,
                model=model,
                system_prompt=None,  # Use default system prompt
                stream=False,
                model_parameters=query.model_parameters,
                conversation_context=conversation_context,
                metadata_filters=metadata_filters,
                user_id=user_id,
                use_rag=query.use_rag
            )
            
            # Get response and sources
            response_text = enhanced_response.get("answer", "")
            sources = enhanced_response.get("sources")
            execution_trace = enhanced_response.get("execution_trace")
            
            if sources is None:
                logger.warning("No sources returned from Enhanced LangGraph RAG Agent")
                sources = []
            
            # Add assistant message to conversation
            assistant_message = await conversation_repository.add_message(
                conversation_id=UUID(conversation_id),
                content=response_text,
                role="assistant"
            )
            
            # Add citations if any
            if sources:
                for source in sources:
                    await conversation_repository.add_citation(
                        message_id=assistant_message.id,
                        document_id=UUID(source.document_id) if hasattr(source, "document_id") else None,
                        chunk_id=UUID(source.chunk_id) if hasattr(source, "chunk_id") else None,
                        relevance_score=source.relevance_score if hasattr(source, "relevance_score") else None,
                        excerpt=source.excerpt if hasattr(source, "excerpt") else ""
                    )
            
            # Return response
            return ChatResponse(
                message=response_text,
                conversation_id=conversation_id,
                citations=sources,
                execution_trace=execution_trace
            )
    except Exception as e:
        logger.error(f"Error generating chat response with Enhanced LangGraph RAG Agent: {str(e)}")
        
        # Create a user-friendly error message
        error_message = "Sorry, there was an error processing your request with the Enhanced LangGraph RAG Agent."
        
        # Return a 200 response with the error message instead of raising an exception
        return ChatResponse(
            message=error_message,
            conversation_id=conversation_id if "conversation_id" in locals() else None,
            citations=None
        )

@router.get("/list")
async def list_conversations(
    request: Request,
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    db: AsyncSession = Depends(get_db),
    conversation_repository: ConversationRepository = Depends(get_conversation_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    List conversations for the current user with pagination
    """
    # The repository already has the user context from the dependency injection
    # Get conversations with pagination
    conversations = await conversation_repository.get_conversations(
        skip=skip,
        limit=limit
    )
    
    # Get total count
    total_count = await conversation_repository.count_conversations()
    
    return {
        "conversations": [
            {
                "id": str(conv.id),
                "user_id": conv.conv_metadata.get("user_id"),
                "created_at": conv.created_at,
                "updated_at": conv.updated_at,
                "message_count": conv.message_count,
                "metadata": conv.conv_metadata
            }
            for conv in conversations
        ],
        "pagination": {
            "skip": skip,
            "limit": limit,
            "total": total_count
        }
    }

================
File: app/api/document_sharing.py
================
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from uuid import UUID
from pydantic import BaseModel
from datetime import datetime

from app.db.dependencies import get_db
from app.db.models import DocumentPermission
from app.db.repositories.document_repository import DocumentRepository
from app.db.repositories.notification_repository import NotificationRepository
from app.db.repositories.user_repository import UserRepository
from app.models.user import User
from app.models.document import DocumentInfo
from app.core.security import get_current_user
from app.core.permissions import has_permission, PERMISSION_SHARE

router = APIRouter()


class ShareDocumentRequest(BaseModel):
    """Request model for sharing a document"""
    user_id: str
    permission_level: str  # 'read', 'write', 'admin'


class DocumentCollaborator(BaseModel):
    """Model for document collaborator information"""
    user_id: str
    username: str
    permission_level: str
    shared_at: str


@router.post("/documents/{document_id}/share", status_code=status.HTTP_201_CREATED)
async def share_document(
    document_id: str,
    share_request: ShareDocumentRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Share a document with another user
    
    Args:
        document_id: Document ID
        share_request: Share request with user_id and permission_level
        
    Returns:
        Success message
    """
    # Validate permission level
    valid_permissions = ["read", "write", "admin"]
    if share_request.permission_level not in valid_permissions:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid permission level. Must be one of: {', '.join(valid_permissions)}"
        )
    
    # Get document
    doc_repo = DocumentRepository(db)
    document = await doc_repo.get_document_by_id(document_id)
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document with ID {document_id} not found"
        )
    
    # Check if user has permission to share the document
    # User must be the owner or have admin permission on the document
    can_share = False
    
    # Check if user is the owner
    if str(document.user_id) == current_user.id:
        can_share = True
    else:
        # Check if user has admin permission on the document
        permission = await doc_repo.get_document_permission(document_id, current_user.id)
        if permission and permission.permission_level == "admin":
            can_share = True
        # Check if user has share permission through roles
        elif await has_permission(PERMISSION_SHARE)(current_user, db):
            can_share = True
    
    if not can_share:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to share this document"
        )
    
    # Share document
    try:
        await doc_repo.share_document(
            document_id=document_id,
            user_id=share_request.user_id,
            permission_level=share_request.permission_level
        )
        
        # Create notification for the user
        notification_repo = NotificationRepository(db)
        user_repo = UserRepository(db)
        
        # Get the target user's information
        target_user = await user_repo.get_by_id(share_request.user_id)
        if not target_user:
            # If user not found, still return success but don't create notification
            return {"message": "Document shared successfully, but user not found for notification"}
        
        # Create notification
        await notification_repo.create_document_shared_notification(
            user_id=share_request.user_id,
            document_id=document_id,
            document_name=document.filename,
            shared_by_username=current_user.username,
            permission_level=share_request.permission_level
        )
        
        return {"message": "Document shared successfully"}
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.delete("/documents/{document_id}/share/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
async def revoke_document_access(
    document_id: str,
    user_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Revoke a user's access to a document
    
    Args:
        document_id: Document ID
        user_id: User ID to revoke access from
        
    Returns:
        No content
    """
    # Get document
    doc_repo = DocumentRepository(db)
    document = await doc_repo.get_document_by_id(document_id)
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document with ID {document_id} not found"
        )
    
    # Check if user has permission to revoke access
    # User must be the owner or have admin permission on the document
    can_revoke = False
    
    # Check if user is the owner
    if str(document.user_id) == current_user.id:
        can_revoke = True
    else:
        # Check if user has admin permission on the document
        permission = await doc_repo.get_document_permission(document_id, current_user.id)
        if permission and permission.permission_level == "admin":
            can_revoke = True
        # Check if user has share permission through roles
        elif await has_permission(PERMISSION_SHARE)(current_user, db):
            can_revoke = True
    
    if not can_revoke:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to revoke access to this document"
        )
    
    # Don't allow revoking access from the owner
    if str(document.user_id) == user_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Cannot revoke access from the document owner"
        )
    
    # Revoke access
    success = await doc_repo.revoke_document_access(document_id, user_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"User with ID {user_id} does not have access to this document"
        )
    
    return None


@router.get("/documents/{document_id}/collaborators", response_model=List[DocumentCollaborator])
async def get_document_collaborators(
    document_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all collaborators for a document
    
    Args:
        document_id: Document ID
        
    Returns:
        List of collaborators
    """
    # Get document
    doc_repo = DocumentRepository(db)
    document = await doc_repo.get_document_by_id(document_id)
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document with ID {document_id} not found"
        )
    
    # Check if user has permission to view collaborators
    # User must have at least read access to the document
    has_access = False
    
    # Check if user is the owner
    if str(document.user_id) == current_user.id:
        has_access = True
    else:
        # Check if user has permission on the document
        permission = await doc_repo.get_document_permission(document_id, current_user.id)
        if permission:
            has_access = True
    
    if not has_access:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to view this document's collaborators"
        )
    
    # Get collaborators
    collaborators = await doc_repo.get_document_collaborators(document_id)
    
    return collaborators


@router.get("/documents/shared-with-me", response_model=List[DocumentInfo])
async def get_documents_shared_with_me(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all documents shared with the current user
    
    Returns:
        List of documents shared with the current user
    """
    doc_repo = DocumentRepository(db)
    documents = await doc_repo.get_documents_shared_with_user(current_user.id)
    
    return documents


@router.get("/documents/shared-by-me", response_model=List[DocumentInfo])
async def get_documents_shared_by_me(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all documents shared by the current user
    
    Returns:
        List of documents shared by the current user
    """
    doc_repo = DocumentRepository(db)
    documents = await doc_repo.get_documents_shared_by_user(current_user.id)
    
    return documents

================
File: app/api/documents.py
================
import logging
import os
from typing import List, Dict, Any, Optional, Set
from uuid import UUID
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks, Query, Depends
from sqlalchemy import text, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.document import (
    Document, DocumentInfo, DocumentProcessRequest,
    TagUpdateRequest, FolderUpdateRequest, DocumentFilterRequest
)
from app.models.user import User
from app.db.models import Document as DBDocument
from app.rag.document_processor import DocumentProcessor
from app.rag.vector_store import VectorStore
from app.utils.file_utils import validate_file, save_upload_file, delete_document_files
from app.core.config import UPLOAD_DIR, CHUNK_SIZE, CHUNK_OVERLAP
from app.db.dependencies import get_db, get_document_repository
from app.db.repositories.document_repository import DocumentRepository
from app.core.security import get_current_active_user

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.documents")

# Document processor
document_processor = DocumentProcessor()

# Vector store
vector_store = VectorStore()

@router.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    tags: str = Form(""),
    folder: str = Form("/"),
    current_user: User = Depends(get_current_active_user)
):
    """
    Upload a document
    """
    try:
        # Validate file
        if not validate_file(file):
            raise HTTPException(status_code=400, detail=f"File type not allowed: {file.filename}")
        
        # Parse tags
        tag_list = [tag.strip() for tag in tags.split(",")] if tags else []
        
        # Validate folder
        if not folder.startswith("/"):
            folder = "/" + folder
        
        # Create a simple document record with minimal information
        import uuid
        from datetime import datetime
        
        # Generate a document ID
        document_id = uuid.uuid4()
        
        # Save file to disk first
        file_path = await save_upload_file(file, str(document_id))
        
        # Create a simple document record in the database
        # We'll use raw SQL to avoid async/sync issues
        from sqlalchemy import text
        from app.db.session import AsyncSessionLocal
        
        db = AsyncSessionLocal()
        try:
            # Create document record
            query = text("""
                INSERT INTO documents (id, filename, folder, uploaded, processing_status, user_id)
                VALUES (:id, :filename, :folder, :uploaded, :status, :user_id)
            """)
            
            await db.execute(query, {
                "id": document_id,
                "filename": file.filename,
                "folder": folder,
                "uploaded": datetime.utcnow(),
                "status": "pending",
                "user_id": current_user.id
            })
            
            # Add tags if provided
            for tag_name in tag_list:
                # Check if tag exists
                tag_query = text("SELECT id FROM tags WHERE name = :name")
                tag_result = await db.execute(tag_query, {"name": tag_name})
                tag_row = tag_result.fetchone()
                
                if tag_row:
                    tag_id = tag_row[0]
                    # Update usage count
                    await db.execute(
                        text("UPDATE tags SET usage_count = usage_count + 1 WHERE id = :id"),
                        {"id": tag_id}
                    )
                else:
                    # Create new tag
                    tag_insert = text("""
                        INSERT INTO tags (name, created_at, usage_count)
                        VALUES (:name, :created_at, 1)
                        RETURNING id
                    """)
                    tag_result = await db.execute(
                        tag_insert,
                        {"name": tag_name, "created_at": datetime.utcnow()}
                    )
                    tag_id = tag_result.fetchone()[0]
                
                # Link tag to document
                await db.execute(
                    text("INSERT INTO document_tags (document_id, tag_id) VALUES (:doc_id, :tag_id)"),
                    {"doc_id": document_id, "tag_id": tag_id}
                )
            
            # Commit transaction
            await db.commit()
            
            return {
                "success": True,
                "message": f"Document {file.filename} uploaded successfully",
                "document_id": str(document_id)
            }
        except Exception as e:
            await db.rollback()
            raise e
        finally:
            await db.close()
    except Exception as e:
        logger.error(f"Error uploading document: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error uploading document: {str(e)}")
@router.get("/list", response_model=List[DocumentInfo])
async def list_documents(
    tags: Optional[List[str]] = Query(None),
    folder: Optional[str] = Query(None),
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    List all documents with optional filtering and pagination
    """
    try:
        # Use raw SQL query with text function
        sql_query = """
        SELECT id, filename, content, doc_metadata, folder, uploaded, processing_status,
               processing_strategy, file_size, file_type, last_accessed
        FROM documents
        """
        
        # Add folder and user_id filters
        where_clauses = ["user_id = :user_id"]
        params = {"user_id": current_user.id}
        if folder:
            where_clauses.append("folder = :folder")
            params["folder"] = folder
        
        # Add WHERE clause if needed
        if where_clauses:
            sql_query += " WHERE " + " AND ".join(where_clauses)
        
        # Add pagination
        sql_query += " LIMIT :limit OFFSET :skip"
        params["limit"] = limit
        params["skip"] = skip
        
        # Execute query
        result = await db.execute(text(sql_query), params)
        rows = result.fetchall()
        
        # Convert rows to DocumentInfo objects
        document_infos = []
        for row in rows:
            # Create DocumentInfo object
            doc_info = DocumentInfo(
                id=str(row.id),
                filename=row.filename,
                chunk_count=0,  # We don't have chunk information in this query
                metadata=row.doc_metadata or {},
                tags=[],  # We'll need to fetch tags separately if needed
                folder=row.folder,
                uploaded=row.uploaded
            )
            document_infos.append(doc_info)
        
        return document_infos
    except Exception as e:
        logger.error(f"Error listing documents: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error listing documents: {str(e)}")

@router.post("/filter", response_model=List[DocumentInfo])
async def filter_documents(
    filter_request: DocumentFilterRequest,
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Filter documents by tags, folder, and metadata with pagination
    """
    try:
        # Use raw SQL query with text function
        sql_query = """
        SELECT id, filename, content, doc_metadata, folder, uploaded, processing_status,
               processing_strategy, file_size, file_type, last_accessed
        FROM documents
        """
        
        # Add folder and user_id filters
        where_clauses = ["user_id = :user_id"]
        params = {"user_id": current_user.id}
        if filter_request.folder:
            where_clauses.append("folder = :folder")
            params["folder"] = filter_request.folder
        
        # Add search query filter if provided
        search_query = filter_request.query if hasattr(filter_request, 'query') else ""
        if search_query:
            where_clauses.append("(filename ILIKE :search_query OR content ILIKE :search_query)")
            params["search_query"] = f"%{search_query}%"
        
        # Add WHERE clause if needed
        if where_clauses:
            sql_query += " WHERE " + " AND ".join(where_clauses)
        
        # Add pagination
        sql_query += " LIMIT :limit OFFSET :skip"
        params["limit"] = limit
        params["skip"] = skip
        
        # Execute query
        result = await db.execute(text(sql_query), params)
        rows = result.fetchall()
        
        # Convert rows to DocumentInfo objects
        document_infos = []
        for row in rows:
            # Create DocumentInfo object
            doc_info = DocumentInfo(
                id=str(row.id),
                filename=row.filename,
                chunk_count=0,  # We don't have chunk information in this query
                metadata=row.doc_metadata or {},
                tags=[],  # We'll need to fetch tags separately if needed
                folder=row.folder,
                uploaded=row.uploaded
            )
            document_infos.append(doc_info)
        
        # Convert documents to DocumentInfo
        document_infos = [
            DocumentInfo(
                id=str(doc.id),
                filename=doc.filename,
                chunk_count=len(doc.chunks) if hasattr(doc, 'chunks') else 0,
                metadata=doc.metadata or {},
                tags=[tag.name for tag in doc.tags] if hasattr(doc, 'tags') else [],
                folder=doc.folder,
                uploaded=doc.uploaded
            )
            for doc in documents
        ]
        
        return document_infos
    except Exception as e:
        logger.error(f"Error filtering documents: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error filtering documents: {str(e)}")

@router.get("/tags")
async def get_all_tags(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Get all tags used by the current user
    """
    try:
        # Query tags used by the current user
        query = text("""
            SELECT DISTINCT t.name
            FROM tags t
            JOIN document_tags dt ON t.id = dt.tag_id
            JOIN documents d ON dt.document_id = d.id
            WHERE d.user_id = :user_id
            ORDER BY t.name
        """)
        result = await db.execute(query, {"user_id": current_user.id})
        tags = result.all()
        return {"tags": [tag[0] for tag in tags]}
    except Exception as e:
        logger.error(f"Error getting tags: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting tags: {str(e)}")

@router.get("/folders")
async def get_all_folders(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Get all folders used by the current user
    """
    try:
        # Query folders used by the current user
        query = text("""
            SELECT DISTINCT d.folder
            FROM documents d
            WHERE d.user_id = :user_id
            ORDER BY d.folder
        """)
        result = await db.execute(query, {"user_id": current_user.id})
        folders = result.all()
        return {"folders": [folder[0] for folder in folders]}
    except Exception as e:
        logger.error(f"Error getting folders: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting folders: {str(e)}")

@router.get("/{document_id}")
async def get_document(
    document_id: UUID,
    db: AsyncSession = Depends(get_db),
    document_repository: DocumentRepository = Depends(get_document_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Get a document by ID
    """
    document = await document_repository.get_document_with_chunks(document_id)
    if not document:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    # Check if the document belongs to the current user
    if str(document.user_id) != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized to access this document")
    
    # Update last accessed timestamp
    await document_repository.update_document(
        document_id=document_id,
        metadata={"last_accessed": "now"}  # This will be converted to a timestamp in the repository
    )
    
    return document

@router.put("/{document_id}/tags")
async def update_document_tags(
    document_id: UUID,
    tag_request: TagUpdateRequest,
    db: AsyncSession = Depends(get_db),
    document_repository: DocumentRepository = Depends(get_document_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Update document tags
    """
    document = await document_repository.get_by_id(document_id)
    if not document:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    # Check if the document belongs to the current user
    if str(document.user_id) != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized to update this document")
    
    try:
        # Update document tags in database
        updated_document = await document_repository.update_document_tags(document_id, tag_request.tags)
        
        # Update vector store metadata - convert tags list to string for ChromaDB
        await vector_store.update_document_metadata(
            str(document_id),
            {
                "tags": ",".join(tag_request.tags) if tag_request.tags else "",
                "tags_list": tag_request.tags  # Keep original list for internal use
            }
        )
        
        return {
            "success": True,
            "message": f"Tags updated for document {document_id}",
            "tags": tag_request.tags
        }
    except Exception as e:
        logger.error(f"Error updating document tags: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error updating document tags: {str(e)}")

@router.put("/{document_id}/folder")
async def update_document_folder(
    document_id: UUID,
    folder_request: FolderUpdateRequest,
    db: AsyncSession = Depends(get_db),
    document_repository: DocumentRepository = Depends(get_document_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Update document folder
    """
    document = await document_repository.get_by_id(document_id)
    if not document:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    # Check if the document belongs to the current user
    if str(document.user_id) != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized to update this document")
    
    try:
        # Validate folder
        folder = folder_request.folder
        if not folder.startswith("/"):
            folder = "/" + folder
        
        # Update document folder in database
        updated_document = await document_repository.update_document(
            document_id=document_id,
            folder=folder
        )
        
        # Update vector store metadata
        await vector_store.update_document_metadata(
            str(document_id),
            {"folder": folder}
        )
        
        return {
            "success": True,
            "message": f"Folder updated for document {document_id}",
            "folder": folder
        }
    except Exception as e:
        logger.error(f"Error updating document folder: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error updating document folder: {str(e)}")

@router.delete("/{document_id}")
async def delete_document(
    document_id: UUID,
    db: AsyncSession = Depends(get_db),
    document_repository: DocumentRepository = Depends(get_document_repository),
    current_user: User = Depends(get_current_active_user)
):
    """
    Delete a document
    """
    document = await document_repository.get_by_id(document_id)
    if not document:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    # Check if the document belongs to the current user
    if str(document.user_id) != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized to delete this document")
    
    try:
        # Delete from vector store
        await vector_store.delete_document(str(document_id))
        
        # Delete document files
        delete_document_files(str(document_id))
        
        # Delete from database
        await document_repository.delete(document_id)
        
        return {"success": True, "message": f"Document {document_id} deleted"}
    except Exception as e:
        logger.error(f"Error deleting document: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting document: {str(e)}")

async def process_document_background(
    document_ids: List[UUID],
    force_reprocess: bool = False,
    chunking_strategy: str = "recursive",
    chunk_size: Optional[int] = None,
    chunk_overlap: Optional[int] = None
):
    """
    Background task to process documents with configurable chunking strategy
    """
    # Create a new session for the background task
    from app.db.session import AsyncSessionLocal
    db = AsyncSessionLocal()
    
    try:
        # Create a document processor with the specified parameters
        processor = DocumentProcessor(
            chunk_size=chunk_size or CHUNK_SIZE,
            chunk_overlap=chunk_overlap or CHUNK_OVERLAP,
            chunking_strategy=chunking_strategy
        )
        
        for document_id in document_ids:
            try:
                # Get document using raw SQL
                from sqlalchemy import text
                query = text("""
                    SELECT id, filename, content, doc_metadata, folder, uploaded, processing_status
                    FROM documents WHERE id = :id
                """)
                result = await db.execute(query, {"id": document_id})
                doc_row = result.fetchone()
                
                if not doc_row:
                    logger.warning(f"Document {document_id} not found, skipping processing")
                    continue
                
                # Update processing status
                update_query = text("""
                    UPDATE documents
                    SET processing_status = :status, processing_strategy = :strategy
                    WHERE id = :id
                """)
                await db.execute(
                    update_query,
                    {
                        "id": document_id,
                        "status": "processing",
                        "strategy": chunking_strategy
                    }
                )
                await db.commit()
                
                # Create a document object for processing
                from app.models.document import Document
                document = Document(
                    id=str(doc_row.id),
                    filename=doc_row.filename,
                    content=doc_row.content or "",
                    metadata=doc_row.doc_metadata or {},
                    folder=doc_row.folder,
                    uploaded=doc_row.uploaded
                )
                
                # Process document with the configured processor
                processed_document = await processor.process_document(document)
                
                # Add to vector store
                await vector_store.add_document(processed_document)
                
                # Update processing status to completed
                await db.execute(
                    update_query,
                    {
                        "id": document_id,
                        "status": "completed",
                        "strategy": chunking_strategy
                    }
                )
                await db.commit()
                
                logger.info(f"Document {document_id} processed successfully with {chunking_strategy} chunking strategy")
            except Exception as e:
                # Update processing status to failed
                await db.execute(
                    update_query,
                    {
                        "id": document_id,
                        "status": "failed",
                        "strategy": chunking_strategy
                    }
                )
                await db.commit()
                logger.error(f"Error processing document {document_id}: {str(e)}")
    except Exception as e:
        logger.error(f"Error in background processing task: {str(e)}")
    finally:
        await db.close()

@router.post("/process")
async def process_documents(
    request: DocumentProcessRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_active_user)
):
    """
    Process documents with configurable chunking strategy
    """
    try:
        # Convert string IDs to UUID
        document_ids = [UUID(doc_id) for doc_id in request.document_ids]
        
        # Create a new session for validation
        from app.db.session import AsyncSessionLocal
        db = AsyncSessionLocal()
        
        try:
            # Validate document IDs using raw SQL
            for doc_id in document_ids:
                # Check if document exists and belongs to the current user
                from sqlalchemy import text
                query = text("SELECT id FROM documents WHERE id = :id AND user_id = :user_id")
                result = await db.execute(query, {"id": doc_id, "user_id": current_user.id})
                if not result.fetchone():
                    raise HTTPException(
                        status_code=404,
                        detail=f"Document {doc_id} not found or not authorized to access"
                    )
            
            # Log chunking strategy
            logger.info(f"Processing documents with chunking strategy: {request.chunking_strategy}")
            
            # Add the background task
            background_tasks.add_task(
                process_document_background,
                document_ids,
                request.force_reprocess,
                request.chunking_strategy,
                request.chunk_size,
                request.chunk_overlap
            )
            
            return {
                "success": True,
                "message": f"Processing started for {len(request.document_ids)} documents"
            }
        finally:
            await db.close()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing documents: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing documents: {str(e)}")

================
File: app/api/health.py
================
import logging
from typing import Dict, Any
from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

from app.db.dependencies import get_db
from app.db.session import engine
from app.rag.vector_store import VectorStore
from app.core.config import SETTINGS

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.health")

@router.get("/")
async def health_check(db: AsyncSession = Depends(get_db)):
    """
    Health check endpoint
    
    Returns:
        Health status of the application and its dependencies
    """
    try:
        # Check database connection
        db_status = "healthy"
        db_error = None
        try:
            # Execute a simple query to check database connection
            result = await db.execute(text("SELECT 1"))
            await result.fetchall()
        except Exception as e:
            db_status = "unhealthy"
            db_error = str(e)
            logger.error(f"Database health check failed: {str(e)}")
        
        # Check vector store
        vector_store_status = "healthy"
        vector_store_error = None
        try:
            vector_store = VectorStore()
            stats = vector_store.get_stats()
        except Exception as e:
            vector_store_status = "unhealthy"
            vector_store_error = str(e)
            logger.error(f"Vector store health check failed: {str(e)}")
        
        # Check Mem0 if enabled
        mem0_status = "disabled"
        mem0_error = None
        if SETTINGS.use_mem0:
            try:
                from app.rag.mem0_client import get_mem0_client
                mem0_client = get_mem0_client()
                if mem0_client:
                    mem0_status = "healthy"
                else:
                    mem0_status = "unhealthy"
                    mem0_error = "Failed to initialize Mem0 client"
            except Exception as e:
                mem0_status = "unhealthy"
                mem0_error = str(e)
                logger.error(f"Mem0 health check failed: {str(e)}")
        
        # Check Ollama
        ollama_status = "healthy"
        ollama_error = None
        try:
            from app.rag.ollama_client import OllamaClient
            ollama_client = OllamaClient()
            # Just initialize the client, don't make an actual request
            # to avoid unnecessary load on the Ollama service
        except Exception as e:
            ollama_status = "unhealthy"
            ollama_error = str(e)
            logger.error(f"Ollama health check failed: {str(e)}")
        
        # Overall status is healthy only if all critical components are healthy
        overall_status = "healthy"
        if db_status != "healthy" or vector_store_status != "healthy":
            overall_status = "unhealthy"
        
        # Build response
        response = {
            "status": overall_status,
            "version": SETTINGS.version,
            "components": {
                "database": {
                    "status": db_status,
                    "error": db_error
                },
                "vector_store": {
                    "status": vector_store_status,
                    "error": vector_store_error
                },
                "mem0": {
                    "status": mem0_status,
                    "error": mem0_error
                },
                "ollama": {
                    "status": ollama_status,
                    "error": ollama_error
                }
            }
        }
        
        # Log health check result
        logger.info(f"Health check: {overall_status}")
        
        return response
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@router.get("/readiness")
async def readiness_check():
    """
    Readiness check endpoint
    
    This endpoint checks if the application is ready to accept traffic.
    It's a lightweight check that doesn't verify all dependencies.
    
    Returns:
        Readiness status
    """
    return {"status": "ready"}

@router.get("/liveness")
async def liveness_check():
    """
    Liveness check endpoint
    
    This endpoint checks if the application is alive.
    It's a very lightweight check that doesn't verify any dependencies.
    
    Returns:
        Liveness status
    """
    return {"status": "alive"}

================
File: app/api/notifications.py
================
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.dependencies import get_db
from app.db.repositories.notification_repository import NotificationRepository
from app.models.notification import Notification
from app.models.user import User
from app.core.security import get_current_user

router = APIRouter()


@router.get("/notifications", response_model=List[Notification])
async def get_notifications(
    skip: int = 0,
    limit: int = 20,
    unread_only: bool = False,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get notifications for the current user
    
    Args:
        skip: Number of notifications to skip
        limit: Maximum number of notifications to return
        unread_only: Whether to return only unread notifications
        
    Returns:
        List of notifications
    """
    notification_repo = NotificationRepository(db)
    notifications = await notification_repo.get_user_notifications(
        user_id=current_user.id,
        skip=skip,
        limit=limit,
        unread_only=unread_only
    )
    
    return notifications


@router.get("/notifications/count", response_model=int)
async def count_unread_notifications(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Count unread notifications for the current user
    
    Returns:
        Number of unread notifications
    """
    notification_repo = NotificationRepository(db)
    count = await notification_repo.count_unread_notifications(current_user.id)
    
    return count


@router.get("/notifications/{notification_id}", response_model=Notification)
async def get_notification(
    notification_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a notification by ID
    
    Args:
        notification_id: Notification ID
        
    Returns:
        Notification if found
    """
    notification_repo = NotificationRepository(db)
    notification = await notification_repo.get_by_id(notification_id)
    
    if not notification:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Notification with ID {notification_id} not found"
        )
    
    # Check if notification belongs to the current user
    if notification.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to access this notification"
        )
    
    return notification


@router.post("/notifications/{notification_id}/read", response_model=Notification)
async def mark_notification_as_read(
    notification_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Mark a notification as read
    
    Args:
        notification_id: Notification ID
        
    Returns:
        Updated notification
    """
    notification_repo = NotificationRepository(db)
    
    # Get notification
    notification = await notification_repo.get_by_id(notification_id)
    
    if not notification:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Notification with ID {notification_id} not found"
        )
    
    # Check if notification belongs to the current user
    if notification.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to access this notification"
        )
    
    # Mark as read
    success = await notification_repo.mark_as_read(notification_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to mark notification as read"
        )
    
    # Get updated notification
    updated_notification = await notification_repo.get_by_id(notification_id)
    return updated_notification


@router.post("/notifications/read-all", response_model=int)
async def mark_all_notifications_as_read(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Mark all notifications for the current user as read
    
    Returns:
        Number of notifications marked as read
    """
    notification_repo = NotificationRepository(db)
    count = await notification_repo.mark_all_as_read(current_user.id)
    
    return count


@router.delete("/notifications/{notification_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_notification(
    notification_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Delete a notification
    
    Args:
        notification_id: Notification ID
    """
    notification_repo = NotificationRepository(db)
    
    # Get notification
    notification = await notification_repo.get_by_id(notification_id)
    
    if not notification:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Notification with ID {notification_id} not found"
        )
    
    # Check if notification belongs to the current user
    if notification.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to delete this notification"
        )
    
    # Delete notification
    success = await notification_repo.delete_notification(notification_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete notification"
        )
    
    return None


@router.delete("/notifications", status_code=status.HTTP_204_NO_CONTENT)
async def delete_all_notifications(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Delete all notifications for the current user
    """
    notification_repo = NotificationRepository(db)
    await notification_repo.delete_all_notifications(current_user.id)
    
    return None

================
File: app/api/organizations.py
================
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.dependencies import get_db
from app.db.repositories.organization_repository import OrganizationRepository
from app.models.organization import Organization, OrganizationCreate, OrganizationUpdate, OrganizationMember
from app.models.user import User
from app.core.security import get_current_user

router = APIRouter()


@router.get("/organizations", response_model=List[Organization])
async def get_organizations(
    skip: int = 0,
    limit: int = 100,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all organizations the current user is a member of
    
    Args:
        skip: Number of organizations to skip
        limit: Maximum number of organizations to return
        
    Returns:
        List of organizations
    """
    org_repo = OrganizationRepository(db)
    organizations = await org_repo.get_user_organizations(current_user.id)
    
    # Apply pagination
    start = min(skip, len(organizations))
    end = min(start + limit, len(organizations))
    
    return organizations[start:end]


@router.post("/organizations", response_model=Organization, status_code=status.HTTP_201_CREATED)
async def create_organization(
    organization: OrganizationCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Create a new organization with the current user as owner
    
    Args:
        organization: Organization creation data
        
    Returns:
        Created organization
    """
    try:
        org_repo = OrganizationRepository(db)
        created_org = await org_repo.create_organization(organization, current_user.id)
        return created_org
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.get("/organizations/{organization_id}", response_model=Organization)
async def get_organization(
    organization_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get an organization by ID
    
    Args:
        organization_id: Organization ID
        
    Returns:
        Organization if found
    """
    org_repo = OrganizationRepository(db)
    
    # Check if user is a member of the organization
    is_member = await org_repo.user_is_member(organization_id, current_user.id)
    if not is_member and not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to access this organization"
        )
    
    organization = await org_repo.get_by_id(organization_id)
    if not organization:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Organization with ID {organization_id} not found"
        )
    
    return organization


@router.put("/organizations/{organization_id}", response_model=Organization)
async def update_organization(
    organization_id: str,
    organization_update: OrganizationUpdate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Update an organization
    
    Args:
        organization_id: Organization ID
        organization_update: Organization update data
        
    Returns:
        Updated organization
    """
    org_repo = OrganizationRepository(db)
    
    # Check if user is an admin or owner of the organization
    is_admin_or_owner = await org_repo.user_is_admin_or_owner(organization_id, current_user.id)
    if not is_admin_or_owner and not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to update this organization"
        )
    
    try:
        updated_org = await org_repo.update_organization(organization_id, organization_update)
        if not updated_org:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Organization with ID {organization_id} not found"
            )
        
        return updated_org
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.delete("/organizations/{organization_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_organization(
    organization_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Delete an organization
    
    Args:
        organization_id: Organization ID
    """
    org_repo = OrganizationRepository(db)
    
    # Check if user is an owner of the organization
    is_owner = await org_repo.user_is_owner(organization_id, current_user.id)
    if not is_owner and not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to delete this organization"
        )
    
    success = await org_repo.delete_organization(organization_id)
    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Organization with ID {organization_id} not found"
        )
    
    return None


@router.get("/organizations/{organization_id}/members", response_model=List[OrganizationMember])
async def get_organization_members(
    organization_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all members of an organization
    
    Args:
        organization_id: Organization ID
        
    Returns:
        List of organization members
    """
    org_repo = OrganizationRepository(db)
    
    # Check if user is a member of the organization
    is_member = await org_repo.user_is_member(organization_id, current_user.id)
    if not is_member and not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to access this organization"
        )
    
    members = await org_repo.get_organization_members(organization_id)
    return members


@router.post("/organizations/{organization_id}/members", response_model=OrganizationMember, status_code=status.HTTP_201_CREATED)
async def add_organization_member(
    organization_id: str,
    user_id: str,
    role: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Add a member to an organization
    
    Args:
        organization_id: Organization ID
        user_id: User ID
        role: Member role ('owner', 'admin', 'member')
        
    Returns:
        Organization member
    """
    org_repo = OrganizationRepository(db)
    
    # Check if user is an admin or owner of the organization
    is_admin_or_owner = await org_repo.user_is_admin_or_owner(organization_id, current_user.id)
    if not is_admin_or_owner and not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to add members to this organization"
        )
    
    # Only owners can add other owners
    if role == 'owner':
        is_owner = await org_repo.user_is_owner(organization_id, current_user.id)
        if not is_owner and not current_user.is_admin:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Only owners can add other owners to the organization"
            )
    
    try:
        member = await org_repo.add_member(organization_id, user_id, role)
        return member
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.delete("/organizations/{organization_id}/members/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
async def remove_organization_member(
    organization_id: str,
    user_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Remove a member from an organization
    
    Args:
        organization_id: Organization ID
        user_id: User ID
    """
    org_repo = OrganizationRepository(db)
    
    # Check if user is an admin or owner of the organization
    is_admin_or_owner = await org_repo.user_is_admin_or_owner(organization_id, current_user.id)
    
    # Users can remove themselves
    is_self = current_user.id == user_id
    
    if not is_admin_or_owner and not is_self and not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to remove members from this organization"
        )
    
    # Get the role of the user to be removed
    user_role = await org_repo.get_user_role_in_organization(organization_id, user_id)
    
    # Only owners can remove other owners
    if user_role == 'owner':
        is_owner = await org_repo.user_is_owner(organization_id, current_user.id)
        if not is_owner and not current_user.is_admin:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Only owners can remove other owners from the organization"
            )
    
    try:
        success = await org_repo.remove_member(organization_id, user_id)
        if not success:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"User with ID {user_id} is not a member of this organization"
            )
        
        return None
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.get("/organizations/{organization_id}/role", response_model=str)
async def get_user_role_in_organization(
    organization_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get the current user's role in an organization
    
    Args:
        organization_id: Organization ID
        
    Returns:
        User's role in the organization
    """
    org_repo = OrganizationRepository(db)
    role = await org_repo.get_user_role_in_organization(organization_id, current_user.id)
    
    if not role:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="You are not a member of this organization"
        )
    
    return role

================
File: app/api/password_reset.py
================
from datetime import datetime, timedelta
import uuid
from typing import Optional
from fastapi import APIRouter, HTTPException, Depends, status, Request
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
from pydantic import BaseModel, EmailStr

from app.db.dependencies import get_db, get_user_repository
from app.db.repositories.user_repository import UserRepository
from app.core.config import SETTINGS
from app.core.email import send_email

# Create router
router = APIRouter()

# Models
class PasswordResetRequest(BaseModel):
    """
    Password reset request model
    """
    email: EmailStr

class PasswordResetConfirm(BaseModel):
    """
    Password reset confirmation model
    """
    token: str
    password: str
    confirm_password: str

class PasswordResetToken(BaseModel):
    """
    Password reset token model
    """
    id: str
    user_id: str
    token: str
    created_at: datetime
    expires_at: datetime
    is_used: bool = False

# Endpoints
@router.post("/request-reset", status_code=status.HTTP_200_OK)
async def request_password_reset(
    request_data: PasswordResetRequest,
    request: Request,
    db: AsyncSession = Depends(get_db)
):
    """
    Request a password reset
    
    Args:
        request_data: Password reset request data
        request: Request object
        db: Database session
        
    Returns:
        Success message
    """
    # Get user by email
    user_repository = await get_user_repository(db)
    user = await user_repository.get_by_email(request_data.email)
    
    # Always return success to prevent email enumeration
    if not user:
        return {"message": "If your email is registered, you will receive a password reset link"}
    
    # Generate token
    token = str(uuid.uuid4())
    expires_at = datetime.utcnow() + timedelta(hours=24)
    
    # Store token in database
    try:
        # Use raw SQL to insert the token with text() function
        query = text("""
        INSERT INTO password_reset_tokens (id, user_id, token, created_at, expires_at, is_used)
        VALUES (:id, :user_id, :token, :created_at, :expires_at, :is_used)
        """)
        values = {
            "id": str(uuid.uuid4()),
            "user_id": str(user.id),
            "token": token,
            "created_at": datetime.utcnow(),
            "expires_at": expires_at,
            "is_used": False
        }
        
        await db.execute(query, values)
        await db.commit()
    except Exception as e:
        print(f"Error storing token: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error processing request"
        )
    
    # Generate reset link
    base_url = str(request.base_url).rstrip('/')
    reset_link = f"{base_url}/reset-password?token={token}"
    
    # Send email
    try:
        # In a real application, this would send an actual email
        # For now, we'll just print the reset link to the console
        print(f"Password reset link for {user.email}: {reset_link}")
        
        # Uncomment this to send a real email
        # subject = "Password Reset Request"
        # body = f"""
        # Hello {user.username},
        # 
        # You have requested to reset your password. Please click the link below to reset your password:
        # 
        # {reset_link}
        # 
        # This link will expire in 24 hours.
        # 
        # If you did not request this, please ignore this email.
        # 
        # Regards,
        # The Metis RAG Team
        # """
        # await send_email(user.email, subject, body)
    except Exception as e:
        print(f"Error sending email: {str(e)}")
    
    return {"message": "If your email is registered, you will receive a password reset link"}

@router.post("/reset-password", status_code=status.HTTP_200_OK)
async def reset_password(
    reset_data: PasswordResetConfirm,
    db: AsyncSession = Depends(get_db)
):
    """
    Reset password using a token
    
    Args:
        reset_data: Password reset confirmation data
        db: Database session
        
    Returns:
        Success message
    """
    # Validate passwords match
    if reset_data.password != reset_data.confirm_password:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Passwords do not match"
        )
    
    # Validate password length
    if len(reset_data.password) < 8:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Password must be at least 8 characters long"
        )
    
    # Get token from database
    try:
        query = text("""
        SELECT id, user_id, token, created_at, expires_at, is_used
        FROM password_reset_tokens
        WHERE token = :token
        """)
        result = await db.execute(query, {"token": reset_data.token})
        token_row = result.fetchone()
        
        if not token_row:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid or expired token"
            )
        
        # Convert to model
        token_data = PasswordResetToken(
            id=token_row[0],
            user_id=token_row[1],
            token=token_row[2],
            created_at=token_row[3],
            expires_at=token_row[4],
            is_used=token_row[5]
        )
    except Exception as e:
        print(f"Error retrieving token: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error processing request"
        )
    
    # Check if token is valid
    if token_data.is_used:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Token has already been used"
        )
    
    # Check if token is expired
    if token_data.expires_at < datetime.utcnow():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Token has expired"
        )
    
    # Update user password
    user_repository = await get_user_repository(db)
    user = await user_repository.get_by_id(token_data.user_id)
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="User not found"
        )
    
    # Update password
    await user_repository.update_user(user.id, {"password": reset_data.password})
    
    # Mark token as used
    try:
        query = text("""
        UPDATE password_reset_tokens
        SET is_used = true
        WHERE id = :id
        """)
        await db.execute(query, {"id": token_data.id})
        await db.commit()
    except Exception as e:
        print(f"Error updating token: {str(e)}")
    
    return {"message": "Password reset successfully"}

================
File: app/api/processing.py
================
"""
API endpoints for document processing jobs
"""
import logging
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field

from app.db.dependencies import get_db, get_document_repository, get_document_processor
from app.db.repositories.document_repository import DocumentRepository
from app.rag.document_processor import DocumentProcessor
from app.rag.processing_job import DocumentProcessingService, ProcessingJob

# Initialize router
router = APIRouter()

# Initialize logger
logger = logging.getLogger("app.api.processing")

# Initialize document processor and processing service
document_processor = get_document_processor()
processing_service = DocumentProcessingService(document_processor=document_processor)

# Document repository will be set in the startup event

# Pydantic models for request/response
class ProcessingJobRequest(BaseModel):
    """Request model for creating a processing job"""
    document_ids: List[str] = Field(..., description="List of document IDs to process")
    strategy: Optional[str] = Field(None, description="Processing strategy")

class ProcessingJobResponse(BaseModel):
    """Response model for processing job"""
    id: str = Field(..., description="Job ID")
    document_ids: List[str] = Field(..., description="List of document IDs")
    strategy: Optional[str] = Field(None, description="Processing strategy")
    status: str = Field(..., description="Job status")
    created_at: str = Field(..., description="Creation timestamp")
    completed_at: Optional[str] = Field(None, description="Completion timestamp")
    document_count: int = Field(..., description="Total number of documents")
    processed_count: int = Field(..., description="Number of processed documents")
    progress_percentage: float = Field(..., description="Progress percentage")
    error_message: Optional[str] = Field(None, description="Error message if failed")

class ProcessingJobListResponse(BaseModel):
    """Response model for listing processing jobs"""
    jobs: List[ProcessingJobResponse] = Field(..., description="List of jobs")
    total: int = Field(..., description="Total number of jobs")

# Startup event
@router.on_event("startup")
async def startup_event():
    """Start the processing service on startup"""
    # Get document repository
    from app.db.dependencies import get_document_repository
    from app.db.session import AsyncSessionLocal
    
    # Create a session
    db = AsyncSessionLocal()
    try:
        # Get document repository
        document_repo = await get_document_repository(db)
        
        # Set document repository for processing service
        processing_service.set_document_repository(document_repo)
        logger.info("Document repository set for processing service")
        
        # Start processing service
        await processing_service.start()
        logger.info("Processing service started")
    except Exception as e:
        logger.error(f"Error starting processing service: {str(e)}")
    finally:
        await db.close()

# Shutdown event
@router.on_event("shutdown")
async def shutdown_event():
    """Stop the processing service on shutdown"""
    await processing_service.stop()
    logger.info("Processing service stopped")

@router.post("/jobs", response_model=ProcessingJobResponse, tags=["processing"])
async def create_processing_job(
    job_request: ProcessingJobRequest,
    document_repo: DocumentRepository = Depends(get_document_repository)
) -> Dict[str, Any]:
    """
    Create a new document processing job
    
    Args:
        job_request: Job request
        document_repo: Document repository
        
    Returns:
        Created job
    """
    try:
        # Validate document IDs
        for document_id in job_request.document_ids:
            document = document_repo.get_document_with_chunks(document_id)
            if not document:
                raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
            
            # Update document status to pending processing
            document_repo.update_processing_status(document_id, "pending", job_request.strategy)
        
        # Create job
        job = await processing_service.create_job(
            document_ids=job_request.document_ids,
            strategy=job_request.strategy
        )
        
        logger.info(f"Created processing job {job.id} for {len(job_request.document_ids)} documents")
        
        # Return job
        return job.to_dict()
    except Exception as e:
        logger.error(f"Error creating processing job: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/jobs", response_model=ProcessingJobListResponse, tags=["processing"])
async def list_processing_jobs(
    status: Optional[str] = None
) -> Dict[str, Any]:
    """
    List processing jobs
    
    Args:
        status: Filter by status
        
    Returns:
        List of jobs
    """
    try:
        jobs = await processing_service.list_jobs(status=status)
        return {
            "jobs": [job.to_dict() for job in jobs],
            "total": len(jobs)
        }
    except Exception as e:
        logger.error(f"Error listing processing jobs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/jobs/{job_id}", response_model=ProcessingJobResponse, tags=["processing"])
async def get_processing_job(
    job_id: str
) -> Dict[str, Any]:
    """
    Get a processing job by ID
    
    Args:
        job_id: Job ID
        
    Returns:
        Job
    """
    try:
        job = await processing_service.get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
        return job.to_dict()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting processing job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/jobs/{job_id}/cancel", response_model=ProcessingJobResponse, tags=["processing"])
async def cancel_processing_job(
    job_id: str
) -> Dict[str, Any]:
    """
    Cancel a processing job
    
    Args:
        job_id: Job ID
        
    Returns:
        Cancelled job
    """
    try:
        success = await processing_service.cancel_job(job_id)
        if not success:
            raise HTTPException(status_code=400, detail=f"Cannot cancel job {job_id}")
        
        job = await processing_service.get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
        
        logger.info(f"Cancelled processing job {job_id}")
        
        return job.to_dict()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling processing job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

================
File: app/api/query_analysis.py
================
"""
API endpoints for query analysis
"""
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel

from app.rag.query_analyzer import QueryAnalyzer
from app.rag.tools import ToolRegistry
from app.rag.process_logger import ProcessLogger
from app.db.dependencies import get_db

router = APIRouter(
    prefix="/api/query",
    tags=["query"],
    responses={404: {"description": "Not found"}},
)

class QueryAnalysisRequest(BaseModel):
    """Query analysis request model"""
    query: str
    conversation_id: Optional[str] = None
    context: Optional[Dict[str, Any]] = None

class QueryAnalysisResponse(BaseModel):
    """Query analysis response model"""
    query_id: str
    complexity: str
    requires_tools: List[str]
    sub_queries: List[str]
    reasoning: str

class ToolExecutionRequest(BaseModel):
    """Tool execution request model"""
    query_id: str
    tool_name: str
    tool_input: Dict[str, Any]

class ToolExecutionResponse(BaseModel):
    """Tool execution response model"""
    query_id: str
    tool_name: str
    tool_output: Dict[str, Any]
    execution_time: float

@router.post("/analyze", response_model=QueryAnalysisResponse)
async def analyze_query(
    request: QueryAnalysisRequest,
    query_analyzer: QueryAnalyzer = Depends(),
    process_logger: ProcessLogger = Depends(),
    db_session = Depends(get_db)
):
    """
    Analyze a query to determine its complexity and requirements
    
    Args:
        request: Query analysis request
        
    Returns:
        Query analysis response
    """
    # Generate a unique query ID
    import uuid
    query_id = str(uuid.uuid4())
    
    # Start process logging
    process_logger.start_process(query_id=query_id, query=request.query)
    
    # Analyze the query
    analysis = await query_analyzer.analyze(request.query)
    
    # Log the analysis step
    process_logger.log_step(
        query_id=query_id,
        step_name="query_analysis",
        step_data=analysis
    )
    
    # Return the analysis
    return QueryAnalysisResponse(
        query_id=query_id,
        complexity=analysis.get("complexity", "simple"),
        requires_tools=analysis.get("requires_tools", []),
        sub_queries=analysis.get("sub_queries", []),
        reasoning=analysis.get("reasoning", "")
    )

@router.post("/execute-tool", response_model=ToolExecutionResponse)
async def execute_tool(
    request: ToolExecutionRequest,
    tool_registry: ToolRegistry = Depends(),
    process_logger: ProcessLogger = Depends(),
    db_session = Depends(get_db)
):
    """
    Execute a tool with the given input
    
    Args:
        request: Tool execution request
        
    Returns:
        Tool execution response
    """
    # Get the tool
    tool = tool_registry.get_tool(request.tool_name)
    if not tool:
        raise HTTPException(status_code=404, detail=f"Tool not found: {request.tool_name}")
    
    # Execute the tool
    import time
    start_time = time.time()
    tool_output = await tool.execute(request.tool_input)
    execution_time = time.time() - start_time
    
    # Log the tool execution
    process_logger.log_tool_usage(
        query_id=request.query_id,
        tool_name=request.tool_name,
        input_data=request.tool_input,
        output_data=tool_output
    )
    
    # Return the tool output
    return ToolExecutionResponse(
        query_id=request.query_id,
        tool_name=request.tool_name,
        tool_output=tool_output,
        execution_time=execution_time
    )

@router.get("/available-tools", response_model=List[Dict[str, Any]])
async def list_available_tools(
    tool_registry: ToolRegistry = Depends()
):
    """
    List all available tools
    
    Returns:
        List of tool information dictionaries
    """
    return tool_registry.list_tools()

@router.get("/tool-examples/{tool_name}", response_model=List[Dict[str, Any]])
async def get_tool_examples(
    tool_name: str,
    tool_registry: ToolRegistry = Depends()
):
    """
    Get examples for a specific tool
    
    Args:
        tool_name: Tool name
        
    Returns:
        List of example input/output pairs
    """
    examples = tool_registry.get_tool_examples(tool_name)
    if not examples:
        raise HTTPException(status_code=404, detail=f"No examples found for tool: {tool_name}")
    
    return examples

@router.get("/logs/{query_id}", response_model=Dict[str, Any])
async def get_query_logs(
    query_id: str,
    process_logger: ProcessLogger = Depends()
):
    """
    Get the process log for a query
    
    Args:
        query_id: Query ID
        
    Returns:
        Process log
    """
    log = process_logger.get_process_log(query_id)
    if not log:
        raise HTTPException(status_code=404, detail=f"No log found for query: {query_id}")
    
    return log

================
File: app/api/roles.py
================
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.dependencies import get_db
from app.db.repositories.role_repository import RoleRepository
from app.models.role import Role, RoleCreate, RoleUpdate, UserRole, UserRoleCreate
from app.models.user import User
from app.core.security import get_current_user
from app.core.permissions import has_permission, PERMISSION_MANAGE_ROLES

router = APIRouter()


@router.get("/roles", response_model=List[Role])
async def get_roles(
    skip: int = 0,
    limit: int = 100,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all roles
    """
    # Only admins or users with manage_roles permission can list all roles
    if not current_user.is_admin and not await RoleRepository(db).user_has_permission(current_user.id, PERMISSION_MANAGE_ROLES):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions to list roles"
        )
    
    roles = await RoleRepository(db).get_all_roles(skip=skip, limit=limit)
    return roles


@router.post("/roles", response_model=Role, status_code=status.HTTP_201_CREATED)
async def create_role(
    role: RoleCreate,
    current_user: User = Depends(has_permission(PERMISSION_MANAGE_ROLES)),
    db: AsyncSession = Depends(get_db)
):
    """
    Create a new role
    """
    try:
        created_role = await RoleRepository(db).create_role(role)
        return created_role
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.get("/roles/{role_id}", response_model=Role)
async def get_role(
    role_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a role by ID
    """
    # Only admins or users with manage_roles permission can get role details
    if not current_user.is_admin and not await RoleRepository(db).user_has_permission(current_user.id, PERMISSION_MANAGE_ROLES):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions to view role details"
        )
    
    role = await RoleRepository(db).get_by_id(role_id)
    if not role:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Role with ID {role_id} not found"
        )
    
    return role


@router.put("/roles/{role_id}", response_model=Role)
async def update_role(
    role_id: str,
    role_update: RoleUpdate,
    current_user: User = Depends(has_permission(PERMISSION_MANAGE_ROLES)),
    db: AsyncSession = Depends(get_db)
):
    """
    Update a role
    """
    try:
        updated_role = await RoleRepository(db).update_role(role_id, role_update)
        if not updated_role:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Role with ID {role_id} not found"
            )
        
        return updated_role
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.delete("/roles/{role_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_role(
    role_id: str,
    current_user: User = Depends(has_permission(PERMISSION_MANAGE_ROLES)),
    db: AsyncSession = Depends(get_db)
):
    """
    Delete a role
    """
    # Check if role exists
    role = await RoleRepository(db).get_by_id(role_id)
    if not role:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Role with ID {role_id} not found"
        )
    
    # Don't allow deleting the admin role
    if role.name == "admin":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Cannot delete the admin role"
        )
    
    # Delete the role
    deleted = await RoleRepository(db).delete_role(role_id)
    if not deleted:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete role"
        )
    
    return None


@router.get("/users/{user_id}/roles", response_model=List[Role])
async def get_user_roles(
    user_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all roles assigned to a user
    """
    # Users can view their own roles, admins can view anyone's roles
    if not current_user.is_admin and current_user.id != user_id and not await RoleRepository(db).user_has_permission(current_user.id, PERMISSION_MANAGE_ROLES):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions to view user roles"
        )
    
    roles = await RoleRepository(db).get_user_roles(user_id)
    return roles


@router.post("/users/{user_id}/roles", response_model=UserRole, status_code=status.HTTP_201_CREATED)
async def assign_role_to_user(
    user_id: str,
    role_id: str,
    current_user: User = Depends(has_permission(PERMISSION_MANAGE_ROLES)),
    db: AsyncSession = Depends(get_db)
):
    """
    Assign a role to a user
    """
    try:
        user_role = await RoleRepository(db).assign_role_to_user(user_id, role_id)
        return user_role
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )


@router.delete("/users/{user_id}/roles/{role_id}", status_code=status.HTTP_204_NO_CONTENT)
async def remove_role_from_user(
    user_id: str,
    role_id: str,
    current_user: User = Depends(has_permission(PERMISSION_MANAGE_ROLES)),
    db: AsyncSession = Depends(get_db)
):
    """
    Remove a role from a user
    """
    # Check if user has the role
    role_repo = RoleRepository(db)
    roles = await role_repo.get_user_roles(user_id)
    role_ids = [role.id for role in roles]
    
    if role_id not in role_ids:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"User does not have role with ID {role_id}"
        )
    
    # Don't allow removing the admin role from the last admin user
    role = await role_repo.get_by_id(role_id)
    if role and role.name == "admin":
        # Check if this is the last admin user
        admin_users = await role_repo.get_role_users(role_id)
        if len(admin_users) == 1 and admin_users[0] == user_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Cannot remove the admin role from the last admin user"
            )
    
    # Remove the role
    removed = await role_repo.remove_role_from_user(user_id, role_id)
    if not removed:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to remove role from user"
        )
    
    return None


@router.get("/roles/{role_id}/users", response_model=List[str])
async def get_role_users(
    role_id: str,
    current_user: User = Depends(has_permission(PERMISSION_MANAGE_ROLES)),
    db: AsyncSession = Depends(get_db)
):
    """
    Get all users assigned to a role
    """
    # Check if role exists
    role = await RoleRepository(db).get_by_id(role_id)
    if not role:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Role with ID {role_id} not found"
        )
    
    # Get users with this role
    users = await RoleRepository(db).get_role_users(role_id)
    return users


@router.get("/check-permission/{permission}", response_model=bool)
async def check_user_permission(
    permission: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Check if the current user has a specific permission
    """
    # Admin users have all permissions
    if current_user.is_admin:
        return True
    
    # Check if user has the permission
    has_perm = await RoleRepository(db).user_has_permission(current_user.id, permission)
    return has_perm

================
File: app/api/schema.py
================
"""
API endpoints for database schema introspection
"""
from typing import Dict, List, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.dependencies import get_db
from app.db.connection_manager import connection_manager
from app.db.schema_inspector import schema_inspector
from app.core.security import get_current_user
from app.models.user import User

router = APIRouter(
    prefix="/api/schema",
    tags=["schema"],
    responses={404: {"description": "Not found"}},
)

@router.get("/connections")
async def list_connections(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    List all PostgreSQL connections available to the user
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view database connections")
    
    # Get all connections
    connections = []
    for conn_id, conn_string in connection_manager._connection_map.items():
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type == 'postgres':
            # Mask password in connection string for security
            masked_conn_string = mask_connection_string(conn_string)
            connections.append({
                "id": conn_id,
                "connection_string": masked_conn_string,
                "type": conn_type
            })
    
    return {"connections": connections}

@router.get("/schemas")
async def get_schemas(
    connection_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a list of schemas in the database
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view database schemas")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get schemas
        schemas = await schema_inspector.get_schemas(connection_id)
        return {"schemas": schemas}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving schemas: {str(e)}")

@router.get("/tables")
async def get_tables(
    connection_id: str,
    schema: str = "public",
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a list of tables in the specified schema
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view database tables")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get tables
        tables = await schema_inspector.get_tables(connection_id, schema)
        return {"tables": tables, "schema": schema}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving tables: {str(e)}")

@router.get("/columns")
async def get_columns(
    connection_id: str,
    table_name: str,
    schema: str = "public",
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a list of columns for the specified table
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view table columns")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get columns
        columns = await schema_inspector.get_columns(connection_id, table_name, schema)
        return {"columns": columns, "table_name": table_name, "schema": schema}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving columns: {str(e)}")

@router.get("/indexes")
async def get_indexes(
    connection_id: str,
    table_name: str,
    schema: str = "public",
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a list of indexes for the specified table
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view table indexes")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get indexes
        indexes = await schema_inspector.get_indexes(connection_id, table_name, schema)
        return {"indexes": indexes, "table_name": table_name, "schema": schema}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving indexes: {str(e)}")

@router.get("/constraints")
async def get_constraints(
    connection_id: str,
    table_name: str,
    schema: str = "public",
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a list of constraints for the specified table
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view table constraints")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get constraints
        constraints = await schema_inspector.get_constraints(connection_id, table_name, schema)
        return {"constraints": constraints, "table_name": table_name, "schema": schema}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving constraints: {str(e)}")

@router.get("/foreign-keys")
async def get_foreign_keys(
    connection_id: str,
    table_name: str,
    schema: str = "public",
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get a list of foreign keys for the specified table
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view foreign keys")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get foreign keys
        foreign_keys = await schema_inspector.get_foreign_keys(connection_id, table_name, schema)
        return {"foreign_keys": foreign_keys, "table_name": table_name, "schema": schema}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving foreign keys: {str(e)}")

@router.get("/table-structure")
async def get_table_structure(
    connection_id: str,
    table_name: str,
    schema: str = "public",
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get comprehensive structure information for a table
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view table structure")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get table structure
        table_structure = await schema_inspector.get_table_structure(connection_id, table_name, schema)
        return {"table_structure": table_structure}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving table structure: {str(e)}")

@router.get("/database-structure")
async def get_database_structure(
    connection_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get comprehensive structure information for the entire database
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view database structure")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get database structure
        database_structure = await schema_inspector.get_database_structure(connection_id)
        return {"database_structure": database_structure}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving database structure: {str(e)}")

@router.get("/extensions")
async def get_extensions(
    connection_id: str,
    extension_name: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get information about installed PostgreSQL extensions
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view extensions")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get extensions
        extensions = await schema_inspector.get_extension_info(connection_id, extension_name)
        return {"extensions": extensions}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving extensions: {str(e)}")

@router.get("/pgvector-info")
async def get_pgvector_info(
    connection_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Get information about pgvector extension if installed
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can view pgvector information")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get pgvector info
        pgvector_info = await schema_inspector.get_pgvector_info(connection_id)
        return {"pgvector_info": pgvector_info}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving pgvector information: {str(e)}")

@router.post("/explain-query")
async def explain_query(
    connection_id: str,
    query: str,
    explain_type: str = "simple",
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Explain a SQL query execution plan
    """
    # Check if user is admin
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only administrators can explain queries")
    
    try:
        # Validate connection
        conn_type = connection_manager.get_connection_type(connection_id)
        if conn_type != 'postgres':
            raise HTTPException(status_code=400, detail="Connection is not a PostgreSQL connection")
        
        # Get connection
        conn = await connection_manager.get_postgres_connection(connection_id)
        
        try:
            # Build EXPLAIN command based on type
            explain_options = ""
            if explain_type == "analyze":
                explain_options = "ANALYZE"
            elif explain_type == "verbose":
                explain_options = "VERBOSE"
            elif explain_type == "analyze_verbose":
                explain_options = "ANALYZE, VERBOSE"
            elif explain_type == "analyze_verbose_buffers":
                explain_options = "ANALYZE, VERBOSE, BUFFERS"
            elif explain_type == "json":
                explain_options = "FORMAT JSON"
            elif explain_type == "analyze_json":
                explain_options = "ANALYZE, FORMAT JSON"
            
            # Execute EXPLAIN
            explain_query = f"EXPLAIN ({explain_options}) {query}"
            rows = await conn.fetch(explain_query)
            
            # Format result
            if explain_type in ["json", "analyze_json"]:
                # For JSON format, return the parsed JSON
                plan = rows[0][0]
                return {
                    "query": query,
                    "plan": plan
                }
            else:
                # For text format, concatenate the rows
                plan_lines = [row[0] for row in rows]
                plan_text = "\n".join(plan_lines)
                
                return {
                    "query": query,
                    "plan_text": plan_text
                }
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(connection_id, conn)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error explaining query: {str(e)}")

def mask_connection_string(connection_string: str) -> str:
    """
    Mask password in connection string for security
    
    Args:
        connection_string: Database connection string
        
    Returns:
        Masked connection string
    """
    import re
    
    # Mask password in PostgreSQL connection string
    if connection_string.startswith('postgresql://'):
        # Find the password part
        match = re.search(r'postgresql://([^:]+):([^@]+)@', connection_string)
        if match:
            username = match.group(1)
            password = match.group(2)
            masked_password = '*' * len(password)
            masked_conn_string = connection_string.replace(
                f"{username}:{password}@", 
                f"{username}:{masked_password}@"
            )
            return masked_conn_string
    
    return connection_string

================
File: app/api/system.py
================
import logging
import os
import platform
import psutil
from typing import List, Dict, Any
from fastapi import APIRouter, HTTPException

from app.models.system import SystemStats, ModelInfo, HealthCheck
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore
from app.db.dependencies import get_db, get_document_repository
from app.db.repositories.document_repository import DocumentRepository
from sqlalchemy.ext.asyncio import AsyncSession
from fastapi import Depends
from app.core.config import API_V1_STR

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.system")

# Vector store
vector_store = VectorStore()

@router.get("/stats", response_model=SystemStats)
async def get_stats(
    db: AsyncSession = Depends(get_db),
    document_repository: DocumentRepository = Depends(get_document_repository)
):
    """
    Get system statistics
    """
    try:
        # Get Ollama client
        async with OllamaClient() as ollama_client:
            # Get available models
            models = await ollama_client.list_models()
            model_names = [model["name"] for model in models]
        
        # Get vector store stats
        vector_store_stats = vector_store.get_stats()
        
        # Get document count and total chunks from repository
        documents = document_repository.get_all()
        total_chunks = sum(len(doc.chunks) if hasattr(doc, 'chunks') else 0 for doc in documents)
        
        return SystemStats(
            documents_count=len(documents),
            total_chunks=total_chunks,
            vector_store_size=vector_store_stats["count"],
            available_models=model_names
        )
    except Exception as e:
        logger.error(f"Error getting system stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting system stats: {str(e)}")

@router.get("/models", response_model=List[ModelInfo])
async def get_models():
    """
    Get available models
    """
    try:
        # Get Ollama client
        async with OllamaClient() as ollama_client:
            # Get available models
            models = await ollama_client.list_models()
        
        # Convert to ModelInfo
        model_infos = [
            ModelInfo(
                name=model["name"],
                size=str(model.get("size")) if model.get("size") is not None else None,
                modified_at=model.get("modified_at"),
                description=model.get("description", f"Ollama model: {model['name']}")
            )
            for model in models
        ]
        
        return model_infos
    except Exception as e:
        logger.error(f"Error getting models: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting models: {str(e)}")

@router.get("/health", response_model=HealthCheck)
async def health_check():
    """
    Health check endpoint
    """
    try:
        # Check Ollama
        ollama_status = "healthy"
        try:
            async with OllamaClient() as ollama_client:
                await ollama_client.list_models()
        except Exception as e:
            logger.error(f"Ollama health check failed: {str(e)}")
            ollama_status = "unhealthy"
        
        # Check vector DB
        vector_db_status = "healthy"
        try:
            vector_store.get_stats()
        except Exception as e:
            logger.error(f"Vector DB health check failed: {str(e)}")
            vector_db_status = "unhealthy"
        
        return HealthCheck(
            status="healthy" if ollama_status == "healthy" and vector_db_status == "healthy" else "unhealthy",
            ollama_status=ollama_status,
            vector_db_status=vector_db_status,
            api_version="v1"
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return HealthCheck(
            status="unhealthy",
            ollama_status="unknown",
            vector_db_status="unknown",
            api_version="v1"
        )

================
File: app/api/tasks.py
================
"""
API endpoints for background tasks
"""
import logging
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from datetime import datetime

from app.db.dependencies import get_db
from app.db.session import AsyncSession
from app.tasks.task_manager import TaskManager
from app.tasks.task_models import Task, TaskStatus, TaskPriority, TaskDependency
from app.tasks.task_repository import TaskRepository
from app.tasks.example_tasks import register_example_handlers

# Initialize router
router = APIRouter()

# Initialize logger
logger = logging.getLogger("app.api.tasks")

# Initialize task manager
task_manager = TaskManager()

# Register example task handlers
register_example_handlers(task_manager)

# Pydantic models for request/response
class TaskDependencyModel(BaseModel):
    """Model for task dependency"""
    task_id: str = Field(..., description="Task ID")
    required_status: str = Field("completed", description="Required status")

class TaskCreateRequest(BaseModel):
    """Request model for creating a task"""
    name: str = Field(..., description="Task name")
    task_type: str = Field(..., description="Task type")
    params: Dict[str, Any] = Field(default={}, description="Task parameters")
    priority: str = Field("normal", description="Task priority (low, normal, high, critical)")
    dependencies: List[TaskDependencyModel] = Field(default=[], description="Task dependencies")
    schedule_time: Optional[datetime] = Field(None, description="Schedule time")
    timeout_seconds: Optional[int] = Field(None, description="Timeout in seconds")
    max_retries: int = Field(0, description="Maximum number of retries")
    metadata: Dict[str, Any] = Field(default={}, description="Additional metadata")

class TaskResponse(BaseModel):
    """Response model for task"""
    id: str = Field(..., description="Task ID")
    name: str = Field(..., description="Task name")
    task_type: str = Field(..., description="Task type")
    params: Dict[str, Any] = Field(..., description="Task parameters")
    priority: str = Field(..., description="Task priority")
    dependencies: List[Dict[str, Any]] = Field(..., description="Task dependencies")
    schedule_time: Optional[str] = Field(None, description="Schedule time")
    timeout_seconds: Optional[int] = Field(None, description="Timeout in seconds")
    max_retries: int = Field(..., description="Maximum number of retries")
    metadata: Dict[str, Any] = Field(..., description="Additional metadata")
    status: str = Field(..., description="Task status")
    created_at: str = Field(..., description="Creation timestamp")
    scheduled_at: Optional[str] = Field(None, description="Scheduled timestamp")
    started_at: Optional[str] = Field(None, description="Start timestamp")
    completed_at: Optional[str] = Field(None, description="Completion timestamp")
    retry_count: int = Field(..., description="Retry count")
    result: Optional[Any] = Field(None, description="Task result")
    error: Optional[str] = Field(None, description="Error message")
    progress: float = Field(..., description="Progress percentage")
    resource_usage: Dict[str, Any] = Field(..., description="Resource usage")
    execution_time_ms: Optional[float] = Field(None, description="Execution time in milliseconds")

class TaskListResponse(BaseModel):
    """Response model for task list"""
    tasks: List[TaskResponse] = Field(..., description="List of tasks")
    total: int = Field(..., description="Total number of tasks")

class TaskStatsResponse(BaseModel):
    """Response model for task statistics"""
    pending_tasks: int = Field(..., description="Number of pending tasks")
    scheduled_tasks: int = Field(..., description="Number of scheduled tasks")
    running_tasks: int = Field(..., description="Number of running tasks")
    completed_tasks: int = Field(..., description="Number of completed tasks")
    failed_tasks: int = Field(..., description="Number of failed tasks")
    cancelled_tasks: int = Field(..., description="Number of cancelled tasks")
    total_tasks: int = Field(..., description="Total number of tasks")
    system_load: float = Field(..., description="System load factor (0.0-1.0)")
    resource_alerts: List[Dict[str, Any]] = Field(..., description="Recent resource alerts")

# Startup event
@router.on_event("startup")
async def startup_event():
    """Start the task manager on startup"""
    await task_manager.start()
    logger.info("Task manager started")

# Shutdown event
@router.on_event("shutdown")
async def shutdown_event():
    """Stop the task manager on shutdown"""
    await task_manager.stop()
    logger.info("Task manager stopped")

@router.post("/tasks", response_model=TaskResponse, tags=["tasks"])
async def create_task(
    task_request: TaskCreateRequest,
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Create a new background task
    
    Args:
        task_request: Task request
        db: Database session
        
    Returns:
        Created task
    """
    try:
        # Convert priority string to enum
        priority_map = {
            "low": TaskPriority.LOW,
            "normal": TaskPriority.NORMAL,
            "high": TaskPriority.HIGH,
            "critical": TaskPriority.CRITICAL
        }
        priority = priority_map.get(task_request.priority.lower(), TaskPriority.NORMAL)
        
        # Convert dependencies
        dependencies = []
        for dep in task_request.dependencies:
            status_map = {
                "pending": TaskStatus.PENDING,
                "scheduled": TaskStatus.SCHEDULED,
                "running": TaskStatus.RUNNING,
                "completed": TaskStatus.COMPLETED,
                "failed": TaskStatus.FAILED,
                "cancelled": TaskStatus.CANCELLED,
                "waiting": TaskStatus.WAITING
            }
            required_status = status_map.get(dep.required_status.lower(), TaskStatus.COMPLETED)
            dependencies.append(TaskDependency(task_id=dep.task_id, required_status=required_status))
        
        # Submit task
        task_id = await task_manager.submit(
            name=task_request.name,
            task_type=task_request.task_type,
            params=task_request.params,
            priority=priority,
            dependencies=dependencies,
            schedule_time=task_request.schedule_time,
            timeout_seconds=task_request.timeout_seconds,
            max_retries=task_request.max_retries,
            metadata=task_request.metadata
        )
        
        # Get task
        task = task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=500, detail="Task creation failed")
        
        # Save to database
        task_repo = TaskRepository(db)
        await task_repo.create(task)
        
        logger.info(f"Created task {task_id} of type {task_request.task_type}")
        
        # Return task
        return task.to_dict()
    except ValueError as e:
        logger.error(f"Error creating task: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error creating task: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/tasks", response_model=TaskListResponse, tags=["tasks"])
async def list_tasks(
    status: Optional[str] = None,
    task_type: Optional[str] = None,
    limit: int = 100,
    offset: int = 0,
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    List background tasks
    
    Args:
        status: Filter by status
        task_type: Filter by task type
        limit: Maximum number of tasks to return
        offset: Offset for pagination
        db: Database session
        
    Returns:
        List of tasks
    """
    try:
        # Convert status string to enum
        task_status = None
        if status:
            status_map = {
                "pending": TaskStatus.PENDING,
                "scheduled": TaskStatus.SCHEDULED,
                "running": TaskStatus.RUNNING,
                "completed": TaskStatus.COMPLETED,
                "failed": TaskStatus.FAILED,
                "cancelled": TaskStatus.CANCELLED,
                "waiting": TaskStatus.WAITING
            }
            task_status = status_map.get(status.lower())
            if not task_status:
                raise ValueError(f"Invalid status: {status}")
        
        # Get tasks from repository
        task_repo = TaskRepository(db)
        if task_status and task_type:
            tasks = await task_repo.get_by_type(task_type, task_status, limit, offset)
        elif task_status:
            tasks = await task_repo.get_by_status(task_status, limit, offset)
        elif task_type:
            tasks = await task_repo.get_by_type(task_type, None, limit, offset)
        else:
            # Get all tasks
            tasks = task_manager.get_tasks(limit=limit, offset=offset)
        
        # Count tasks
        status_counts = await task_repo.count_by_status()
        total = sum(status_counts.values())
        
        return {
            "tasks": [task.to_dict() for task in tasks],
            "total": total
        }
    except ValueError as e:
        logger.error(f"Error listing tasks: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error listing tasks: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/tasks/{task_id}", response_model=TaskResponse, tags=["tasks"])
async def get_task(
    task_id: str,
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Get a task by ID
    
    Args:
        task_id: Task ID
        db: Database session
        
    Returns:
        Task
    """
    try:
        # Get task from repository
        task_repo = TaskRepository(db)
        task = await task_repo.get_by_id(task_id)
        
        if not task:
            # Try to get from task manager
            task = task_manager.get_task(task_id)
            
        if not task:
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")
        
        return task.to_dict()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting task {task_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/tasks/{task_id}/cancel", response_model=TaskResponse, tags=["tasks"])
async def cancel_task(
    task_id: str,
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Cancel a task
    
    Args:
        task_id: Task ID
        db: Database session
        
    Returns:
        Cancelled task
    """
    try:
        # Cancel task
        cancelled = await task_manager.cancel(task_id)
        if not cancelled:
            raise HTTPException(status_code=400, detail=f"Cannot cancel task {task_id}")
        
        # Get updated task
        task = task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")
        
        # Update in database
        task_repo = TaskRepository(db)
        await task_repo.update(task)
        
        logger.info(f"Cancelled task {task_id}")
        
        return task.to_dict()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling task {task_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/tasks/stats", response_model=TaskStatsResponse, tags=["tasks"])
async def get_task_stats(
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Get task statistics
    
    Args:
        db: Database session
        
    Returns:
        Task statistics
    """
    try:
        # Get stats from task manager
        stats = task_manager.get_stats()
        
        # Get status counts from repository
        task_repo = TaskRepository(db)
        status_counts = await task_repo.count_by_status()
        
        # Get resource alerts
        resource_alerts = task_manager.get_resource_alerts(limit=5)
        
        return {
            "pending_tasks": status_counts.get(TaskStatus.PENDING.value, 0),
            "scheduled_tasks": status_counts.get(TaskStatus.SCHEDULED.value, 0),
            "running_tasks": status_counts.get(TaskStatus.RUNNING.value, 0),
            "completed_tasks": status_counts.get(TaskStatus.COMPLETED.value, 0),
            "failed_tasks": status_counts.get(TaskStatus.FAILED.value, 0),
            "cancelled_tasks": status_counts.get(TaskStatus.CANCELLED.value, 0),
            "total_tasks": sum(status_counts.values()),
            "system_load": stats["resources"]["system_load"],
            "resource_alerts": resource_alerts
        }
    except Exception as e:
        logger.error(f"Error getting task stats: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

================
File: app/api/test.py
================
import logging
from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.dependencies import get_db
from app.db.repositories.document_repository import DocumentRepository

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.test")

@router.get("/test-document-repository")
async def test_document_repository(db: AsyncSession = Depends(get_db)):
    """
    Test the document repository
    """
    try:
        # Create a document repository
        document_repository = DocumentRepository(db)
        
        # Create a document
        document = await document_repository.create_document(
            filename="test_document.txt",
            content="This is a test document.",
            metadata={"source": "test"},
            tags=["test", "sample"],
            folder="/"
        )
        
        return {
            "success": True,
            "message": "Document created successfully",
            "document_id": str(document.id)
        }
    except Exception as e:
        logger.error(f"Error testing document repository: {str(e)}")
        return {
            "success": False,
            "message": f"Error testing document repository: {str(e)}"
        }

================
File: app/cache/__init__.py
================
"""
Cache module for Metis_RAG.

This module provides caching implementations for various components of the system.
"""

from app.cache.base import Cache
from app.cache.vector_search_cache import VectorSearchCache
from app.cache.document_cache import DocumentCache
from app.cache.llm_response_cache import LLMResponseCache
from app.cache.cache_manager import CacheManager

__all__ = [
    "Cache",
    "VectorSearchCache",
    "DocumentCache",
    "LLMResponseCache",
    "CacheManager",
]

================
File: app/cache/base.py
================
"""
Base cache implementation for Metis_RAG.
"""

import os
import time
import json
import logging
import pickle
from typing import Dict, Any, Optional, Generic, TypeVar, List

T = TypeVar('T')

class Cache(Generic[T]):
    """
    Generic cache implementation with disk persistence.
    
    This class provides a generic caching mechanism with optional disk persistence,
    TTL-based expiration, and size-based pruning.
    
    Attributes:
        name (str): Name of the cache, used for logging and persistence
        ttl (int): Time-to-live in seconds for cache entries
        max_size (int): Maximum number of entries in the cache
        persist (bool): Whether to persist the cache to disk
        persist_dir (str): Directory for cache persistence
        cache (Dict[str, Dict[str, Any]]): In-memory cache storage
        hits (int): Number of cache hits
        misses (int): Number of cache misses
        logger (logging.Logger): Logger instance
    """
    
    def __init__(
        self,
        name: str,
        ttl: int = 3600,
        max_size: int = 1000,
        persist: bool = True,
        persist_dir: str = "data/cache"
    ):
        """
        Initialize a new cache instance.
        
        Args:
            name: Name of the cache, used for logging and persistence
            ttl: Time-to-live in seconds for cache entries (default: 3600)
            max_size: Maximum number of entries in the cache (default: 1000)
            persist: Whether to persist the cache to disk (default: True)
            persist_dir: Directory for cache persistence (default: "data/cache")
        """
        self.name = name
        self.ttl = ttl
        self.max_size = max_size
        self.persist = persist
        self.persist_dir = persist_dir
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.hits = 0
        self.misses = 0
        self.logger = logging.getLogger(f"app.cache.{name}")
        
        # Create persist directory if needed
        if self.persist:
            os.makedirs(os.path.join(self.persist_dir, self.name), exist_ok=True)
            self._load_from_disk()
            self.logger.info(f"Loaded {len(self.cache)} items from disk cache")
    
    def get(self, key: str) -> Optional[T]:
        """
        Get a value from the cache.
        
        Args:
            key: Cache key
            
        Returns:
            The cached value if found and not expired, None otherwise
        """
        if key in self.cache:
            entry = self.cache[key]
            if time.time() - entry["timestamp"] < self.ttl:
                self.hits += 1
                self.logger.debug(f"Cache hit for key: {key}")
                return entry["value"]
            else:
                # Expired, remove from cache
                del self.cache[key]
                self.logger.debug(f"Cache entry expired for key: {key}")
        
        self.misses += 1
        self.logger.debug(f"Cache miss for key: {key}")
        return None
    
    def set(self, key: str, value: T) -> None:
        """
        Set a value in the cache.
        
        Args:
            key: Cache key
            value: Value to cache
        """
        self.cache[key] = {
            "value": value,
            "timestamp": time.time()
        }
        self.logger.debug(f"Cache set for key: {key}")
        
        # Prune cache if it gets too large
        if len(self.cache) > self.max_size:
            self._prune()
            
        # Persist to disk if enabled
        if self.persist:
            self._save_to_disk()
    
    def delete(self, key: str) -> bool:
        """
        Delete a value from the cache.
        
        Args:
            key: Cache key
            
        Returns:
            True if the key was found and deleted, False otherwise
        """
        if key in self.cache:
            del self.cache[key]
            self.logger.debug(f"Cache entry deleted for key: {key}")
            
            # Persist changes to disk if enabled
            if self.persist:
                self._save_to_disk()
            
            return True
        
        return False
    
    def clear(self) -> None:
        """
        Clear all entries from the cache.
        """
        self.cache = {}
        self.logger.info(f"Cache '{self.name}' cleared")
        
        # Persist changes to disk if enabled
        if self.persist:
            self._save_to_disk()
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the cache.
        
        Returns:
            Dictionary with cache statistics
        """
        total_requests = self.hits + self.misses
        hit_ratio = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            "name": self.name,
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_ratio": hit_ratio,
            "ttl_seconds": self.ttl,
            "persist": self.persist
        }
    
    def _prune(self) -> None:
        """
        Remove oldest entries from the cache when it exceeds max_size.
        """
        # Sort by timestamp and keep the newest entries
        sorted_cache = sorted(
            self.cache.items(),
            key=lambda x: x[1]["timestamp"],
            reverse=True
        )
        
        # Keep only half of the max cache size
        keep_count = self.max_size // 2
        self.cache = dict(sorted_cache[:keep_count])
        self.logger.info(f"Pruned cache to {keep_count} entries")
        
        # Persist changes to disk if enabled
        if self.persist:
            self._save_to_disk()
    
    def _save_to_disk(self) -> None:
        """
        Save the cache to disk.
        """
        try:
            cache_file = os.path.join(self.persist_dir, self.name, "cache.pickle")
            with open(cache_file, "wb") as f:
                pickle.dump(self.cache, f)
            
            # Save stats separately as JSON for easier inspection
            stats_file = os.path.join(self.persist_dir, self.name, "stats.json")
            with open(stats_file, "w") as f:
                json.dump(self.get_stats(), f, indent=2)
                
            self.logger.debug(f"Cache saved to disk: {cache_file}")
        except Exception as e:
            self.logger.error(f"Error saving cache to disk: {str(e)}")
    
    def _load_from_disk(self) -> None:
        """
        Load the cache from disk.
        """
        try:
            cache_file = os.path.join(self.persist_dir, self.name, "cache.pickle")
            if os.path.exists(cache_file):
                with open(cache_file, "rb") as f:
                    loaded_cache = pickle.load(f)
                
                # Filter out expired entries
                current_time = time.time()
                self.cache = {
                    key: entry
                    for key, entry in loaded_cache.items()
                    if current_time - entry["timestamp"] < self.ttl
                }
                
                self.logger.debug(f"Cache loaded from disk: {cache_file}")
            else:
                self.logger.debug(f"No cache file found at {cache_file}")
        except Exception as e:
            self.logger.error(f"Error loading cache from disk: {str(e)}")
            # Start with an empty cache if loading fails
            self.cache = {}
    
    def get_keys(self) -> List[str]:
        """
        Get all keys in the cache.
        
        Returns:
            List of cache keys
        """
        return list(self.cache.keys())
    
    def get_size(self) -> int:
        """
        Get the current size of the cache.
        
        Returns:
            Number of entries in the cache
        """
        return len(self.cache)
    
    def has_key(self, key: str) -> bool:
        """
        Check if a key exists in the cache and is not expired.
        
        Args:
            key: Cache key
            
        Returns:
            True if the key exists and is not expired, False otherwise
        """
        if key in self.cache:
            entry = self.cache[key]
            if time.time() - entry["timestamp"] < self.ttl:
                return True
            else:
                # Expired, remove from cache
                del self.cache[key]
        
        return False
    
    def update_ttl(self, ttl: int) -> None:
        """
        Update the TTL for the cache.
        
        Args:
            ttl: New TTL in seconds
        """
        self.ttl = ttl
        self.logger.info(f"Cache TTL updated to {ttl} seconds")

================
File: app/cache/cache_manager.py
================
"""
Cache manager for Metis_RAG.
"""

import os
import json
import logging
from typing import Dict, Any, Optional, List, Type, TypeVar, Generic

from app.cache.base import Cache
from app.cache.vector_search_cache import VectorSearchCache
from app.cache.document_cache import DocumentCache
from app.cache.llm_response_cache import LLMResponseCache

T = TypeVar('T')

class CacheManager:
    """
    Manager for all cache instances in the system.
    
    This class provides a centralized way to access and manage all caches,
    including initialization, configuration, and monitoring.
    
    Attributes:
        vector_search_cache: Cache for vector search results
        document_cache: Cache for document content and metadata
        llm_response_cache: Cache for LLM responses
        logger: Logger instance
    """
    
    def __init__(
        self,
        cache_dir: str = "data/cache",
        config_file: Optional[str] = None,
        enable_caching: bool = True
    ):
        """
        Initialize the cache manager.
        
        Args:
            cache_dir: Directory for cache persistence (default: "data/cache")
            config_file: Path to cache configuration file (default: None)
            enable_caching: Whether to enable caching (default: True)
        """
        self.cache_dir = cache_dir
        self.enable_caching = enable_caching
        self.logger = logging.getLogger("app.cache.manager")
        
        # Create cache directory if it doesn't exist
        os.makedirs(cache_dir, exist_ok=True)
        
        # Load configuration if provided
        self.config = self._load_config(config_file)
        
        # Initialize caches
        self._initialize_caches()
        
        self.logger.info(f"Cache manager initialized with caching {'enabled' if enable_caching else 'disabled'}")
    
    def _load_config(self, config_file: Optional[str]) -> Dict[str, Any]:
        """
        Load cache configuration from a file.
        
        Args:
            config_file: Path to configuration file
            
        Returns:
            Configuration dictionary
        """
        default_config = {
            "vector_search_cache": {
                "ttl": 3600,
                "max_size": 1000,
                "persist": True
            },
            "document_cache": {
                "ttl": 7200,
                "max_size": 500,
                "persist": True
            },
            "llm_response_cache": {
                "ttl": 86400,
                "max_size": 2000,
                "persist": True
            }
        }
        
        if not config_file or not os.path.exists(config_file):
            self.logger.info("Using default cache configuration")
            return default_config
        
        try:
            with open(config_file, "r") as f:
                config = json.load(f)
                self.logger.info(f"Loaded cache configuration from {config_file}")
                
                # Merge with defaults for any missing values
                for cache_name, default_values in default_config.items():
                    if cache_name not in config:
                        config[cache_name] = default_values
                    else:
                        for key, value in default_values.items():
                            if key not in config[cache_name]:
                                config[cache_name][key] = value
                
                return config
        except Exception as e:
            self.logger.error(f"Error loading cache configuration: {str(e)}")
            return default_config
    
    def _initialize_caches(self) -> None:
        """
        Initialize all cache instances.
        """
        # Only create actual cache instances if caching is enabled
        if self.enable_caching:
            # Initialize vector search cache
            vector_config = self.config["vector_search_cache"]
            self.vector_search_cache = VectorSearchCache(
                ttl=vector_config["ttl"],
                max_size=vector_config["max_size"],
                persist=vector_config["persist"],
                persist_dir=self.cache_dir
            )
            
            # Initialize document cache
            document_config = self.config["document_cache"]
            self.document_cache = DocumentCache(
                ttl=document_config["ttl"],
                max_size=document_config["max_size"],
                persist=document_config["persist"],
                persist_dir=self.cache_dir
            )
            
            # Initialize LLM response cache
            llm_config = self.config["llm_response_cache"]
            self.llm_response_cache = LLMResponseCache(
                ttl=llm_config["ttl"],
                max_size=llm_config["max_size"],
                persist=llm_config["persist"],
                persist_dir=self.cache_dir
            )
        else:
            # Create dummy cache instances that don't actually cache anything
            self.vector_search_cache = self._create_dummy_cache(VectorSearchCache)
            self.document_cache = self._create_dummy_cache(DocumentCache)
            self.llm_response_cache = self._create_dummy_cache(LLMResponseCache)
    
    def _create_dummy_cache(self, cache_class: Type[Cache[T]]) -> Cache[T]:
        """
        Create a dummy cache instance that doesn't actually cache anything.
        
        Args:
            cache_class: Cache class to instantiate
            
        Returns:
            Dummy cache instance
        """
        # Create a cache instance
        dummy_cache = cache_class(
            ttl=1,  # 1 second TTL
            max_size=1,
            persist=False,
            persist_dir=self.cache_dir
        )
        
        # Override the get and set methods to make it truly non-caching
        original_set = dummy_cache.set
        
        def dummy_set(key: str, value: Any) -> None:
            # Do nothing when setting values
            pass
            
        def dummy_get(key: str) -> Optional[T]:
            # Always return None
            return None
            
        # Replace the methods with our dummy implementations
        dummy_cache.set = dummy_set
        dummy_cache.get = dummy_get
        
        return dummy_cache
    
    def clear_all_caches(self) -> None:
        """
        Clear all caches.
        """
        if not self.enable_caching:
            self.logger.info("Caching is disabled, no caches to clear")
            return
            
        self.vector_search_cache.clear()
        self.document_cache.clear()
        self.llm_response_cache.clear()
        self.logger.info("All caches cleared")
    
    def get_all_cache_stats(self) -> Dict[str, Dict[str, Any]]:
        """
        Get statistics for all caches.
        
        Returns:
            Dictionary mapping cache names to statistics dictionaries
        """
        if not self.enable_caching:
            return {
                "caching_enabled": False
            }
            
        return {
            "caching_enabled": True,
            "vector_search_cache": self.vector_search_cache.get_stats(),
            "document_cache": self.document_cache.get_stats(),
            "llm_response_cache": self.llm_response_cache.get_stats()
        }
    
    def update_cache_config(self, config: Dict[str, Any]) -> None:
        """
        Update cache configuration.
        
        Args:
            config: New configuration dictionary
        """
        if not self.enable_caching:
            self.logger.info("Caching is disabled, configuration update ignored")
            return
            
        # Update vector search cache configuration
        if "vector_search_cache" in config:
            vector_config = config["vector_search_cache"]
            if "ttl" in vector_config:
                self.vector_search_cache.update_ttl(vector_config["ttl"])
        
        # Update document cache configuration
        if "document_cache" in config:
            document_config = config["document_cache"]
            if "ttl" in document_config:
                self.document_cache.update_ttl(document_config["ttl"])
        
        # Update LLM response cache configuration
        if "llm_response_cache" in config:
            llm_config = config["llm_response_cache"]
            if "ttl" in llm_config:
                self.llm_response_cache.update_ttl(llm_config["ttl"])
        
        # Update internal configuration
        self.config.update(config)
        self.logger.info("Cache configuration updated")
    
    def save_config(self, config_file: str) -> None:
        """
        Save current cache configuration to a file.
        
        Args:
            config_file: Path to configuration file
        """
        try:
            with open(config_file, "w") as f:
                json.dump(self.config, f, indent=2)
            self.logger.info(f"Cache configuration saved to {config_file}")
        except Exception as e:
            self.logger.error(f"Error saving cache configuration: {str(e)}")
    
    def invalidate_document(self, document_id: str) -> None:
        """
        Invalidate all caches for a specific document.
        
        Args:
            document_id: Document ID to invalidate
        """
        if not self.enable_caching:
            return
            
        # Invalidate document in document cache
        self.document_cache.invalidate_document(document_id)
        
        # Invalidate document chunks in document cache
        self.document_cache.invalidate_document_chunks(document_id)
        
        # Invalidate vector search results containing the document
        self.vector_search_cache.invalidate_by_document_id(document_id)
        
        self.logger.info(f"All caches invalidated for document {document_id}")

================
File: app/cache/document_cache.py
================
"""
Document cache implementation for Metis_RAG.
"""

import hashlib
from typing import Dict, Any, Optional, List, Tuple

from app.cache.base import Cache

class DocumentCache(Cache[Dict[str, Any]]):
    """
    Cache implementation for document content and metadata.
    
    This cache stores document content and metadata to avoid redundant
    database queries and file system access.
    
    Attributes:
        Inherits all attributes from the base Cache class
    """
    
    def __init__(
        self,
        ttl: int = 7200,  # 2 hours default TTL for documents
        max_size: int = 500,  # Lower default max size due to potentially larger entries
        persist: bool = True,
        persist_dir: str = "data/cache"
    ):
        """
        Initialize a new document cache.
        
        Args:
            ttl: Time-to-live in seconds for cache entries (default: 7200)
            max_size: Maximum number of entries in the cache (default: 500)
            persist: Whether to persist the cache to disk (default: True)
            persist_dir: Directory for cache persistence (default: "data/cache")
        """
        super().__init__(
            name="document",
            ttl=ttl,
            max_size=max_size,
            persist=persist,
            persist_dir=persist_dir
        )
    
    def get_document(self, document_id: str) -> Optional[Dict[str, Any]]:
        """
        Get a cached document by ID.
        
        Args:
            document_id: Document ID
            
        Returns:
            Cached document if found, None otherwise
        """
        cache_key = self._create_document_key(document_id)
        return self.get(cache_key)
    
    def set_document(self, document_id: str, document: Dict[str, Any]) -> None:
        """
        Cache a document.
        
        Args:
            document_id: Document ID
            document: Document data to cache
        """
        cache_key = self._create_document_key(document_id)
        self.set(cache_key, document)
    
    def get_document_content(self, document_id: str) -> Optional[str]:
        """
        Get cached document content by ID.
        
        Args:
            document_id: Document ID
            
        Returns:
            Cached document content if found, None otherwise
        """
        document = self.get_document(document_id)
        if document:
            return document.get("content")
        return None
    
    def get_document_metadata(self, document_id: str) -> Optional[Dict[str, Any]]:
        """
        Get cached document metadata by ID.
        
        Args:
            document_id: Document ID
            
        Returns:
            Cached document metadata if found, None otherwise
        """
        document = self.get_document(document_id)
        if document:
            return document.get("metadata", {})
        return None
    
    def invalidate_document(self, document_id: str) -> bool:
        """
        Invalidate a cached document.
        
        Args:
            document_id: Document ID to invalidate
            
        Returns:
            True if the document was found and invalidated, False otherwise
        """
        cache_key = self._create_document_key(document_id)
        return self.delete(cache_key)
    
    def get_documents_by_tag(self, tag: str) -> List[Dict[str, Any]]:
        """
        Get all cached documents with a specific tag.
        
        Args:
            tag: Tag to filter by
            
        Returns:
            List of cached documents with the specified tag
        """
        documents = []
        
        for key in self.get_keys():
            if not key.startswith("doc:"):
                continue
                
            document = self.get(key)
            if document and tag in document.get("metadata", {}).get("tags", []):
                documents.append(document)
        
        return documents
    
    def get_documents_by_folder(self, folder: str) -> List[Dict[str, Any]]:
        """
        Get all cached documents in a specific folder.
        
        Args:
            folder: Folder path to filter by
            
        Returns:
            List of cached documents in the specified folder
        """
        documents = []
        
        for key in self.get_keys():
            if not key.startswith("doc:"):
                continue
                
            document = self.get(key)
            if document and document.get("metadata", {}).get("folder") == folder:
                documents.append(document)
        
        return documents
    
    def _create_document_key(self, document_id: str) -> str:
        """
        Create a cache key for a document.
        
        Args:
            document_id: Document ID
            
        Returns:
            Cache key string
        """
        # Use a simple prefix for document keys
        return f"doc:{document_id}"
    
    def get_document_chunk(self, chunk_id: str) -> Optional[Dict[str, Any]]:
        """
        Get a cached document chunk by ID.
        
        Args:
            chunk_id: Chunk ID
            
        Returns:
            Cached document chunk if found, None otherwise
        """
        cache_key = f"chunk:{chunk_id}"
        return self.get(cache_key)
    
    def set_document_chunk(self, chunk_id: str, chunk: Dict[str, Any]) -> None:
        """
        Cache a document chunk.
        
        Args:
            chunk_id: Chunk ID
            chunk: Chunk data to cache
        """
        cache_key = f"chunk:{chunk_id}"
        self.set(cache_key, chunk)
    
    def invalidate_document_chunks(self, document_id: str) -> int:
        """
        Invalidate all cached chunks for a document.
        
        Args:
            document_id: Document ID
            
        Returns:
            Number of chunks invalidated
        """
        invalidated_count = 0
        
        for key in list(self.get_keys()):
            if not key.startswith("chunk:"):
                continue
                
            chunk = self.get(key)
            if chunk and chunk.get("document_id") == document_id:
                self.delete(key)
                invalidated_count += 1
        
        self.logger.info(f"Invalidated {invalidated_count} chunks for document {document_id}")
        return invalidated_count
    
    def get_cache_stats_by_type(self) -> Dict[str, Tuple[int, int]]:
        """
        Get hit/miss statistics by entry type (document vs. chunk).
        
        Returns:
            Dictionary mapping entry types to (hits, misses) tuples
        """
        # This is a simplified implementation that would need to be enhanced
        # with actual tracking of per-type statistics in a real system
        return {
            "document": (0, 0),
            "chunk": (0, 0)
        }

================
File: app/cache/llm_response_cache.py
================
"""
LLM response cache implementation for Metis_RAG.
"""

import hashlib
import json
from typing import Dict, Any, Optional, List, Tuple

from app.cache.base import Cache

class LLMResponseCache(Cache[Dict[str, Any]]):
    """
    Cache implementation for LLM responses.
    
    This cache stores responses from language models to avoid redundant API calls,
    reducing latency and costs.
    
    Attributes:
        Inherits all attributes from the base Cache class
    """
    
    def __init__(
        self,
        ttl: int = 86400,  # 24 hours default TTL for LLM responses
        max_size: int = 2000,  # Higher default max size for LLM responses
        persist: bool = True,
        persist_dir: str = "data/cache"
    ):
        """
        Initialize a new LLM response cache.
        
        Args:
            ttl: Time-to-live in seconds for cache entries (default: 86400)
            max_size: Maximum number of entries in the cache (default: 2000)
            persist: Whether to persist the cache to disk (default: True)
            persist_dir: Directory for cache persistence (default: "data/cache")
        """
        super().__init__(
            name="llm_response",
            ttl=ttl,
            max_size=max_size,
            persist=persist,
            persist_dir=persist_dir
        )
    
    def get_response(
        self,
        prompt: str,
        model: str,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        additional_params: Optional[Dict[str, Any]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Get a cached LLM response.
        
        Args:
            prompt: The prompt sent to the LLM
            model: The model identifier
            temperature: The temperature parameter (default: 0.0)
            max_tokens: The maximum tokens parameter (default: None)
            additional_params: Additional parameters sent to the LLM (default: None)
            
        Returns:
            Cached LLM response if found, None otherwise
        """
        cache_key = self._create_response_key(prompt, model, temperature, max_tokens, additional_params)
        return self.get(cache_key)
    
    def set_response(
        self,
        prompt: str,
        model: str,
        response: Dict[str, Any],
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        additional_params: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Cache an LLM response.
        
        Args:
            prompt: The prompt sent to the LLM
            model: The model identifier
            response: The LLM response to cache
            temperature: The temperature parameter (default: 0.0)
            max_tokens: The maximum tokens parameter (default: None)
            additional_params: Additional parameters sent to the LLM (default: None)
        """
        cache_key = self._create_response_key(prompt, model, temperature, max_tokens, additional_params)
        self.set(cache_key, response)
    
    def _create_response_key(
        self,
        prompt: str,
        model: str,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        additional_params: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Create a cache key for an LLM response.
        
        Args:
            prompt: The prompt sent to the LLM
            model: The model identifier
            temperature: The temperature parameter
            max_tokens: The maximum tokens parameter
            additional_params: Additional parameters sent to the LLM
            
        Returns:
            Cache key string
        """
        # Normalize the prompt by removing extra whitespace
        normalized_prompt = " ".join(prompt.split())
        
        # Create a dictionary of all parameters
        params = {
            "model": model,
            "temperature": temperature
        }
        
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
            
        if additional_params:
            params.update(additional_params)
        
        # Convert parameters to a stable string representation
        params_str = json.dumps(params, sort_keys=True)
        
        # Create a hash of the combined parameters for a shorter key
        key_data = f"{normalized_prompt}:{params_str}"
        key_hash = hashlib.md5(key_data.encode()).hexdigest()
        
        return f"llm:{key_hash}"
    
    def invalidate_by_model(self, model: str) -> int:
        """
        Invalidate all cache entries for a specific model.
        
        Args:
            model: Model identifier to invalidate
            
        Returns:
            Number of cache entries invalidated
        """
        invalidated_count = 0
        keys_to_delete = []
        
        # Find all cache entries for the specified model
        for key, entry in list(self.cache.items()):
            response = entry["value"]
            if response.get("model") == model:
                keys_to_delete.append(key)
                invalidated_count += 1
        
        # Delete the identified entries
        for key in keys_to_delete:
            self.delete(key)
        
        self.logger.info(f"Invalidated {invalidated_count} cache entries for model {model}")
        return invalidated_count
    
    def get_response_by_prompt_prefix(self, prefix: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get cached responses for prompts starting with a specific prefix.
        
        Args:
            prefix: Prompt prefix to filter by
            limit: Maximum number of responses to return (default: 10)
            
        Returns:
            List of cached responses for prompts with the specified prefix
        """
        responses = []
        count = 0
        
        for key, entry in list(self.cache.items()):
            if count >= limit:
                break
                
            response = entry["value"]
            if "prompt" in response and response["prompt"].startswith(prefix):
                responses.append(response)
                count += 1
        
        return responses
    
    def get_cache_stats_by_model(self) -> Dict[str, Tuple[int, int]]:
        """
        Get hit/miss statistics by model.
        
        Returns:
            Dictionary mapping model identifiers to (hits, misses) tuples
        """
        # This is a simplified implementation that would need to be enhanced
        # with actual tracking of per-model statistics in a real system
        return {}
    
    def should_cache_response(
        self,
        prompt: str,
        model: str,
        temperature: float,
        response: Dict[str, Any]
    ) -> bool:
        """
        Determine whether a response should be cached based on various factors.
        
        Args:
            prompt: The prompt sent to the LLM
            model: The model identifier
            temperature: The temperature parameter
            response: The LLM response
            
        Returns:
            True if the response should be cached, False otherwise
        """
        # Don't cache responses from high-temperature requests (more random)
        if temperature > 0.5:
            return False
            
        # Don't cache very short responses
        if len(response.get("response", "")) < 10:
            return False
            
        # Don't cache error responses
        if response.get("error"):
            return False
            
        return True

================
File: app/cache/README.md
================
# Metis_RAG Cache System

This module provides a comprehensive caching system for the Metis_RAG application, designed to improve performance by reducing redundant operations and speeding up response times.

## Overview

The caching system consists of the following components:

1. **Base Cache**: A generic cache implementation with disk persistence, TTL-based expiration, and size-based pruning.
2. **VectorSearchCache**: Specialized cache for vector search results.
3. **DocumentCache**: Specialized cache for document content and metadata.
4. **LLMResponseCache**: Specialized cache for LLM responses.
5. **CacheManager**: Central manager for all cache instances.

## Usage

### Basic Cache Usage

```python
from app.cache import Cache

# Create a cache instance
cache = Cache[str](
    name="my_cache",
    ttl=3600,  # 1 hour TTL
    max_size=1000,
    persist=True,
    persist_dir="data/cache"
)

# Set a value
cache.set("key", "value")

# Get a value
value = cache.get("key")

# Delete a value
cache.delete("key")

# Clear the cache
cache.clear()

# Get cache statistics
stats = cache.get_stats()
```

### Vector Search Cache

```python
from app.cache import VectorSearchCache

# Create a vector search cache
vector_cache = VectorSearchCache(
    ttl=3600,
    max_size=1000,
    persist=True,
    persist_dir="data/cache"
)

# Cache search results
vector_cache.set_results(
    query="What is RAG?",
    top_k=5,
    results=[...],  # List of search results
    filter_criteria={"tags": "AI"}
)

# Get cached search results
results = vector_cache.get_results(
    query="What is RAG?",
    top_k=5,
    filter_criteria={"tags": "AI"}
)

# Invalidate cache entries for a document
vector_cache.invalidate_by_document_id("doc123")
```

### Document Cache

```python
from app.cache import DocumentCache

# Create a document cache
document_cache = DocumentCache(
    ttl=7200,  # 2 hours TTL
    max_size=500,
    persist=True,
    persist_dir="data/cache"
)

# Cache a document
document_cache.set_document(
    document_id="doc123",
    document={
        "id": "doc123",
        "content": "Document content...",
        "metadata": {
            "filename": "example.txt",
            "tags": ["example", "document"],
            "folder": "/examples"
        }
    }
)

# Get a cached document
document = document_cache.get_document("doc123")

# Get document content
content = document_cache.get_document_content("doc123")

# Get document metadata
metadata = document_cache.get_document_metadata("doc123")

# Invalidate a document
document_cache.invalidate_document("doc123")

# Get documents by tag
documents = document_cache.get_documents_by_tag("example")

# Get documents by folder
documents = document_cache.get_documents_by_folder("/examples")
```

### LLM Response Cache

```python
from app.cache import LLMResponseCache

# Create an LLM response cache
llm_cache = LLMResponseCache(
    ttl=86400,  # 24 hours TTL
    max_size=2000,
    persist=True,
    persist_dir="data/cache"
)

# Cache an LLM response
llm_cache.set_response(
    prompt="What is RAG?",
    model="gpt-4",
    response={
        "response": "RAG stands for Retrieval-Augmented Generation...",
        "model": "gpt-4",
        "tokens": 150,
        "finish_reason": "stop"
    },
    temperature=0.0
)

# Get a cached LLM response
response = llm_cache.get_response(
    prompt="What is RAG?",
    model="gpt-4",
    temperature=0.0
)

# Invalidate responses for a model
llm_cache.invalidate_by_model("gpt-4")

# Check if a response should be cached
should_cache = llm_cache.should_cache_response(
    prompt="What is RAG?",
    model="gpt-4",
    temperature=0.2,
    response={...}
)
```

### Cache Manager

```python
from app.cache import CacheManager

# Create a cache manager
cache_manager = CacheManager(
    cache_dir="data/cache",
    config_file="config/cache_config.json",
    enable_caching=True
)

# Access individual caches
vector_cache = cache_manager.vector_search_cache
document_cache = cache_manager.document_cache
llm_cache = cache_manager.llm_response_cache

# Clear all caches
cache_manager.clear_all_caches()

# Get statistics for all caches
stats = cache_manager.get_all_cache_stats()

# Update cache configuration
cache_manager.update_cache_config({
    "vector_search_cache": {
        "ttl": 7200
    }
})

# Save configuration to file
cache_manager.save_config("config/cache_config.json")

# Invalidate a document in all caches
cache_manager.invalidate_document("doc123")
```

## Configuration

The cache system can be configured through a JSON configuration file with the following structure:

```json
{
  "vector_search_cache": {
    "ttl": 3600,
    "max_size": 1000,
    "persist": true
  },
  "document_cache": {
    "ttl": 7200,
    "max_size": 500,
    "persist": true
  },
  "llm_response_cache": {
    "ttl": 86400,
    "max_size": 2000,
    "persist": true
  }
}
```

## Cache Persistence

By default, all caches are persisted to disk in the specified `persist_dir`. This allows the cache to survive application restarts. The persistence format is:

- `{persist_dir}/{cache_name}/cache.pickle`: The serialized cache data
- `{persist_dir}/{cache_name}/stats.json`: Cache statistics in JSON format for easier inspection

## Cache Invalidation

The cache system provides several ways to invalidate cache entries:

1. **TTL-based expiration**: Entries automatically expire after the specified TTL.
2. **Size-based pruning**: When the cache exceeds its maximum size, the oldest entries are removed.
3. **Manual invalidation**: Entries can be manually invalidated using the `delete`, `clear`, and specialized invalidation methods.

## Performance Considerations

- **TTL**: Choose appropriate TTL values based on how frequently the underlying data changes.
- **Max Size**: Set the maximum cache size based on memory constraints and usage patterns.
- **Persistence**: Disable persistence for temporary caches or when disk I/O is a concern.
- **Key Generation**: The cache implementations use optimized key generation to minimize collisions.

## Testing

The cache system includes comprehensive unit tests in `tests/unit/test_cache.py`. Run the tests with:

```bash
python -m unittest tests/unit/test_cache.py

================
File: app/cache/vector_search_cache.py
================
"""
Vector search cache implementation for Metis_RAG.
"""

import json
import hashlib
from typing import Dict, List, Any, Optional, Tuple

from app.cache.base import Cache

class VectorSearchCache(Cache[List[Dict[str, Any]]]):
    """
    Cache implementation for vector search results.
    
    This cache stores the results of vector searches to avoid redundant
    embedding generation and vector database queries.
    
    Attributes:
        Inherits all attributes from the base Cache class
    """
    
    def __init__(
        self,
        ttl: int = 3600,
        max_size: int = 1000,
        persist: bool = True,
        persist_dir: str = "data/cache"
    ):
        """
        Initialize a new vector search cache.
        
        Args:
            ttl: Time-to-live in seconds for cache entries (default: 3600)
            max_size: Maximum number of entries in the cache (default: 1000)
            persist: Whether to persist the cache to disk (default: True)
            persist_dir: Directory for cache persistence (default: "data/cache")
        """
        super().__init__(
            name="vector_search",
            ttl=ttl,
            max_size=max_size,
            persist=persist,
            persist_dir=persist_dir
        )
    
    def get_results(
        self,
        query: str,
        top_k: int,
        filter_criteria: Optional[Dict[str, Any]] = None
    ) -> Optional[List[Dict[str, Any]]]:
        """
        Get cached search results for a query.
        
        Args:
            query: Search query
            top_k: Number of results to return
            filter_criteria: Optional filter criteria
            
        Returns:
            Cached search results if found, None otherwise
        """
        cache_key = self._create_cache_key(query, top_k, filter_criteria)
        return self.get(cache_key)
    
    def set_results(
        self,
        query: str,
        top_k: int,
        results: List[Dict[str, Any]],
        filter_criteria: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Cache search results for a query.
        
        Args:
            query: Search query
            top_k: Number of results
            results: Search results to cache
            filter_criteria: Optional filter criteria
        """
        cache_key = self._create_cache_key(query, top_k, filter_criteria)
        self.set(cache_key, results)
    
    def _create_cache_key(
        self,
        query: str,
        top_k: int,
        filter_criteria: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Create a cache key from the search parameters.
        
        Args:
            query: Search query
            top_k: Number of results
            filter_criteria: Optional filter criteria
            
        Returns:
            Cache key string
        """
        # Normalize the query by removing extra whitespace and lowercasing
        normalized_query = " ".join(query.lower().split())
        
        # Convert filter criteria to a stable string representation
        filter_str = "none"
        if filter_criteria:
            # Sort keys to ensure consistent ordering
            filter_str = json.dumps(filter_criteria, sort_keys=True)
        
        # Create a hash of the combined parameters for a shorter key
        key_data = f"{normalized_query}:{top_k}:{filter_str}"
        key_hash = hashlib.md5(key_data.encode()).hexdigest()
        
        return f"vsearch:{key_hash}"
    
    def invalidate_by_document_id(self, document_id: str) -> int:
        """
        Invalidate all cache entries that might contain results from a specific document.
        This is a more complex operation that requires examining the cached results.
        
        Args:
            document_id: Document ID to invalidate
            
        Returns:
            Number of cache entries invalidated
        """
        invalidated_count = 0
        keys_to_delete = []
        
        # Find all cache entries that contain the document ID
        for key, entry in list(self.cache.items()):
            results = entry["value"]
            for result in results:
                if result.get("document_id") == document_id or result.get("metadata", {}).get("document_id") == document_id:
                    keys_to_delete.append(key)
                    invalidated_count += 1
                    break
        
        # Delete the identified entries
        for key in keys_to_delete:
            self.delete(key)
        
        self.logger.info(f"Invalidated {invalidated_count} cache entries for document {document_id}")
        return invalidated_count
    
    def get_cache_stats_by_query_prefix(self, prefix: str) -> Tuple[int, int]:
        """
        Get hit/miss statistics for queries with a specific prefix.
        
        Args:
            prefix: Query prefix to filter by
            
        Returns:
            Tuple of (hits, misses) for queries with the given prefix
        """
        hits = 0
        misses = 0
        
        # This is a simplified implementation that would need to be enhanced
        # with actual tracking of per-query statistics in a real system
        
        return hits, misses

================
File: app/core/__init__.py
================
from app.core.config import *
from app.core.security import setup_security
from app.core.logging import setup_logging, get_logger

================
File: app/core/config.py
================
import os
from pathlib import Path
from dotenv import load_dotenv
from types import SimpleNamespace

# Load environment variables from .env file
load_dotenv()

print(f"DATABASE_URL from environment: {os.getenv('DATABASE_URL')}")  # Debug print

# Base directory
BASE_DIR = Path(__file__).resolve().parent.parent.parent

# API settings
API_V1_STR = "/api"
PROJECT_NAME = "Metis RAG"

# Ollama settings
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "gemma3:4b")
DEFAULT_EMBEDDING_MODEL = os.getenv("DEFAULT_EMBEDDING_MODEL", "nomic-embed-text")

# LLM Judge settings
CHUNKING_JUDGE_MODEL = os.getenv("CHUNKING_JUDGE_MODEL", "gemma3:4b")
RETRIEVAL_JUDGE_MODEL = os.getenv("RETRIEVAL_JUDGE_MODEL", "gemma3:4b")
USE_CHUNKING_JUDGE = os.getenv("USE_CHUNKING_JUDGE", "True").lower() == "true"
USE_RETRIEVAL_JUDGE = os.getenv("USE_RETRIEVAL_JUDGE", "True").lower() == "true"

# LangGraph RAG Agent settings
LANGGRAPH_RAG_MODEL = os.getenv("LANGGRAPH_RAG_MODEL", "gemma3:4b")
USE_LANGGRAPH_RAG = os.getenv("USE_LANGGRAPH_RAG", "True").lower() == "true"
USE_ENHANCED_LANGGRAPH_RAG = os.getenv("USE_ENHANCED_LANGGRAPH_RAG", "True").lower() == "true"

# Document settings
UPLOAD_DIR = os.getenv("UPLOAD_DIR", str(BASE_DIR / "uploads"))
CHROMA_DB_DIR = os.getenv("CHROMA_DB_DIR", str(BASE_DIR / "chroma_db"))
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))

# Database settings
DATABASE_TYPE = os.getenv("DATABASE_TYPE", "postgresql")
DATABASE_USER = os.getenv("DATABASE_USER", "postgres")
DATABASE_PASSWORD = os.getenv("DATABASE_PASSWORD", "postgres")
DATABASE_HOST = os.getenv("DATABASE_HOST", "localhost")
DATABASE_PORT = os.getenv("DATABASE_PORT", "5432")
DATABASE_NAME = os.getenv("DATABASE_NAME", "metis_rag")

# Handle SQLite URLs differently
if DATABASE_TYPE.startswith("sqlite"):
    DATABASE_URL = os.getenv("DATABASE_URL", f"sqlite+aiosqlite:///./test.db")
elif DATABASE_TYPE == "postgresql":
    # Always use asyncpg for PostgreSQL
    db_url = os.getenv("DATABASE_URL")
    if db_url and "+asyncpg" not in db_url and db_url.startswith("postgresql"):
        # Replace the URL with one that includes asyncpg and credentials
        if "localhost" in db_url and "@" not in db_url:
            # URL is missing credentials, add them
            DATABASE_URL = f"postgresql+asyncpg://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_NAME}"
            print(f"Modified DATABASE_URL to include asyncpg and credentials: {DATABASE_URL}")
        else:
            # Just add asyncpg
            DATABASE_URL = db_url.replace("postgresql", "postgresql+asyncpg", 1)
            print(f"Modified DATABASE_URL to include asyncpg: {DATABASE_URL}")
    else:
        DATABASE_URL = os.getenv(
            "DATABASE_URL",
            f"postgresql+asyncpg://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_NAME}"
        )
else:
    DATABASE_URL = os.getenv(
        "DATABASE_URL",
        f"{DATABASE_TYPE}://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_NAME}"
    )

print(f"DATABASE_URL after default construction: {DATABASE_URL}")  # Debug print
DATABASE_POOL_SIZE = int(os.getenv("DATABASE_POOL_SIZE", "5"))
DATABASE_MAX_OVERFLOW = int(os.getenv("DATABASE_MAX_OVERFLOW", "10"))

# Security settings
CORS_ORIGINS = os.getenv("CORS_ORIGINS", "*").split(",")
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", "30"))
REFRESH_TOKEN_EXPIRE_DAYS = int(os.getenv("REFRESH_TOKEN_EXPIRE_DAYS", "7"))
TOKEN_URL = f"{API_V1_STR}/auth/token"
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
RATE_LIMITING_ENABLED = os.getenv("RATE_LIMITING_ENABLED", "True").lower() == "true"
JWT_AUDIENCE = os.getenv("JWT_AUDIENCE", "metis-rag-api")
JWT_ISSUER = os.getenv("JWT_ISSUER", "metis-rag")

# Email settings
SMTP_SERVER = os.getenv("SMTP_SERVER", "smtp.gmail.com")
SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
SMTP_USERNAME = os.getenv("SMTP_USERNAME", "")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD", "")
SMTP_SENDER = os.getenv("SMTP_SENDER", "noreply@metisrag.com")
SMTP_USE_TLS = os.getenv("SMTP_USE_TLS", "True").lower() == "true"
EMAIL_ENABLED = os.getenv("EMAIL_ENABLED", "False").lower() == "true"
BASE_URL = os.getenv("BASE_URL", "http://localhost:8000")

# Mem0 settings
MEM0_ENDPOINT = os.getenv("MEM0_ENDPOINT", "http://localhost:8050")
MEM0_API_KEY = os.getenv("MEM0_API_KEY", None)
USE_MEM0 = os.getenv("USE_MEM0", "True").lower() == "true"

# Make sure upload directory exists
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(CHROMA_DB_DIR, exist_ok=True)

# Create a settings object for easy access
SETTINGS = SimpleNamespace(
    version="0.1.0",
    api_v1_str=API_V1_STR,
    project_name=PROJECT_NAME,
    
    # Ollama settings
    ollama_base_url=OLLAMA_BASE_URL,
    default_model=DEFAULT_MODEL,
    default_embedding_model=DEFAULT_EMBEDDING_MODEL,
    
    # LLM Judge settings
    chunking_judge_model=CHUNKING_JUDGE_MODEL,
    retrieval_judge_model=RETRIEVAL_JUDGE_MODEL,
    use_chunking_judge=USE_CHUNKING_JUDGE,
    use_retrieval_judge=USE_RETRIEVAL_JUDGE,
    
    # LangGraph RAG Agent settings
    langgraph_rag_model=LANGGRAPH_RAG_MODEL,
    use_langgraph_rag=USE_LANGGRAPH_RAG,
    use_enhanced_langgraph_rag=USE_ENHANCED_LANGGRAPH_RAG,
    
    # Document settings
    upload_dir=UPLOAD_DIR,
    chroma_db_dir=CHROMA_DB_DIR,
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    
    # Database settings
    database_type=DATABASE_TYPE,
    database_user=DATABASE_USER,
    database_password=DATABASE_PASSWORD,
    database_host=DATABASE_HOST,
    database_port=DATABASE_PORT,
    database_name=DATABASE_NAME,
    database_url=DATABASE_URL,
    database_pool_size=DATABASE_POOL_SIZE,
    database_max_overflow=DATABASE_MAX_OVERFLOW,
    
    # Security settings
    cors_origins=CORS_ORIGINS,
    secret_key=SECRET_KEY,
    algorithm=ALGORITHM,
    access_token_expire_minutes=ACCESS_TOKEN_EXPIRE_MINUTES,
    refresh_token_expire_days=REFRESH_TOKEN_EXPIRE_DAYS,
    token_url=TOKEN_URL,
    redis_url=REDIS_URL,
    rate_limiting_enabled=RATE_LIMITING_ENABLED,
    jwt_audience=JWT_AUDIENCE,
    jwt_issuer=JWT_ISSUER,
    
    # Email settings
    smtp_server=SMTP_SERVER,
    smtp_port=SMTP_PORT,
    smtp_username=SMTP_USERNAME,
    smtp_password=SMTP_PASSWORD,
    smtp_sender=SMTP_SENDER,
    smtp_tls=SMTP_USE_TLS,
    email_enabled=EMAIL_ENABLED,
    email_sender=SMTP_SENDER,
    base_url=BASE_URL,
    
    # Mem0 settings
    mem0_endpoint=MEM0_ENDPOINT,
    mem0_api_key=MEM0_API_KEY,
    use_mem0=USE_MEM0
)

print(f"Final DATABASE_URL in settings: {SETTINGS.database_url}")  # Debug print

================
File: app/core/email.py
================
import logging
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import List, Optional

from app.core.config import SETTINGS

logger = logging.getLogger(__name__)

async def send_email(
    recipient_email: str,
    subject: str,
    body: str,
    cc: Optional[List[str]] = None,
    bcc: Optional[List[str]] = None,
    html_body: Optional[str] = None
) -> bool:
    """
    Send an email
    
    Args:
        recipient_email: Recipient email address
        subject: Email subject
        body: Email body (plain text)
        cc: CC recipients
        bcc: BCC recipients
        html_body: Email body (HTML)
        
    Returns:
        True if email was sent successfully, False otherwise
    """
    # In development mode, just log the email
    if not SETTINGS.email_enabled:
        logger.info(f"Email would be sent to {recipient_email}")
        logger.info(f"Subject: {subject}")
        logger.info(f"Body: {body}")
        return True
    
    # Create message
    message = MIMEMultipart("alternative")
    message["Subject"] = subject
    message["From"] = SETTINGS.email_sender
    message["To"] = recipient_email
    
    # Add CC recipients
    if cc:
        message["Cc"] = ", ".join(cc)
    
    # Add BCC recipients
    if bcc:
        message["Bcc"] = ", ".join(bcc)
    
    # Add plain text body
    message.attach(MIMEText(body, "plain"))
    
    # Add HTML body if provided
    if html_body:
        message.attach(MIMEText(html_body, "html"))
    
    try:
        # Connect to SMTP server
        with smtplib.SMTP(SETTINGS.smtp_server, SETTINGS.smtp_port) as server:
            # Use TLS if enabled
            if SETTINGS.smtp_tls:
                server.starttls()
            
            # Login if credentials are provided
            if SETTINGS.smtp_username and SETTINGS.smtp_password:
                server.login(SETTINGS.smtp_username, SETTINGS.smtp_password)
            
            # Send email
            recipients = [recipient_email]
            if cc:
                recipients.extend(cc)
            if bcc:
                recipients.extend(bcc)
            
            server.sendmail(SETTINGS.email_sender, recipients, message.as_string())
            
            return True
    except Exception as e:
        logger.error(f"Error sending email: {str(e)}")
        return False

================
File: app/core/logging.py
================
import logging
import sys
from typing import Any, Dict, List

# Configure logging
def setup_logging() -> None:
    """
    Configure logging for the application
    """
    logging_config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "default": {
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                "datefmt": "%Y-%m-%d %H:%M:%S",
            },
        },
        "handlers": {
            "console": {
                "level": "INFO",
                "class": "logging.StreamHandler",
                "formatter": "default",
                "stream": sys.stdout,
            },
        },
        "loggers": {
            "app": {"handlers": ["console"], "level": "INFO", "propagate": False},
            "uvicorn": {"handlers": ["console"], "level": "INFO", "propagate": False},
            "fastapi": {"handlers": ["console"], "level": "INFO", "propagate": False},
        },
    }
    
    logging.config.dictConfig(logging_config)

def get_logger(name: str) -> logging.Logger:
    """
    Get a logger instance
    """
    return logging.getLogger(name)

================
File: app/core/permissions.py
================
from typing import List, Optional, Union, Dict, Any
from uuid import UUID
from fastapi import Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.dependencies import get_db
from app.db.repositories.role_repository import RoleRepository
from app.core.security import get_current_user
from app.models.user import User


class RoleChecker:
    """
    Role-based permission checker
    """
    
    def __init__(self, required_roles: List[str] = None, required_permissions: List[str] = None):
        """
        Initialize the role checker
        
        Args:
            required_roles: List of required role names (any one is sufficient)
            required_permissions: List of required permissions (any one is sufficient)
        """
        self.required_roles = required_roles or []
        self.required_permissions = required_permissions or []
    
    async def __call__(
        self,
        current_user: User = Depends(get_current_user),
        db: AsyncSession = Depends(get_db)
    ) -> User:
        """
        Check if the user has the required roles or permissions
        
        Args:
            current_user: Current authenticated user
            db: Database session
            
        Returns:
            Current user if they have the required roles or permissions
            
        Raises:
            HTTPException: If the user doesn't have the required roles or permissions
        """
        # Admin users bypass all permission checks
        if current_user.is_admin:
            return current_user
        
        # If no roles or permissions are required, allow access
        if not self.required_roles and not self.required_permissions:
            return current_user
        
        # Check roles and permissions
        role_repo = RoleRepository(db)
        
        # Check if user has any of the required roles
        for role_name in self.required_roles:
            if await role_repo.user_has_role(current_user.id, role_name):
                return current_user
        
        # Check if user has any of the required permissions
        for permission in self.required_permissions:
            if await role_repo.user_has_permission(current_user.id, permission):
                return current_user
        
        # If we get here, the user doesn't have the required roles or permissions
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to access this resource"
        )


# Convenience functions for common permission checks

def has_role(role_name: str):
    """
    Check if the user has a specific role
    
    Args:
        role_name: Role name
        
    Returns:
        RoleChecker dependency
    """
    return RoleChecker(required_roles=[role_name])


def has_permission(permission: str):
    """
    Check if the user has a specific permission
    
    Args:
        permission: Permission name
        
    Returns:
        RoleChecker dependency
    """
    return RoleChecker(required_permissions=[permission])


def has_any_role(roles: List[str]):
    """
    Check if the user has any of the specified roles
    
    Args:
        roles: List of role names
        
    Returns:
        RoleChecker dependency
    """
    return RoleChecker(required_roles=roles)


def has_any_permission(permissions: List[str]):
    """
    Check if the user has any of the specified permissions
    
    Args:
        permissions: List of permission names
        
    Returns:
        RoleChecker dependency
    """
    return RoleChecker(required_permissions=permissions)


def has_role_or_permission(role_name: str, permission: str):
    """
    Check if the user has a specific role or permission
    
    Args:
        role_name: Role name
        permission: Permission name
        
    Returns:
        RoleChecker dependency
    """
    return RoleChecker(required_roles=[role_name], required_permissions=[permission])


# Common role names
ROLE_ADMIN = "admin"
ROLE_EDITOR = "editor"
ROLE_VIEWER = "viewer"

# Common permission names
PERMISSION_READ = "read"
PERMISSION_WRITE = "write"
PERMISSION_DELETE = "delete"
PERMISSION_SHARE = "share"
PERMISSION_MANAGE_USERS = "manage_users"
PERMISSION_MANAGE_ROLES = "manage_roles"

================
File: app/core/rate_limit.py
================
"""
Rate limiting implementation for the application.
This module provides rate limiting functionality to protect against brute force attacks
and other forms of abuse.
"""

import logging
from fastapi import Request, Response
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter
import redis.asyncio as redis
from typing import Callable, Optional
import time

from app.core.config import SETTINGS

# Setup logging
logger = logging.getLogger("app.core.rate_limit")

# Rate limit configurations
LOGIN_RATE_LIMIT = "5/minute"  # 5 login attempts per minute per IP
API_GENERAL_RATE_LIMIT = "60/minute"  # 60 API requests per minute per IP
SENSITIVE_ENDPOINTS_RATE_LIMIT = "10/minute"  # 10 requests per minute for sensitive endpoints

# IP ban threshold (number of rate limit violations before temporary ban)
IP_BAN_THRESHOLD = 10
IP_BAN_DURATION = 3600  # 1 hour in seconds

# Redis key prefixes
RATE_LIMIT_VIOLATIONS_PREFIX = "rate_limit_violations:"
IP_BAN_PREFIX = "ip_ban:"


async def setup_rate_limiting():
    """
    Initialize the rate limiter with Redis
    """
    try:
        redis_url = SETTINGS.redis_url or "redis://localhost:6379/0"
        redis_instance = redis.from_url(redis_url, encoding="utf-8", decode_responses=True)
        await FastAPILimiter.init(redis_instance)
        logger.info("Rate limiting initialized with Redis")
        return True
    except Exception as e:
        logger.error(f"Failed to initialize rate limiting: {str(e)}")
        return False


def login_rate_limit():
    """
    Rate limiter for login endpoints
    """
    return RateLimiter(times=5, seconds=60, callback=login_rate_limit_callback)


def api_general_rate_limit():
    """
    General rate limiter for API endpoints
    """
    return RateLimiter(times=60, seconds=60)


def sensitive_endpoints_rate_limit():
    """
    Rate limiter for sensitive endpoints (password reset, etc.)
    """
    return RateLimiter(times=10, seconds=60)


async def login_rate_limit_callback(request: Request, response: Response, pexpire: int):
    """
    Callback for login rate limit violations
    Logs the violation and increments the violation counter for the IP
    """
    client_ip = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("user-agent", "unknown")
    
    # Log the rate limit violation
    logger.warning(
        f"Rate limit exceeded for login endpoint. "
        f"IP: {client_ip}, "
        f"User-Agent: {user_agent}, "
        f"Reset in {pexpire/1000:.1f} seconds"
    )
    
    # Increment violation counter in Redis
    if hasattr(FastAPILimiter, "redis"):
        violation_key = f"{RATE_LIMIT_VIOLATIONS_PREFIX}{client_ip}"
        try:
            # Increment the counter and set expiry
            await FastAPILimiter.redis.incr(violation_key)
            await FastAPILimiter.redis.expire(violation_key, 86400)  # 24 hours
            
            # Check if we should ban this IP
            violations = int(await FastAPILimiter.redis.get(violation_key) or 0)
            if violations >= IP_BAN_THRESHOLD:
                ban_key = f"{IP_BAN_PREFIX}{client_ip}"
                await FastAPILimiter.redis.set(ban_key, time.time(), ex=IP_BAN_DURATION)
                logger.warning(f"IP {client_ip} has been temporarily banned due to excessive rate limit violations")
        except Exception as e:
            logger.error(f"Error tracking rate limit violations: {str(e)}")


async def check_ip_ban(request: Request) -> bool:
    """
    Check if an IP is banned
    Returns True if the IP is banned, False otherwise
    """
    # If rate limiting is disabled or not initialized, skip the check
    if not hasattr(FastAPILimiter, "redis") or FastAPILimiter.redis is None or not SETTINGS.rate_limiting_enabled:
        return False
        
    client_ip = request.client.host if request.client else "unknown"
    ban_key = f"{IP_BAN_PREFIX}{client_ip}"
    
    try:
        banned_until = await FastAPILimiter.redis.get(ban_key)
        if banned_until:
            # IP is banned
            logger.info(f"Blocked request from banned IP: {client_ip}")
            return True
    except Exception as e:
        logger.error(f"Error checking IP ban status: {str(e)}")
    
    return False


async def ip_ban_middleware(request: Request, call_next: Callable):
    """
    Middleware to check if an IP is banned before processing the request
    """
    is_banned = await check_ip_ban(request)
    if is_banned:
        from fastapi.responses import JSONResponse
        return JSONResponse(
            status_code=403,
            content={"detail": "Too many failed attempts. Please try again later."}
        )
    
    return await call_next(request)

================
File: app/core/security_alerts.py
================
"""
Security alerts system for the application.
This module provides functionality to detect and alert on suspicious security events.
"""

import logging
import json
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime
from typing import Dict, Any, List, Optional
import os
from pathlib import Path

from app.core.config import SETTINGS

# Setup logging
logger = logging.getLogger("app.core.security_alerts")

# Security events log file
SECURITY_LOG_DIR = Path(os.getenv("SECURITY_LOG_DIR", "logs/security"))
SECURITY_LOG_FILE = SECURITY_LOG_DIR / "security_events.log"

# Ensure the security log directory exists
os.makedirs(SECURITY_LOG_DIR, exist_ok=True)

# Alert thresholds
LOGIN_FAILURE_THRESHOLD = 5  # Number of failed logins before alerting
SUSPICIOUS_IP_THRESHOLD = 3  # Number of different usernames from same IP before alerting
TIME_WINDOW_MINUTES = 10     # Time window for counting events (in minutes)


class SecurityEvent:
    """
    Represents a security event in the system
    """
    def __init__(
        self,
        event_type: str,
        severity: str,
        source_ip: str,
        username: Optional[str] = None,
        user_agent: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        timestamp: Optional[datetime] = None
    ):
        self.event_type = event_type
        self.severity = severity
        self.source_ip = source_ip
        self.username = username
        self.user_agent = user_agent
        self.details = details or {}
        self.timestamp = timestamp or datetime.utcnow()
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the event to a dictionary
        """
        return {
            "event_type": self.event_type,
            "severity": self.severity,
            "source_ip": self.source_ip,
            "username": self.username,
            "user_agent": self.user_agent,
            "details": self.details,
            "timestamp": self.timestamp.isoformat()
        }
    
    def to_json(self) -> str:
        """
        Convert the event to a JSON string
        """
        return json.dumps(self.to_dict())
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'SecurityEvent':
        """
        Create a SecurityEvent from a dictionary
        """
        timestamp = datetime.fromisoformat(data.pop("timestamp"))
        return cls(timestamp=timestamp, **data)


def log_security_event(event: SecurityEvent) -> None:
    """
    Log a security event to the security log file
    """
    try:
        with open(SECURITY_LOG_FILE, "a") as f:
            f.write(f"{event.to_json()}\n")
        
        logger.info(f"Security event logged: {event.event_type} from {event.source_ip}")
        
        # Check if this event should trigger an alert
        check_alert_triggers(event)
    except Exception as e:
        logger.error(f"Error logging security event: {str(e)}")


def get_recent_events(event_type: Optional[str] = None, minutes: int = TIME_WINDOW_MINUTES) -> List[SecurityEvent]:
    """
    Get recent security events from the log file
    """
    events = []
    now = datetime.utcnow()
    
    try:
        if not os.path.exists(SECURITY_LOG_FILE):
            return []
        
        with open(SECURITY_LOG_FILE, "r") as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    event = SecurityEvent.from_dict(data)
                    
                    # Check if the event is within the time window
                    event_time = event.timestamp
                    time_diff = (now - event_time).total_seconds() / 60
                    
                    if time_diff <= minutes:
                        if event_type is None or event.event_type == event_type:
                            events.append(event)
                except Exception as e:
                    logger.error(f"Error parsing security event: {str(e)}")
    except Exception as e:
        logger.error(f"Error reading security events: {str(e)}")
    
    return events


def check_alert_triggers(event: SecurityEvent) -> None:
    """
    Check if the event should trigger an alert
    """
    # Check for failed login attempts
    if event.event_type == "failed_login":
        check_failed_login_threshold(event)
    
    # Check for suspicious IP activity
    if event.event_type in ["failed_login", "successful_login"]:
        check_suspicious_ip_activity(event)
    
    # Check for credentials in URL
    if event.event_type == "credentials_in_url":
        # Always alert on credentials in URL
        send_security_alert(
            "Credentials detected in URL",
            f"Credentials were detected in a URL from IP {event.source_ip}",
            event
        )


def check_failed_login_threshold(event: SecurityEvent) -> None:
    """
    Check if the number of failed logins exceeds the threshold
    """
    # Get recent failed login events for this username or IP
    username_events = []
    ip_events = []
    
    if event.username:
        username_events = [
            e for e in get_recent_events("failed_login")
            if e.username == event.username
        ]
    
    ip_events = [
        e for e in get_recent_events("failed_login")
        if e.source_ip == event.source_ip
    ]
    
    # Check thresholds
    if len(username_events) >= LOGIN_FAILURE_THRESHOLD:
        send_security_alert(
            f"Multiple failed login attempts for user {event.username}",
            f"There have been {len(username_events)} failed login attempts for user {event.username} in the last {TIME_WINDOW_MINUTES} minutes.",
            event
        )
    
    if len(ip_events) >= LOGIN_FAILURE_THRESHOLD:
        send_security_alert(
            f"Multiple failed login attempts from IP {event.source_ip}",
            f"There have been {len(ip_events)} failed login attempts from IP {event.source_ip} in the last {TIME_WINDOW_MINUTES} minutes.",
            event
        )


def check_suspicious_ip_activity(event: SecurityEvent) -> None:
    """
    Check for suspicious IP activity (multiple usernames from same IP)
    """
    # Get all login events (failed and successful) from this IP
    login_events = [
        e for e in get_recent_events()
        if e.source_ip == event.source_ip and e.event_type in ["failed_login", "successful_login"]
    ]
    
    # Count unique usernames
    usernames = set(e.username for e in login_events if e.username)
    
    if len(usernames) >= SUSPICIOUS_IP_THRESHOLD:
        send_security_alert(
            f"Multiple users from same IP {event.source_ip}",
            f"There have been login attempts for {len(usernames)} different users from IP {event.source_ip} in the last {TIME_WINDOW_MINUTES} minutes.",
            event
        )


def send_security_alert(subject: str, message: str, event: SecurityEvent) -> None:
    """
    Send a security alert via email and log it
    """
    # Log the alert
    logger.warning(f"SECURITY ALERT: {subject} - {message}")
    
    # Send email alert if email is configured
    if SETTINGS.email_enabled:
        try:
            send_email_alert(subject, message, event)
        except Exception as e:
            logger.error(f"Error sending email alert: {str(e)}")


def send_email_alert(subject: str, message: str, event: SecurityEvent) -> None:
    """
    Send an email alert
    """
    # Create email
    email = MIMEMultipart()
    email["From"] = SETTINGS.email_sender
    email["To"] = SETTINGS.smtp_username  # Send to admin email
    email["Subject"] = f"SECURITY ALERT: {subject}"
    
    # Create email body
    body = f"""
    <html>
    <body>
        <h2>Security Alert</h2>
        <p><strong>{subject}</strong></p>
        <p>{message}</p>
        <h3>Event Details:</h3>
        <ul>
            <li><strong>Event Type:</strong> {event.event_type}</li>
            <li><strong>Severity:</strong> {event.severity}</li>
            <li><strong>Source IP:</strong> {event.source_ip}</li>
            <li><strong>Username:</strong> {event.username or 'N/A'}</li>
            <li><strong>User Agent:</strong> {event.user_agent or 'N/A'}</li>
            <li><strong>Timestamp:</strong> {event.timestamp.isoformat()}</li>
        </ul>
        <h3>Additional Details:</h3>
        <pre>{json.dumps(event.details, indent=2)}</pre>
    </body>
    </html>
    """
    
    email.attach(MIMEText(body, "html"))
    
    # Send email
    with smtplib.SMTP(SETTINGS.smtp_server, SETTINGS.smtp_port) as server:
        if SETTINGS.smtp_tls:
            server.starttls()
        
        if SETTINGS.smtp_username and SETTINGS.smtp_password:
            server.login(SETTINGS.smtp_username, SETTINGS.smtp_password)
        
        server.send_message(email)

================
File: app/core/security.py
================
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, Union
import logging
from fastapi import FastAPI, Request, Response, Depends, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from jose import JWTError, jwt
from passlib.context import CryptContext
from pydantic import BaseModel
from uuid import UUID

from app.core.config import CORS_ORIGINS, SETTINGS
from app.core.security_alerts import SecurityEvent, log_security_event

# Setup logging
logger = logging.getLogger("app.core.security")

# Password hashing
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# OAuth2 scheme
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{SETTINGS.api_v1_str}/auth/token")

# Token models
class Token(BaseModel):
    """Token response model"""
    access_token: str
    token_type: str
    expires_in: int
    refresh_token: Optional[str] = None

class TokenData(BaseModel):
    """Token data model for internal use"""
    username: Optional[str] = None
    user_id: Optional[str] = None
    exp: Optional[datetime] = None
    token_type: Optional[str] = None

class RefreshToken(BaseModel):
    """Refresh token request model"""
    refresh_token: str

def setup_security(app: FastAPI) -> None:
    """
    Setup security middleware for the application
    """
    # Setup CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=CORS_ORIGINS,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        response: Response = await call_next(request)
        
        # Add security headers
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        
        # Add Referrer-Policy to prevent leaking URL parameters to external sites
        response.headers["Referrer-Policy"] = "same-origin"
        
        # Add HSTS header for HTTPS enforcement
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        
        # Add Cache-Control for sensitive pages
        path = request.url.path
        if path in ["/login", "/register", "/forgot-password", "/reset-password"]:
            response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            response.headers["Pragma"] = "no-cache"
            response.headers["Expires"] = "0"
        
        # Enhanced Content Security Policy
        response.headers["Content-Security-Policy"] = (
            "default-src 'self'; "
            "script-src 'self'; "
            "style-src 'self'; "
            "img-src 'self' data:; "
            "connect-src 'self';"
            "form-action 'self';"
        )
        
        return response

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """
    Verify a password against a hash
    
    Args:
        plain_password: The plain text password
        hashed_password: The hashed password
        
    Returns:
        True if the password matches the hash, False otherwise
    """
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """
    Hash a password using bcrypt
    
    Args:
        password: The plain text password
        
    Returns:
        The hashed password
    """
    return pwd_context.hash(password)

def create_access_token(data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
    """
    Create a JWT access token
    
    Args:
        data: The data to encode in the token
        expires_delta: Optional expiration time delta
        
    Returns:
        The encoded JWT token
    """
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=SETTINGS.access_token_expire_minutes)
    
    to_encode.update({
        "exp": expire,
        "iat": datetime.utcnow(),
        "token_type": "access"
    })
    
    encoded_jwt = jwt.encode(to_encode, SETTINGS.secret_key, algorithm=SETTINGS.algorithm)
    
    return encoded_jwt

def create_refresh_token(data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
    """
    Create a JWT refresh token
    
    Args:
        data: The data to encode in the token
        expires_delta: Optional expiration time delta
        
    Returns:
        The encoded JWT refresh token
    """
    to_encode = data.copy()
    
    # Refresh tokens should have longer expiration
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        # Default to 7 days for refresh tokens
        expire = datetime.utcnow() + timedelta(days=7)
    
    to_encode.update({
        "exp": expire,
        "iat": datetime.utcnow(),
        "token_type": "refresh"
    })
    
    encoded_jwt = jwt.encode(to_encode, SETTINGS.secret_key, algorithm=SETTINGS.algorithm)
    
    return encoded_jwt

def decode_token(token: str) -> Dict[str, Any]:
    """
    Decode a JWT token
    
    Args:
        token: The JWT token to decode
        
    Returns:
        The decoded token payload
        
    Raises:
        JWTError: If the token is invalid
    """
    return jwt.decode(
        token,
        SETTINGS.secret_key,
        algorithms=[SETTINGS.algorithm],
        options={"verify_aud": False}  # Don't verify audience claim for now
    )

def verify_refresh_token(refresh_token: str) -> Optional[Dict[str, Any]]:
    """
    Verify a refresh token and return the payload if valid
    
    Args:
        refresh_token: The refresh token to verify
        
    Returns:
        The decoded token payload if valid, None otherwise
    """
    try:
        # Use direct jwt.decode instead of decode_token to specify options
        payload = jwt.decode(
            refresh_token,
            SETTINGS.secret_key,
            algorithms=[SETTINGS.algorithm],
            options={"verify_aud": False}  # Don't verify audience claim for now
        )
        
        # Check if it's a refresh token
        if payload.get("token_type") != "refresh":
            logger.warning("Invalid token type for refresh token")
            return None
        
        # Check required claims
        if "sub" not in payload or "user_id" not in payload:
            logger.warning("Missing required claims in refresh token")
            return None
        
        return payload
    except JWTError as e:
        logger.warning(f"Invalid refresh token: {str(e)}")
        return None

async def get_current_user(token: str = Depends(oauth2_scheme)):
    """
    Get the current user from a JWT token
    
    Args:
        token: The JWT token
        
    Returns:
        The user if the token is valid
        
    Raises:
        HTTPException: If the token is invalid or the user is not found
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        # Decode token
        payload = jwt.decode(
            token,
            SETTINGS.secret_key,
            algorithms=[SETTINGS.algorithm],
            options={"verify_aud": False}  # Don't verify audience claim for now
        )
        username: str = payload.get("sub")
        user_id: str = payload.get("user_id")
        token_type: str = payload.get("token_type")
        
        # Validate token data
        if username is None or user_id is None:
            logger.warning("Missing username or user_id in token")
            raise credentials_exception
        
        # Ensure it's an access token
        if token_type != "access":
            logger.warning(f"Invalid token type: {token_type}")
            raise credentials_exception
        
        token_data = TokenData(
            username=username,
            user_id=user_id,
            exp=datetime.fromtimestamp(payload.get("exp")),
            token_type=token_type
        )
    except JWTError as e:
        logger.warning(f"JWT validation error: {str(e)}")
        raise credentials_exception
    
    # Get user from database
    from app.db.dependencies import get_user_repository
    from app.db.session import AsyncSessionLocal
    
    db = AsyncSessionLocal()
    try:
        user_repository = await get_user_repository(db)
        user = await user_repository.get_by_username(token_data.username)
        
        if user is None:
            logger.warning(f"User not found: {token_data.username}")
            raise credentials_exception
        
        if not user.is_active:
            logger.warning(f"Inactive user attempted access: {token_data.username}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Inactive user"
            )
        
        return user
    finally:
        await db.close()

async def get_current_active_user(current_user = Depends(get_current_user)):
    """
    Get the current active user
    
    Args:
        current_user: The current user from the token
        
    Returns:
        The user if active
        
    Raises:
        HTTPException: If the user is inactive
    """
    if not current_user.is_active:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Inactive user"
        )
    
    return current_user

async def get_current_admin_user(current_user = Depends(get_current_user)):
    """
    Get the current admin user
    
    Args:
        current_user: The current user from the token
        
    Returns:
        The user if admin
        
    Raises:
        HTTPException: If the user is not an admin
    """
    if not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not an admin user"
        )
    
    return current_user

async def get_current_user_optional(token: Optional[str] = Depends(oauth2_scheme)):
    """
    Get the current user from a JWT token, but return None if no valid token
    
    Args:
        token: The JWT token (optional)
        
    Returns:
        The user if the token is valid, None otherwise
    """
    if not token:
        return None
        
    try:
        # Decode token
        payload = jwt.decode(
            token,
            SETTINGS.secret_key,
            algorithms=[SETTINGS.algorithm],
            options={"verify_aud": False}  # Don't verify audience claim for now
        )
        username: str = payload.get("sub")
        user_id: str = payload.get("user_id")
        token_type: str = payload.get("token_type")
        
        # Validate token data
        if username is None or user_id is None or token_type != "access":
            logger.warning("Invalid token data")
            return None
            
        # Get user from database
        from app.db.dependencies import get_user_repository
        from app.db.session import AsyncSessionLocal
        
        db = AsyncSessionLocal()
        try:
            user_repository = await get_user_repository(db)
            user = await user_repository.get_by_username(username)
            
            if user is None or not user.is_active:
                logger.warning(f"User not found or inactive: {username}")
                return None
                
            return user
        finally:
            await db.close()
    except JWTError as e:
        logger.warning(f"JWT validation error in optional auth: {str(e)}")
        return None

================
File: app/db/repositories/__init__.py
================
# Repository package

================
File: app/db/repositories/analytics_repository.py
================
from typing import List, Optional, Dict, Any, Union
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import func, desc, cast, Date, extract

from app.db.models import AnalyticsQuery, Document, Chunk, Message, Citation
from app.db.repositories.base import BaseRepository


class AnalyticsRepository(BaseRepository[AnalyticsQuery]):
    """
    Repository for AnalyticsQuery model
    """
    
    def __init__(self, session: Session):
        super().__init__(session, AnalyticsQuery)
    
    def log_query(self, 
                 query: str, 
                 model: Optional[str] = None, 
                 use_rag: bool = True, 
                 response_time_ms: Optional[float] = None, 
                 token_count: Optional[int] = None, 
                 document_ids: Optional[List[str]] = None, 
                 query_type: Optional[str] = None, 
                 successful: bool = True) -> AnalyticsQuery:
        """
        Log a query for analytics
        
        Args:
            query: Query text
            model: Model used
            use_rag: Whether RAG was used
            response_time_ms: Response time in milliseconds
            token_count: Token count
            document_ids: List of document IDs used
            query_type: Query type (simple, complex, agentic)
            successful: Whether the query was successful
            
        Returns:
            Created analytics query
        """
        analytics_query = AnalyticsQuery(
            query=query,
            model=model,
            use_rag=use_rag,
            timestamp=datetime.utcnow(),
            response_time_ms=response_time_ms,
            token_count=token_count,
            document_ids=document_ids or [],
            query_type=query_type,
            successful=successful
        )
        
        self.session.add(analytics_query)
        self.session.commit()
        self.session.refresh(analytics_query)
        return analytics_query
    
    def get_query_count_by_date(self, days: int = 30) -> List[Dict[str, Any]]:
        """
        Get query count by date
        
        Args:
            days: Number of days to include
            
        Returns:
            List of dictionaries with date and count
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        query_result = (
            self.session.query(
                cast(AnalyticsQuery.timestamp, Date).label("date"),
                func.count(AnalyticsQuery.id).label("count")
            )
            .filter(AnalyticsQuery.timestamp >= start_date)
            .group_by(cast(AnalyticsQuery.timestamp, Date))
            .order_by(cast(AnalyticsQuery.timestamp, Date))
            .all()
        )
        
        return [{"date": str(row.date), "count": row.count} for row in query_result]
    
    def get_average_response_time(self, days: int = 30) -> Dict[str, float]:
        """
        Get average response time
        
        Args:
            days: Number of days to include
            
        Returns:
            Dictionary with average response times
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        avg_time_overall = (
            self.session.query(func.avg(AnalyticsQuery.response_time_ms))
            .filter(AnalyticsQuery.timestamp >= start_date)
            .scalar() or 0
        )
        
        avg_time_with_rag = (
            self.session.query(func.avg(AnalyticsQuery.response_time_ms))
            .filter(AnalyticsQuery.timestamp >= start_date)
            .filter(AnalyticsQuery.use_rag == True)
            .scalar() or 0
        )
        
        avg_time_without_rag = (
            self.session.query(func.avg(AnalyticsQuery.response_time_ms))
            .filter(AnalyticsQuery.timestamp >= start_date)
            .filter(AnalyticsQuery.use_rag == False)
            .scalar() or 0
        )
        
        return {
            "overall": float(avg_time_overall),
            "with_rag": float(avg_time_with_rag),
            "without_rag": float(avg_time_without_rag)
        }
    
    def get_query_type_distribution(self, days: int = 30) -> List[Dict[str, Any]]:
        """
        Get query type distribution
        
        Args:
            days: Number of days to include
            
        Returns:
            List of dictionaries with query type and count
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        query_result = (
            self.session.query(
                AnalyticsQuery.query_type,
                func.count(AnalyticsQuery.id).label("count")
            )
            .filter(AnalyticsQuery.timestamp >= start_date)
            .filter(AnalyticsQuery.query_type.isnot(None))
            .group_by(AnalyticsQuery.query_type)
            .order_by(desc("count"))
            .all()
        )
        
        return [{"query_type": row.query_type, "count": row.count} for row in query_result]
    
    def get_model_usage_statistics(self, days: int = 30) -> List[Dict[str, Any]]:
        """
        Get model usage statistics
        
        Args:
            days: Number of days to include
            
        Returns:
            List of dictionaries with model and usage statistics
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        query_result = (
            self.session.query(
                AnalyticsQuery.model,
                func.count(AnalyticsQuery.id).label("query_count"),
                func.avg(AnalyticsQuery.response_time_ms).label("avg_response_time"),
                func.avg(AnalyticsQuery.token_count).label("avg_token_count"),
                func.sum(AnalyticsQuery.token_count).label("total_token_count")
            )
            .filter(AnalyticsQuery.timestamp >= start_date)
            .filter(AnalyticsQuery.model.isnot(None))
            .group_by(AnalyticsQuery.model)
            .order_by(desc("query_count"))
            .all()
        )
        
        return [
            {
                "model": row.model,
                "query_count": row.query_count,
                "avg_response_time": float(row.avg_response_time or 0),
                "avg_token_count": float(row.avg_token_count or 0),
                "total_token_count": row.total_token_count or 0
            }
            for row in query_result
        ]
    
    def get_success_rate(self, days: int = 30) -> Dict[str, Any]:
        """
        Get query success rate
        
        Args:
            days: Number of days to include
            
        Returns:
            Dictionary with success rate statistics
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        total_queries = (
            self.session.query(func.count(AnalyticsQuery.id))
            .filter(AnalyticsQuery.timestamp >= start_date)
            .scalar() or 0
        )
        
        successful_queries = (
            self.session.query(func.count(AnalyticsQuery.id))
            .filter(AnalyticsQuery.timestamp >= start_date)
            .filter(AnalyticsQuery.successful == True)
            .scalar() or 0
        )
        
        success_rate = (successful_queries / total_queries) * 100 if total_queries > 0 else 0
        
        return {
            "total_queries": total_queries,
            "successful_queries": successful_queries,
            "failed_queries": total_queries - successful_queries,
            "success_rate": success_rate
        }
    
    def get_document_usage_statistics(self, days: int = 30, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get document usage statistics
        
        Args:
            days: Number of days to include
            limit: Maximum number of documents to return
            
        Returns:
            List of dictionaries with document and usage statistics
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        # This is a more complex query that requires JSON array processing
        # For PostgreSQL, we can use the jsonb_array_elements function
        # This is a simplified version that counts documents in the document_ids array
        
        # In a real implementation, you would need to use a database-specific approach
        # to extract and count elements from the JSON array
        
        # For now, we'll return a placeholder
        return []
    
    def get_hourly_query_distribution(self, days: int = 30) -> List[Dict[str, Any]]:
        """
        Get hourly query distribution
        
        Args:
            days: Number of days to include
            
        Returns:
            List of dictionaries with hour and count
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        query_result = (
            self.session.query(
                extract('hour', AnalyticsQuery.timestamp).label("hour"),
                func.count(AnalyticsQuery.id).label("count")
            )
            .filter(AnalyticsQuery.timestamp >= start_date)
            .group_by(extract('hour', AnalyticsQuery.timestamp))
            .order_by(extract('hour', AnalyticsQuery.timestamp))
            .all()
        )
        
        return [{"hour": int(row.hour), "count": row.count} for row in query_result]

================
File: app/db/repositories/base.py
================
from typing import Generic, TypeVar, Type, List, Optional, Dict, Any, Union
from uuid import UUID
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import asc, desc, func, select, exists
from sqlalchemy.sql import select, delete, update

from app.db.session import Base

# Define a generic type variable for SQLAlchemy models
ModelType = TypeVar("ModelType", bound=Base)

class BaseRepository(Generic[ModelType]):
    """
    Base repository class with common CRUD operations
    """
    
    def __init__(self, session: AsyncSession, model_class: Type[ModelType]):
        self.session = session
        self.model_class = model_class
    
    async def get_by_id(self, id: Union[int, str, UUID]) -> Optional[ModelType]:
        """
        Get a record by ID
        
        Args:
            id: Record ID
            
        Returns:
            Record if found, None otherwise
        """
        stmt = select(self.model_class).where(self.model_class.id == id)
        result = await self.session.execute(stmt)
        return result.scalars().first()
    
    async def get_all(self,
                skip: int = 0,
                limit: int = 100,
                order_by: str = None,
                order_direction: str = "asc") -> List[ModelType]:
        """
        Get all records with pagination and ordering
        
        Args:
            skip: Number of records to skip
            limit: Maximum number of records to return
            order_by: Column to order by
            order_direction: Order direction (asc or desc)
            
        Returns:
            List of records
        """
        stmt = select(self.model_class)
        
        if order_by:
            column = getattr(self.model_class, order_by, None)
            if column:
                if order_direction.lower() == "desc":
                    stmt = stmt.order_by(desc(column))
                else:
                    stmt = stmt.order_by(asc(column))
        
        stmt = stmt.offset(skip).limit(limit)
        result = await self.session.execute(stmt)
        return result.scalars().all()
    
    async def create(self, obj_in: Dict[str, Any]) -> ModelType:
        """
        Create a new record
        
        Args:
            obj_in: Dictionary with record data
            
        Returns:
            Created record
        """
        db_obj = self.model_class(**obj_in)
        self.session.add(db_obj)
        await self.session.commit()
        await self.session.refresh(db_obj)
        return db_obj
    
    async def update(self, id: Union[int, str, UUID], obj_in: Dict[str, Any]) -> Optional[ModelType]:
        """
        Update a record
        
        Args:
            id: Record ID
            obj_in: Dictionary with record data
            
        Returns:
            Updated record if found, None otherwise
        """
        db_obj = await self.get_by_id(id)
        if db_obj:
            for key, value in obj_in.items():
                if hasattr(db_obj, key):
                    setattr(db_obj, key, value)
            
            await self.session.commit()
            await self.session.refresh(db_obj)
            return db_obj
        return None
    
    async def delete(self, id: Union[int, str, UUID]) -> bool:
        """
        Delete a record
        
        Args:
            id: Record ID
            
        Returns:
            True if record was deleted, False otherwise
        """
        db_obj = await self.get_by_id(id)
        if db_obj:
            await self.session.delete(db_obj)
            await self.session.commit()
            return True
        return False
    
    async def count(self) -> int:
        """
        Count all records
        
        Returns:
            Number of records
        """
        stmt = select(func.count()).select_from(self.model_class)
        result = await self.session.execute(stmt)
        return result.scalar()
    
    async def exists(self, id: Union[int, str, UUID]) -> bool:
        """
        Check if a record exists
        
        Args:
            id: Record ID
            
        Returns:
            True if record exists, False otherwise
        """
        stmt = select(exists().where(self.model_class.id == id))
        result = await self.session.execute(stmt)
        return result.scalar()

================
File: app/db/repositories/conversation_repository.py
================
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import func, or_, and_, select, delete
from app.db.models import Conversation, Message, Citation, Document, Chunk

from app.db.models import Conversation, Message, Citation, Document, Chunk
from app.db.repositories.base import BaseRepository


class ConversationRepository(BaseRepository[Conversation]):
    """
    Repository for Conversation model with user context and permission handling
    """
    
    def __init__(self, session: Session, user_id: Optional[UUID] = None):
        super().__init__(session, Conversation)
        self.user_id = user_id
    
    async def create_conversation(self, metadata: Optional[Dict[str, Any]] = None) -> Conversation:
        """
        Create a new conversation
        
        Args:
            metadata: Conversation metadata
            
        Returns:
            Created conversation
        """
        # Initialize metadata if None
        meta = metadata or {}
        
        conversation = Conversation(
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
            conv_metadata=meta,  # Changed to conv_metadata
            message_count=0,
            user_id=self.user_id  # Set user_id directly from context
        )
        
        self.session.add(conversation)
        await self.session.commit()
        await self.session.refresh(conversation)
        return conversation
    
    async def add_message(self,
                   conversation_id: UUID,
                   content: str,
                   role: str,
                   citations: Optional[List[Dict[str, Any]]] = None,
                   token_count: Optional[int] = None) -> Optional[Message]:
        """
        Add a message to a conversation
        
        Args:
            conversation_id: Conversation ID
            content: Message content
            role: Message role (user, assistant, system)
            citations: List of citation data
            token_count: Message token count
            
        Returns:
            Created message if conversation found, None otherwise
        """
        conversation = await self.get_by_id(conversation_id)
        if not conversation:
            return None
        
        # Check if user has permission to add messages to this conversation
        if not self._can_modify_conversation(conversation):
            return None
        
        # Create message
        message = Message(
            conversation_id=conversation_id,
            content=content,
            role=role,
            timestamp=datetime.utcnow(),
            token_count=token_count
        )
        
        self.session.add(message)
        await self.session.flush()  # Flush to get the message ID
        
        # Add citations if provided
        if citations:
            for citation_data in citations:
                citation = Citation(
                    message_id=message.id,
                    document_id=citation_data.get("document_id"),
                    chunk_id=citation_data.get("chunk_id"),
                    relevance_score=citation_data.get("relevance_score"),
                    excerpt=citation_data.get("excerpt"),
                    character_range_start=citation_data.get("character_range_start"),
                    character_range_end=citation_data.get("character_range_end")
                )
                self.session.add(citation)
        
        # Update conversation
        conversation.message_count += 1
        conversation.updated_at = datetime.utcnow()
        
        await self.session.commit()
        await self.session.refresh(message)
        return message
    
    async def get_conversation_with_messages(self, conversation_id: UUID) -> Optional[Conversation]:
        """
        Get a conversation with its messages
        
        Args:
            conversation_id: Conversation ID
            
        Returns:
            Conversation with messages if found, None otherwise
        """
        stmt = select(Conversation).filter(Conversation.id == conversation_id)
        result = await self.session.execute(stmt)
        conversation = result.scalars().first()
        
        # Check if user has permission to view this conversation
        if conversation and not self._can_view_conversation(conversation):
            return None
        
        return conversation
    
    async def get_message(self, message_id: int) -> Optional[Message]:
        """
        Get a message by ID
        
        Args:
            message_id: Message ID
            
        Returns:
            Message if found, None otherwise
        """
        stmt = select(Message).filter(Message.id == message_id)
        result = await self.session.execute(stmt)
        message = result.scalars().first()
        
        # Check if user has permission to view this message
        if message:
            conversation = await self.get_by_id(message.conversation_id)
            if not conversation or not self._can_view_conversation(conversation):
                return None
        
        return message
    
    async def get_message_with_citations(self, message_id: int) -> Optional[Message]:
        """
        Get a message with its citations
        
        Args:
            message_id: Message ID
            
        Returns:
            Message with citations if found, None otherwise
        """
        # Reuse get_message which already has permission checking
        return await self.get_message(message_id)
    
    async def get_recent_conversations(self, limit: int = 10) -> List[Conversation]:
        """
        Get recent conversations for the current user
        
        Args:
            limit: Maximum number of conversations to return
            
        Returns:
            List of recent conversations
        """
        # Filter by user_id if available
        if self.user_id:
            stmt = select(Conversation).filter(
                Conversation.user_id == self.user_id
            ).order_by(
                Conversation.updated_at.desc()
            ).limit(limit)
        else:
            # Return empty list if no user context
            return []
        
        result = await self.session.execute(stmt)
        return result.scalars().all()
    
    async def search_conversations(self, query: str, skip: int = 0, limit: int = 100) -> List[Conversation]:
        """
        Search conversations by message content for the current user
        
        Args:
            query: Search query
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            List of matching conversations
        """
        # Return empty list if no user context
        if not self.user_id:
            return []
        
        # Find messages matching the query
        message_subquery = select(Message.conversation_id).join(
            Conversation, Conversation.id == Message.conversation_id
        ).filter(
            Message.content.ilike(f"%{query}%"),
            Conversation.user_id == self.user_id  # Filter by user_id
        ).distinct().subquery()
        
        # Get conversations with matching messages
        stmt = select(Conversation).join(
            message_subquery, Conversation.id == message_subquery.c.conversation_id
        ).order_by(Conversation.updated_at.desc()).offset(skip).limit(limit)
        
        result = await self.session.execute(stmt)
        return result.scalars().all()
    
    async def update_conversation(self, conversation_id: UUID, metadata: Optional[Dict[str, Any]] = None) -> Optional[Conversation]:
        """
        Update a conversation
        
        Args:
            conversation_id: Conversation ID
            metadata: New metadata
            
        Returns:
            Updated conversation if found, None otherwise
        """
        conversation = await self.get_by_id(conversation_id)
        if not conversation:
            return None
        
        # Check if user has permission to modify this conversation
        if not self._can_modify_conversation(conversation):
            return None
        
        # Update metadata if provided
        if metadata:
            # Get current metadata
            current_metadata = conversation.conv_metadata or {}
            
            # Merge new metadata
            current_metadata = {**current_metadata, **metadata}
            
            # Update conversation metadata
            conversation.conv_metadata = current_metadata
            
        # Update timestamp
        conversation.updated_at = datetime.utcnow()
        
        await self.session.commit()
        await self.session.refresh(conversation)
        return conversation
        
    async def update_conversation_metadata(self, conversation_id: UUID, metadata: Dict[str, Any]) -> Optional[Conversation]:
        """
        Update conversation metadata
        
        Args:
            conversation_id: Conversation ID
            metadata: New metadata
            
        Returns:
            Updated conversation if found, None otherwise
        """
        return await self.update_conversation(conversation_id=conversation_id, metadata=metadata)
    
    async def delete_conversation_with_messages(self, conversation_id: UUID) -> bool:
        """
        Delete a conversation and all its messages
        
        Args:
            conversation_id: Conversation ID
            
        Returns:
            True if conversation was deleted, False otherwise
        """
        conversation = await self.get_by_id(conversation_id)
        if not conversation:
            return False
        
        # Check if user has permission to delete this conversation
        if not self._can_delete_conversation(conversation):
            return False
        
        # Delete all messages (cascade will delete citations)
        stmt = delete(Message).where(Message.conversation_id == conversation_id)
        await self.session.execute(stmt)
        
        # Delete conversation
        await self.session.delete(conversation)
        await self.session.commit()
        return True
    
    async def get_conversation_messages(self,
                                  conversation_id: UUID,
                                  skip: int = 0,
                                  limit: int = 100) -> List[Message]:
        """
        Get messages for a conversation with pagination
        
        Args:
            conversation_id: Conversation ID
            skip: Number of messages to skip
            limit: Maximum number of messages to return
            
        Returns:
            List of messages
        """
        # Check if user has permission to view this conversation
        conversation = await self.get_by_id(conversation_id)
        if not conversation or not self._can_view_conversation(conversation):
            return []
        
        stmt = select(Message).filter(
            Message.conversation_id == conversation_id
        ).order_by(
            Message.timestamp.asc()
        ).offset(skip).limit(limit)
        
        result = await self.session.execute(stmt)
        return result.scalars().all()
    
    async def get_last_user_message(self, conversation_id: UUID) -> Optional[Message]:
        """
        Get the last user message in a conversation
        
        Args:
            conversation_id: Conversation ID
            
        Returns:
            Last user message if found, None otherwise
        """
        # Check if user has permission to view this conversation
        conversation = await self.get_by_id(conversation_id)
        if not conversation or not self._can_view_conversation(conversation):
            return None
        
        stmt = select(Message).filter(
            Message.conversation_id == conversation_id,
            Message.role == "user"
        ).order_by(
            Message.timestamp.desc()
        ).limit(1)
        
        result = await self.session.execute(stmt)
        return result.scalars().first()
    
    async def get_conversations(self, skip: int = 0, limit: int = 100) -> List[Conversation]:
        """
        Get conversations for the current user with pagination
        
        Args:
            skip: Number of conversations to skip
            limit: Maximum number of conversations to return
            
        Returns:
            List of conversations
        """
        # Return empty list if no user context
        if not self.user_id:
            return []
        
        # Filter by user_id
        stmt = select(Conversation).filter(
            Conversation.user_id == self.user_id
        ).order_by(
            Conversation.updated_at.desc()
        ).offset(skip).limit(limit)
        
        result = await self.session.execute(stmt)
        return result.scalars().all()
    
    async def count_conversations(self) -> int:
        """
        Count conversations for the current user
        
        Returns:
            Number of conversations
        """
        # Return 0 if no user context
        if not self.user_id:
            return 0
        
        # Filter by user_id
        stmt = select(func.count()).select_from(Conversation).filter(
            Conversation.user_id == self.user_id
        )
        
        result = await self.session.execute(stmt)
        return result.scalar() or 0
    
    async def add_citation(self,
                     message_id: int,
                     document_id: Optional[UUID] = None,
                     chunk_id: Optional[UUID] = None,
                     relevance_score: Optional[float] = None,
                     excerpt: Optional[str] = None,
                     character_range_start: Optional[int] = None,
                     character_range_end: Optional[int] = None) -> Optional[Citation]:
        """
        Add a citation to a message
        
        Args:
            message_id: Message ID
            document_id: Document ID
            chunk_id: Chunk ID
            relevance_score: Relevance score
            excerpt: Excerpt from the document
            character_range_start: Start of character range
            character_range_end: End of character range
            
        Returns:
            Created citation if message found, None otherwise
        """
        # Check if user has permission to modify this message
        message = await self.get_message(message_id)
        if not message:
            return None
        
        conversation = await self.get_by_id(message.conversation_id)
        if not conversation or not self._can_modify_conversation(conversation):
            return None
        
        # Create citation
        citation = Citation(
            message_id=message_id,
            document_id=document_id,
            chunk_id=chunk_id,
            relevance_score=relevance_score,
            excerpt=excerpt,
            character_range_start=character_range_start,
            character_range_end=character_range_end
        )
        
        self.session.add(citation)
        await self.session.commit()
        await self.session.refresh(citation)
        return citation
    
    async def get_conversation_statistics(self) -> Dict[str, Any]:
        """
        Get conversation statistics for the current user
        
        Returns:
            Dictionary with statistics
        """
        # Return empty stats if no user context
        if not self.user_id:
            return {
                "total_conversations": 0,
                "total_messages": 0,
                "avg_messages_per_conversation": 0.0
            }
        
        # Get total conversations for this user
        stmt_conversations = select(func.count(Conversation.id)).filter(
            Conversation.user_id == self.user_id
        )
        result_conversations = await self.session.execute(stmt_conversations)
        total_conversations = result_conversations.scalar() or 0
        
        # Get total messages for this user's conversations
        stmt_messages = select(func.count(Message.id)).join(
            Conversation, Conversation.id == Message.conversation_id
        ).filter(
            Conversation.user_id == self.user_id
        )
        result_messages = await self.session.execute(stmt_messages)
        total_messages = result_messages.scalar() or 0
        
        # Get average messages per conversation
        avg_messages_per_conversation = 0.0
        if total_conversations > 0:
            avg_messages_per_conversation = total_messages / total_conversations
        
        return {
            "total_conversations": total_conversations,
            "total_messages": total_messages,
            "avg_messages_per_conversation": float(avg_messages_per_conversation)
        }
    
    def _can_view_conversation(self, conversation: Conversation) -> bool:
        """
        Check if the current user can view a conversation
        
        Args:
            conversation: Conversation to check
            
        Returns:
            True if user can view the conversation, False otherwise
        """
        # If no user context, no access
        if not self.user_id:
            return False
        
        # User can view their own conversations
        return conversation.user_id == self.user_id
    
    def _can_modify_conversation(self, conversation: Conversation) -> bool:
        """
        Check if the current user can modify a conversation
        
        Args:
            conversation: Conversation to check
            
        Returns:
            True if user can modify the conversation, False otherwise
        """
        # Same as view permissions for now - only owner can modify
        return self._can_view_conversation(conversation)
    
    def _can_delete_conversation(self, conversation: Conversation) -> bool:
        """
        Check if the current user can delete a conversation
        
        Args:
            conversation: Conversation to check
            
        Returns:
            True if user can delete the conversation, False otherwise
        """
        # Same as modify permissions - only owner can delete
        return self._can_modify_conversation(conversation)
        
    async def get_by_id(self, id: UUID) -> Optional[Conversation]:
        """
        Get a conversation by ID with permission check
        
        Args:
            id: Conversation ID
            
        Returns:
            Conversation if found and user has permission, None otherwise
        """
        # Get the conversation using the parent method
        conversation = await super().get_by_id(id)
        
        # If conversation not found, return None
        if not conversation:
            return None
            
        # Check if user has permission to view this conversation
        if not self._can_view_conversation(conversation):
            return None
            
        return conversation

================
File: app/db/repositories/document_repository.py
================
from typing import List, Optional, Dict, Any, Union, Tuple
from uuid import UUID
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import func, or_, and_, select, exists, String

from app.db.models import Document, Chunk, Tag, Folder, document_tags, DocumentPermission, User
from app.db.repositories.base import BaseRepository


class DocumentRepository(BaseRepository[Document]):
    """
    Repository for Document model with user context and permission handling
    """
    
    def __init__(self, session: Session, user_id: Optional[UUID] = None):
        super().__init__(session, Document)
        self.user_id = user_id
    
    def create_document(self,
                       filename: str,
                       content: Optional[str] = None,
                       metadata: Optional[Dict[str, Any]] = None,
                       tags: Optional[List[str]] = None,
                       folder: str = "/",
                       is_public: bool = False,
                       organization_id: Optional[Union[str, UUID]] = None) -> Document:
        """
        Create a new document
        
        Args:
            filename: Document filename
            content: Document content
            metadata: Document metadata
            tags: List of tag names
            folder: Document folder path
            is_public: Whether the document is publicly accessible
            organization_id: Organization ID (if the document belongs to an organization)
            
        Returns:
            Created document
        """
        # Ensure folder exists
        self._ensure_folder_exists(folder)
        
        # Convert organization_id to UUID if provided as string
        if organization_id and isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                organization_id = None
        
        # Create document
        document = Document(
            filename=filename,
            content=content,
            doc_metadata=metadata or {},  # Changed from metadata to doc_metadata
            folder=folder,
            uploaded=datetime.utcnow(),
            processing_status="pending",
            user_id=self.user_id,  # Set the user ID from context
            is_public=is_public,
            organization_id=organization_id
        )
        
        self.session.add(document)
        self.session.flush()  # Flush to get the document ID
        
        # Add tags if provided
        if tags:
            self._add_tags_to_document(document, tags)
        
        # Update folder document count
        folder_obj = self.session.query(Folder).filter(Folder.path == folder).first()
        if folder_obj:
            folder_obj.document_count += 1
        
        self.session.commit()
        self.session.refresh(document)
        return document
    
    def update_document(self, 
                        document_id: Union[str, UUID], 
                        filename: Optional[str] = None, 
                        content: Optional[str] = None, 
                        metadata: Optional[Dict[str, Any]] = None, 
                        folder: Optional[str] = None,
                        is_public: Optional[bool] = None) -> Optional[Document]:
        """
        Update a document
        
        Args:
            document_id: Document ID
            filename: New filename
            content: New content
            metadata: New metadata
            folder: New folder path
            is_public: Whether the document is publicly accessible
            
        Returns:
            Updated document or None if not found
        """
        document = self.get_document(document_id)
        if not document:
            return None
        
        # Check if user has permission to update the document
        if not self._can_modify_document(document):
            return None
        
        # Update fields if provided
        if filename:
            document.filename = filename
        
        if content is not None:  # Allow empty content
            document.content = content
        
        if metadata:
            # Merge metadata instead of replacing
            document.doc_metadata = {**document.doc_metadata, **metadata}  # Changed from metadata to doc_metadata
        
        if is_public is not None:
            document.is_public = is_public
        
        if folder and folder != document.folder:
            # Ensure new folder exists
            self._ensure_folder_exists(folder)
            
            # Update folder document counts
            old_folder = self.session.query(Folder).filter(Folder.path == document.folder).first()
            if old_folder:
                old_folder.document_count -= 1
            
            new_folder = self.session.query(Folder).filter(Folder.path == folder).first()
            if new_folder:
                new_folder.document_count += 1
            
            document.folder = folder
        
        document.last_accessed = datetime.utcnow()
        
        self.session.commit()
        self.session.refresh(document)
        return document
    
    def get_document(self, document_id: Union[str, UUID]) -> Optional[Document]:
        """
        Get a document by ID
        
        Args:
            document_id: Document ID
            
        Returns:
            Document or None if not found
        """
        if isinstance(document_id, str):
            try:
                document_id = UUID(document_id)
            except ValueError:
                return None
        
        document = self.session.query(Document).filter(Document.id == document_id).first()
        
        # Check if user has permission to view the document
        if document and not self._can_view_document(document):
            return None
        
        if document:
            document.last_accessed = datetime.utcnow()
            self.session.commit()
        
        return document
    
    def delete_document(self, document_id: Union[str, UUID]) -> bool:
        """
        Delete a document by ID
        
        Args:
            document_id: Document ID
            
        Returns:
            True if deleted successfully, False otherwise
        """
        if isinstance(document_id, str):
            try:
                document_id = UUID(document_id)
            except ValueError:
                return False
        
        document = self.session.query(Document).filter(Document.id == document_id).first()
        if not document:
            return False
        
        # Check if user has permission to delete the document
        if not self._can_delete_document(document):
            return False
        
        # Update folder document count
        folder_obj = self.session.query(Folder).filter(Folder.path == document.folder).first()
        if folder_obj:
            folder_obj.document_count -= 1
        
        # Delete document (and chunks via cascade)
        self.session.delete(document)
        self.session.commit()
        
        return True
    
    def search_documents(self, 
                         query: Optional[str] = None, 
                         folder: Optional[str] = None,
                         tags: Optional[List[str]] = None,
                         limit: int = 100,
                         offset: int = 0,
                         include_public: bool = True) -> List[Document]:
        """
        Search documents by content, filename, or metadata
        
        Args:
            query: Search query (searches in content, filename, and metadata)
            folder: Filter by folder
            tags: Filter by tags
            limit: Maximum number of results
            offset: Offset for pagination
            include_public: Whether to include public documents
            
        Returns:
            List of documents
        """
        # Start with base query
        query_obj = self.session.query(Document)
        
        # Apply filters
        filters = []
        
        # Filter by user permissions
        if self.user_id:
            # Documents owned by the user
            user_filter = Document.user_id == self.user_id
            
            # Documents shared with the user
            shared_subquery = self.session.query(DocumentPermission.document_id).filter(
                DocumentPermission.user_id == self.user_id
            ).subquery()
            shared_filter = Document.id.in_(shared_subquery)
            
            # Public documents if requested
            if include_public:
                public_filter = Document.is_public == True
                filters.append(or_(user_filter, shared_filter, public_filter))
            else:
                filters.append(or_(user_filter, shared_filter))
        else:
            # Only public documents for anonymous users
            filters.append(Document.is_public == True)
        
        if query:
            filters.append(or_(
                Document.content.ilike(f"%{query}%"),
                Document.filename.ilike(f"%{query}%"),
                func.cast(Document.doc_metadata, type_=String).ilike(f"%{query}%")  # Changed from metadata to doc_metadata
            ))
        
        if folder:
            if folder.endswith('*'):
                # Folder path prefix search
                prefix = folder[:-1]
                filters.append(Document.folder.like(f"{prefix}%"))
            else:
                # Exact folder match
                filters.append(Document.folder == folder)
        
        if tags:
            # Filter by tags using a subquery
            for tag in tags:
                tag_subquery = self.session.query(document_tags.c.document_id).join(
                    Tag, Tag.id == document_tags.c.tag_id
                ).filter(Tag.name == tag).subquery()
                
                filters.append(Document.id.in_(tag_subquery))
        
        # Apply all filters
        if filters:
            query_obj = query_obj.filter(and_(*filters))
        
        # Order by last accessed (most recent first)
        query_obj = query_obj.order_by(Document.last_accessed.desc())
        
        # Apply pagination
        query_obj = query_obj.limit(limit).offset(offset)
        
        # Execute query
        documents = query_obj.all()
        
        # Update last_accessed for all returned documents
        for doc in documents:
            doc.last_accessed = datetime.utcnow()
        
        self.session.commit()
        
        return documents
    
    def get_documents_by_folder(self, folder: str, limit: int = 100, offset: int = 0) -> List[Document]:
        """
        Get documents by folder
        
        Args:
            folder: Folder path
            limit: Maximum number of results
            offset: Offset for pagination
            
        Returns:
            List of documents
        """
        query = self.session.query(Document).filter(Document.folder == folder)
        
        # Apply permission filtering
        if self.user_id:
            # Documents owned by the user
            user_filter = Document.user_id == self.user_id
            
            # Documents shared with the user
            shared_subquery = self.session.query(DocumentPermission.document_id).filter(
                DocumentPermission.user_id == self.user_id
            ).subquery()
            shared_filter = Document.id.in_(shared_subquery)
            
            # Public documents
            public_filter = Document.is_public == True
            
            # Documents in organizations the user is a member of
            org_subquery = self.session.query(
                Document.id
            ).join(
                "organization"  # Join using the relationship name
            ).join(
                "members"  # Join using the relationship name
            ).filter(
                Document.organization_id.isnot(None),
                Document.organization_id == Document.organization.id,
                Document.organization.members.user_id == self.user_id
            ).subquery()
            org_filter = Document.id.in_(org_subquery)
            
            query = query.filter(or_(user_filter, shared_filter, public_filter, org_filter))
        else:
            # Only public documents for anonymous users
            query = query.filter(Document.is_public == True)
        
        query = query.order_by(Document.uploaded.desc())
        query = query.limit(limit).offset(offset)
        
        documents = query.all()
        
        # Update last_accessed for all returned documents
        for doc in documents:
            doc.last_accessed = datetime.utcnow()
        
        self.session.commit()
        
        return documents
    
    def get_all_documents(self, limit: int = 100, offset: int = 0) -> List[Document]:
        """
        Get all documents accessible to the current user
        
        Args:
            limit: Maximum number of results
            offset: Offset for pagination
            
        Returns:
            List of documents
        """
        query = self.session.query(Document)
        
        # Apply permission filtering
        if self.user_id:
            # Documents owned by the user
            user_filter = Document.user_id == self.user_id
            
            # Documents shared with the user
            shared_subquery = self.session.query(DocumentPermission.document_id).filter(
                DocumentPermission.user_id == self.user_id
            ).subquery()
            shared_filter = Document.id.in_(shared_subquery)
            
            # Public documents
            public_filter = Document.is_public == True
            
            # Documents in organizations the user is a member of
            org_subquery = self.session.query(
                Document.id
            ).join(
                "organization"  # Join using the relationship name
            ).join(
                "members"  # Join using the relationship name
            ).filter(
                Document.organization_id.isnot(None),
                Document.organization_id == Document.organization.id,
                Document.organization.members.user_id == self.user_id
            ).subquery()
            org_filter = Document.id.in_(org_subquery)
            
            query = query.filter(or_(user_filter, shared_filter, public_filter, org_filter))
        else:
            # Only public documents for anonymous users
            query = query.filter(Document.is_public == True)
        
        query = query.order_by(Document.uploaded.desc())
        query = query.limit(limit).offset(offset)
        
        documents = query.all()
        
        return documents
    
    def count_documents(self, folder: Optional[str] = None, tag: Optional[str] = None) -> int:
        """
        Count documents
        
        Args:
            folder: Filter by folder
            tag: Filter by tag
            
        Returns:
            Number of documents
        """
        query = self.session.query(func.count(Document.id))
        
        # Apply permission filtering
        if self.user_id:
            # Documents owned by the user
            user_filter = Document.user_id == self.user_id
            
            # Documents shared with the user
            shared_subquery = self.session.query(DocumentPermission.document_id).filter(
                DocumentPermission.user_id == self.user_id
            ).subquery()
            shared_filter = Document.id.in_(shared_subquery)
            
            # Public documents
            public_filter = Document.is_public == True
            
            # Documents in organizations the user is a member of
            org_subquery = self.session.query(
                Document.id
            ).join(
                "organization"  # Join using the relationship name
            ).join(
                "members"  # Join using the relationship name
            ).filter(
                Document.organization_id.isnot(None),
                Document.organization_id == Document.organization.id,
                Document.organization.members.user_id == self.user_id
            ).subquery()
            org_filter = Document.id.in_(org_subquery)
            
            query = query.filter(or_(user_filter, shared_filter, public_filter, org_filter))
        else:
            # Only public documents for anonymous users
            query = query.filter(Document.is_public == True)
        
        if folder:
            query = query.filter(Document.folder == folder)
        
        if tag:
            tag_subquery = self.session.query(document_tags.c.document_id).join(
                Tag, Tag.id == document_tags.c.tag_id
            ).filter(Tag.name == tag).subquery()
            
            query = query.filter(Document.id.in_(tag_subquery))
        
        return query.scalar()
    
    def get_document_chunks(self, document_id: Union[str, UUID]) -> List[Chunk]:
        """
        Get all chunks for a document
        
        Args:
            document_id: Document ID
            
        Returns:
            List of chunks
        """
        if isinstance(document_id, str):
            try:
                document_id = UUID(document_id)
            except ValueError:
                return []
        
        # Check if user has permission to view the document
        document = self.session.query(Document).filter(Document.id == document_id).first()
        if not document or not self._can_view_document(document):
            return []
        
        return self.session.query(Chunk).filter(Chunk.document_id == document_id).order_by(Chunk.index).all()
    
    def update_document_chunks(self, document_id: Union[str, UUID], chunks: List[Dict[str, Any]]) -> Optional[Document]:
        """
        Update document chunks
        
        Args:
            document_id: Document ID
            chunks: List of chunk data
            
        Returns:
            Updated document or None if not found
        """
        document = self.get_document(document_id)
        if not document:
            return None
        
        # Check if user has permission to modify the document
        if not self._can_modify_document(document):
            return None
        
        # Delete existing chunks
        self.session.query(Chunk).filter(Chunk.document_id == document.id).delete()
        
        # Create new chunks
        for i, chunk_data in enumerate(chunks):
            content = chunk_data.get('content', '')
            metadata = chunk_data.get('metadata', {})
            
            chunk = Chunk(
                document_id=document.id,
                content=content,
                chunk_metadata=metadata,  # Changed from metadata to chunk_metadata
                index=i
            )
            
            self.session.add(chunk)
        
        # Update document status
        document.processing_status = "completed"
        document.last_accessed = datetime.utcnow()
        
        self.session.commit()
        self.session.refresh(document)
        
        return document
    
    def add_tags_to_document(self, document_id: Union[str, UUID], tags: List[str]) -> Optional[Document]:
        """
        Add tags to a document
        
        Args:
            document_id: Document ID
            tags: List of tag names
            
        Returns:
            Updated document or None if not found
        """
        document = self.get_document(document_id)
        if not document:
            return None
        
        # Check if user has permission to modify the document
        if not self._can_modify_document(document):
            return None
        
        self._add_tags_to_document(document, tags)
        
        self.session.commit()
        self.session.refresh(document)
        
        return document
    
    def share_document(self, document_id: Union[str, UUID], user_id: Union[str, UUID], permission_level: str = "read") -> bool:
        """
        Share a document with another user
        
        Args:
            document_id: Document ID
            user_id: User ID to share with
            permission_level: Permission level (read, write, admin)
            
        Returns:
            True if shared successfully, False otherwise
        """
        if isinstance(document_id, str):
            try:
                document_id = UUID(document_id)
            except ValueError:
                return False
        
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        # Check if document exists
        document = self.session.query(Document).filter(Document.id == document_id).first()
        if not document:
            return False
        
        # Check if user has permission to share the document
        if not self._can_share_document(document):
            return False
        
        # Check if user exists
        user_exists = self.session.query(exists().where(User.id == user_id)).scalar()
        if not user_exists:
            return False
        
        # Check if permission already exists
        existing_permission = self.session.query(DocumentPermission).filter(
            DocumentPermission.document_id == document_id,
            DocumentPermission.user_id == user_id
        ).first()
        
        if existing_permission:
            # Update existing permission
            existing_permission.permission_level = permission_level
        else:
            # Create new permission
            permission = DocumentPermission(
                document_id=document_id,
                user_id=user_id,
                permission_level=permission_level
            )
            self.session.add(permission)
        
        self.session.commit()
        return True
    
    def revoke_document_access(self, document_id: Union[str, UUID], user_id: Union[str, UUID]) -> bool:
        """
        Revoke a user's access to a document
        
        Args:
            document_id: Document ID
            user_id: User ID to revoke access from
            
        Returns:
            True if revoked successfully, False otherwise
        """
        if isinstance(document_id, str):
            try:
                document_id = UUID(document_id)
            except ValueError:
                return False
        
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        # Check if document exists
        document = self.session.query(Document).filter(Document.id == document_id).first()
        if not document:
            return False
        
        # Check if user has permission to modify document sharing
        if not self._can_share_document(document):
            return False
        
        # Delete permission
        result = self.session.query(DocumentPermission).filter(
            DocumentPermission.document_id == document_id,
            DocumentPermission.user_id == user_id
        ).delete()
        
        self.session.commit()
        return result > 0
    
    def get_document_permissions(self, document_id: Union[str, UUID]) -> List[Dict[str, Any]]:
        """
        Get all permissions for a document
        
        Args:
            document_id: Document ID
            
        Returns:
            List of permissions with user information
        """
        if isinstance(document_id, str):
            try:
                document_id = UUID(document_id)
            except ValueError:
                return []
        
        # Check if document exists
        document = self.session.query(Document).filter(Document.id == document_id).first()
        if not document:
            return []
        
        # Check if user has permission to view document permissions
        if not self._can_view_permissions(document):
            return []
        
        # Get permissions with user information
        permissions = self.session.query(
            DocumentPermission, User.username, User.email
        ).join(
            User, User.id == DocumentPermission.user_id
        ).filter(
            DocumentPermission.document_id == document_id
        ).all()
        
        return [
            {
                "user_id": str(perm.DocumentPermission.user_id),
                "username": username,
                "email": email,
                "permission_level": perm.DocumentPermission.permission_level,
                "created_at": perm.DocumentPermission.created_at
            }
            for perm, username, email in permissions
        ]
    
    def get_shared_documents(self, limit: int = 100, offset: int = 0) -> List[Document]:
        """
        Get documents shared with the current user
        
        Args:
            limit: Maximum number of results
            offset: Offset for pagination
            
        Returns:
            List of documents
        """
        if not self.user_id:
            return []
        
        # Get documents shared with the user
        shared_subquery = self.session.query(DocumentPermission.document_id).filter(
            DocumentPermission.user_id == self.user_id
        ).subquery()
        
        query = self.session.query(Document).filter(Document.id.in_(shared_subquery))
        query = query.order_by(Document.last_accessed.desc())
        query = query.limit(limit).offset(offset)
        
        documents = query.all()
        
        # Update last_accessed for all returned documents
        for doc in documents:
            doc.last_accessed = datetime.utcnow()
        
        self.session.commit()
        
        return documents
    
    def _can_view_document(self, document: Document) -> bool:
        """
        Check if the current user can view a document
        
        Args:
            document: Document to check
            
        Returns:
            True if user can view the document, False otherwise
        """
        # Public documents can be viewed by anyone
        if document.is_public:
            return True
        
        # If no user context, only public documents can be viewed
        if not self.user_id:
            return False
        
        # Document owner can view
        if document.user_id == self.user_id:
            return True
        
        # Check if document is shared with user
        permission = self.session.query(DocumentPermission).filter(
            DocumentPermission.document_id == document.id,
            DocumentPermission.user_id == self.user_id
        ).first()
        
        return permission is not None
    
    def _can_modify_document(self, document: Document) -> bool:
        """
        Check if the current user can modify a document
        
        Args:
            document: Document to check
            
        Returns:
            True if user can modify the document, False otherwise
        """
        # If no user context, no modifications allowed
        if not self.user_id:
            return False
        
        # Document owner can modify
        if document.user_id == self.user_id:
            return True
        
        # Check if document is shared with user with write or admin permission
        permission = self.session.query(DocumentPermission).filter(
            DocumentPermission.document_id == document.id,
            DocumentPermission.user_id == self.user_id,
            DocumentPermission.permission_level.in_(["write", "admin"])
        ).first()
        
        return permission is not None
    
    def _can_delete_document(self, document: Document) -> bool:
        """
        Check if the current user can delete a document
        
        Args:
            document: Document to check
            
        Returns:
            True if user can delete the document, False otherwise
        """
        # If no user context, no deletions allowed
        if not self.user_id:
            return False
        
        # Document owner can delete
        if document.user_id == self.user_id:
            return True
        
        # Check if document is shared with user with admin permission
        permission = self.session.query(DocumentPermission).filter(
            DocumentPermission.document_id == document.id,
            DocumentPermission.user_id == self.user_id,
            DocumentPermission.permission_level == "admin"
        ).first()
        
        return permission is not None
    
    def _can_share_document(self, document: Document) -> bool:
        """
        Check if the current user can share a document
        
        Args:
            document: Document to check
            
        Returns:
            True if user can share the document, False otherwise
        """
        # Same as delete permissions - only owner or admin can share
        return self._can_delete_document(document)
    
    def _can_view_permissions(self, document: Document) -> bool:
        """
        Check if the current user can view document permissions
        
        Args:
            document: Document to check
            
        Returns:
            True if user can view document permissions, False otherwise
        """
        # Same as share permissions - only owner or admin can view permissions
        return self._can_share_document(document)
    
    def _add_tags_to_document(self, document: Document, tags: List[str]) -> None:
        """
        Add tags to a document (helper method)
        
        Args:
            document: Document
            tags: List of tag names
        """
        for tag_name in tags:
            # Get or create tag
            tag = self.session.query(Tag).filter(Tag.name == tag_name).first()
            if not tag:
                tag = Tag(name=tag_name)
                self.session.add(tag)
                self.session.flush()
            
            # Add tag to document if not already present
            if tag not in document.tags:
                document.tags.append(tag)
                tag.usage_count += 1
    
    def _ensure_folder_exists(self, folder_path: str) -> None:
        """
        Ensure a folder exists (create if not)
        
        Args:
            folder_path: Folder path
        """
        # Skip for root folder
        if folder_path == "/":
            # Ensure root folder exists
            root_folder = self.session.query(Folder).filter(Folder.path == "/").first()
            if not root_folder:
                root_folder = Folder(path="/", name="Root", parent_path=None)
                self.session.add(root_folder)
                self.session.flush()
            return
        
        # Check if folder already exists
        folder = self.session.query(Folder).filter(Folder.path == folder_path).first()
        if folder:
            return
        
        # Split path components
        parts = folder_path.strip('/').split('/')
        name = parts[-1]  # Last component is the folder name
        
        # Determine parent path
        if len(parts) == 1:
            parent_path = "/"
        else:
            parent_path = "/" + "/".join(parts[:-1])
        
        # Ensure parent folder exists
        self._ensure_folder_exists(parent_path)
        
        # Create folder
        folder = Folder(
            path=folder_path,
            name=name,
            parent_path=parent_path
        )
        
        self.session.add(folder)
        self.session.flush()
        
    def get_organization_documents(self, organization_id: Union[str, UUID], limit: int = 100, offset: int = 0) -> List[Document]:
        """
        Get all documents in an organization
        
        Args:
            organization_id: Organization ID
            limit: Maximum number of results
            offset: Offset for pagination
            
        Returns:
            List of documents
        """
        # Convert string ID to UUID if needed
        if isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                return []
        
        query = self.session.query(Document).filter(Document.organization_id == organization_id)
        
        # Apply permission filtering
        if self.user_id:
            # Check if user is a member of the organization
            is_member_subquery = self.session.query(
                func.count()
            ).filter(
                and_(
                    Document.organization_id == organization_id,
                    Document.organization.has(
                        members=self.user_id
                    )
                )
            ).scalar_subquery()
            
            # If user is not a member, they can only see their own documents or documents shared with them
            user_filter = Document.user_id == self.user_id
            
            # Documents shared with the user
            shared_subquery = self.session.query(DocumentPermission.document_id).filter(
                DocumentPermission.user_id == self.user_id
            ).subquery()
            shared_filter = Document.id.in_(shared_subquery)
            
            # Public documents
            public_filter = Document.is_public == True
            
            # User is a member of the organization
            member_filter = is_member_subquery > 0
            
            query = query.filter(or_(user_filter, shared_filter, public_filter, member_filter))
        else:
            # Only public documents for anonymous users
            query = query.filter(Document.is_public == True)
        
        query = query.order_by(Document.uploaded.desc())
        query = query.limit(limit).offset(offset)
        
        documents = query.all()
        
        return documents

================
File: app/db/repositories/memory_repository.py
================
"""
Memory repository for managing memory operations
"""
from typing import List, Optional, Dict, Any
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc

from app.db.repositories.base import BaseRepository
from app.models.memory import Memory

class MemoryRepository(BaseRepository[Memory]):
    """
    Repository for Memory model
    
    This repository provides methods for managing memory operations,
    including storing and retrieving memories.
    """
    
    def __init__(self, session: AsyncSession):
        """
        Initialize the memory repository
        
        Args:
            session: Database session
        """
        super().__init__(session, Memory)
    
    async def create_memory(self, conversation_id: UUID, content: str, label: str = "explicit_memory") -> Memory:
        """
        Create a new memory
        
        Args:
            conversation_id: Conversation ID
            content: Memory content
            label: Memory label
            
        Returns:
            Created memory
        """
        memory_data = {
            "conversation_id": conversation_id,
            "content": content,
            "label": label
        }
        
        return await self.create(memory_data)
    
    async def get_memories_by_conversation(
        self,
        conversation_id: UUID,
        label: Optional[str] = None,
        limit: int = 10
    ) -> List[Memory]:
        """
        Get memories for a conversation
        
        Args:
            conversation_id: Conversation ID
            label: Optional memory label filter
            limit: Maximum number of memories to return
            
        Returns:
            List of memories
        """
        query = select(Memory).where(Memory.conversation_id == conversation_id)
        
        if label:
            query = query.where(Memory.label == label)
        
        query = query.order_by(desc(Memory.created_at)).limit(limit)
        
        result = await self.session.execute(query)
        return list(result.scalars().all())
    
    async def search_memories(
        self,
        conversation_id: UUID,
        search_term: str,
        label: Optional[str] = None,
        limit: int = 10
    ) -> List[Memory]:
        """
        Search memories by content
        
        Args:
            conversation_id: Conversation ID
            search_term: Search term
            label: Optional memory label filter
            limit: Maximum number of memories to return
            
        Returns:
            List of matching memories
        """
        # First get all memories for the conversation
        memories = await self.get_memories_by_conversation(
            conversation_id=conversation_id,
            label=label
        )
        
        # Filter by search term
        if search_term:
            search_term_lower = search_term.lower()
            filtered_memories = [
                memory for memory in memories
                if search_term_lower in memory.content.lower()
            ]
            memories = filtered_memories
        
        # Apply limit
        return memories[:limit]
    
    async def delete_memories_by_conversation(self, conversation_id: UUID) -> int:
        """
        Delete all memories for a conversation
        
        Args:
            conversation_id: Conversation ID
            
        Returns:
            Number of deleted memories
        """
        query = select(Memory).where(Memory.conversation_id == conversation_id)
        result = await self.session.execute(query)
        memories = result.scalars().all()
        
        count = 0
        for memory in memories:
            await self.session.delete(memory)
            count += 1
        
        await self.session.commit()
        return count

# Factory function for dependency injection
async def get_memory_repository(session: AsyncSession) -> MemoryRepository:
    """
    Get a memory repository instance
    
    Args:
        session: Database session
        
    Returns:
        Memory repository
    """
    return MemoryRepository(session)

================
File: app/db/repositories/notification_repository.py
================
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, or_, and_, select, text, update, desc
from sqlalchemy.dialects.postgresql import UUID as SQLUUID

from app.db.models import Notification as DBNotification, User as DBUser
from app.models.notification import Notification as PydanticNotification, NotificationCreate
from app.db.repositories.base import BaseRepository


class NotificationRepository(BaseRepository[DBNotification]):
    """
    Repository for Notification model
    """
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, DBNotification)
    
    async def get_by_id(self, id: Union[str, UUID]) -> Optional[PydanticNotification]:
        """
        Get a notification by ID
        
        Args:
            id: Notification ID
            
        Returns:
            Notification if found, None otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(id, str):
            try:
                id = UUID(id)
            except ValueError:
                return None
        
        stmt = select(DBNotification).where(DBNotification.id == id)
        result = await self.session.execute(stmt)
        notification = result.scalars().first()
        
        if not notification:
            return None
        
        return self._db_notification_to_pydantic(notification)
    
    async def create_notification(self, notification_data: NotificationCreate) -> PydanticNotification:
        """
        Create a new notification
        
        Args:
            notification_data: Notification creation data
            
        Returns:
            Created notification
        """
        # Create notification
        notification = DBNotification(
            user_id=UUID(notification_data.user_id) if isinstance(notification_data.user_id, str) else notification_data.user_id,
            type=notification_data.type,
            title=notification_data.title,
            message=notification_data.message,
            data=notification_data.data,
            is_read=notification_data.is_read,
            created_at=datetime.utcnow()
        )
        
        self.session.add(notification)
        await self.session.commit()
        await self.session.refresh(notification)
        
        return self._db_notification_to_pydantic(notification)
    
    async def get_user_notifications(
        self, 
        user_id: Union[str, UUID], 
        skip: int = 0, 
        limit: int = 20,
        unread_only: bool = False
    ) -> List[PydanticNotification]:
        """
        Get notifications for a user
        
        Args:
            user_id: User ID
            skip: Number of notifications to skip
            limit: Maximum number of notifications to return
            unread_only: Whether to return only unread notifications
            
        Returns:
            List of notifications
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return []
        
        # Build query
        query = select(DBNotification).where(DBNotification.user_id == user_id)
        
        if unread_only:
            query = query.where(DBNotification.is_read == False)
        
        # Order by created_at (newest first) and apply pagination
        query = query.order_by(desc(DBNotification.created_at)).offset(skip).limit(limit)
        
        # Execute query
        result = await self.session.execute(query)
        notifications = result.scalars().all()
        
        return [self._db_notification_to_pydantic(notification) for notification in notifications]
    
    async def mark_as_read(self, notification_id: Union[str, UUID]) -> bool:
        """
        Mark a notification as read
        
        Args:
            notification_id: Notification ID
            
        Returns:
            True if notification was marked as read, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(notification_id, str):
            try:
                notification_id = UUID(notification_id)
            except ValueError:
                return False
        
        # Get notification
        stmt = select(DBNotification).where(DBNotification.id == notification_id)
        result = await self.session.execute(stmt)
        notification = result.scalars().first()
        
        if not notification:
            return False
        
        # Mark as read
        notification.is_read = True
        notification.read_at = datetime.utcnow()
        
        await self.session.commit()
        return True
    
    async def mark_all_as_read(self, user_id: Union[str, UUID]) -> int:
        """
        Mark all notifications for a user as read
        
        Args:
            user_id: User ID
            
        Returns:
            Number of notifications marked as read
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return 0
        
        # Get unread notifications
        stmt = select(DBNotification).where(
            and_(
                DBNotification.user_id == user_id,
                DBNotification.is_read == False
            )
        )
        result = await self.session.execute(stmt)
        notifications = result.scalars().all()
        
        if not notifications:
            return 0
        
        # Mark all as read
        now = datetime.utcnow()
        for notification in notifications:
            notification.is_read = True
            notification.read_at = now
        
        await self.session.commit()
        return len(notifications)
    
    async def delete_notification(self, notification_id: Union[str, UUID]) -> bool:
        """
        Delete a notification
        
        Args:
            notification_id: Notification ID
            
        Returns:
            True if notification was deleted, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(notification_id, str):
            try:
                notification_id = UUID(notification_id)
            except ValueError:
                return False
        
        # Get notification
        stmt = select(DBNotification).where(DBNotification.id == notification_id)
        result = await self.session.execute(stmt)
        notification = result.scalars().first()
        
        if not notification:
            return False
        
        # Delete notification
        await self.session.delete(notification)
        await self.session.commit()
        
        return True
    
    async def delete_all_notifications(self, user_id: Union[str, UUID]) -> int:
        """
        Delete all notifications for a user
        
        Args:
            user_id: User ID
            
        Returns:
            Number of notifications deleted
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return 0
        
        # Get notifications
        stmt = select(DBNotification).where(DBNotification.user_id == user_id)
        result = await self.session.execute(stmt)
        notifications = result.scalars().all()
        
        if not notifications:
            return 0
        
        # Delete all notifications
        for notification in notifications:
            await self.session.delete(notification)
        
        await self.session.commit()
        return len(notifications)
    
    async def count_unread_notifications(self, user_id: Union[str, UUID]) -> int:
        """
        Count unread notifications for a user
        
        Args:
            user_id: User ID
            
        Returns:
            Number of unread notifications
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return 0
        
        # Count unread notifications
        stmt = select(func.count()).where(
            and_(
                DBNotification.user_id == user_id,
                DBNotification.is_read == False
            )
        )
        result = await self.session.execute(stmt)
        count = result.scalar()
        
        return count or 0
    
    async def create_document_shared_notification(
        self, 
        user_id: Union[str, UUID], 
        document_id: Union[str, UUID], 
        document_name: str, 
        shared_by_username: str,
        permission_level: str
    ) -> PydanticNotification:
        """
        Create a notification for a document being shared with a user
        
        Args:
            user_id: User ID
            document_id: Document ID
            document_name: Document name
            shared_by_username: Username of the user who shared the document
            permission_level: Permission level granted
            
        Returns:
            Created notification
        """
        # Create notification data
        notification_data = NotificationCreate(
            user_id=str(user_id) if isinstance(user_id, UUID) else user_id,
            type="document_shared",
            title=f"{shared_by_username} shared a document with you",
            message=f"{shared_by_username} shared '{document_name}' with you with {permission_level} permission.",
            data={
                "document_id": str(document_id) if isinstance(document_id, UUID) else document_id,
                "document_name": document_name,
                "shared_by": shared_by_username,
                "permission_level": permission_level
            },
            is_read=False
        )
        
        # Create notification
        return await self.create_notification(notification_data)
    
    def _db_notification_to_pydantic(self, db_notification: DBNotification) -> PydanticNotification:
        """
        Convert a database notification to a Pydantic notification
        
        Args:
            db_notification: Database notification
            
        Returns:
            Pydantic notification
        """
        return PydanticNotification(
            id=str(db_notification.id),
            user_id=str(db_notification.user_id),
            type=db_notification.type,
            title=db_notification.title,
            message=db_notification.message,
            data=db_notification.data,
            is_read=db_notification.is_read,
            created_at=db_notification.created_at,
            read_at=db_notification.read_at
        )

================
File: app/db/repositories/organization_repository.py
================
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, or_, and_, select, text, update, desc
from sqlalchemy.dialects.postgresql import UUID as SQLUUID

from app.db.models import Organization as DBOrganization, OrganizationMember as DBOrganizationMember, User as DBUser
from app.models.organization import Organization as PydanticOrganization, OrganizationCreate, OrganizationUpdate
from app.models.organization import OrganizationMember as PydanticOrganizationMember, OrganizationMemberCreate
from app.db.repositories.base import BaseRepository


class OrganizationRepository(BaseRepository[DBOrganization]):
    """
    Repository for Organization model
    """
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, DBOrganization)
    
    async def get_by_id(self, id: Union[str, UUID]) -> Optional[PydanticOrganization]:
        """
        Get an organization by ID
        
        Args:
            id: Organization ID
            
        Returns:
            Organization if found, None otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(id, str):
            try:
                id = UUID(id)
            except ValueError:
                return None
        
        stmt = select(DBOrganization).where(DBOrganization.id == id)
        result = await self.session.execute(stmt)
        organization = result.scalars().first()
        
        if not organization:
            return None
        
        return self._db_organization_to_pydantic(organization)
    
    async def get_by_name(self, name: str) -> Optional[PydanticOrganization]:
        """
        Get an organization by name
        
        Args:
            name: Organization name
            
        Returns:
            Organization if found, None otherwise
        """
        stmt = select(DBOrganization).where(DBOrganization.name == name)
        result = await self.session.execute(stmt)
        organization = result.scalars().first()
        
        if not organization:
            return None
        
        return self._db_organization_to_pydantic(organization)
    
    async def create_organization(self, organization_data: OrganizationCreate, owner_id: Union[str, UUID]) -> PydanticOrganization:
        """
        Create a new organization with the specified owner
        
        Args:
            organization_data: Organization creation data
            owner_id: User ID of the organization owner
            
        Returns:
            Created organization
        """
        # Check if organization name already exists
        existing_org = await self.get_by_name(organization_data.name)
        if existing_org:
            raise ValueError(f"Organization with name '{organization_data.name}' already exists")
        
        # Convert owner_id to UUID if needed
        if isinstance(owner_id, str):
            try:
                owner_id = UUID(owner_id)
            except ValueError:
                raise ValueError(f"Invalid owner ID: {owner_id}")
        
        # Create organization
        organization = DBOrganization(
            name=organization_data.name,
            description=organization_data.description,
            settings=organization_data.settings,
            created_at=datetime.utcnow()
        )
        
        self.session.add(organization)
        await self.session.flush()  # Flush to get the organization ID
        
        # Create owner membership
        owner_membership = DBOrganizationMember(
            organization_id=organization.id,
            user_id=owner_id,
            role='owner',
            created_at=datetime.utcnow()
        )
        
        self.session.add(owner_membership)
        await self.session.commit()
        await self.session.refresh(organization)
        
        return self._db_organization_to_pydantic(organization)
    
    async def update_organization(self, organization_id: Union[str, UUID], organization_data: OrganizationUpdate) -> Optional[PydanticOrganization]:
        """
        Update an organization
        
        Args:
            organization_id: Organization ID
            organization_data: Organization update data
            
        Returns:
            Updated organization if found, None otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                return None
        
        # Get the organization
        stmt = select(DBOrganization).where(DBOrganization.id == organization_id)
        result = await self.session.execute(stmt)
        db_organization = result.scalars().first()
        
        if not db_organization:
            return None
        
        # Update fields if provided
        if organization_data.name is not None:
            # Check if name already exists
            existing_org = await self.get_by_name(organization_data.name)
            if existing_org and str(existing_org.id) != str(organization_id):
                raise ValueError(f"Organization with name '{organization_data.name}' already exists")
            db_organization.name = organization_data.name
        
        if organization_data.description is not None:
            db_organization.description = organization_data.description
        
        if organization_data.settings is not None:
            db_organization.settings = organization_data.settings
        
        await self.session.commit()
        await self.session.refresh(db_organization)
        
        return self._db_organization_to_pydantic(db_organization)
    
    async def delete_organization(self, organization_id: Union[str, UUID]) -> bool:
        """
        Delete an organization
        
        Args:
            organization_id: Organization ID
            
        Returns:
            True if organization was deleted, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                return False
        
        # Get the organization
        stmt = select(DBOrganization).where(DBOrganization.id == organization_id)
        result = await self.session.execute(stmt)
        db_organization = result.scalars().first()
        
        if not db_organization:
            return False
        
        # Delete the organization (cascade will handle members)
        await self.session.delete(db_organization)
        await self.session.commit()
        
        return True
    
    async def get_all_organizations(self, skip: int = 0, limit: int = 100) -> List[PydanticOrganization]:
        """
        Get all organizations with pagination
        
        Args:
            skip: Number of organizations to skip
            limit: Maximum number of organizations to return
            
        Returns:
            List of organizations
        """
        stmt = select(DBOrganization).offset(skip).limit(limit)
        result = await self.session.execute(stmt)
        organizations = result.scalars().all()
        
        return [self._db_organization_to_pydantic(org) for org in organizations]
    
    async def add_member(self, organization_id: Union[str, UUID], user_id: Union[str, UUID], role: str) -> PydanticOrganizationMember:
        """
        Add a member to an organization
        
        Args:
            organization_id: Organization ID
            user_id: User ID
            role: Member role ('owner', 'admin', 'member')
            
        Returns:
            Organization member
        """
        # Convert string IDs to UUID if needed
        if isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                raise ValueError(f"Invalid organization ID: {organization_id}")
        
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                raise ValueError(f"Invalid user ID: {user_id}")
        
        # Validate role
        valid_roles = ['owner', 'admin', 'member']
        if role not in valid_roles:
            raise ValueError(f"Invalid role: {role}. Must be one of: {', '.join(valid_roles)}")
        
        # Check if organization exists
        org_stmt = select(DBOrganization).where(DBOrganization.id == organization_id)
        org_result = await self.session.execute(org_stmt)
        organization = org_result.scalars().first()
        
        if not organization:
            raise ValueError(f"Organization with ID {organization_id} not found")
        
        # Check if user exists
        user_stmt = select(DBUser).where(DBUser.id == user_id)
        user_result = await self.session.execute(user_stmt)
        user = user_result.scalars().first()
        
        if not user:
            raise ValueError(f"User with ID {user_id} not found")
        
        # Check if user is already a member
        member_stmt = select(DBOrganizationMember).where(
            and_(
                DBOrganizationMember.organization_id == organization_id,
                DBOrganizationMember.user_id == user_id
            )
        )
        member_result = await self.session.execute(member_stmt)
        existing_member = member_result.scalars().first()
        
        if existing_member:
            # Update role if different
            if existing_member.role != role:
                existing_member.role = role
                await self.session.commit()
            
            return PydanticOrganizationMember(
                organization_id=str(existing_member.organization_id),
                user_id=str(existing_member.user_id),
                role=existing_member.role,
                created_at=existing_member.created_at
            )
        
        # Create new member
        member = DBOrganizationMember(
            organization_id=organization_id,
            user_id=user_id,
            role=role,
            created_at=datetime.utcnow()
        )
        
        self.session.add(member)
        await self.session.commit()
        
        return PydanticOrganizationMember(
            organization_id=str(member.organization_id),
            user_id=str(member.user_id),
            role=member.role,
            created_at=member.created_at
        )
    
    async def remove_member(self, organization_id: Union[str, UUID], user_id: Union[str, UUID]) -> bool:
        """
        Remove a member from an organization
        
        Args:
            organization_id: Organization ID
            user_id: User ID
            
        Returns:
            True if member was removed, False otherwise
        """
        # Convert string IDs to UUID if needed
        if isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                return False
        
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        # Get the member
        stmt = select(DBOrganizationMember).where(
            and_(
                DBOrganizationMember.organization_id == organization_id,
                DBOrganizationMember.user_id == user_id
            )
        )
        result = await self.session.execute(stmt)
        member = result.scalars().first()
        
        if not member:
            return False
        
        # Check if this is the last owner
        if member.role == 'owner':
            # Count owners
            owner_stmt = select(func.count()).where(
                and_(
                    DBOrganizationMember.organization_id == organization_id,
                    DBOrganizationMember.role == 'owner'
                )
            )
            owner_result = await self.session.execute(owner_stmt)
            owner_count = owner_result.scalar()
            
            if owner_count == 1:
                raise ValueError("Cannot remove the last owner of an organization")
        
        # Delete the member
        await self.session.delete(member)
        await self.session.commit()
        
        return True
    
    async def get_organization_members(self, organization_id: Union[str, UUID]) -> List[PydanticOrganizationMember]:
        """
        Get all members of an organization
        
        Args:
            organization_id: Organization ID
            
        Returns:
            List of organization members
        """
        # Convert string ID to UUID if needed
        if isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                return []
        
        # Get all members
        stmt = select(DBOrganizationMember).where(DBOrganizationMember.organization_id == organization_id)
        result = await self.session.execute(stmt)
        members = result.scalars().all()
        
        return [
            PydanticOrganizationMember(
                organization_id=str(member.organization_id),
                user_id=str(member.user_id),
                role=member.role,
                created_at=member.created_at
            )
            for member in members
        ]
    
    async def get_user_organizations(self, user_id: Union[str, UUID]) -> List[PydanticOrganization]:
        """
        Get all organizations a user is a member of
        
        Args:
            user_id: User ID
            
        Returns:
            List of organizations
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return []
        
        # Get all organizations the user is a member of
        stmt = select(DBOrganization).join(
            DBOrganizationMember,
            DBOrganizationMember.organization_id == DBOrganization.id
        ).where(DBOrganizationMember.user_id == user_id)
        
        result = await self.session.execute(stmt)
        organizations = result.scalars().all()
        
        return [self._db_organization_to_pydantic(org) for org in organizations]
    
    async def get_user_role_in_organization(self, organization_id: Union[str, UUID], user_id: Union[str, UUID]) -> Optional[str]:
        """
        Get a user's role in an organization
        
        Args:
            organization_id: Organization ID
            user_id: User ID
            
        Returns:
            User's role if they are a member, None otherwise
        """
        # Convert string IDs to UUID if needed
        if isinstance(organization_id, str):
            try:
                organization_id = UUID(organization_id)
            except ValueError:
                return None
        
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return None
        
        # Get the member
        stmt = select(DBOrganizationMember).where(
            and_(
                DBOrganizationMember.organization_id == organization_id,
                DBOrganizationMember.user_id == user_id
            )
        )
        result = await self.session.execute(stmt)
        member = result.scalars().first()
        
        if not member:
            return None
        
        return member.role
    
    async def user_is_member(self, organization_id: Union[str, UUID], user_id: Union[str, UUID]) -> bool:
        """
        Check if a user is a member of an organization
        
        Args:
            organization_id: Organization ID
            user_id: User ID
            
        Returns:
            True if user is a member, False otherwise
        """
        role = await self.get_user_role_in_organization(organization_id, user_id)
        return role is not None
    
    async def user_is_admin_or_owner(self, organization_id: Union[str, UUID], user_id: Union[str, UUID]) -> bool:
        """
        Check if a user is an admin or owner of an organization
        
        Args:
            organization_id: Organization ID
            user_id: User ID
            
        Returns:
            True if user is an admin or owner, False otherwise
        """
        role = await self.get_user_role_in_organization(organization_id, user_id)
        return role in ['admin', 'owner']
    
    async def user_is_owner(self, organization_id: Union[str, UUID], user_id: Union[str, UUID]) -> bool:
        """
        Check if a user is an owner of an organization
        
        Args:
            organization_id: Organization ID
            user_id: User ID
            
        Returns:
            True if user is an owner, False otherwise
        """
        role = await self.get_user_role_in_organization(organization_id, user_id)
        return role == 'owner'
    
    def _db_organization_to_pydantic(self, db_organization: DBOrganization) -> PydanticOrganization:
        """
        Convert a database organization to a Pydantic organization
        
        Args:
            db_organization: Database organization
            
        Returns:
            Pydantic organization
        """
        return PydanticOrganization(
            id=str(db_organization.id),
            name=db_organization.name,
            description=db_organization.description,
            settings=db_organization.settings,
            created_at=db_organization.created_at
        )

================
File: app/db/repositories/password_reset_repository.py
================
from typing import Optional
from datetime import datetime, timedelta
from uuid import UUID
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_

from app.db.models import PasswordResetToken, User
from app.db.repositories.base import BaseRepository
from app.models.password_reset import PasswordResetToken as PydanticPasswordResetToken

class PasswordResetRepository(BaseRepository[PasswordResetToken]):
    """
    Repository for PasswordResetToken model
    """
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, PasswordResetToken)
    
    async def create_token(self, user_id: str, expires_in_hours: int = 24) -> PydanticPasswordResetToken:
        """
        Create a new password reset token
        
        Args:
            user_id: User ID
            expires_in_hours: Token expiration time in hours
            
        Returns:
            Created token (Pydantic model)
        """
        # Calculate expiration time
        expires_at = datetime.utcnow() + timedelta(hours=expires_in_hours)
        
        # Create token
        token = PasswordResetToken(
            user_id=user_id,
            expires_at=expires_at
        )
        
        self.session.add(token)
        await self.session.commit()
        await self.session.refresh(token)
        
        # Convert to Pydantic model
        return self._db_token_to_pydantic(token)
    
    async def get_valid_token(self, token: str) -> Optional[PydanticPasswordResetToken]:
        """
        Get a valid token by token string
        
        Args:
            token: Token string
            
        Returns:
            Token if found and valid, None otherwise
        """
        stmt = select(PasswordResetToken).where(
            and_(
                PasswordResetToken.token == token,
                PasswordResetToken.expires_at > datetime.utcnow(),
                PasswordResetToken.is_used == False
            )
        )
        result = await self.session.execute(stmt)
        db_token = result.scalars().first()
        
        if not db_token:
            return None
        
        return self._db_token_to_pydantic(db_token)
    
    async def invalidate_token(self, token: str) -> bool:
        """
        Invalidate a token by marking it as used
        
        Args:
            token: Token string
            
        Returns:
            True if token was invalidated, False otherwise
        """
        stmt = select(PasswordResetToken).where(PasswordResetToken.token == token)
        result = await self.session.execute(stmt)
        db_token = result.scalars().first()
        
        if not db_token:
            return False
        
        db_token.is_used = True
        await self.session.commit()
        
        return True
    
    async def invalidate_all_user_tokens(self, user_id: str) -> int:
        """
        Invalidate all tokens for a user
        
        Args:
            user_id: User ID
            
        Returns:
            Number of tokens invalidated
        """
        stmt = select(PasswordResetToken).where(
            and_(
                PasswordResetToken.user_id == user_id,
                PasswordResetToken.is_used == False
            )
        )
        result = await self.session.execute(stmt)
        tokens = result.scalars().all()
        
        count = 0
        for token in tokens:
            token.is_used = True
            count += 1
        
        if count > 0:
            await self.session.commit()
        
        return count
    
    async def cleanup_expired_tokens(self) -> int:
        """
        Delete all expired tokens
        
        Returns:
            Number of tokens deleted
        """
        stmt = select(PasswordResetToken).where(PasswordResetToken.expires_at < datetime.utcnow())
        result = await self.session.execute(stmt)
        tokens = result.scalars().all()
        
        count = 0
        for token in tokens:
            await self.session.delete(token)
            count += 1
        
        if count > 0:
            await self.session.commit()
        
        return count
    
    def _db_token_to_pydantic(self, db_token: PasswordResetToken) -> PydanticPasswordResetToken:
        """
        Convert a database token to a Pydantic token
        
        Args:
            db_token: Database token
            
        Returns:
            Pydantic token
        """
        return PydanticPasswordResetToken(
            id=str(db_token.id),
            user_id=str(db_token.user_id),
            token=db_token.token,
            created_at=db_token.created_at,
            expires_at=db_token.expires_at,
            is_used=db_token.is_used
        )

================
File: app/db/repositories/role_repository.py
================
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, or_, and_, select, text, update
from sqlalchemy.dialects.postgresql import UUID as SQLUUID

from app.db.models import Role as DBRole, UserRole as DBUserRole, User as DBUser
from app.models.role import Role as PydanticRole, RoleCreate, RoleUpdate, UserRole as PydanticUserRole, UserRoleCreate
from app.db.repositories.base import BaseRepository


class RoleRepository(BaseRepository[DBRole]):
    """
    Repository for Role model
    """
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, DBRole)
    
    async def get_by_id(self, id: Union[str, UUID]) -> Optional[PydanticRole]:
        """
        Get a role by ID
        
        Args:
            id: Role ID
            
        Returns:
            Role if found, None otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(id, str):
            try:
                id = UUID(id)
            except ValueError:
                return None
        
        stmt = select(DBRole).where(DBRole.id == id)
        result = await self.session.execute(stmt)
        role = result.scalars().first()
        
        if not role:
            return None
        
        return self._db_role_to_pydantic(role)
    
    async def get_by_name(self, name: str) -> Optional[PydanticRole]:
        """
        Get a role by name
        
        Args:
            name: Role name
            
        Returns:
            Role if found, None otherwise
        """
        stmt = select(DBRole).where(DBRole.name == name)
        result = await self.session.execute(stmt)
        role = result.scalars().first()
        
        if not role:
            return None
        
        return self._db_role_to_pydantic(role)
    
    async def create_role(self, role_data: RoleCreate) -> PydanticRole:
        """
        Create a new role
        
        Args:
            role_data: Role creation data
            
        Returns:
            Created role
        """
        # Check if role name already exists
        existing_role = await self.get_by_name(role_data.name)
        if existing_role:
            raise ValueError(f"Role with name '{role_data.name}' already exists")
        
        # Create role
        role = DBRole(
            name=role_data.name,
            description=role_data.description,
            permissions=role_data.permissions,
            created_at=datetime.utcnow()
        )
        
        self.session.add(role)
        await self.session.commit()
        await self.session.refresh(role)
        
        return self._db_role_to_pydantic(role)
    
    async def update_role(self, role_id: Union[str, UUID], role_data: RoleUpdate) -> Optional[PydanticRole]:
        """
        Update a role
        
        Args:
            role_id: Role ID
            role_data: Role update data
            
        Returns:
            Updated role if found, None otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(role_id, str):
            try:
                role_id = UUID(role_id)
            except ValueError:
                return None
        
        # Get the role
        stmt = select(DBRole).where(DBRole.id == role_id)
        result = await self.session.execute(stmt)
        db_role = result.scalars().first()
        
        if not db_role:
            return None
        
        # Update fields if provided
        if role_data.name is not None:
            # Check if name already exists
            existing_role = await self.get_by_name(role_data.name)
            if existing_role and str(existing_role.id) != str(role_id):
                raise ValueError(f"Role with name '{role_data.name}' already exists")
            db_role.name = role_data.name
        
        if role_data.description is not None:
            db_role.description = role_data.description
        
        if role_data.permissions is not None:
            db_role.permissions = role_data.permissions
        
        await self.session.commit()
        await self.session.refresh(db_role)
        
        return self._db_role_to_pydantic(db_role)
    
    async def delete_role(self, role_id: Union[str, UUID]) -> bool:
        """
        Delete a role
        
        Args:
            role_id: Role ID
            
        Returns:
            True if role was deleted, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(role_id, str):
            try:
                role_id = UUID(role_id)
            except ValueError:
                return False
        
        # Get the role
        stmt = select(DBRole).where(DBRole.id == role_id)
        result = await self.session.execute(stmt)
        db_role = result.scalars().first()
        
        if not db_role:
            return False
        
        # Delete the role (cascade will handle user-role associations)
        await self.session.delete(db_role)
        await self.session.commit()
        
        return True
    
    async def get_all_roles(self, skip: int = 0, limit: int = 100) -> List[PydanticRole]:
        """
        Get all roles with pagination
        
        Args:
            skip: Number of roles to skip
            limit: Maximum number of roles to return
            
        Returns:
            List of roles
        """
        stmt = select(DBRole).offset(skip).limit(limit)
        result = await self.session.execute(stmt)
        roles = result.scalars().all()
        
        return [self._db_role_to_pydantic(role) for role in roles]
    
    async def assign_role_to_user(self, user_id: Union[str, UUID], role_id: Union[str, UUID]) -> PydanticUserRole:
        """
        Assign a role to a user
        
        Args:
            user_id: User ID
            role_id: Role ID
            
        Returns:
            User-role association
        """
        # Convert string IDs to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                raise ValueError(f"Invalid user ID: {user_id}")
        
        if isinstance(role_id, str):
            try:
                role_id = UUID(role_id)
            except ValueError:
                raise ValueError(f"Invalid role ID: {role_id}")
        
        # Check if user exists
        user_stmt = select(DBUser).where(DBUser.id == user_id)
        user_result = await self.session.execute(user_stmt)
        user = user_result.scalars().first()
        
        if not user:
            raise ValueError(f"User with ID {user_id} not found")
        
        # Check if role exists
        role_stmt = select(DBRole).where(DBRole.id == role_id)
        role_result = await self.session.execute(role_stmt)
        role = role_result.scalars().first()
        
        if not role:
            raise ValueError(f"Role with ID {role_id} not found")
        
        # Check if user already has this role
        user_role_stmt = select(DBUserRole).where(
            and_(DBUserRole.user_id == user_id, DBUserRole.role_id == role_id)
        )
        user_role_result = await self.session.execute(user_role_stmt)
        existing_user_role = user_role_result.scalars().first()
        
        if existing_user_role:
            # User already has this role, return the existing association
            return PydanticUserRole(
                user_id=str(existing_user_role.user_id),
                role_id=str(existing_user_role.role_id),
                created_at=existing_user_role.created_at
            )
        
        # Create user-role association
        user_role = DBUserRole(
            user_id=user_id,
            role_id=role_id,
            created_at=datetime.utcnow()
        )
        
        self.session.add(user_role)
        await self.session.commit()
        
        return PydanticUserRole(
            user_id=str(user_role.user_id),
            role_id=str(user_role.role_id),
            created_at=user_role.created_at
        )
    
    async def remove_role_from_user(self, user_id: Union[str, UUID], role_id: Union[str, UUID]) -> bool:
        """
        Remove a role from a user
        
        Args:
            user_id: User ID
            role_id: Role ID
            
        Returns:
            True if role was removed, False otherwise
        """
        # Convert string IDs to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        if isinstance(role_id, str):
            try:
                role_id = UUID(role_id)
            except ValueError:
                return False
        
        # Get the user-role association
        stmt = select(DBUserRole).where(
            and_(DBUserRole.user_id == user_id, DBUserRole.role_id == role_id)
        )
        result = await self.session.execute(stmt)
        user_role = result.scalars().first()
        
        if not user_role:
            return False
        
        # Delete the user-role association
        await self.session.delete(user_role)
        await self.session.commit()
        
        return True
    
    async def get_user_roles(self, user_id: Union[str, UUID]) -> List[PydanticRole]:
        """
        Get all roles assigned to a user
        
        Args:
            user_id: User ID
            
        Returns:
            List of roles assigned to the user
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return []
        
        # Get all roles assigned to the user
        stmt = select(DBRole).join(DBUserRole, DBUserRole.role_id == DBRole.id).where(DBUserRole.user_id == user_id)
        result = await self.session.execute(stmt)
        roles = result.scalars().all()
        
        return [self._db_role_to_pydantic(role) for role in roles]
    
    async def get_role_users(self, role_id: Union[str, UUID]) -> List[str]:
        """
        Get all users assigned to a role
        
        Args:
            role_id: Role ID
            
        Returns:
            List of user IDs assigned to the role
        """
        # Convert string ID to UUID if needed
        if isinstance(role_id, str):
            try:
                role_id = UUID(role_id)
            except ValueError:
                return []
        
        # Get all users assigned to the role
        stmt = select(DBUserRole.user_id).where(DBUserRole.role_id == role_id)
        result = await self.session.execute(stmt)
        user_ids = result.scalars().all()
        
        return [str(user_id) for user_id in user_ids]
    
    async def user_has_role(self, user_id: Union[str, UUID], role_name: str) -> bool:
        """
        Check if a user has a specific role
        
        Args:
            user_id: User ID
            role_name: Role name
            
        Returns:
            True if user has the role, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        # Check if user has the role
        stmt = select(DBUserRole).join(DBRole, DBUserRole.role_id == DBRole.id).where(
            and_(DBUserRole.user_id == user_id, DBRole.name == role_name)
        )
        result = await self.session.execute(stmt)
        user_role = result.scalars().first()
        
        return user_role is not None
    
    async def user_has_permission(self, user_id: Union[str, UUID], permission: str) -> bool:
        """
        Check if a user has a specific permission through any of their roles
        
        Args:
            user_id: User ID
            permission: Permission name
            
        Returns:
            True if user has the permission, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        # Get all roles assigned to the user
        roles = await self.get_user_roles(user_id)
        
        # Check if any role has the permission
        for role in roles:
            if role.permissions and permission in role.permissions:
                return True
        
        return False
    
    def _db_role_to_pydantic(self, db_role: DBRole) -> PydanticRole:
        """
        Convert a database role to a Pydantic role
        
        Args:
            db_role: Database role
            
        Returns:
            Pydantic role
        """
        return PydanticRole(
            id=str(db_role.id),
            name=db_role.name,
            description=db_role.description,
            permissions=db_role.permissions,
            created_at=db_role.created_at
        )

================
File: app/db/repositories/user_repository_updated.py
================
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, or_, and_, select, text, update
from sqlalchemy.dialects.postgresql import UUID as SQLUUID

from app.db.models import User as DBUser, Document
from app.models.user import User as PydanticUser, UserCreate, UserUpdate, UserInDB
from app.db.repositories.base import BaseRepository
from app.core.security import get_password_hash, verify_password


class UserRepository(BaseRepository[DBUser]):
    """
    Repository for User model
    """
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, DBUser)
    
    async def get_by_id(self, id: Union[int, str, UUID]) -> Optional[PydanticUser]:
        """
        Get a user by ID with improved UUID handling
        
        Args:
            id: User ID (can be string, UUID, or int)
            
        Returns:
            User if found, None otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(id, str):
            try:
                id = UUID(id)
            except ValueError:
                return None
        
        # Use SQLAlchemy's native UUID handling
        stmt = select(DBUser).where(DBUser.id == id)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def create_user(self, user_data: UserCreate) -> PydanticUser:
        """
        Create a new user
        
        Args:
            user_data: User creation data
            
        Returns:
            Created user (Pydantic model)
        """
        # Check if username or email already exists
        existing_user = await self.get_by_username_or_email(user_data.username, user_data.email)
        if existing_user:
            raise ValueError("Username or email already exists")
        
        # Create password hash
        password_hash = get_password_hash(user_data.password)
        
        # Create user
        user = DBUser(
            username=user_data.username,
            email=user_data.email,
            password_hash=password_hash,
            full_name=user_data.full_name,
            is_active=user_data.is_active,
            is_admin=user_data.is_admin,
            created_at=datetime.utcnow()
        )
        
        self.session.add(user)
        await self.session.commit()
        await self.session.refresh(user)
        
        # Convert to Pydantic model
        return self._db_user_to_pydantic(user)
    
    async def get_by_username(self, username: str) -> Optional[PydanticUser]:
        """
        Get a user by username
        
        Args:
            username: Username
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(DBUser.username == username)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def get_by_email(self, email: str) -> Optional[PydanticUser]:
        """
        Get a user by email
        
        Args:
            email: Email
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(DBUser.email == email)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def get_by_username_or_email(self, username: str, email: str) -> Optional[PydanticUser]:
        """
        Get a user by username or email
        
        Args:
            username: Username
            email: Email
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(or_(DBUser.username == username, DBUser.email == email))
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def update_user(self, user_id: Union[UUID, str], user_data: Union[UserUpdate, Dict[str, Any]]) -> Optional[PydanticUser]:
        """
        Update a user
        
        Args:
            user_id: User ID
            user_data: User update data
            
        Returns:
            Updated user if found, None otherwise
        """
        # Convert user_data to UserUpdate if it's a dict
        if isinstance(user_data, dict):
            user_data = UserUpdate(**user_data)
        
        user = await self.get_by_id(user_id)
        if not user:
            return None
        
        # Get the DB user
        stmt = select(DBUser).where(DBUser.id == user_id)
        result = await self.session.execute(stmt)
        db_user = result.scalars().first()
        
        if not db_user:
            return None
        
        # Update fields if provided
        if user_data.username is not None:
            # Check if username already exists
            existing_user = await self.get_by_username(user_data.username)
            if existing_user and str(existing_user.id) != str(user_id):
                raise ValueError("Username already exists")
            db_user.username = user_data.username
        
        if user_data.email is not None:
            # Check if email already exists
            existing_user = await self.get_by_email(user_data.email)
            if existing_user and str(existing_user.id) != str(user_id):
                raise ValueError("Email already exists")
            db_user.email = user_data.email
        
        if user_data.full_name is not None:
            db_user.full_name = user_data.full_name
        
        if user_data.password is not None:
            db_user.password_hash = get_password_hash(user_data.password)
        
        if user_data.is_active is not None:
            db_user.is_active = user_data.is_active
        
        if user_data.is_admin is not None:
            db_user.is_admin = user_data.is_admin
        
        await self.session.commit()
        await self.session.refresh(db_user)
        
        return self._db_user_to_pydantic(db_user)
    
    async def authenticate_user(self, username: str, password: str) -> Optional[PydanticUser]:
        """
        Authenticate a user
        
        Args:
            username: Username
            password: Password
            
        Returns:
            User if authentication successful, None otherwise
        """
        # Get user by username
        stmt = select(DBUser).where(DBUser.username == username)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        # Verify password
        if not verify_password(password, user.password_hash):
            return None
        
        # Update last login
        user.last_login = datetime.utcnow()
        await self.session.commit()
        
        return self._db_user_to_pydantic(user)
    
    async def get_all_users(self, skip: int = 0, limit: int = 100) -> List[PydanticUser]:
        """
        Get all users with pagination
        
        Args:
            skip: Number of users to skip
            limit: Maximum number of users to return
            
        Returns:
            List of users
        """
        stmt = select(DBUser).offset(skip).limit(limit)
        result = await self.session.execute(stmt)
        users = result.scalars().all()
        
        return [self._db_user_to_pydantic(user) for user in users]
    
    async def search_users(self, search_term: str, skip: int = 0, limit: int = 100) -> List[PydanticUser]:
        """
        Search users by username or email
        
        Args:
            search_term: Search term
            skip: Number of users to skip
            limit: Maximum number of users to return
            
        Returns:
            List of users matching the search term
        """
        # Create a search pattern with wildcards
        search_pattern = f"%{search_term}%"
        
        # Search by username or email
        stmt = select(DBUser).where(
            or_(
                DBUser.username.ilike(search_pattern),
                DBUser.email.ilike(search_pattern),
                DBUser.full_name.ilike(search_pattern)
            )
        ).offset(skip).limit(limit)
        
        result = await self.session.execute(stmt)
        users = result.scalars().all()
        
        return [self._db_user_to_pydantic(user) for user in users]
    
    async def delete_user(self, user_id: Union[UUID, str]) -> bool:
        """
        Delete a user and reassign their documents to the system user
        
        Args:
            user_id: User ID
            
        Returns:
            True if user was deleted, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        # Get the system user
        system_user_stmt = select(DBUser).where(DBUser.username == 'system')
        system_user_result = await self.session.execute(system_user_stmt)
        system_user = system_user_result.scalars().first()
        
        if not system_user:
            # System user doesn't exist, create one
            raise ValueError("System user not found. Please run scripts/create_system_user.py first.")
        
        # Start a transaction
        async with self.session.begin():
            # Reassign documents to system user
            update_stmt = update(Document).where(Document.user_id == user_id).values(
                user_id=system_user.id,
                # Store original owner info in metadata
                doc_metadata=func.jsonb_set(
                    Document.doc_metadata,
                    '{previous_owner}',
                    func.to_jsonb(str(user_id))
                )
            )
            await self.session.execute(update_stmt)
            
            # Get the DB user
            user_stmt = select(DBUser).where(DBUser.id == user_id)
            user_result = await self.session.execute(user_stmt)
            db_user = user_result.scalars().first()
            
            if not db_user:
                return False
            
            # Delete the user
            await self.session.delete(db_user)
            
            return True
    
    def _db_user_to_pydantic(self, db_user: DBUser) -> PydanticUser:
        """
        Convert a database user to a Pydantic user
        
        Args:
            db_user: Database user
            
        Returns:
            Pydantic user
        """
        # Check if user_metadata attribute exists
        metadata = {}
        if hasattr(db_user, 'user_metadata') and db_user.user_metadata is not None:
            metadata = db_user.user_metadata
            
        return PydanticUser(
            id=str(db_user.id),
            username=db_user.username,
            email=db_user.email,
            full_name=db_user.full_name,
            is_active=db_user.is_active,
            is_admin=db_user.is_admin,
            created_at=db_user.created_at,
            last_login=db_user.last_login,
            metadata=metadata
        )

================
File: app/db/repositories/user_repository.py
================
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, or_, and_, select, text, update
from sqlalchemy.dialects.postgresql import UUID as SQLUUID

from app.db.models import User as DBUser, Document
from app.models.user import User as PydanticUser, UserCreate, UserUpdate, UserInDB
from app.db.repositories.base import BaseRepository
from app.core.security import get_password_hash, verify_password


class UserRepository(BaseRepository[DBUser]):
    """
    Repository for User model
    """
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, DBUser)
    
    async def get_by_id(self, id: Union[int, str, UUID]) -> Optional[PydanticUser]:
        """
        Get a user by ID with improved UUID handling
        
        Args:
            id: User ID (can be string, UUID, or int)
            
        Returns:
            User if found, None otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(id, str):
            try:
                id = UUID(id)
            except ValueError:
                return None
        
        # Use SQLAlchemy's native UUID handling
        stmt = select(DBUser).where(DBUser.id == id)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def create_user(self, user_data: UserCreate) -> PydanticUser:
        """
        Create a new user
        
        Args:
            user_data: User creation data
            
        Returns:
            Created user (Pydantic model)
        """
        # Check if username or email already exists
        existing_user = await self.get_by_username_or_email(user_data.username, user_data.email)
        if existing_user:
            raise ValueError("Username or email already exists")
        
        # Create password hash
        password_hash = get_password_hash(user_data.password)
        
        # Create user
        user = DBUser(
            username=user_data.username,
            email=user_data.email,
            password_hash=password_hash,
            full_name=user_data.full_name,
            is_active=user_data.is_active,
            is_admin=user_data.is_admin,
            created_at=datetime.utcnow()
        )
        
        self.session.add(user)
        await self.session.commit()
        await self.session.refresh(user)
        
        # Convert to Pydantic model
        return self._db_user_to_pydantic(user)
    
    async def get_by_username(self, username: str) -> Optional[PydanticUser]:
        """
        Get a user by username
        
        Args:
            username: Username
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(DBUser.username == username)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def get_by_email(self, email: str) -> Optional[PydanticUser]:
        """
        Get a user by email
        
        Args:
            email: Email
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(DBUser.email == email)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def get_by_username_or_email(self, username: str, email: str) -> Optional[PydanticUser]:
        """
        Get a user by username or email
        
        Args:
            username: Username
            email: Email
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(or_(DBUser.username == username, DBUser.email == email))
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def update_user(self, user_id: Union[UUID, str], user_data: Union[UserUpdate, Dict[str, Any]]) -> Optional[PydanticUser]:
        """
        Update a user
        
        Args:
            user_id: User ID
            user_data: User update data
            
        Returns:
            Updated user if found, None otherwise
        """
        # Convert user_data to UserUpdate if it's a dict
        if isinstance(user_data, dict):
            user_data = UserUpdate(**user_data)
        
        user = await self.get_by_id(user_id)
        if not user:
            return None
        
        # Get the DB user
        stmt = select(DBUser).where(DBUser.id == user_id)
        result = await self.session.execute(stmt)
        db_user = result.scalars().first()
        
        if not db_user:
            return None
        
        # Update fields if provided
        if user_data.username is not None:
            # Check if username already exists
            existing_user = await self.get_by_username(user_data.username)
            if existing_user and str(existing_user.id) != str(user_id):
                raise ValueError("Username already exists")
            db_user.username = user_data.username
        
        if user_data.email is not None:
            # Check if email already exists
            existing_user = await self.get_by_email(user_data.email)
            if existing_user and str(existing_user.id) != str(user_id):
                raise ValueError("Email already exists")
            db_user.email = user_data.email
        
        if user_data.full_name is not None:
            db_user.full_name = user_data.full_name
        
        if user_data.password is not None:
            db_user.password_hash = get_password_hash(user_data.password)
        
        if user_data.is_active is not None:
            db_user.is_active = user_data.is_active
        
        if user_data.is_admin is not None:
            db_user.is_admin = user_data.is_admin
        
        await self.session.commit()
        await self.session.refresh(db_user)
        
        return self._db_user_to_pydantic(db_user)
    
    async def authenticate_user(self, username: str, password: str) -> Optional[PydanticUser]:
        """
        Authenticate a user
        
        Args:
            username: Username
            password: Password
            
        Returns:
            User if authentication successful, None otherwise
        """
        # Get user by username
        stmt = select(DBUser).where(DBUser.username == username)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        # Verify password
        if not verify_password(password, user.password_hash):
            return None
        
        # Update last login
        user.last_login = datetime.utcnow()
        await self.session.commit()
        
        return self._db_user_to_pydantic(user)
    
    async def get_all_users(self, skip: int = 0, limit: int = 100) -> List[PydanticUser]:
        """
        Get all users with pagination
        
        Args:
            skip: Number of users to skip
            limit: Maximum number of users to return
            
        Returns:
            List of users
        """
        stmt = select(DBUser).offset(skip).limit(limit)
        result = await self.session.execute(stmt)
        users = result.scalars().all()
        
        return [self._db_user_to_pydantic(user) for user in users]
    
    async def search_users(self, search_term: str, skip: int = 0, limit: int = 100) -> List[PydanticUser]:
        """
        Search users by username or email
        
        Args:
            search_term: Search term
            skip: Number of users to skip
            limit: Maximum number of users to return
            
        Returns:
            List of users matching the search term
        """
        # Create a search pattern with wildcards
        search_pattern = f"%{search_term}%"
        
        # Search by username or email
        stmt = select(DBUser).where(
            or_(
                DBUser.username.ilike(search_pattern),
                DBUser.email.ilike(search_pattern),
                DBUser.full_name.ilike(search_pattern)
            )
        ).offset(skip).limit(limit)
        
        result = await self.session.execute(stmt)
        users = result.scalars().all()
        
        return [self._db_user_to_pydantic(user) for user in users]
    
    async def delete_user(self, user_id: Union[UUID, str]) -> bool:
        """
        Delete a user and reassign their documents to the system user
        
        Args:
            user_id: User ID
            
        Returns:
            True if user was deleted, False otherwise
        """
        # Convert string ID to UUID if needed
        if isinstance(user_id, str):
            try:
                user_id = UUID(user_id)
            except ValueError:
                return False
        
        # Get the system user
        system_user_stmt = select(DBUser).where(DBUser.username == 'system')
        system_user_result = await self.session.execute(system_user_stmt)
        system_user = system_user_result.scalars().first()
        
        if not system_user:
            # System user doesn't exist, create one
            raise ValueError("System user not found. Please run scripts/create_system_user.py first.")
        
        # First get all documents owned by this user
        docs_stmt = select(Document).where(Document.user_id == user_id)
        docs_result = await self.session.execute(docs_stmt)
        documents = docs_result.scalars().all()
        
        # Update each document individually
        for doc in documents:
            # Create updated metadata with previous owner info
            if not doc.doc_metadata:
                doc.doc_metadata = {}
            
            # Convert to dict if it's not already
            if not isinstance(doc.doc_metadata, dict):
                doc.doc_metadata = {}
                
            # Add previous owner info
            doc.doc_metadata['previous_owner'] = str(user_id)
            
            # Reassign to system user
            doc.user_id = system_user.id
            self.session.add(doc)
        
        # Get the DB user
        user_stmt = select(DBUser).where(DBUser.id == user_id)
        user_result = await self.session.execute(user_stmt)
        db_user = user_result.scalars().first()
        
        if not db_user:
            return False
        
        # Delete the user
        await self.session.delete(db_user)
        await self.session.commit()
        
        return True
    
    def _db_user_to_pydantic(self, db_user: DBUser) -> PydanticUser:
        """
        Convert a database user to a Pydantic user
        
        Args:
            db_user: Database user
            
        Returns:
            Pydantic user
        """
        # Check if user_metadata attribute exists
        metadata = {}
        if hasattr(db_user, 'user_metadata') and db_user.user_metadata is not None:
            metadata = db_user.user_metadata
            
        return PydanticUser(
            id=str(db_user.id),
            username=db_user.username,
            email=db_user.email,
            full_name=db_user.full_name,
            is_active=db_user.is_active,
            is_admin=db_user.is_admin,
            created_at=db_user.created_at,
            last_login=db_user.last_login,
            metadata=metadata
        )

================
File: app/db/__init__.py
================
# Database package

================
File: app/db/adapters.py
================
"""
Adapter functions to convert between Pydantic and SQLAlchemy models.

This module provides functions to convert between Pydantic models (used in the API and domain layers)
and SQLAlchemy models (used in the database layer). This separation allows for clean domain models
while maintaining efficient database operations.
"""
import uuid
from typing import List, Optional, Union, Dict, Any
from uuid import UUID

from app.models import document as pydantic_models
from app.db import models as db_models
from app.core.config import DATABASE_TYPE

def to_str_id(id_value: Union[str, UUID, None]) -> Optional[str]:
    """
    Convert ID to string format.
    
    Args:
        id_value: ID value (can be string, UUID, or None)
        
    Returns:
        String representation of the ID, or None if input is None
    """
    if id_value is None:
        return None
    return str(id_value)

def to_uuid_or_str(id_value: Union[str, UUID, None]) -> Optional[Union[str, UUID]]:
    """
    Convert ID to appropriate type based on database.
    For PostgreSQL: UUID
    For SQLite: string
    
    Args:
        id_value: ID value (can be string, UUID, or None)
        
    Returns:
        UUID for PostgreSQL, string for other databases, or None if input is None
    """
    if id_value is None:
        return None
        
    if DATABASE_TYPE == 'postgresql':
        if isinstance(id_value, UUID):
            return id_value
        try:
            return UUID(id_value)
        except ValueError:
            # If the string is not a valid UUID format, return it as is
            return id_value
    else:
        return to_str_id(id_value)

def pydantic_document_to_sqlalchemy(doc: pydantic_models.Document) -> db_models.Document:
    """
    Convert Pydantic Document to SQLAlchemy Document.
    
    Args:
        doc: Pydantic Document model
        
    Returns:
        SQLAlchemy Document model
    """
    # Handle UUID conversion based on database type
    doc_id = to_uuid_or_str(doc.id)
    
    # Create SQLAlchemy Document
    sqlalchemy_doc = db_models.Document(
        id=doc_id,
        filename=doc.filename,
        content=doc.content,
        doc_metadata=doc.metadata,  # Note the attribute name change
        folder=doc.folder,
        uploaded=doc.uploaded,
        processing_status=doc.metadata.get("processing_status", "pending"),
        processing_strategy=doc.metadata.get("processing_strategy", None),
        file_size=doc.metadata.get("file_size", None),
        file_type=doc.metadata.get("file_type", None),
        last_accessed=doc.metadata.get("last_accessed", doc.uploaded)
    )
    
    # Convert chunks if available
    if hasattr(doc, 'chunks') and doc.chunks:
        sqlalchemy_doc.chunks = [
            pydantic_chunk_to_sqlalchemy(chunk, doc_id) 
            for chunk in doc.chunks
        ]
    
    return sqlalchemy_doc

def sqlalchemy_document_to_pydantic(doc: db_models.Document) -> pydantic_models.Document:
    """
    Convert SQLAlchemy Document to Pydantic Document.
    
    Args:
        doc: SQLAlchemy Document model
        
    Returns:
        Pydantic Document model
    """
    pydantic_doc = pydantic_models.Document(
        id=to_str_id(doc.id),
        filename=doc.filename,
        content=doc.content,
        metadata=doc.doc_metadata,  # Note the attribute name change
        folder=doc.folder,
        uploaded=doc.uploaded
    )
    
    # Convert chunks if available
    if hasattr(doc, 'chunks') and doc.chunks:
        pydantic_doc.chunks = [
            sqlalchemy_chunk_to_pydantic(chunk) 
            for chunk in doc.chunks
        ]
    
    # Convert tags if available
    if hasattr(doc, 'tags') and doc.tags:
        pydantic_doc.tags = [tag.name for tag in doc.tags]
    
    return pydantic_doc

def pydantic_chunk_to_sqlalchemy(chunk: pydantic_models.Chunk, document_id: Union[str, UUID]) -> db_models.Chunk:
    """
    Convert Pydantic Chunk to SQLAlchemy Chunk.
    
    Args:
        chunk: Pydantic Chunk model
        document_id: ID of the parent document
        
    Returns:
        SQLAlchemy Chunk model
    """
    # Handle UUID conversion based on database type
    chunk_id = to_uuid_or_str(chunk.id)
    doc_id = to_uuid_or_str(document_id)
    
    # Extract index from metadata or default to 0
    index = chunk.metadata.get('index', 0) if chunk.metadata else 0
    
    sqlalchemy_chunk = db_models.Chunk(
        id=chunk_id,
        document_id=doc_id,
        content=chunk.content,
        chunk_metadata=chunk.metadata,  # Note the attribute name change
        index=index,
        embedding_quality=chunk.metadata.get('embedding_quality', None) if chunk.metadata else None
    )
    return sqlalchemy_chunk

def sqlalchemy_chunk_to_pydantic(chunk: db_models.Chunk) -> pydantic_models.Chunk:
    """
    Convert SQLAlchemy Chunk to Pydantic Chunk.
    
    Args:
        chunk: SQLAlchemy Chunk model
        
    Returns:
        Pydantic Chunk model
    """
    pydantic_chunk = pydantic_models.Chunk(
        id=to_str_id(chunk.id),
        content=chunk.content,
        metadata=chunk.chunk_metadata  # Note the attribute name change
    )
    
    # Add embedding if available
    if hasattr(chunk, 'embedding') and chunk.embedding:
        pydantic_chunk.embedding = chunk.embedding
    
    return pydantic_chunk

def is_sqlalchemy_model(obj: Any) -> bool:
    """
    Check if an object is a SQLAlchemy model.
    
    Args:
        obj: Object to check
        
    Returns:
        True if the object is a SQLAlchemy model, False otherwise
    """
    return hasattr(obj, '_sa_instance_state')

def is_pydantic_model(obj: Any) -> bool:
    """
    Check if an object is a Pydantic model.
    
    Args:
        obj: Object to check
        
    Returns:
        True if the object is a Pydantic model, False otherwise
    """
    return hasattr(obj, '__fields__')

================
File: app/db/connection_manager.py
================
"""
Database connection manager for async database operations
"""
import logging
import uuid
import urllib.parse
from typing import Dict, Any, Optional, Union
import aiosqlite
import asyncpg

class DatabaseConnectionManager:
    """
    Manages async database connections with connection pooling and secure IDs
    
    This class provides a unified interface for managing both SQLite and PostgreSQL
    connections using aiosqlite and asyncpg respectively. It implements connection
    pooling, secure connection ID generation, and proper lifecycle management.
    """
    
    def __init__(self):
        """Initialize the database connection manager"""
        self.logger = logging.getLogger("app.db.connection_manager")
        self._pools = {}  # Connection pools by connection ID
        self._connection_map = {}  # Map connection IDs to connection strings
        self._reverse_map = {}  # Map connection strings to IDs
        self._connection_types = {}  # Track connection types (sqlite or postgres)
    
    def connection_to_uuid(self, connection_string: str) -> str:
        """
        Convert a connection string to a deterministic UUID
        
        Args:
            connection_string: Database connection string
            
        Returns:
            str: UUID representing the connection
        """
        # For SQLite, use the file path
        if connection_string.endswith(('.db', '.sqlite', '.sqlite3')) or connection_string.startswith('sqlite:'):
            # Extract just the path part for SQLite
            if connection_string.startswith('sqlite:'):
                # Handle SQLAlchemy-style connection strings
                path = connection_string.replace('sqlite:///', '')
                path = connection_string.replace('sqlite://', '')
            else:
                path = connection_string
            
            # Create a Version 5 UUID (SHA-1 based)
            return str(uuid.uuid5(uuid.NAMESPACE_URL, f"sqlite:{path}"))
        
        # For PostgreSQL, use a similar approach to pg-mcp
        elif connection_string.startswith('postgresql:'):
            # Parse the connection string
            parsed = urllib.parse.urlparse(connection_string)
            
            # Extract the netloc (user:password@host:port) and path (database name)
            connection_id_string = parsed.netloc + parsed.path
            
            # Create a Version 5 UUID (SHA-1 based)
            return str(uuid.uuid5(uuid.NAMESPACE_URL, connection_id_string))
        
        # Default case - just hash the whole string
        return str(uuid.uuid5(uuid.NAMESPACE_URL, connection_string))
    
    def register_connection(self, connection_string: str) -> str:
        """
        Register a connection string and return its UUID identifier
        
        Args:
            connection_string: Database connection string
            
        Returns:
            str: UUID identifier for this connection
        """
        # Check if already registered
        if connection_string in self._reverse_map:
            return self._reverse_map[connection_string]
        
        # Normalize PostgreSQL connection strings
        if connection_string.startswith('postgres://'):
            connection_string = connection_string.replace('postgres://', 'postgresql://')
        
        # Determine connection type
        if connection_string == ':memory:' or connection_string == 'sqlite::memory:':
            conn_type = 'sqlite'
        elif connection_string.endswith(('.db', '.sqlite', '.sqlite3')) or connection_string.startswith('sqlite:'):
            conn_type = 'sqlite'
        elif connection_string.startswith('postgresql://'):
            conn_type = 'postgres'
        else:
            # Try to infer from file extension
            lower_conn = connection_string.lower()
            if any(lower_conn.endswith(ext) for ext in ['.db', '.sqlite', '.sqlite3']):
                conn_type = 'sqlite'
            else:
                raise ValueError(f"Unable to determine connection type for: {connection_string}")
        
        # Generate a new UUID
        conn_id = self.connection_to_uuid(connection_string)
        
        # Store mappings
        self._connection_map[conn_id] = connection_string
        self._reverse_map[connection_string] = conn_id
        self._connection_types[conn_id] = conn_type
        
        self.logger.info(f"Registered new {conn_type} connection with ID {conn_id}")
        return conn_id
    
    def get_connection_string(self, conn_id: str) -> str:
        """
        Get the connection string for a connection ID
        
        Args:
            conn_id: Connection ID
            
        Returns:
            str: Connection string
            
        Raises:
            ValueError: If the connection ID is unknown
        """
        if conn_id not in self._connection_map:
            raise ValueError(f"Unknown connection ID: {conn_id}")
        return self._connection_map[conn_id]
    
    def get_connection_type(self, conn_id: str) -> str:
        """
        Get the connection type (sqlite or postgres) for a connection ID
        
        Args:
            conn_id: Connection ID
            
        Returns:
            str: Connection type ('sqlite' or 'postgres')
            
        Raises:
            ValueError: If the connection ID is unknown
        """
        if conn_id not in self._connection_types:
            raise ValueError(f"Unknown connection ID: {conn_id}")
        return self._connection_types[conn_id]
    
    async def get_connection(self, conn_id: str) -> Union[aiosqlite.Connection, asyncpg.Connection]:
        """
        Get a database connection for the given connection ID
        
        This method returns the appropriate connection type based on the
        registered connection string.
        
        Args:
            conn_id: Connection ID
            
        Returns:
            Union[aiosqlite.Connection, asyncpg.Connection]: Database connection
            
        Raises:
            ValueError: If the connection ID is unknown
        """
        conn_type = self.get_connection_type(conn_id)
        
        if conn_type == 'sqlite':
            return await self.get_sqlite_connection(conn_id)
        elif conn_type == 'postgres':
            return await self.get_postgres_connection(conn_id)
        else:
            raise ValueError(f"Unsupported connection type: {conn_type}")
    
    async def get_sqlite_connection(self, conn_id: str) -> aiosqlite.Connection:
        """
        Get an aiosqlite connection for the given connection ID
        
        Args:
            conn_id: Connection ID
            
        Returns:
            aiosqlite.Connection: SQLite connection
            
        Raises:
            ValueError: If the connection ID is unknown or not a SQLite connection
        """
        if self.get_connection_type(conn_id) != 'sqlite':
            raise ValueError(f"Connection ID {conn_id} is not a SQLite connection")
        
        connection_string = self.get_connection_string(conn_id)
        
        # For SQLite, the connection string is the file path
        # Handle SQLAlchemy-style connection strings
        if connection_string.startswith('sqlite:'):
            if connection_string.startswith('sqlite:///'):
                # Absolute path
                db_path = connection_string[10:]
            elif connection_string.startswith('sqlite://'):
                # Relative path
                db_path = connection_string[9:]
            else:
                db_path = ':memory:'
        else:
            db_path = connection_string
        
        # Create connection if it doesn't exist
        if conn_id not in self._pools:
            self.logger.debug(f"Creating new SQLite connection to {db_path}")
            self._pools[conn_id] = await aiosqlite.connect(db_path)
            # Enable row factory for dict-like access
            self._pools[conn_id].row_factory = aiosqlite.Row
            
        return self._pools[conn_id]
    
    async def get_postgres_connection(self, conn_id: str) -> asyncpg.Connection:
        """
        Get an asyncpg connection from the pool for the given connection ID
        
        Args:
            conn_id: Connection ID
            
        Returns:
            asyncpg.Connection: PostgreSQL connection
            
        Raises:
            ValueError: If the connection ID is unknown or not a PostgreSQL connection
        """
        if self.get_connection_type(conn_id) != 'postgres':
            raise ValueError(f"Connection ID {conn_id} is not a PostgreSQL connection")
        
        connection_string = self.get_connection_string(conn_id)
        
        # Create pool if it doesn't exist
        if conn_id not in self._pools:
            self.logger.debug(f"Creating new PostgreSQL connection pool for {conn_id}")
            self._pools[conn_id] = await asyncpg.create_pool(
                connection_string,
                min_size=2,
                max_size=10,
                command_timeout=60.0,
                # Read-only mode for safety by default
                server_settings={"default_transaction_read_only": "true"}
            )
            
        # Get connection from pool
        return await self._pools[conn_id].acquire()
    
    async def release_postgres_connection(self, conn_id: str, connection: asyncpg.Connection):
        """
        Release a PostgreSQL connection back to the pool
        
        Args:
            conn_id: Connection ID
            connection: PostgreSQL connection to release
            
        Raises:
            ValueError: If the connection ID is unknown or not a PostgreSQL connection
        """
        if self.get_connection_type(conn_id) != 'postgres':
            raise ValueError(f"Connection ID {conn_id} is not a PostgreSQL connection")
            
        if conn_id in self._pools:
            await self._pools[conn_id].release(connection)
    
    async def close(self, conn_id: Optional[str] = None):
        """
        Close a specific or all database connections
        
        Args:
            conn_id: If provided, close only this specific connection.
                    If None, close all connections.
        """
        if conn_id:
            if conn_id in self._pools:
                conn_type = self.get_connection_type(conn_id)
                self.logger.info(f"Closing {conn_type} connection for ID {conn_id}")
                
                pool = self._pools[conn_id]
                if conn_type == 'sqlite':
                    await pool.close()
                else:  # postgres
                    await pool.close()
                    
                del self._pools[conn_id]
                # Keep the mapping for potential reconnection
        else:
            # Close all connection pools
            self.logger.info("Closing all database connections")
            for id, pool in list(self._pools.items()):
                conn_type = self.get_connection_type(id)
                if conn_type == 'sqlite':
                    await pool.close()
                else:  # postgres
                    await pool.close()
                del self._pools[id]

# Create a singleton instance
connection_manager = DatabaseConnectionManager()

================
File: app/db/dependencies.py
================
from typing import AsyncGenerator, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from fastapi import Depends
from uuid import UUID

from app.db.session import get_session
from app.db.repositories.document_repository import DocumentRepository
from app.db.repositories.conversation_repository import ConversationRepository
from app.db.repositories.analytics_repository import AnalyticsRepository
from app.db.repositories.user_repository import UserRepository
from app.db.repositories.password_reset_repository import PasswordResetRepository
from app.rag.document_processor import DocumentProcessor
from app.core.config import UPLOAD_DIR, CHUNK_SIZE, CHUNK_OVERLAP
from app.core.security import get_current_user_optional
from app.models.user import User


# Async dependency for database session
get_db = get_session


async def get_document_repository(db: AsyncSession = Depends(get_db)) -> DocumentRepository:
    """
    Get a document repository
    
    Args:
        db: Database session
        
    Returns:
        Document repository
    """
    return DocumentRepository(db)

async def get_conversation_repository(
    db: AsyncSession = Depends(get_db),
    current_user: Optional[User] = Depends(get_current_user_optional)
) -> ConversationRepository:
    """
    Get a conversation repository with user context
    
    Args:
        db: Database session
        current_user: Current user (optional)
        
    Returns:
        Conversation repository with user context
    """
    user_id = current_user.id if current_user else None
    return ConversationRepository(db, user_id=user_id)

async def get_analytics_repository(db: AsyncSession = Depends(get_db)) -> AnalyticsRepository:
    """
    Get an analytics repository
    
    Args:
        db: Database session
        
    Returns:
        Analytics repository
    """
    return AnalyticsRepository(db)


async def get_user_repository(db: AsyncSession = Depends(get_db)) -> UserRepository:
    """
    Get a user repository
    
    Args:
        db: Database session
        
    Returns:
        User repository
    """
    return UserRepository(db)


async def get_password_reset_repository(db: AsyncSession = Depends(get_db)) -> PasswordResetRepository:
    """
    Get a password reset repository
    
    Args:
        db: Database session
        
    Returns:
        Password reset repository
    """
    return PasswordResetRepository(db)


def get_document_processor(
    upload_dir: str = UPLOAD_DIR,
    chunk_size: int = CHUNK_SIZE,
    chunk_overlap: int = CHUNK_OVERLAP,
    chunking_strategy: str = "recursive",
    llm_provider = None
) -> DocumentProcessor:
    """
    Get a document processor with the specified parameters
    
    Args:
        upload_dir: Upload directory
        chunk_size: Chunk size
        chunk_overlap: Chunk overlap
        chunking_strategy: Chunking strategy
        llm_provider: LLM provider
        
    Returns:
        Document processor
    """
    return DocumentProcessor(
        upload_dir=upload_dir,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        chunking_strategy=chunking_strategy,
        llm_provider=llm_provider
    )

================
File: app/db/mem0_integration.py
================
"""
Mem0 integration for Metis_RAG repositories
"""
import logging
from typing import Dict, Any, Optional, List, Type, TypeVar, Generic, Union
from uuid import UUID
from datetime import datetime

from mem0ai import Mem0Client
from sqlalchemy.orm import Session

from app.db.session import Base
from app.core.config import SETTINGS

# Define a generic type variable for SQLAlchemy models
ModelType = TypeVar("ModelType", bound=Base)

logger = logging.getLogger("app.db.mem0_integration")

class Mem0Integration(Generic[ModelType]):
    """
    Mem0 integration for repositories
    
    This class provides memory-enhanced operations for repositories using mem0.
    It stores and retrieves memories related to database operations.
    """
    
    def __init__(self, model_class: Type[ModelType], session: Session):
        """
        Initialize mem0 integration
        
        Args:
            model_class: SQLAlchemy model class
            session: Database session
        """
        self.model_class = model_class
        self.session = session
        self.client = Mem0Client()
        self.model_name = model_class.__name__.lower()
        
    async def store_memory(self, operation: str, entity_id: Union[str, int, UUID], data: Dict[str, Any]) -> None:
        """
        Store a memory related to a database operation
        
        Args:
            operation: Operation type (create, read, update, delete)
            entity_id: Entity ID
            data: Memory data
        """
        try:
            memory = {
                "operation": operation,
                "entity_id": str(entity_id),
                "entity_type": self.model_name,
                "timestamp": datetime.utcnow().isoformat(),
                "data": data
            }
            
            # Store memory in mem0
            await self.client.add_memory(
                memory=memory,
                collection=f"{self.model_name}_operations"
            )
            
            logger.debug(f"Stored memory for {self.model_name} {entity_id} ({operation})")
        except Exception as e:
            logger.error(f"Error storing memory: {str(e)}")
    
    async def retrieve_memories(self, entity_id: Union[str, int, UUID], limit: int = 10) -> List[Dict[str, Any]]:
        """
        Retrieve memories related to an entity
        
        Args:
            entity_id: Entity ID
            limit: Maximum number of memories to retrieve
            
        Returns:
            List of memories
        """
        try:
            # Retrieve memories from mem0
            memories = await self.client.get_memories(
                query=f"entity_id:{str(entity_id)} AND entity_type:{self.model_name}",
                collection=f"{self.model_name}_operations",
                limit=limit
            )
            
            logger.debug(f"Retrieved {len(memories)} memories for {self.model_name} {entity_id}")
            return memories
        except Exception as e:
            logger.error(f"Error retrieving memories: {str(e)}")
            return []
    
    async def retrieve_user_preferences(self, user_id: str) -> Dict[str, Any]:
        """
        Retrieve user preferences
        
        Args:
            user_id: User ID
            
        Returns:
            User preferences
        """
        try:
            # Retrieve user preferences from mem0
            preferences = await self.client.get_memories(
                query=f"user_id:{user_id} AND type:preferences",
                collection="user_memory",
                limit=1
            )
            
            if preferences:
                return preferences[0].get("data", {})
            return {}
        except Exception as e:
            logger.error(f"Error retrieving user preferences: {str(e)}")
            return {}
    
    async def store_user_preferences(self, user_id: str, preferences: Dict[str, Any]) -> None:
        """
        Store user preferences
        
        Args:
            user_id: User ID
            preferences: User preferences
        """
        try:
            memory = {
                "user_id": user_id,
                "type": "preferences",
                "timestamp": datetime.utcnow().isoformat(),
                "data": preferences
            }
            
            # Store user preferences in mem0
            await self.client.add_memory(
                memory=memory,
                collection="user_memory"
            )
            
            logger.debug(f"Stored preferences for user {user_id}")
        except Exception as e:
            logger.error(f"Error storing user preferences: {str(e)}")
    
    async def retrieve_document_interactions(self, document_id: Union[str, UUID], user_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Retrieve document interactions
        
        Args:
            document_id: Document ID
            user_id: Optional user ID to filter by
            
        Returns:
            List of document interactions
        """
        try:
            query = f"document_id:{str(document_id)}"
            if user_id:
                query += f" AND user_id:{user_id}"
                
            # Retrieve document interactions from mem0
            interactions = await self.client.get_memories(
                query=query,
                collection="document_memory",
                limit=20
            )
            
            logger.debug(f"Retrieved {len(interactions)} interactions for document {document_id}")
            return interactions
        except Exception as e:
            logger.error(f"Error retrieving document interactions: {str(e)}")
            return []
    
    async def store_document_interaction(self, document_id: Union[str, UUID], user_id: str, interaction_type: str, data: Dict[str, Any]) -> None:
        """
        Store a document interaction
        
        Args:
            document_id: Document ID
            user_id: User ID
            interaction_type: Interaction type (view, search, cite, etc.)
            data: Interaction data
        """
        try:
            memory = {
                "document_id": str(document_id),
                "user_id": user_id,
                "type": interaction_type,
                "timestamp": datetime.utcnow().isoformat(),
                "data": data
            }
            
            # Store document interaction in mem0
            await self.client.add_memory(
                memory=memory,
                collection="document_memory"
            )
            
            logger.debug(f"Stored {interaction_type} interaction for document {document_id} by user {user_id}")
        except Exception as e:
            logger.error(f"Error storing document interaction: {str(e)}")

================
File: app/db/models.py
================
import uuid
from datetime import datetime
from sqlalchemy import (
    Column, Integer, String, Text, Float, Boolean, 
    DateTime, ForeignKey, JSON, Table, Index, UniqueConstraint
)
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import relationship, backref

from app.db.session import Base

# Association table for document-tag many-to-many relationship
document_tags = Table(
    'document_tags',
    Base.metadata,
    Column('document_id', UUID(as_uuid=True), ForeignKey('documents.id'), primary_key=True),
    Column('tag_id', Integer, ForeignKey('tags.id'), primary_key=True),
    Column('added_at', DateTime, default=datetime.utcnow)
)

class Document(Base):
    """Document model for database"""
    __tablename__ = "documents"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    filename = Column(String, nullable=False)
    content = Column(Text, nullable=True)  # Can be null if we only store metadata
    doc_metadata = Column(JSONB, default={})  # Renamed from 'metadata' to 'doc_metadata'
    folder = Column(String, ForeignKey('folders.path'), default="/")
    uploaded = Column(DateTime, default=datetime.utcnow)
    processing_status = Column(String, default="pending")  # pending, processing, completed, failed
    processing_strategy = Column(String, nullable=True)
    file_size = Column(Integer, nullable=True)
    file_type = Column(String, nullable=True)
    last_accessed = Column(DateTime, default=datetime.utcnow)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=True)
    is_public = Column(Boolean, default=False)  # Whether the document is publicly accessible
    organization_id = Column(UUID(as_uuid=True), ForeignKey('organizations.id'), nullable=True)

    # Relationships
    chunks = relationship("Chunk", back_populates="document", cascade="all, delete-orphan")
    tags = relationship("Tag", secondary=document_tags, back_populates="documents")
    folder_rel = relationship("Folder", back_populates="documents")
    citations = relationship("Citation", back_populates="document")
    user = relationship("User", back_populates="documents")
    shared_with = relationship("DocumentPermission", back_populates="document", cascade="all, delete-orphan")
    organization = relationship("Organization", back_populates="documents")

    # Indexes
    __table_args__ = (
        Index('ix_documents_filename', filename),
        Index('ix_documents_folder', folder),
        Index('ix_documents_processing_status', processing_status),
        Index('ix_documents_is_public', is_public),
        Index('ix_documents_organization_id', organization_id),
    )

    def __repr__(self):
        return f"<Document(id={self.id}, filename='{self.filename}')>"


class DocumentPermission(Base):
    """Document permission model for database"""
    __tablename__ = "document_permissions"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), ForeignKey('documents.id', ondelete='CASCADE'), nullable=False)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id', ondelete='CASCADE'), nullable=False)
    permission_level = Column(String, nullable=False)  # 'read', 'write', 'admin'
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    document = relationship("Document", back_populates="shared_with")
    user = relationship("User", back_populates="document_permissions")

    # Indexes and constraints
    __table_args__ = (
        Index('ix_document_permissions_document_id', document_id),
        Index('ix_document_permissions_user_id', user_id),
        UniqueConstraint('document_id', 'user_id', name='uq_document_permissions_document_user'),
    )

    def __repr__(self):
        return f"<DocumentPermission(document_id={self.document_id}, user_id={self.user_id}, level='{self.permission_level}')>"


class Chunk(Base):
    """Chunk model for database"""
    __tablename__ = "chunks"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), ForeignKey('documents.id'), nullable=False)
    content = Column(Text, nullable=False)
    chunk_metadata = Column(JSONB, default={})  # Renamed from 'metadata' to 'chunk_metadata'
    index = Column(Integer, nullable=False)  # Position in the document
    embedding_quality = Column(Float, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    document = relationship("Document", back_populates="chunks")
    citations = relationship("Citation", back_populates="chunk")

    # Indexes
    __table_args__ = (
        Index('ix_chunks_document_id', document_id),
        Index('ix_chunks_document_id_index', document_id, index),
    )

    def __repr__(self):
        return f"<Chunk(id={self.id}, document_id={self.document_id}, index={self.index})>"


class Tag(Base):
    """Tag model for database"""
    __tablename__ = "tags"

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    usage_count = Column(Integer, default=0)

    # Relationships
    documents = relationship("Document", secondary=document_tags, back_populates="tags")

    # Indexes
    __table_args__ = (
        Index('ix_tags_name', name),
    )

    def __repr__(self):
        return f"<Tag(id={self.id}, name='{self.name}')>"


class Folder(Base):
    """Folder model for database"""
    __tablename__ = "folders"

    path = Column(String, primary_key=True)
    name = Column(String, nullable=False)
    parent_path = Column(String, ForeignKey('folders.path'), nullable=True)
    document_count = Column(Integer, default=0)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    documents = relationship("Document", back_populates="folder_rel")
    subfolders = relationship("Folder",
                             backref=backref("parent", remote_side=[path]),
                             cascade="all, delete-orphan")

    # Indexes
    __table_args__ = (
        Index('ix_folders_parent_path', parent_path),
    )

    def __repr__(self):
        return f"<Folder(path='{self.path}', name='{self.name}')>"


class Conversation(Base):
    """Conversation model for database"""
    __tablename__ = "conversations"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    conv_metadata = Column(JSONB, default={})  # Renamed from 'metadata' to 'conv_metadata'
    message_count = Column(Integer, default=0)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=True)

    # Relationships
    messages = relationship("Message", back_populates="conversation", cascade="all, delete-orphan")
    user = relationship("User", back_populates="conversations")
    memories = relationship("Memory", back_populates="conversation", cascade="all, delete-orphan")

    # Indexes
    __table_args__ = (
        Index('ix_conversations_created_at', created_at),
        Index('ix_conversations_updated_at', updated_at),
    )

    def __repr__(self):
        return f"<Conversation(id={self.id}, message_count={self.message_count})>"


class Message(Base):
    """Message model for database"""
    __tablename__ = "messages"

    id = Column(Integer, primary_key=True)
    conversation_id = Column(UUID(as_uuid=True), ForeignKey('conversations.id'), nullable=False)
    content = Column(Text, nullable=False)
    role = Column(String, nullable=False)  # user, assistant, system
    timestamp = Column(DateTime, default=datetime.utcnow)
    token_count = Column(Integer, nullable=True)

    # Relationships
    conversation = relationship("Conversation", back_populates="messages")
    citations = relationship("Citation", back_populates="message", cascade="all, delete-orphan")

    # Indexes
    __table_args__ = (
        Index('ix_messages_conversation_id', conversation_id),
        Index('ix_messages_timestamp', timestamp),
    )

    def __repr__(self):
        return f"<Message(id={self.id}, conversation_id={self.conversation_id}, role='{self.role}')>"


class Citation(Base):
    """Citation model for database"""
    __tablename__ = "citations"

    id = Column(Integer, primary_key=True)
    message_id = Column(Integer, ForeignKey('messages.id'), nullable=False)
    document_id = Column(UUID(as_uuid=True), ForeignKey('documents.id'), nullable=True)
    chunk_id = Column(UUID(as_uuid=True), ForeignKey('chunks.id'), nullable=True)
    relevance_score = Column(Float, nullable=True)
    excerpt = Column(Text, nullable=True)
    character_range_start = Column(Integer, nullable=True)
    character_range_end = Column(Integer, nullable=True)

    # Relationships
    message = relationship("Message", back_populates="citations")
    document = relationship("Document", back_populates="citations")
    chunk = relationship("Chunk", back_populates="citations")

    # Indexes
    __table_args__ = (
        Index('ix_citations_message_id', message_id),
        Index('ix_citations_document_id', document_id),
        Index('ix_citations_chunk_id', chunk_id),
    )

    def __repr__(self):
        return f"<Citation(id={self.id}, message_id={self.message_id}, document_id={self.document_id})>"


class ProcessingJob(Base):
    """Processing job model for database"""
    __tablename__ = "processing_jobs"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    status = Column(String, nullable=False, default="pending")  # pending, processing, completed, failed
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)
    document_count = Column(Integer, default=0)
    processed_count = Column(Integer, default=0)
    strategy = Column(String, nullable=True)
    job_metadata = Column(JSONB, default={})  # Renamed from 'metadata' to 'job_metadata'
    progress_percentage = Column(Float, default=0.0)
    error_message = Column(Text, nullable=True)

    # Indexes
    __table_args__ = (
        Index('ix_processing_jobs_status', status),
        Index('ix_processing_jobs_created_at', created_at),
    )

    def __repr__(self):
        return f"<ProcessingJob(id={self.id}, status='{self.status}', progress={self.progress_percentage}%)>"


class AnalyticsQuery(Base):
    """Analytics query model for database"""
    __tablename__ = "analytics_queries"

    id = Column(Integer, primary_key=True)
    query = Column(Text, nullable=False)
    model = Column(String, nullable=True)
    use_rag = Column(Boolean, default=True)
    timestamp = Column(DateTime, default=datetime.utcnow)
    response_time_ms = Column(Float, nullable=True)
    token_count = Column(Integer, nullable=True)
    document_id_list = Column(JSONB, default=[])  # Renamed from 'document_ids' to 'document_id_list'
    query_type = Column(String, nullable=True)  # simple, complex, agentic
    successful = Column(Boolean, default=True)

    # Indexes
    __table_args__ = (
        Index('ix_analytics_queries_timestamp', timestamp),
        Index('ix_analytics_queries_model', model),
        Index('ix_analytics_queries_query_type', query_type),
    )

    def __repr__(self):
        return f"<AnalyticsQuery(id={self.id}, query_type='{self.query_type}', response_time={self.response_time_ms}ms)>"


class Organization(Base):
    """Organization model for database"""
    __tablename__ = "organizations"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String, nullable=False)
    description = Column(String, nullable=True)
    settings = Column(JSONB, default={})
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    members = relationship("OrganizationMember", back_populates="organization", cascade="all, delete-orphan")
    documents = relationship("Document", back_populates="organization")

    # Indexes
    __table_args__ = (
        Index('ix_organizations_name', name),
    )

    def __repr__(self):
        return f"<Organization(id={self.id}, name='{self.name}')>"


class OrganizationMember(Base):
    """Organization member model for database"""
    __tablename__ = "organization_members"

    organization_id = Column(UUID(as_uuid=True), ForeignKey('organizations.id', ondelete='CASCADE'), primary_key=True)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id', ondelete='CASCADE'), primary_key=True)
    role = Column(String, nullable=False)  # 'owner', 'admin', 'member'
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    organization = relationship("Organization", back_populates="members")
    user = relationship("User", back_populates="organizations")

    # Indexes
    __table_args__ = (
        Index('ix_organization_members_organization_id', organization_id),
        Index('ix_organization_members_user_id', user_id),
    )

    def __repr__(self):
        return f"<OrganizationMember(organization_id={self.organization_id}, user_id={self.user_id}, role='{self.role}')>"


class User(Base):
    """User model for database"""
    __tablename__ = "users"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    username = Column(String, nullable=False, unique=True)
    email = Column(String, nullable=False, unique=True)
    password_hash = Column(String, nullable=False)
    full_name = Column(String, nullable=True)
    is_active = Column(Boolean, default=True)
    is_admin = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime, nullable=True)
    user_metadata = Column('metadata', JSONB, default={})  # Map user_metadata attribute to 'metadata' column

    # Relationships
    documents = relationship("Document", back_populates="user")
    conversations = relationship("Conversation", back_populates="user")
    password_reset_tokens = relationship("PasswordResetToken", back_populates="user", cascade="all, delete-orphan")
    document_permissions = relationship("DocumentPermission", back_populates="user", cascade="all, delete-orphan")
    roles = relationship("UserRole", back_populates="user", cascade="all, delete-orphan")
    notifications = relationship("Notification", back_populates="user", cascade="all, delete-orphan")
    organizations = relationship("OrganizationMember", back_populates="user", cascade="all, delete-orphan")

    # Indexes
    __table_args__ = (
        Index('ix_users_username', username),
        Index('ix_users_email', email),
    )

    def __repr__(self):
        return f"<User(id={self.id}, username='{self.username}')>"


class Role(Base):
    """Role model for database"""
    __tablename__ = "roles"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String, nullable=False, unique=True)
    description = Column(String, nullable=True)
    permissions = Column(JSONB, default={})
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    users = relationship("UserRole", back_populates="role", cascade="all, delete-orphan")

    # Indexes
    __table_args__ = (
        Index('ix_roles_name', name),
    )

    def __repr__(self):
        return f"<Role(id={self.id}, name='{self.name}')>"


class UserRole(Base):
    """User-role association model for database"""
    __tablename__ = "user_roles"

    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id', ondelete='CASCADE'), primary_key=True)
    role_id = Column(UUID(as_uuid=True), ForeignKey('roles.id', ondelete='CASCADE'), primary_key=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    user = relationship("User", back_populates="roles")
    role = relationship("Role", back_populates="users")

    # Indexes
    __table_args__ = (
        Index('ix_user_roles_user_id', user_id),
        Index('ix_user_roles_role_id', role_id),
    )

    def __repr__(self):
        return f"<UserRole(user_id={self.user_id}, role_id={self.role_id})>"


class PasswordResetToken(Base):
    """Password reset token model for database"""
    __tablename__ = "password_reset_tokens"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=False)
    token = Column(String, nullable=False, unique=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    expires_at = Column(DateTime, nullable=False)
    is_used = Column(Boolean, default=False)

    # Relationships
    user = relationship("User", back_populates="password_reset_tokens")

    # Indexes
    __table_args__ = (
        Index('ix_password_reset_tokens_token', token),
        Index('ix_password_reset_tokens_user_id', user_id),
        Index('ix_password_reset_tokens_expires_at', expires_at),
    )

    def __repr__(self):
        return f"<PasswordResetToken(id={self.id}, user_id={self.user_id}, is_used={self.is_used})>"


class Notification(Base):
    """Notification model for database"""
    __tablename__ = "notifications"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id', ondelete='CASCADE'), nullable=False)
    type = Column(String, nullable=False)  # e.g., 'document_shared', 'mention', 'system'
    title = Column(String, nullable=False)
    message = Column(Text, nullable=False)
    data = Column(JSONB, default={})
    is_read = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    read_at = Column(DateTime, nullable=True)

    # Relationships
    user = relationship("User", back_populates="notifications")

    # Indexes
    __table_args__ = (
        Index('ix_notifications_user_id', user_id),
        Index('ix_notifications_created_at', created_at),
        Index('ix_notifications_is_read', is_read),
    )

    def __repr__(self):
        return f"<Notification(id={self.id}, user_id={self.user_id}, type='{self.type}')>"


class BackgroundTask(Base):
    """Background task model for database"""
    __tablename__ = "background_tasks"

    id = Column(String, primary_key=True)
    name = Column(String, nullable=False)
    task_type = Column(String, nullable=False)
    params = Column(JSONB, nullable=True)
    priority = Column(Integer, nullable=True, default=50)
    dependencies = Column(Text, nullable=True)
    schedule_time = Column(DateTime, nullable=True)
    timeout_seconds = Column(Integer, nullable=True)
    max_retries = Column(Integer, nullable=True, default=0)
    task_metadata = Column(JSONB, nullable=True)  # Renamed from 'metadata' to 'task_metadata'
    status = Column(String, nullable=False, default="pending")
    created_at = Column(DateTime, nullable=True, default=datetime.utcnow)
    scheduled_at = Column(DateTime, nullable=True)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    retry_count = Column(Integer, nullable=True, default=0)
    result = Column(Text, nullable=True)
    error = Column(Text, nullable=True)
    progress = Column(Float, nullable=True, default=0.0)
    resource_usage = Column(JSONB, nullable=True)
    execution_time_ms = Column(Float, nullable=True)

    # Indexes
    __table_args__ = (
        Index('ix_background_tasks_status', status),
        Index('ix_background_tasks_task_type', task_type),
        Index('ix_background_tasks_created_at', created_at),
        Index('ix_background_tasks_schedule_time', schedule_time),
    )

    def __repr__(self):
        return f"<BackgroundTask(id={self.id}, name='{self.name}', status='{self.status}')>"

================
File: app/db/README.md
================
# Database Integration

This directory contains the database integration components for the Metis_RAG system.

## Overview

The database integration provides a PostgreSQL-based persistence layer for the Metis_RAG system, allowing for efficient storage and retrieval of documents, conversations, and analytics data.

## Components

### Database Connection (`session.py`)

Provides database connection management with:
- Connection pooling for efficient database access
- Session management with context manager support
- Transaction management with automatic rollback on exceptions
- Database initialization function

### SQLAlchemy Models (`models.py`)

Defines the database schema using SQLAlchemy ORM:
- Document: Stores document metadata and content
- Chunk: Stores document chunks for retrieval
- Tag: Stores document tags
- Folder: Stores folder structure for documents
- Conversation: Stores conversation metadata
  - Uses conv_metadata (JSON field) to store flexible metadata including user_id
  - Tracks message count and timestamps
- Message: Stores conversation messages
- Citation: Stores citations for messages
- ProcessingJob: Stores document processing job information
- AnalyticsQuery: Stores query analytics data

#### Important Note on Metadata Fields

Several models use JSON fields for flexible metadata storage:
- Conversation.conv_metadata: Stores user_id and other conversation metadata
- Document.doc_metadata: Stores document-specific metadata
- Chunk.chunk_metadata: Stores chunk-specific metadata
- ProcessingJob.job_metadata: Stores job-specific metadata

When accessing these fields in code, always use the attribute name (e.g., `conversation.conv_metadata.get("user_id")`) rather than trying to access properties directly (e.g., `conversation.user_id`).

#### Working with Pydantic Models vs. SQLAlchemy Models

The system uses both SQLAlchemy models (in `app/db/models.py`) and Pydantic models (in `app/models/*.py`). It's important to understand the difference:

- **SQLAlchemy Models**: Used for database operations, with attributes accessed directly (e.g., `citation.document_id`)
- **Pydantic Models**: Used for API requests/responses, with attributes accessed directly (e.g., `citation.document_id`)

When working with objects that could be either type:
- For SQLAlchemy models with JSON fields, use the get method (e.g., `conversation.conv_metadata.get("user_id")`)
- For Pydantic models, check if attributes exist with hasattr (e.g., `if hasattr(source, "document_id")`)

Example with Citation objects:
```python
# For SQLAlchemy Citation model
citation.document_id  # Direct access

# For Pydantic Citation model
if hasattr(source, "document_id"):
    document_id = source.document_id
else:
    document_id = None
```

### Repository Classes (`repositories/`)

Implements the repository pattern for database operations:
- BaseRepository: Common CRUD operations
- DocumentRepository: Document management operations
- ConversationRepository: Conversation and message management with async operations
  - Handles conversations, messages, and citations
  - Stores user_id and other metadata in the conv_metadata JSON field
  - Provides methods for conversation history retrieval and search
- AnalyticsRepository: Query logging and analytics

### Dependencies (`dependencies.py`)

Provides FastAPI dependency injection for repositories:
- get_db: Database session dependency
- get_document_repository: Document repository dependency
- get_conversation_repository: Conversation repository dependency
- get_analytics_repository: Analytics repository dependency

## Database Migrations

Database migrations are managed using Alembic:
- Configuration: `alembic.ini` in the project root
- Environment: `alembic/env.py`
- Migrations: `alembic/versions/`

## Usage

### Document Repository Example

To use the document repository in API endpoints:

```python
from fastapi import Depends
from app.db.dependencies import get_document_repository
from app.db.repositories.document_repository import DocumentRepository

@router.get("/documents/{document_id}")
async def get_document(
    document_id: str,
    document_repo: DocumentRepository = Depends(get_document_repository)
):
    document = await document_repo.get_by_id(document_id)
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    return document
```

### Conversation Repository Example

To use the conversation repository with proper metadata handling:

```python
from fastapi import Depends
from app.db.dependencies import get_conversation_repository
from app.db.repositories.conversation_repository import ConversationRepository

@router.get("/conversations/{conversation_id}")
async def get_conversation(
    conversation_id: UUID,
    conversation_repo: ConversationRepository = Depends(get_conversation_repository)
):
    conversation = await conversation_repo.get_by_id(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    # Access user_id from metadata correctly
    user_id = conversation.conv_metadata.get("user_id")
    
    # Get messages for this conversation
    messages = await conversation_repo.get_conversation_messages(
        conversation_id=conversation_id,
        limit=100
    )
    
    return {
        "id": str(conversation.id),
        "user_id": user_id,
        "created_at": conversation.created_at,
        "updated_at": conversation.updated_at,
        "messages": messages,
        "message_count": conversation.message_count
    }
```

================
File: app/db/schema_inspector.py
================
"""
Schema Inspector - Module for PostgreSQL schema introspection

This module provides methods to retrieve database schema information from PostgreSQL,
including schemas, tables, columns, indexes, and constraints.
"""
import logging
from typing import Dict, List, Any, Optional, Tuple
import asyncpg

from app.db.connection_manager import connection_manager

class SchemaInspector:
    """
    PostgreSQL schema introspection utility
    
    This class provides methods to retrieve database schema information from PostgreSQL,
    including schemas, tables, columns, indexes, and constraints.
    """
    
    def __init__(self):
        """Initialize the schema inspector"""
        self.logger = logging.getLogger("app.db.schema_inspector")
        self._cache = {}  # Cache for schema information
    
    async def get_schemas(self, conn_id: str) -> List[Dict[str, Any]]:
        """
        Get a list of schemas in the database
        
        Args:
            conn_id: Connection ID
            
        Returns:
            List of schema information dictionaries
        """
        # Check cache
        cache_key = f"{conn_id}:schemas"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Query schemas
            query = """
            SELECT 
                n.nspname AS schema_name,
                pg_catalog.pg_get_userbyid(n.nspowner) AS owner,
                pg_catalog.obj_description(n.oid, 'pg_namespace') AS description
            FROM pg_catalog.pg_namespace n
            WHERE n.nspname !~ '^pg_' 
              AND n.nspname <> 'information_schema'
            ORDER BY 1;
            """
            
            rows = await conn.fetch(query)
            
            # Convert to list of dictionaries
            schemas = [dict(row) for row in rows]
            
            # Cache results
            self._cache[cache_key] = schemas
            
            return schemas
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def get_tables(self, conn_id: str, schema: str = 'public') -> List[Dict[str, Any]]:
        """
        Get a list of tables in the specified schema
        
        Args:
            conn_id: Connection ID
            schema: Schema name (default: 'public')
            
        Returns:
            List of table information dictionaries
        """
        # Check cache
        cache_key = f"{conn_id}:tables:{schema}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Query tables with row counts
            query = """
            SELECT 
                c.relname AS table_name,
                pg_catalog.pg_get_userbyid(c.relowner) AS owner,
                pg_catalog.obj_description(c.oid, 'pg_class') AS description,
                c.reltuples::bigint AS row_estimate,
                pg_catalog.pg_size_pretty(pg_catalog.pg_total_relation_size(c.oid)) AS total_size,
                CASE 
                    WHEN c.relkind = 'r' THEN 'table'
                    WHEN c.relkind = 'v' THEN 'view'
                    WHEN c.relkind = 'm' THEN 'materialized view'
                    WHEN c.relkind = 'f' THEN 'foreign table'
                    ELSE c.relkind::text
                END AS type
            FROM pg_catalog.pg_class c
            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
            WHERE c.relkind IN ('r', 'v', 'm', 'f')
              AND n.nspname = $1
              AND pg_catalog.pg_table_is_visible(c.oid)
            ORDER BY c.relname;
            """
            
            rows = await conn.fetch(query, schema)
            
            # Convert to list of dictionaries
            tables = [dict(row) for row in rows]
            
            # Get actual row counts for tables with reasonable size
            for table in tables:
                if table['type'] == 'table' and table['row_estimate'] < 1000000:
                    try:
                        count_query = f"SELECT COUNT(*) AS exact_count FROM {schema}.{table['table_name']}"
                        count_result = await conn.fetchval(count_query)
                        table['exact_row_count'] = count_result
                    except Exception as e:
                        self.logger.warning(f"Could not get exact row count for {schema}.{table['table_name']}: {str(e)}")
                        table['exact_row_count'] = None
                else:
                    table['exact_row_count'] = None
            
            # Cache results
            self._cache[cache_key] = tables
            
            return tables
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def get_columns(self, conn_id: str, table_name: str, schema: str = 'public') -> List[Dict[str, Any]]:
        """
        Get a list of columns for the specified table
        
        Args:
            conn_id: Connection ID
            table_name: Table name
            schema: Schema name (default: 'public')
            
        Returns:
            List of column information dictionaries
        """
        # Check cache
        cache_key = f"{conn_id}:columns:{schema}.{table_name}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Query columns
            query = """
            SELECT 
                a.attname AS column_name,
                pg_catalog.format_type(a.atttypid, a.atttypmod) AS data_type,
                CASE 
                    WHEN a.attnotnull THEN 'NO' 
                    ELSE 'YES' 
                END AS is_nullable,
                (SELECT pg_catalog.pg_get_expr(d.adbin, d.adrelid) 
                 FROM pg_catalog.pg_attrdef d 
                 WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) AS default_value,
                col_description(a.attrelid, a.attnum) AS description,
                a.attnum AS ordinal_position
            FROM pg_catalog.pg_attribute a
            JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
            JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
            WHERE a.attnum > 0 
              AND NOT a.attisdropped
              AND n.nspname = $1
              AND c.relname = $2
            ORDER BY a.attnum;
            """
            
            rows = await conn.fetch(query, schema, table_name)
            
            # Convert to list of dictionaries
            columns = [dict(row) for row in rows]
            
            # Cache results
            self._cache[cache_key] = columns
            
            return columns
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def get_indexes(self, conn_id: str, table_name: str, schema: str = 'public') -> List[Dict[str, Any]]:
        """
        Get a list of indexes for the specified table
        
        Args:
            conn_id: Connection ID
            table_name: Table name
            schema: Schema name (default: 'public')
            
        Returns:
            List of index information dictionaries
        """
        # Check cache
        cache_key = f"{conn_id}:indexes:{schema}.{table_name}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Query indexes
            query = """
            SELECT
                i.relname AS index_name,
                am.amname AS index_type,
                pg_catalog.pg_get_indexdef(i.oid, 0, true) AS index_definition,
                CASE 
                    WHEN ix.indisunique THEN 'YES' 
                    ELSE 'NO' 
                END AS is_unique,
                CASE 
                    WHEN ix.indisprimary THEN 'YES' 
                    ELSE 'NO' 
                END AS is_primary,
                pg_catalog.pg_size_pretty(pg_catalog.pg_relation_size(i.oid)) AS index_size,
                pg_catalog.obj_description(i.oid, 'pg_class') AS description
            FROM pg_catalog.pg_class c
            JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
            JOIN pg_catalog.pg_index ix ON c.oid = ix.indrelid
            JOIN pg_catalog.pg_class i ON i.oid = ix.indexrelid
            JOIN pg_catalog.pg_am am ON i.relam = am.oid
            WHERE c.relkind = 'r'
              AND n.nspname = $1
              AND c.relname = $2
            ORDER BY i.relname;
            """
            
            rows = await conn.fetch(query, schema, table_name)
            
            # Convert to list of dictionaries
            indexes = [dict(row) for row in rows]
            
            # Cache results
            self._cache[cache_key] = indexes
            
            return indexes
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def get_constraints(self, conn_id: str, table_name: str, schema: str = 'public') -> List[Dict[str, Any]]:
        """
        Get a list of constraints for the specified table
        
        Args:
            conn_id: Connection ID
            table_name: Table name
            schema: Schema name (default: 'public')
            
        Returns:
            List of constraint information dictionaries
        """
        # Check cache
        cache_key = f"{conn_id}:constraints:{schema}.{table_name}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Query constraints
            query = """
            SELECT
                c.conname AS constraint_name,
                CASE c.contype
                    WHEN 'c' THEN 'CHECK'
                    WHEN 'f' THEN 'FOREIGN KEY'
                    WHEN 'p' THEN 'PRIMARY KEY'
                    WHEN 'u' THEN 'UNIQUE'
                    WHEN 't' THEN 'TRIGGER'
                    WHEN 'x' THEN 'EXCLUSION'
                    ELSE c.contype::text
                END AS constraint_type,
                pg_catalog.pg_get_constraintdef(c.oid, true) AS definition,
                c.condeferrable AS is_deferrable,
                c.condeferred AS is_deferred,
                c.convalidated AS is_validated,
                pg_catalog.obj_description(c.oid, 'pg_constraint') AS description
            FROM pg_catalog.pg_constraint c
            JOIN pg_catalog.pg_class r ON r.oid = c.conrelid
            JOIN pg_catalog.pg_namespace n ON n.oid = r.relnamespace
            WHERE r.relname = $2
              AND n.nspname = $1
            ORDER BY c.contype, c.conname;
            """
            
            rows = await conn.fetch(query, schema, table_name)
            
            # Convert to list of dictionaries
            constraints = [dict(row) for row in rows]
            
            # Cache results
            self._cache[cache_key] = constraints
            
            return constraints
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def get_foreign_keys(self, conn_id: str, table_name: str, schema: str = 'public') -> List[Dict[str, Any]]:
        """
        Get a list of foreign keys for the specified table
        
        Args:
            conn_id: Connection ID
            table_name: Table name
            schema: Schema name (default: 'public')
            
        Returns:
            List of foreign key information dictionaries
        """
        # Check cache
        cache_key = f"{conn_id}:foreign_keys:{schema}.{table_name}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Query foreign keys
            query = """
            SELECT
                c.conname AS constraint_name,
                n2.nspname AS referenced_schema,
                c2.relname AS referenced_table,
                ARRAY(
                    SELECT attname FROM pg_catalog.pg_attribute
                    WHERE attrelid = c.conrelid AND attnum = ANY(c.conkey)
                ) AS column_names,
                ARRAY(
                    SELECT attname FROM pg_catalog.pg_attribute
                    WHERE attrelid = c.confrelid AND attnum = ANY(c.confkey)
                ) AS referenced_columns,
                CASE c.confupdtype
                    WHEN 'a' THEN 'NO ACTION'
                    WHEN 'r' THEN 'RESTRICT'
                    WHEN 'c' THEN 'CASCADE'
                    WHEN 'n' THEN 'SET NULL'
                    WHEN 'd' THEN 'SET DEFAULT'
                    ELSE NULL
                END AS update_rule,
                CASE c.confdeltype
                    WHEN 'a' THEN 'NO ACTION'
                    WHEN 'r' THEN 'RESTRICT'
                    WHEN 'c' THEN 'CASCADE'
                    WHEN 'n' THEN 'SET NULL'
                    WHEN 'd' THEN 'SET DEFAULT'
                    ELSE NULL
                END AS delete_rule
            FROM pg_catalog.pg_constraint c
            JOIN pg_catalog.pg_class r ON r.oid = c.conrelid
            JOIN pg_catalog.pg_namespace n ON n.oid = r.relnamespace
            JOIN pg_catalog.pg_class c2 ON c2.oid = c.confrelid
            JOIN pg_catalog.pg_namespace n2 ON n2.oid = c2.relnamespace
            WHERE r.relname = $2
              AND n.nspname = $1
              AND c.contype = 'f'
            ORDER BY c.conname;
            """
            
            rows = await conn.fetch(query, schema, table_name)
            
            # Convert to list of dictionaries
            foreign_keys = [dict(row) for row in rows]
            
            # Cache results
            self._cache[cache_key] = foreign_keys
            
            return foreign_keys
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def get_table_structure(self, conn_id: str, table_name: str, schema: str = 'public') -> Dict[str, Any]:
        """
        Get comprehensive structure information for a table
        
        Args:
            conn_id: Connection ID
            table_name: Table name
            schema: Schema name (default: 'public')
            
        Returns:
            Dictionary with table structure information
        """
        # Check cache
        cache_key = f"{conn_id}:table_structure:{schema}.{table_name}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get all table components
        tables = await self.get_tables(conn_id, schema)
        table_info = next((t for t in tables if t['table_name'] == table_name), None)
        
        if not table_info:
            raise ValueError(f"Table {schema}.{table_name} not found")
        
        columns = await self.get_columns(conn_id, table_name, schema)
        indexes = await self.get_indexes(conn_id, table_name, schema)
        constraints = await self.get_constraints(conn_id, table_name, schema)
        foreign_keys = await self.get_foreign_keys(conn_id, table_name, schema)
        
        # Combine into a single structure
        table_structure = {
            "table_name": table_name,
            "schema": schema,
            "description": table_info.get('description'),
            "owner": table_info.get('owner'),
            "row_estimate": table_info.get('row_estimate'),
            "exact_row_count": table_info.get('exact_row_count'),
            "total_size": table_info.get('total_size'),
            "type": table_info.get('type'),
            "columns": columns,
            "indexes": indexes,
            "constraints": constraints,
            "foreign_keys": foreign_keys
        }
        
        # Cache results
        self._cache[cache_key] = table_structure
        
        return table_structure
    
    async def get_database_structure(self, conn_id: str) -> Dict[str, Any]:
        """
        Get comprehensive structure information for the entire database
        
        Args:
            conn_id: Connection ID
            
        Returns:
            Dictionary with database structure information
        """
        # Check cache
        cache_key = f"{conn_id}:database_structure"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Get database name and version
            db_name = await conn.fetchval("SELECT current_database()")
            version = await conn.fetchval("SELECT version()")
            
            # Get schemas
            schemas = await self.get_schemas(conn_id)
            
            # Get tables for each schema
            schema_data = []
            for schema in schemas:
                schema_name = schema['schema_name']
                tables = await self.get_tables(conn_id, schema_name)
                
                # Get columns for each table
                table_data = []
                for table in tables:
                    table_name = table['table_name']
                    columns = await self.get_columns(conn_id, table_name, schema_name)
                    indexes = await self.get_indexes(conn_id, table_name, schema_name)
                    constraints = await self.get_constraints(conn_id, table_name, schema_name)
                    
                    table_data.append({
                        "table_name": table_name,
                        "description": table.get('description'),
                        "owner": table.get('owner'),
                        "row_estimate": table.get('row_estimate'),
                        "exact_row_count": table.get('exact_row_count'),
                        "total_size": table.get('total_size'),
                        "type": table.get('type'),
                        "columns": columns,
                        "indexes": indexes,
                        "constraints": constraints
                    })
                
                schema_data.append({
                    "schema_name": schema_name,
                    "description": schema.get('description'),
                    "owner": schema.get('owner'),
                    "tables": table_data
                })
            
            # Get extensions
            extensions_query = """
            SELECT 
                e.extname AS name,
                e.extversion AS version,
                n.nspname AS schema,
                c.description
            FROM pg_catalog.pg_extension e
            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = e.extnamespace
            LEFT JOIN pg_catalog.pg_description c ON c.objoid = e.oid
            ORDER BY e.extname;
            """
            
            extension_rows = await conn.fetch(extensions_query)
            extensions = [dict(row) for row in extension_rows]
            
            # Combine into a single structure
            database_structure = {
                "database_name": db_name,
                "version": version,
                "schemas": schema_data,
                "extensions": extensions
            }
            
            # Cache results
            self._cache[cache_key] = database_structure
            
            return database_structure
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def get_extension_info(self, conn_id: str, extension_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Get information about installed PostgreSQL extensions
        
        Args:
            conn_id: Connection ID
            extension_name: Optional extension name to filter by
            
        Returns:
            List of extension information dictionaries
        """
        # Check cache
        cache_key = f"{conn_id}:extensions:{extension_name or 'all'}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Get connection
        conn_type = connection_manager.get_connection_type(conn_id)
        if conn_type != 'postgres':
            raise ValueError("Schema introspection is only supported for PostgreSQL")
        
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Query extensions
            query = """
            SELECT 
                e.extname AS name,
                e.extversion AS version,
                n.nspname AS schema,
                c.description,
                pg_catalog.pg_get_userbyid(e.extowner) AS owner,
                e.extrelocatable AS relocatable,
                array_to_string(e.extconfig, ', ') AS config_tables
            FROM pg_catalog.pg_extension e
            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = e.extnamespace
            LEFT JOIN pg_catalog.pg_description c ON c.objoid = e.oid
            """
            
            if extension_name:
                query += " WHERE e.extname = $1"
                rows = await conn.fetch(query, extension_name)
            else:
                query += " ORDER BY e.extname"
                rows = await conn.fetch(query)
            
            # Convert to list of dictionaries
            extensions = [dict(row) for row in rows]
            
            # Cache results
            self._cache[cache_key] = extensions
            
            return extensions
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    async def check_extension_installed(self, conn_id: str, extension_name: str) -> bool:
        """
        Check if a specific PostgreSQL extension is installed
        
        Args:
            conn_id: Connection ID
            extension_name: Extension name to check
            
        Returns:
            True if the extension is installed, False otherwise
        """
        extensions = await self.get_extension_info(conn_id, extension_name)
        return len(extensions) > 0
    
    async def get_pgvector_info(self, conn_id: str) -> Dict[str, Any]:
        """
        Get information about pgvector extension if installed
        
        Args:
            conn_id: Connection ID
            
        Returns:
            Dictionary with pgvector information or None if not installed
        """
        # Check if pgvector is installed
        is_installed = await self.check_extension_installed(conn_id, 'vector')
        if not is_installed:
            return {"installed": False}
        
        # Get connection
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Get pgvector version
            version = await conn.fetchval("SELECT extversion FROM pg_extension WHERE extname = 'vector'")
            
            # Get vector columns
            vector_columns_query = """
            SELECT 
                n.nspname AS schema,
                c.relname AS table,
                a.attname AS column,
                t.typname AS type,
                a.atttypmod AS dimensions
            FROM pg_attribute a
            JOIN pg_class c ON a.attrelid = c.oid
            JOIN pg_namespace n ON c.relnamespace = n.oid
            JOIN pg_type t ON a.atttypid = t.oid
            WHERE t.typname = 'vector'
              AND n.nspname NOT IN ('pg_catalog', 'information_schema')
              AND a.attnum > 0
              AND NOT a.attisdropped
            ORDER BY n.nspname, c.relname, a.attnum;
            """
            
            vector_columns = await conn.fetch(vector_columns_query)
            
            # Get vector indexes
            vector_indexes_query = """
            SELECT 
                n.nspname AS schema,
                c.relname AS table,
                a.attname AS column,
                i.relname AS index_name,
                am.amname AS index_method
            FROM pg_index x
            JOIN pg_class c ON c.oid = x.indrelid
            JOIN pg_class i ON i.oid = x.indexrelid
            JOIN pg_attribute a ON a.attrelid = c.oid AND a.attnum = ANY(x.indkey)
            JOIN pg_namespace n ON n.oid = c.relnamespace
            JOIN pg_am am ON am.oid = i.relam
            JOIN pg_type t ON a.atttypid = t.oid
            WHERE t.typname = 'vector'
              AND n.nspname NOT IN ('pg_catalog', 'information_schema')
            ORDER BY n.nspname, c.relname, i.relname;
            """
            
            vector_indexes = await conn.fetch(vector_indexes_query)
            
            return {
                "installed": True,
                "version": version,
                "vector_columns": [dict(row) for row in vector_columns],
                "vector_indexes": [dict(row) for row in vector_indexes]
            }
        finally:
            # Release connection back to pool
            await connection_manager.release_postgres_connection(conn_id, conn)
    
    def clear_cache(self, conn_id: Optional[str] = None):
        """
        Clear the schema information cache
        
        Args:
            conn_id: Optional connection ID to clear cache for.
                    If None, clear the entire cache.
        """
        if conn_id:
            # Clear cache for specific connection
            keys_to_delete = [key for key in self._cache if key.startswith(f"{conn_id}:")]
            for key in keys_to_delete:
                del self._cache[key]
        else:
            # Clear entire cache
            self._cache = {}

# Create a singleton instance
schema_inspector = SchemaInspector()

================
File: app/db/session.py
================
"""
Database session management
"""
import logging
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import declarative_base

from app.core.config import SETTINGS

logger = logging.getLogger("app.db.session")

# Create SQLAlchemy base
Base = declarative_base()

# Create async engine with connection pooling
print(f"DATABASE_URL in session.py: {SETTINGS.database_url}")  # Debug print
if SETTINGS.database_type.startswith("sqlite"):
    # SQLite doesn't support pool_size and max_overflow
    engine = create_async_engine(
        SETTINGS.database_url,
        echo=False,  # Set to True for SQL query logging
        pool_pre_ping=True,  # Verify connections before using them
    )
else:
    engine = create_async_engine(
        SETTINGS.database_url,
        echo=False,  # Set to True for SQL query logging
        pool_size=SETTINGS.database_pool_size,
        max_overflow=SETTINGS.database_max_overflow,
        pool_pre_ping=True,  # Verify connections before using them
        pool_recycle=3600,   # Recycle connections after 1 hour
        # Use a specific execution option to handle async operations
        execution_options={
            "isolation_level": "READ COMMITTED"
        }
    )

# Create async session factory
AsyncSessionLocal = async_sessionmaker(
    bind=engine,
    expire_on_commit=False,
    autoflush=False,
    autocommit=False,
    future=True,
    class_=AsyncSession
)

# Create a function to get a session
async def get_session():
    async with AsyncSessionLocal() as session:
        yield session

async def init_db():
    """
    Initialize the database by creating all tables.
    This should be called during application startup.
    """
    logger.info("Initializing database")
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

================
File: app/middleware/__init__.py
================
from app.middleware.auth import AuthMiddleware

__all__ = ["AuthMiddleware"]

================
File: app/middleware/auth.py
================
from fastapi import Request, status
from fastapi.responses import RedirectResponse, JSONResponse
from typing import List, Callable, Optional, Dict, Any
from starlette.types import ASGIApp, Scope, Receive, Send
import logging
from jose import jwt, JWTError

from app.core.security_alerts import SecurityEvent, log_security_event
from app.core.config import SETTINGS

# Setup logging
logger = logging.getLogger("app.middleware.auth")

async def log_suspicious_requests(request: Request, call_next):
    """
    Middleware to log suspicious requests that might pose security risks
    """
    path = request.url.path
    query_params = request.query_params
    
    # Check for sensitive parameters in URLs
    sensitive_params = ['username', 'password', 'token', 'key', 'secret', 'api_key', 'auth']
    found_sensitive = [param for param in sensitive_params if param in query_params]
    
    if found_sensitive:
        # Collect request metadata for security analysis
        client_ip = request.client.host if request.client else "unknown"
        user_agent = request.headers.get("user-agent", "unknown")
        referrer = request.headers.get("referer", "none")
        # Log security event (without logging the actual sensitive values)
        logger.warning(
            f"Security alert: Sensitive parameters ({', '.join(found_sensitive)}) "
            f"detected in URL for path: {path}. "
            f"IP: {client_ip}, "
            f"User-Agent: {user_agent}, "
            f"Referrer: {referrer}"
        )
        
        # Create and log security event
        security_event = SecurityEvent(
            event_type="sensitive_params_in_url",
            severity="medium",
            source_ip=client_ip,
            user_agent=user_agent,
            details={
                "path": path,
                "sensitive_params": found_sensitive,
                "referrer": referrer,
                "query_params": str(query_params)
            }
        )
        log_security_event(security_event)
    
    # Continue with the request
    response = await call_next(request)
    return response


class AuthMiddleware:
    """
    Middleware to check for authentication on protected routes
    
    This middleware validates JWT tokens for protected routes and API endpoints.
    It redirects unauthenticated browser requests to the login page and
    returns 401 Unauthorized for unauthenticated API requests.
    """
    
    def __init__(
        self,
        app: ASGIApp,
        protected_routes: List[str] = None,
        api_routes: List[str] = None,
        exclude_routes: List[str] = None,
        login_url: str = "/login"
    ):
        """
        Initialize the middleware
        
        Args:
            app: The ASGI application
            protected_routes: List of routes that require authentication
            api_routes: List of API routes that require authentication
            exclude_routes: List of routes to exclude from authentication
            login_url: URL to redirect to for login
        """
        self.app = app
        self.protected_routes = protected_routes or [
            "/documents",
            "/chat",
            "/analytics",
            "/system",
            "/tasks"
        ]
        self.api_routes = api_routes or [
            "/api/documents",
            "/api/chat",
            "/api/analytics",
            "/api/system",
            "/api/tasks",
            "/api/processing",
            "/api/query"
        ]
        self.exclude_routes = exclude_routes or [
            "/login",
            "/register",
            "/api/auth/token",
            "/api/auth/refresh",
            "/api/auth/register",
            "/api/health",
            "/health",
            "/static",
            "/forgot-password",
            "/reset-password"
        ]
        self.login_url = login_url
    
    async def __call__(self, scope: Scope, receive: Receive, send: Send):
        """
        Process the request
        
        Args:
            scope: The ASGI scope
            receive: The ASGI receive function
            send: The ASGI send function
            
        Returns:
            The response from the next middleware or route handler
        """
        if scope["type"] != "http":
            return await self.app(scope, receive, send)
            
        # Create a request object
        request = Request(scope)
        # Check if the route is protected
        path = request.url.path
        
        # Skip authentication for excluded routes
        for route in self.exclude_routes:
            if path.startswith(route):
                return await self.app(scope, receive, send)
        
        # Check if the route is protected
        is_protected = False
        for route in self.protected_routes:
            if path.startswith(route):
                is_protected = True
                break
        
        # Check if the route is an API route
        is_api = False
        for route in self.api_routes:
            if path.startswith(route):
                is_api = True
                break
        
        # If the route is not protected, continue
        if not is_protected and not is_api:
            return await self.app(scope, receive, send)
        
        # Check for authentication
        auth_header = request.headers.get("Authorization")
        auth_cookie = request.cookies.get("auth_token")
        
        # For API routes, validate JWT token
        if is_api:
            if not auth_header or not auth_header.startswith("Bearer "):
                # Log unauthorized API access attempt
                client_ip = request.client.host if request.client else "unknown"
                user_agent = request.headers.get("user-agent", "unknown")
                logger.warning(
                    f"Unauthorized API access attempt: {path}, "
                    f"IP: {client_ip}, User-Agent: {user_agent}"
                )
                
                # Create a 401 response
                response = JSONResponse(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    content={"detail": "Not authenticated"},
                    headers={"WWW-Authenticate": "Bearer"}
                )
                await response(scope, receive, send)
                return
            
            # Extract and validate the token
            token = auth_header.split(" ")[1]
            is_valid = self._validate_jwt_token(token, request)
            
            if not is_valid:
                # Log invalid token
                client_ip = request.client.host if request.client else "unknown"
                user_agent = request.headers.get("user-agent", "unknown")
                logger.warning(
                    f"Invalid token for API access: {path}, "
                    f"IP: {client_ip}, User-Agent: {user_agent}"
                )
                
                # Create a 401 response
                response = JSONResponse(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    content={"detail": "Invalid or expired token"},
                    headers={"WWW-Authenticate": "Bearer"}
                )
                await response(scope, receive, send)
                return
            
            # Continue with the request
            return await self.app(scope, receive, send)
        
        # For protected routes, check for authentication cookie or header
        if is_protected:
            # First check for auth cookie (for browser requests)
            if auth_cookie:
                is_valid = self._validate_jwt_token(auth_cookie, request)
                if is_valid:
                    # Continue with the request
                    return await self.app(scope, receive, send)
            
            # Then check for auth header (for programmatic requests)
            if auth_header and auth_header.startswith("Bearer "):
                token = auth_header.split(" ")[1]
                is_valid = self._validate_jwt_token(token, request)
                if is_valid:
                    # Continue with the request
                    return await self.app(scope, receive, send)
            
            # If no valid authentication, redirect to login
            redirect_url = f"{self.login_url}?redirect={path}"
            response = RedirectResponse(url=redirect_url, status_code=status.HTTP_303_SEE_OTHER)
            await response(scope, receive, send)
            return
        
        # Continue with the request
        return await self.app(scope, receive, send)
    
    def _validate_jwt_token(self, token: str, request: Request) -> bool:
        """
        Validate a JWT token
        
        Args:
            token: The JWT token to validate
            request: The request object for logging
            
        Returns:
            True if the token is valid, False otherwise
        """
        try:
            # Decode and validate the token
            payload = jwt.decode(
                token,
                SETTINGS.secret_key,
                algorithms=[SETTINGS.algorithm],
                options={
                    "verify_signature": True,
                    "verify_aud": False  # Don't verify audience claim for now
                }
            )
            
            # Check required claims
            if "sub" not in payload or "user_id" not in payload:
                return False
            
            # Check token type
            if payload.get("token_type") != "access":
                return False
            
            # Token is valid
            return True
            
        except JWTError as e:
            # Log the error
            client_ip = request.client.host if request.client else "unknown"
            user_agent = request.headers.get("user-agent", "unknown")
            logger.warning(
                f"JWT validation error: {str(e)}, "
                f"IP: {client_ip}, User-Agent: {user_agent}"
            )
            
            # Create and log security event
            security_event = SecurityEvent(
                event_type="invalid_jwt",
                severity="medium",
                source_ip=client_ip,
                user_agent=user_agent,
                details={
                    "error": str(e),
                    "path": request.url.path
                }
            )
            log_security_event(security_event)
            
            return False

================
File: app/middleware/db_context.py
================
from fastapi import Request, Depends
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession
import logging
import asyncio
from jose import jwt, JWTError
from starlette.types import ASGIApp, Scope, Receive, Send

from app.db.dependencies import get_db
from app.core.config import SETTINGS

# Setup logging
logger = logging.getLogger("app.middleware.db_context")

async def extract_user_id_from_request(request: Request) -> str:
    """
    Extract the user ID from the request's Authorization header
    
    Args:
        request: FastAPI request
        
    Returns:
        User ID if found, None otherwise
    """
    # Skip token validation for certain paths
    path = request.url.path.lower()
    if path.startswith("/static") or path == "/login" or path == "/favicon.ico" or path == "/register" or path.startswith("/api/auth"):
        return None
    
    # Get authorization header
    auth_header = request.headers.get("Authorization")
    auth_cookie = request.cookies.get("auth_token")
    
    # Check for auth header
    if auth_header and auth_header.startswith("Bearer "):
        token = auth_header.split(" ")[1]
    # Check for auth cookie
    elif auth_cookie:
        token = auth_cookie
    else:
        return None
    
    try:
        # Decode token
        payload = jwt.decode(
            token,
            SETTINGS.secret_key,
            algorithms=[SETTINGS.algorithm],
            options={"verify_aud": False}  # Don't verify audience claim for now
        )
        
        # Get user ID from token
        user_id = payload.get("user_id")
        if not user_id:
            return None
        
        # Check token type
        token_type = payload.get("token_type")
        if token_type != "access":
            return None
        
        return user_id
    except JWTError as e:
        # Only log warnings for non-public paths
        if not (path.startswith("/static") or path == "/login" or path == "/favicon.ico" or path == "/register" or path.startswith("/api/auth")):
            logger.warning(f"JWT validation error in DB context: {str(e)}")
        return None


async def set_db_context(db: AsyncSession, user_id: str = None):
    """
    Set the database context for Row Level Security
    
    Args:
        db: Database session
        user_id: User ID (optional, will be extracted from request if not provided)
    
    Returns:
        Database session with context set
    """
    try:
        if user_id:
            # Set the current_user_id for RLS policies
            await db.execute(text(f"SET app.current_user_id = '{user_id}'"))
        else:
            # Set to NULL if no user - use empty string instead of NULL
            await db.execute(text("SET app.current_user_id = ''"))
    except Exception as e:
        # Log the error but continue
        logger.error(f"Error setting database context: {e}")
        # Set to empty string if error
        await db.execute(text("SET app.current_user_id = ''"))
    
    return db


class DBContextMiddleware:
    """
    Middleware to set database context for Row Level Security
    """
    
    def __init__(self, app):
        """
        Initialize the middleware
        
        Args:
            app: The ASGI application
        """
        self.app = app
    
    async def __call__(self, scope: Scope, receive: Receive, send: Send):
        """
        Process the request and set the database context
        
        Args:
            scope: The ASGI scope
            receive: The ASGI receive function
            send: The ASGI send function
            
        Returns:
            The response from the next middleware or route handler
        """
        if scope["type"] != "http":
            return await self.app(scope, receive, send)
            
        # Create a request object
        request = Request(scope)
        
        try:
            # Try to get current user ID
            user_id = await extract_user_id_from_request(request)
            
            # Get database session using anext() since get_db() is an async generator
            db_gen = get_db()
            db = await anext(db_gen)
            
            try:
                if user_id:
                    # Set the current_user_id for RLS policies
                    await db.execute(text(f"SET app.current_user_id = '{user_id}'"))
                    logger.debug(f"Set database context for user_id: {user_id}")
                else:
                    # Set to empty string instead of NULL
                    await db.execute(text("SET app.current_user_id = ''"))
                    logger.debug("Set database context to empty string (no user)")
            except Exception as e:
                # Log the error but continue
                logger.error(f"Error setting database context: {e}")
                # Set to empty string if error
                await db.execute(text("SET app.current_user_id = ''"))
        except Exception as e:
            logger.error(f"Error in DB context middleware: {e}")
        
        # Process the request
        return await self.app(scope, receive, send)

================
File: app/middleware/jwt_bearer.py
================
from fastapi import Request, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import jwt, JWTError
import logging

from app.core.config import SETTINGS
from app.core.security_alerts import SecurityEvent, log_security_event

# Setup logging
logger = logging.getLogger("app.middleware.jwt_bearer")

class JWTBearer(HTTPBearer):
    """
    JWT Bearer token authentication dependency for FastAPI routes
    
    This class extends HTTPBearer to provide JWT token validation.
    It can be used as a dependency in FastAPI route functions.
    """
    
    def __init__(self, auto_error: bool = True):
        """
        Initialize the JWT Bearer authentication
        
        Args:
            auto_error: Whether to automatically raise an HTTPException on authentication failure
        """
        super(JWTBearer, self).__init__(auto_error=auto_error)
    
    async def __call__(self, request: Request) -> dict:
        """
        Validate the JWT token from the Authorization header
        
        Args:
            request: The FastAPI request object
            
        Returns:
            The decoded JWT payload if valid
            
        Raises:
            HTTPException: If the token is invalid or missing
        """
        credentials: HTTPAuthorizationCredentials = await super(JWTBearer, self).__call__(request)
        
        if not credentials:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Invalid authentication credentials",
                headers={"WWW-Authenticate": "Bearer"}
            )
            
        if credentials.scheme != "Bearer":
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Invalid authentication scheme. Use Bearer token.",
                headers={"WWW-Authenticate": "Bearer"}
            )
            
        # Validate the token
        payload = self.verify_jwt(credentials.credentials, request)
        return payload
    
    def verify_jwt(self, token: str, request: Request) -> dict:
        """
        Verify the JWT token and return the payload
        
        Args:
            token: The JWT token to verify
            request: The FastAPI request object
            
        Returns:
            The decoded JWT payload if valid
            
        Raises:
            HTTPException: If the token is invalid
        """
        try:
            # Decode the token
            payload = jwt.decode(
                token, 
                SETTINGS.secret_key, 
                algorithms=[SETTINGS.algorithm]
            )
            
            # Check if token has required claims
            if "sub" not in payload or "user_id" not in payload:
                self._log_invalid_token(request, token, "Missing required claims")
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token payload",
                    headers={"WWW-Authenticate": "Bearer"}
                )
                
            return payload
            
        except JWTError as e:
            self._log_invalid_token(request, token, str(e))
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token or token expired",
                headers={"WWW-Authenticate": "Bearer"}
            )
    
    def _log_invalid_token(self, request: Request, token: str, reason: str):
        """
        Log an invalid token attempt
        
        Args:
            request: The FastAPI request object
            token: The invalid token
            reason: The reason the token is invalid
        """
        # Get request metadata
        client_ip = request.client.host if request.client else "unknown"
        user_agent = request.headers.get("user-agent", "unknown")
        
        # Log the event
        logger.warning(
            f"Invalid token detected. Reason: {reason}. "
            f"IP: {client_ip}, User-Agent: {user_agent}"
        )
        
        # Create and log security event
        security_event = SecurityEvent(
            event_type="invalid_token",
            severity="medium",
            source_ip=client_ip,
            user_agent=user_agent,
            details={
                "reason": reason,
                "path": request.url.path,
                # Don't log the full token for security reasons
                "token_prefix": token[:10] + "..." if len(token) > 10 else token
            }
        )
        log_security_event(security_event)

================
File: app/models/__init__.py
================
from app.models.chat import Citation, Message, Conversation, ChatQuery, ChatResponse
from app.models.document import Chunk, Document, DocumentInfo, DocumentProcessRequest
from app.models.system import SystemStats, ModelInfo, HealthCheck

================
File: app/models/chat.py
================
from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field
from datetime import datetime
import uuid


class Citation(BaseModel):
    """Citation for a message"""
    document_id: str
    chunk_id: str
    relevance_score: float
    excerpt: str

    class Config:
        arbitrary_types_allowed = True


class Message(BaseModel):
    """Chat message"""
    content: str
    role: str = "user"  # "user" or "assistant"
    citations: Optional[List[Citation]] = None
    timestamp: datetime = Field(default_factory=datetime.now)

    class Config:
        arbitrary_types_allowed = True


class Conversation(BaseModel):
    """Chat conversation"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: Optional[str] = None
    messages: List[Message] = []
    metadata: Dict[str, Any] = {}
    created: datetime = Field(default_factory=datetime.now)
    updated: datetime = Field(default_factory=datetime.now)
    
    def add_message(self, message: Message) -> None:
        """Add a message to the conversation"""
        self.messages.append(message)
        self.updated = datetime.now()

    class Config:
        arbitrary_types_allowed = True


class ChatQuery(BaseModel):
    """Chat query model"""
    message: str
    conversation_id: Optional[str] = None
    user_id: Optional[str] = None
    model: Optional[str] = None
    use_rag: bool = True
    stream: bool = True
    model_parameters: Dict[str, Any] = {}
    metadata_filters: Optional[Dict[str, Any]] = None

    class Config:
        arbitrary_types_allowed = True


class ChatResponse(BaseModel):
    """Chat response model"""
    message: str
    conversation_id: Optional[str] = None
    citations: Optional[List[Citation]] = None
    execution_trace: Optional[List[Dict[str, Any]]] = None

    class Config:
        arbitrary_types_allowed = True

================
File: app/models/conversation.py
================
"""
Conversation model for storing chat conversations
"""
from datetime import datetime
from typing import List, Optional, Dict, Any
from uuid import UUID, uuid4

from sqlalchemy import Column, String, DateTime, ForeignKey, Text, JSON, Float, Integer
from sqlalchemy.dialects.postgresql import UUID as PostgresUUID
from sqlalchemy.orm import relationship

from app.db.session import Base

class Conversation(Base):
    """
    Conversation model for storing chat conversations
    
    This model stores chat conversations between users and the system,
    including messages, metadata, and associated memories.
    """
    __tablename__ = "conversations"
    
    id = Column(PostgresUUID(as_uuid=True), primary_key=True, default=uuid4)
    title = Column(String(255), nullable=True)
    created_at = Column(DateTime, nullable=False, default=datetime.now)
    updated_at = Column(DateTime, nullable=False, default=datetime.now, onupdate=datetime.now)
    conv_metadata = Column(JSON, nullable=True)
    
    # Relationships
    messages = relationship("Message", back_populates="conversation", cascade="all, delete-orphan")
    memories = relationship("Memory", back_populates="conversation", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<Conversation(id={self.id}, title={self.title})>"
    
    def to_dict(self):
        """Convert conversation to dictionary"""
        return {
            "id": str(self.id),
            "title": self.title,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
            "metadata": self.conv_metadata
        }
    
    @property
    def user_id(self) -> Optional[str]:
        """Get user ID from metadata"""
        if self.conv_metadata and "user_id" in self.conv_metadata:
            return self.conv_metadata["user_id"]
        return None

class Message(Base):
    """
    Message model for storing chat messages
    
    This model stores individual messages within a conversation,
    including the content, role, and associated citations.
    """
    __tablename__ = "messages"
    
    id = Column(PostgresUUID(as_uuid=True), primary_key=True, default=uuid4)
    conversation_id = Column(PostgresUUID(as_uuid=True), ForeignKey("conversations.id"), nullable=False)
    content = Column(Text, nullable=False)
    role = Column(String(50), nullable=False)
    created_at = Column(DateTime, nullable=False, default=datetime.now)
    token_count = Column(Integer, nullable=True)
    
    # Relationships
    conversation = relationship("Conversation", back_populates="messages")
    citations = relationship("Citation", back_populates="message", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<Message(id={self.id}, role={self.role})>"
    
    def to_dict(self):
        """Convert message to dictionary"""
        return {
            "id": str(self.id),
            "conversation_id": str(self.conversation_id),
            "content": self.content,
            "role": self.role,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "token_count": self.token_count,
            "citations": [citation.to_dict() for citation in self.citations] if self.citations else []
        }

class Citation(Base):
    """
    Citation model for storing document citations
    
    This model stores citations to documents or chunks that were used
    to generate a response.
    """
    __tablename__ = "citations"
    
    id = Column(PostgresUUID(as_uuid=True), primary_key=True, default=uuid4)
    message_id = Column(PostgresUUID(as_uuid=True), ForeignKey("messages.id"), nullable=False)
    document_id = Column(PostgresUUID(as_uuid=True), ForeignKey("documents.id"), nullable=True)
    chunk_id = Column(PostgresUUID(as_uuid=True), nullable=True)
    relevance_score = Column(Float, nullable=True)
    excerpt = Column(Text, nullable=True)
    
    # Relationships
    message = relationship("Message", back_populates="citations")
    document = relationship("Document", back_populates="citations")
    
    def __repr__(self):
        return f"<Citation(id={self.id}, document_id={self.document_id})>"
    
    def to_dict(self):
        """Convert citation to dictionary"""
        return {
            "id": str(self.id),
            "message_id": str(self.message_id),
            "document_id": str(self.document_id) if self.document_id else None,
            "chunk_id": str(self.chunk_id) if self.chunk_id else None,
            "relevance_score": self.relevance_score,
            "excerpt": self.excerpt
        }

================
File: app/models/document.py
================
from typing import List, Dict, Optional, Any, Set
from pydantic import BaseModel, Field
from datetime import datetime
import uuid


class Chunk(BaseModel):
    """Document chunk model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: str
    metadata: Dict[str, Any] = {}
    embedding: Optional[List[float]] = None

    class Config:
        arbitrary_types_allowed = True


class Document(BaseModel):
    """Document model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    filename: str
    content: str
    chunks: List[Chunk] = []
    metadata: Dict[str, Any] = {}
    tags: List[str] = []
    folder: str = "/"  # Root folder by default
    uploaded: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True


class DocumentInfo(BaseModel):
    """Document information (without content)"""
    id: str
    filename: str
    chunk_count: int
    metadata: Dict[str, Any]
    tags: List[str]
    folder: str
    uploaded: datetime

    class Config:
        arbitrary_types_allowed = True


class DocumentProcessRequest(BaseModel):
    """Document processing request"""
    document_ids: List[str]
    force_reprocess: bool = False
    chunking_strategy: Optional[str] = "recursive"
    chunk_size: Optional[int] = None
    chunk_overlap: Optional[int] = None

    class Config:
        arbitrary_types_allowed = True


class TagUpdateRequest(BaseModel):
    """Request to update document tags"""
    tags: List[str]

    class Config:
        arbitrary_types_allowed = True


class FolderUpdateRequest(BaseModel):
    """Request to update document folder"""
    folder: str

    class Config:
        arbitrary_types_allowed = True


class DocumentFilterRequest(BaseModel):
    """Request to filter documents"""
    tags: Optional[List[str]] = None
    folder: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

    class Config:
        arbitrary_types_allowed = True

================
File: app/models/memory.py
================
"""
Memory model for storing explicit memories
"""
from datetime import datetime
from typing import Optional
from uuid import UUID, uuid4

from sqlalchemy import Column, String, DateTime, ForeignKey, Text
from sqlalchemy.dialects.postgresql import UUID as PostgresUUID
from sqlalchemy.orm import relationship

from app.db.session import Base

class Memory(Base):
    """
    Memory model for storing explicit memories
    
    This model stores explicit memories that users want the system to remember,
    such as preferences, facts, or other information.
    """
    __tablename__ = "memories"
    
    id = Column(PostgresUUID(as_uuid=True), primary_key=True, default=uuid4)
    conversation_id = Column(PostgresUUID(as_uuid=True), ForeignKey("conversations.id"), nullable=False)
    content = Column(Text, nullable=False)
    label = Column(String(50), nullable=False, default="explicit_memory")
    created_at = Column(DateTime, nullable=False, default=datetime.now)
    
    # Relationships
    conversation = relationship("Conversation", back_populates="memories")
    
    def __repr__(self):
        return f"<Memory(id={self.id}, label={self.label})>"
    
    def to_dict(self):
        """Convert memory to dictionary"""
        return {
            "id": str(self.id),
            "conversation_id": str(self.conversation_id),
            "content": self.content,
            "label": self.label,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }

================
File: app/models/notification.py
================
from typing import Optional, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class NotificationBase(BaseModel):
    """Base notification model"""
    type: str  # e.g., 'document_shared', 'mention', 'system'
    title: str
    message: str
    data: Dict[str, Any] = {}
    is_read: bool = False
    
    class Config:
        arbitrary_types_allowed = True

class NotificationCreate(NotificationBase):
    """Notification creation model"""
    user_id: str
    
    class Config:
        arbitrary_types_allowed = True

class Notification(NotificationBase):
    """Notification model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: str
    created_at: datetime = Field(default_factory=datetime.now)
    read_at: Optional[datetime] = None
    
    class Config:
        arbitrary_types_allowed = True

================
File: app/models/organization.py
================
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class OrganizationBase(BaseModel):
    """Base organization model"""
    name: str
    description: Optional[str] = None
    settings: Dict[str, Any] = {}
    
    class Config:
        arbitrary_types_allowed = True

class OrganizationCreate(OrganizationBase):
    """Organization creation model"""
    
    class Config:
        arbitrary_types_allowed = True

class OrganizationUpdate(BaseModel):
    """Organization update model"""
    name: Optional[str] = None
    description: Optional[str] = None
    settings: Optional[Dict[str, Any]] = None
    
    class Config:
        arbitrary_types_allowed = True

class Organization(OrganizationBase):
    """Organization model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True

class OrganizationMemberBase(BaseModel):
    """Base organization member model"""
    organization_id: str
    user_id: str
    role: str  # 'owner', 'admin', 'member'
    
    class Config:
        arbitrary_types_allowed = True

class OrganizationMemberCreate(OrganizationMemberBase):
    """Organization member creation model"""
    
    class Config:
        arbitrary_types_allowed = True

class OrganizationMember(OrganizationMemberBase):
    """Organization member model"""
    created_at: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True

================
File: app/models/password_reset.py
================
from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class PasswordResetRequest(BaseModel):
    """Password reset request model"""
    email: str

class PasswordResetToken(BaseModel):
    """Password reset token model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: str
    token: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.now)
    expires_at: datetime
    is_used: bool = False

class PasswordReset(BaseModel):
    """Password reset model"""
    token: str
    password: str
    confirm_password: str

================
File: app/models/role.py
================
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class RoleBase(BaseModel):
    """Base role model"""
    name: str
    description: Optional[str] = None
    permissions: Dict[str, Any] = {}
    
    class Config:
        arbitrary_types_allowed = True

class RoleCreate(RoleBase):
    """Role creation model"""
    
    class Config:
        arbitrary_types_allowed = True

class RoleUpdate(BaseModel):
    """Role update model"""
    name: Optional[str] = None
    description: Optional[str] = None
    permissions: Optional[Dict[str, Any]] = None
    
    class Config:
        arbitrary_types_allowed = True

class Role(RoleBase):
    """Role model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True

class UserRoleBase(BaseModel):
    """Base user-role association model"""
    user_id: str
    role_id: str
    
    class Config:
        arbitrary_types_allowed = True

class UserRoleCreate(UserRoleBase):
    """User-role association creation model"""
    
    class Config:
        arbitrary_types_allowed = True

class UserRole(UserRoleBase):
    """User-role association model"""
    created_at: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True

================
File: app/models/system.py
================
from typing import List, Dict, Optional, Any
from pydantic import BaseModel


class SystemStats(BaseModel):
    """System statistics model"""
    documents_count: int
    total_chunks: int
    vector_store_size: Optional[int] = None
    available_models: List[str]

    class Config:
        arbitrary_types_allowed = True


class ModelInfo(BaseModel):
    """Model information"""
    name: str
    size: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None
    description: Optional[str] = None
    modified_at: Optional[str] = None
    
    class Config:
        arbitrary_types_allowed = True
    
    
class HealthCheck(BaseModel):
    """Health check model"""
    status: str
    ollama_status: str
    vector_db_status: str
    api_version: str
    
    class Config:
        arbitrary_types_allowed = True

================
File: app/models/user.py
================
from typing import Optional, Dict, Any
from pydantic import BaseModel, Field, EmailStr
from datetime import datetime
import uuid

class UserBase(BaseModel):
    """Base user model"""
    username: str
    email: EmailStr
    full_name: Optional[str] = None
    is_active: bool = True
    is_admin: bool = False
    
    class Config:
        arbitrary_types_allowed = True

class UserCreate(UserBase):
    """User creation model"""
    password: str
    
    class Config:
        arbitrary_types_allowed = True

class UserUpdate(BaseModel):
    """User update model"""
    username: Optional[str] = None
    email: Optional[EmailStr] = None
    full_name: Optional[str] = None
    password: Optional[str] = None
    is_active: Optional[bool] = None
    is_admin: Optional[bool] = None
    
    class Config:
        arbitrary_types_allowed = True

class User(UserBase):
    """User model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.now)
    last_login: Optional[datetime] = None
    metadata: Dict[str, Any] = {}
    
    class Config:
        arbitrary_types_allowed = True

class UserInDB(User):
    """User model with password hash (for internal use)"""
    password_hash: str
    
    class Config:
        arbitrary_types_allowed = True

================
File: app/rag/agents/__init__.py
================
"""
LLM-based agents for enhancing the RAG pipeline.
"""

from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.agents.retrieval_judge import RetrievalJudge

__all__ = ["ChunkingJudge", "RetrievalJudge"]

================
File: app/rag/agents/chunking_judge.py
================
"""
Chunking Judge - LLM-based agent that analyzes documents and recommends optimal chunking strategies
"""
import logging
import json
import re
from typing import Dict, Any, List, Optional

from app.models.document import Document
from app.rag.ollama_client import OllamaClient
from app.core.config import CHUNKING_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.chunking_judge")

class ChunkingJudge:
    """
    LLM-based agent that analyzes documents and recommends optimal chunking strategies
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = CHUNKING_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_document(self, document: Document) -> Dict[str, Any]:
        """
        Analyze a document and recommend the best chunking strategy and parameters
        
        Returns:
            Dict with keys:
            - strategy: The recommended chunking strategy
            - parameters: Dict of parameters for the chosen strategy
            - justification: Explanation of the recommendation
        """
        # Extract a sample of the document content (to avoid exceeding context window)
        content_sample = self._extract_representative_sample(document.content, document.filename)
        
        # Create prompt for the LLM
        prompt = self._create_analysis_prompt(document.filename, content_sample)
        
        # Get recommendation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        recommendation = self._parse_recommendation(response.get("response", ""))
        
        logger.info(f"Chunking Judge recommended strategy '{recommendation['strategy']}' for document {document.filename}")
        
        return recommendation
    
    def _extract_representative_sample(self, content: str, filename: str, max_length: int = 5000) -> str:
        """
        Extract a representative sample of the document content
        
        This function prioritizes:
        1. Headers (especially for markdown files)
        2. Introduction and conclusion sections
        3. A mix of content from throughout the document
        """
        if len(content) <= max_length:
            return content
        
        # Check if it's a markdown file
        is_markdown = filename.lower().endswith(('.md', '.markdown'))
        
        # For markdown files, prioritize headers
        if is_markdown:
            # Extract headers
            header_pattern = r'^(#{1,6}\s+.+)$'
            headers = re.findall(header_pattern, content, re.MULTILINE)
            
            # If we have headers, include them in the sample
            if headers:
                # Take all headers (they're usually short)
                headers_text = "\n".join(headers)
                
                # Calculate remaining space
                remaining_space = max_length - len(headers_text) - 100  # 100 chars buffer
                
                # Divide remaining space between intro, middle, and conclusion
                section_size = remaining_space // 3
                
                # Get intro, middle, and conclusion
                intro = content[:section_size]
                middle_start = (len(content) - section_size) // 2
                middle = content[middle_start:middle_start + section_size]
                conclusion = content[-section_size:]
                
                return f"{headers_text}\n\n--- DOCUMENT SAMPLE ---\n\nINTRO:\n{intro}\n\n[...]\n\nMIDDLE SECTION:\n{middle}\n\n[...]\n\nCONCLUSION:\n{conclusion}"
        
        # For non-markdown files or markdown files without headers
        # Take larger portions from the beginning and end (intro/conclusion)
        intro_size = max_length * 2 // 5  # 40% for intro
        conclusion_size = max_length * 2 // 5  # 40% for conclusion
        middle_size = max_length - intro_size - conclusion_size  # 20% for middle
        
        intro = content[:intro_size]
        middle_start = (len(content) - middle_size) // 2
        middle = content[middle_start:middle_start + middle_size]
        conclusion = content[-conclusion_size:]
        
        return f"INTRO:\n{intro}\n\n[...]\n\nMIDDLE SECTION:\n{middle}\n\n[...]\n\nCONCLUSION:\n{conclusion}"
    
    def _create_analysis_prompt(self, filename: str, content_sample: str) -> str:
        """Create a prompt for the LLM to analyze the document"""
        return f"""You are a document analysis expert. Your task is to analyze the following document and recommend the best chunking strategy for a RAG (Retrieval Augmented Generation) system.

Available Strategies:
- recursive: Splits text recursively by characters. Good for general text with natural separators.
- token: Splits text by tokens. Good for preserving semantic units in technical content.
- markdown: Splits markdown documents by headers. Good for structured documents with clear sections.
- semantic: Uses LLM to identify natural semantic boundaries in text. Best for preserving meaning and context in complex documents.

Document Filename: {filename}

Document Sample:
{content_sample}

Analyze the document structure, content type, and formatting. Consider:
1. Is this a structured document with clear sections or headers?
2. Does it contain code, tables, or other special formatting?
3. What's the typical paragraph and sentence length?
4. Are there natural breaks in the content?
5. Would semantic chunking be more appropriate than fixed-size chunking?

Here are some examples of good chunking strategy recommendations:

Example 1:
Document: technical_documentation.md
Recommendation: 
{{
    "strategy": "markdown",
    "parameters": {{
        "chunk_size": 1000,
        "chunk_overlap": 100
    }},
    "justification": "This is a markdown document with clear header structure. Using markdown chunking will preserve the semantic structure of the document and ensure that related content stays together."
}}

Example 2:
Document: research_paper.txt
Recommendation:
{{
    "strategy": "recursive",
    "parameters": {{
        "chunk_size": 1500,
        "chunk_overlap": 200
    }},
    "justification": "This document has long paragraphs with complex sentences. A larger chunk size with significant overlap will help preserve context and ensure that related concepts aren't split across chunks."
}}

Example 3:
Document: code_examples.py
Recommendation:
{{
    "strategy": "token",
    "parameters": {{
        "chunk_size": 500,
        "chunk_overlap": 50
    }},
    "justification": "This document contains code snippets where preserving token-level semantics is important. Token-based chunking will ensure that code blocks remain coherent."
}}

Example 4:
Document: research_paper.pdf
Recommendation:
{{
    "strategy": "semantic",
    "parameters": {{
        "chunk_size": 1500,
        "chunk_overlap": 200
    }},
    "justification": "This research paper contains complex concepts and arguments that span multiple paragraphs. Semantic chunking will identify natural boundaries in the text based on meaning rather than arbitrary character counts, preserving the logical flow and context of the arguments."
}}

Output your recommendation in JSON format:
{{
    "strategy": "...",  // One of: recursive, token, markdown
    "parameters": {{
        "chunk_size": ...,  // Recommended chunk size (characters or tokens)
        "chunk_overlap": ...  // Recommended overlap size
    }},
    "justification": "..." // Explanation of your reasoning
}}
"""
    
    def _parse_recommendation(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM response to extract the recommendation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                recommendation = json.loads(json_str)
                
                # Validate the recommendation
                if "strategy" not in recommendation:
                    raise ValueError("Missing 'strategy' in recommendation")
                
                # Validate strategy is one of the allowed values
                allowed_strategies = ["recursive", "token", "markdown", "semantic"]
                if recommendation["strategy"] not in allowed_strategies:
                    logger.warning(f"Invalid strategy '{recommendation['strategy']}', falling back to recursive")
                    recommendation["strategy"] = "recursive"
                
                # Set defaults if missing
                if "parameters" not in recommendation:
                    recommendation["parameters"] = {}
                if "chunk_size" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_size"] = 500
                if "chunk_overlap" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_overlap"] = 50
                
                return recommendation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing chunking recommendation: {str(e)}")
            # Return default recommendation
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 500,
                    "chunk_overlap": 50
                },
                "justification": "Failed to parse LLM recommendation, using default strategy."
            }

================
File: app/rag/agents/enhanced_langgraph_rag_agent.py
================
"""
Enhanced LangGraph RAG Agent - Orchestrates the RAG process using a state machine with planning and execution
"""
import logging
import json
import uuid
import time
from typing import Dict, Any, List, Optional, TypedDict, Annotated, Sequence, cast, Tuple
from datetime import datetime

from langgraph.graph import StateGraph, END

from app.models.document import Document, Chunk
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.chunkers.semantic_chunker import SemanticChunker
from app.rag.query_planner import QueryPlanner
from app.rag.plan_executor import PlanExecutor
from app.rag.query_analyzer import QueryAnalyzer
from app.rag.tools import ToolRegistry
from app.rag.process_logger import ProcessLogger
from app.rag.langgraph_states import (
    QueryAnalysisState, PlanningState, ExecutionState, RetrievalState, 
    GenerationState, RAGState, RAGStage
)
from app.core.config import CHUNKING_JUDGE_MODEL, RETRIEVAL_JUDGE_MODEL, DEFAULT_MODEL

logger = logging.getLogger("app.rag.agents.enhanced_langgraph_rag_agent")

class EnhancedLangGraphRAGAgent:
    """
    Enhanced LangGraph-based agent that orchestrates the RAG process using a state machine
    with planning and execution capabilities
    
    This agent integrates:
    - QueryAnalyzer for analyzing query complexity and requirements
    - QueryPlanner for creating execution plans
    - PlanExecutor for executing multi-step plans
    - Retrieval Judge for query refinement and context optimization
    - Semantic Chunker for intelligent text splitting
    
    The state machine follows these stages:
    1. Query Analysis: Analyze the query to determine complexity and retrieval parameters
    2. Query Planning: Create a plan for complex queries that may require multiple tools
    3. Plan Execution: Execute the plan, which may involve multiple tools and steps
    4. Retrieval: Retrieve relevant chunks from the vector store
    5. Query Refinement: Refine the query if needed based on initial retrieval
    6. Context Optimization: Optimize the context assembly for generation
    7. Generation: Generate the final response using the optimized context
    """
    
    def __init__(
        self,
        vector_store: Optional[VectorStore] = None,
        ollama_client: Optional[OllamaClient] = None,
        chunking_judge: Optional[ChunkingJudge] = None,
        retrieval_judge: Optional[RetrievalJudge] = None,
        semantic_chunker: Optional[SemanticChunker] = None,
        query_analyzer: Optional[QueryAnalyzer] = None,
        tool_registry: Optional[ToolRegistry] = None,
        process_logger: Optional[ProcessLogger] = None
    ):
        """
        Initialize the EnhancedLangGraphRAGAgent
        
        Args:
            vector_store: Vector store for retrieval
            ollama_client: Client for LLM interactions
            chunking_judge: Judge for document analysis and chunking strategy selection
            retrieval_judge: Judge for query refinement and context optimization
            semantic_chunker: Chunker for intelligent text splitting
            query_analyzer: Analyzer for query complexity and requirements
            tool_registry: Registry for available tools
            process_logger: Logger for process tracking
        """
        self.vector_store = vector_store or VectorStore()
        self.ollama_client = ollama_client or OllamaClient()
        self.chunking_judge = chunking_judge or ChunkingJudge(ollama_client=self.ollama_client)
        self.retrieval_judge = retrieval_judge or RetrievalJudge(ollama_client=self.ollama_client)
        self.semantic_chunker = semantic_chunker or SemanticChunker(ollama_client=self.ollama_client)
        
        # Initialize components for planning and execution
        self.tool_registry = tool_registry or ToolRegistry()
        self.query_analyzer = query_analyzer or QueryAnalyzer(llm_provider=self.ollama_client)
        self.query_planner = QueryPlanner(query_analyzer=self.query_analyzer, tool_registry=self.tool_registry)
        self.process_logger = process_logger or ProcessLogger()
        self.plan_executor = PlanExecutor(
            tool_registry=self.tool_registry,
            process_logger=self.process_logger,
            llm_provider=self.ollama_client
        )
        
        # Initialize and compile the state graph
        self.graph = self._build_graph()
        self.app = self.graph.compile()
        
        logger.info("EnhancedLangGraphRAGAgent initialized with state machine")
    
    def _build_graph(self) -> StateGraph:
        """
        Build the state graph for the RAG process
        
        Returns:
            StateGraph: The state graph for the RAG process
        """
        # Create the state graph
        graph = StateGraph(RAGState)
        
        # Add nodes for each stage
        graph.add_node(RAGStage.QUERY_ANALYSIS, self._analyze_query)
        graph.add_node(RAGStage.QUERY_PLANNING, self._plan_query)
        graph.add_node(RAGStage.PLAN_EXECUTION, self._execute_plan)
        graph.add_node(RAGStage.RETRIEVAL, self._retrieve_chunks)
        graph.add_node(RAGStage.QUERY_REFINEMENT, self._refine_query)
        graph.add_node(RAGStage.CONTEXT_OPTIMIZATION, self._optimize_context)
        graph.add_node(RAGStage.GENERATION, self._generate_response)
        graph.add_node(RAGStage.COMPLETE, self._finalize_response)
        
        # Define the edges between nodes with conditional routing
        # Start with query analysis
        graph.add_conditional_edges(
            RAGStage.QUERY_ANALYSIS,
            self._needs_planning,
            {
                True: RAGStage.QUERY_PLANNING,
                False: RAGStage.RETRIEVAL
            }
        )
        
        # After planning, execute the plan
        graph.add_edge(RAGStage.QUERY_PLANNING, RAGStage.PLAN_EXECUTION)
        
        # After plan execution, proceed to retrieval
        graph.add_edge(RAGStage.PLAN_EXECUTION, RAGStage.RETRIEVAL)
        
        # After retrieval, decide whether to refine the query or optimize the context
        graph.add_conditional_edges(
            RAGStage.RETRIEVAL,
            self._needs_refinement,
            {
                True: RAGStage.QUERY_REFINEMENT,
                False: RAGStage.CONTEXT_OPTIMIZATION
            }
        )
        
        # After query refinement, go back to retrieval with the refined query
        graph.add_edge(RAGStage.QUERY_REFINEMENT, RAGStage.RETRIEVAL)
        
        # After context optimization, proceed to generation
        graph.add_edge(RAGStage.CONTEXT_OPTIMIZATION, RAGStage.GENERATION)
        
        # After generation, complete the process
        graph.add_edge(RAGStage.GENERATION, RAGStage.COMPLETE)
        
        # After completion, end the process
        graph.add_edge(RAGStage.COMPLETE, END)
        
        # Set the entry point
        graph.set_entry_point(RAGStage.QUERY_ANALYSIS)
        
        return graph
    
    async def query(
        self,
        query: str,
        model: str = DEFAULT_MODEL,
        system_prompt: Optional[str] = None,
        stream: bool = False,
        model_parameters: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Query the RAG agent with the state machine
        
        Args:
            query: The user query
            model: The model to use for generation
            system_prompt: Optional system prompt for generation
            stream: Whether to stream the response
            model_parameters: Optional parameters for the model
            conversation_context: Optional conversation context
            metadata_filters: Optional filters for retrieval
            
        Returns:
            Dict with keys:
            - query: The original query
            - answer: The generated answer (if not streaming)
            - stream: The response stream (if streaming)
            - sources: List of sources used in the response
            - execution_trace: Trace of the execution process (if available)
        """
        # Generate a unique query ID
        query_id = str(uuid.uuid4())
        
        # Initialize the state
        initial_state: RAGState = {
            "query": query,
            "query_id": query_id,
            "conversation_context": conversation_context,
            "metadata_filters": metadata_filters,
            "model": model,
            "system_prompt": system_prompt,
            "stream": stream,
            "model_parameters": model_parameters or {},
            "query_analysis": None,
            "planning": None,
            "execution": None,
            "retrieval": None,
            "generation": None,
            "final_response": None
        }
        
        logger.info(f"Starting enhanced RAG query with LangGraph: {query[:50]}...")
        
        # Log the start of the process
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name="process_start",
                step_data={
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "model": model,
                    "stream": stream
                }
            )
        
        # Run the state machine
        start_time = time.time()
        result = await self.app.ainvoke(initial_state)
        elapsed_time = time.time() - start_time
        
        # Log the completion of the process
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name="process_complete",
                step_data={
                    "execution_time": elapsed_time,
                    "timestamp": datetime.now().isoformat()
                }
            )
        
        logger.info(f"Enhanced RAG query completed in {elapsed_time:.2f}s")
        
        # Return the final response
        return result["final_response"]
    
    async def _analyze_query(self, state: RAGState) -> RAGState:
        """
        Analyze the query to determine complexity and retrieval parameters
        
        Args:
            state: The current state
            
        Returns:
            Updated state with query analysis
        """
        logger.info(f"Analyzing query: {state['query'][:50]}...")
        
        # Extract conversation context if available
        conversation_context = state.get("conversation_context")
        
        # Convert conversation context to chat history format if available
        chat_history = None
        if conversation_context:
            # Parse conversation context into a list of (user, assistant) tuples
            # Assuming format like "User: message\nAssistant: response\n..."
            lines = conversation_context.strip().split('\n')
            history = []
            user_msg = None
            
            for line in lines:
                if line.startswith("User: "):
                    if user_msg is not None and len(history) > 0:
                        # If we have a previous user message without a response, discard it
                        user_msg = line[6:]  # Remove "User: " prefix
                    else:
                        user_msg = line[6:]  # Remove "User: " prefix
                elif line.startswith("Assistant: ") and user_msg is not None:
                    ai_msg = line[11:]  # Remove "Assistant: " prefix
                    history.append((user_msg, ai_msg))
                    user_msg = None
            
            if history:
                chat_history = history
                logger.info(f"Extracted {len(chat_history)} conversation turns from context")
        
        # Use the query analyzer to analyze the query with chat history
        analysis = await self.query_analyzer.analyze(state["query"], chat_history)
        
        # Update the state with the query analysis
        state["query_analysis"] = {
            "query": state["query"],
            "conversation_context": state["conversation_context"],
            "complexity": analysis.get("complexity"),
            "parameters": analysis.get("parameters"),
            "justification": analysis.get("justification"),
            "requires_tools": analysis.get("requires_tools", []),
            "sub_queries": analysis.get("sub_queries", [])
        }
        
        # Log the query analysis
        if self.process_logger:
            self.process_logger.log_step(
                query_id=state["query_id"],
                step_name="query_analysis",
                step_data=state["query_analysis"]
            )
        
        logger.info(f"Query complexity: {analysis.get('complexity', 'unknown')}")
        logger.info(f"Required tools: {analysis.get('requires_tools', [])}")
        
        return state
    
    def _needs_planning(self, state: RAGState) -> bool:
        """
        Determine if query planning is needed based on query analysis
        
        Args:
            state: The current state
            
        Returns:
            True if planning is needed, False otherwise
        """
        query_analysis = state["query_analysis"]
        if not query_analysis:
            return False
        
        # Check if the query is complex or requires tools
        complexity = query_analysis.get("complexity", "simple")
        requires_tools = query_analysis.get("requires_tools", [])
        
        needs_planning = complexity == "complex" or len(requires_tools) > 0
        logger.info(f"Query planning needed: {needs_planning}")
        
        return needs_planning
    
    async def _plan_query(self, state: RAGState) -> RAGState:
        """
        Create a plan for executing a complex query
        
        Args:
            state: The current state
            
        Returns:
            Updated state with query plan
        """
        query = state["query"]
        query_id = state["query_id"]
        query_analysis = state["query_analysis"]
        
        logger.info(f"Planning query execution: {query[:50]}...")
        
        # Extract chat history from the state if available
        chat_history = None
        conversation_context = state.get("conversation_context")
        
        if conversation_context:
            # Parse conversation context into a list of (user, assistant) tuples
            # Assuming format like "User: message\nAssistant: response\n..."
            lines = conversation_context.strip().split('\n')
            history = []
            user_msg = None
            
            for line in lines:
                if line.startswith("User: "):
                    if user_msg is not None and len(history) > 0:
                        # If we have a previous user message without a response, discard it
                        user_msg = line[6:]  # Remove "User: " prefix
                    else:
                        user_msg = line[6:]  # Remove "User: " prefix
                elif line.startswith("Assistant: ") and user_msg is not None:
                    ai_msg = line[11:]  # Remove "Assistant: " prefix
                    history.append((user_msg, ai_msg))
                    user_msg = None
            
            if history:
                chat_history = history
                logger.info(f"Using {len(chat_history)} conversation turns for planning")
        
        # Create a plan using the query planner with chat history
        plan = await self.query_planner.create_plan(
            query_id=query_id,
            query=query,
            chat_history=chat_history
        )
        
        # Update the state with the planning information
        state["planning"] = {
            "query": query,
            "query_id": query_id,
            "analysis": query_analysis,
            "plan": plan.to_dict(),
            "steps": plan.steps,
            "current_step": plan.current_step,
            "completed": plan.completed
        }
        
        # Log the query plan
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name="query_planning",
                step_data=state["planning"]
            )
        
        logger.info(f"Created plan with {len(plan.steps)} steps")
        
        return state
    
    async def _execute_plan(self, state: RAGState) -> RAGState:
        """
        Execute the query plan
        
        Args:
            state: The current state
            
        Returns:
            Updated state with execution results
        """
        planning = state["planning"]
        query_id = state["query_id"]
        query = state["query"]
        
        logger.info(f"Executing query plan: {query[:50]}...")
        
        # Reconstruct the plan from the planning state
        from app.rag.query_planner import QueryPlan
        plan = QueryPlan(
            query_id=query_id,
            query=query,
            steps=planning["steps"]
        )
        plan.current_step = planning["current_step"]
        plan.completed = planning["completed"]
        
        # Execute the plan
        execution_result = await self.plan_executor.execute_plan(plan)
        
        # Update the state with the execution results
        state["execution"] = {
            "query": query,
            "query_id": query_id,
            "plan": planning["plan"],
            "results": execution_result,
            "execution_trace": execution_result.get("steps", []),
            "completed": True,
            "error": None
        }
        
        # Log the plan execution
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name="plan_execution",
                step_data=state["execution"]
            )
        
        logger.info(f"Plan execution completed with {len(execution_result.get('steps', []))} results")
        
        return state
    
    async def _retrieve_chunks(self, state: RAGState) -> RAGState:
        """
        Retrieve relevant chunks from the vector store
        
        Args:
            state: The current state
            
        Returns:
            Updated state with retrieved chunks
        """
        # Get query and parameters from the state
        query = state["query"]
        query_analysis = state["query_analysis"]
        retrieval_state = state.get("retrieval", None)
        
        # If we have a refined query from a previous iteration, use it
        if retrieval_state and retrieval_state.get("refined_query"):
            query = retrieval_state["refined_query"]
            logger.info(f"Using refined query: {query[:50]}...")
        
        # Get recommended parameters from query analysis
        parameters = query_analysis["parameters"] if query_analysis else {}
        recommended_k = parameters.get("k", 10)
        
        # Combine the query with conversation context if available
        search_query = query
        if state["conversation_context"]:
            search_query = f"{query} {state['conversation_context'][-200:]}"
        
        logger.info(f"Retrieving chunks for query: {search_query[:50]}...")
        
        # Retrieve chunks from the vector store
        search_results = await self.vector_store.search(
            query=search_query,
            top_k=max(15, recommended_k + 5),  # Get a few extra for filtering
            filter_criteria=state["metadata_filters"]
        )
        
        logger.info(f"Retrieved {len(search_results)} chunks from vector store")
        
        # Evaluate chunks with the retrieval judge
        evaluation = await self.retrieval_judge.evaluate_chunks(query, search_results)
        
        # Extract relevance scores and refinement decision
        relevance_scores = evaluation.get("relevance_scores", {})
        needs_refinement = evaluation.get("needs_refinement", False)
        
        logger.info(f"Chunk evaluation complete, needs_refinement={needs_refinement}")
        
        # Update the state with retrieval results
        state["retrieval"] = {
            "query": query,
            "refined_query": retrieval_state["refined_query"] if retrieval_state else None,
            "conversation_context": state["conversation_context"],
            "parameters": parameters,
            "chunks": search_results,
            "needs_refinement": needs_refinement,
            "relevance_scores": relevance_scores
        }
        
        # Log the retrieval results
        if self.process_logger:
            self.process_logger.log_step(
                query_id=state["query_id"],
                step_name="retrieval",
                step_data={
                    "query": query,
                    "chunk_count": len(search_results),
                    "needs_refinement": needs_refinement
                }
            )
        
        return state
    
    def _needs_refinement(self, state: RAGState) -> bool:
        """
        Determine if query refinement is needed based on retrieval results
        
        Args:
            state: The current state
            
        Returns:
            True if refinement is needed, False otherwise
        """
        retrieval = state["retrieval"]
        
        # If we've already refined the query once, don't refine again
        if retrieval and retrieval.get("refined_query"):
            logger.info("Query already refined, skipping further refinement")
            return False
        
        # Otherwise, use the needs_refinement flag from the retrieval judge
        needs_refinement = retrieval["needs_refinement"] if retrieval else False
        logger.info(f"Query refinement needed: {needs_refinement}")
        
        return needs_refinement
    
    async def _refine_query(self, state: RAGState) -> RAGState:
        """
        Refine the query based on initial retrieval results
        
        Args:
            state: The current state
            
        Returns:
            Updated state with refined query
        """
        retrieval = state["retrieval"]
        
        logger.info(f"Refining query: {retrieval['query'][:50]}...")
        
        # Refine the query using the retrieval judge
        refined_query = await self.retrieval_judge.refine_query(
            retrieval["query"], 
            retrieval["chunks"]
        )
        
        logger.info(f"Refined query: {refined_query[:50]}...")
        
        # Update the state with the refined query
        retrieval["refined_query"] = refined_query
        state["retrieval"] = retrieval
        
        # Log the query refinement
        if self.process_logger:
            self.process_logger.log_step(
                query_id=state["query_id"],
                step_name="query_refinement",
                step_data={
                    "original_query": retrieval["query"],
                    "refined_query": refined_query
                }
            )
        
        return state
    
    async def _optimize_context(self, state: RAGState) -> RAGState:
        """
        Optimize the context assembly for generation
        
        Args:
            state: The current state
            
        Returns:
            Updated state with optimized context
        """
        retrieval = state["retrieval"]
        query = retrieval["refined_query"] or retrieval["query"]
        chunks = retrieval["chunks"]
        relevance_scores = retrieval["relevance_scores"] or {}
        parameters = retrieval["parameters"]
        
        logger.info(f"Optimizing context for query: {query[:50]}...")
        
        # Filter chunks based on relevance scores
        relevance_threshold = parameters.get("threshold", 0.4)
        apply_reranking = parameters.get("reranking", True)
        
        relevant_results = []
        for result in chunks:
            # Skip results with None content
            if "content" not in result or result["content"] is None:
                continue
            
            chunk_id = result["chunk_id"]
            
            # Get relevance score from evaluation or calculate from distance
            if chunk_id in relevance_scores:
                relevance_score = relevance_scores[chunk_id]
            else:
                # Calculate relevance score (lower distance = higher relevance)
                relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
            
            # Only include chunks that are sufficiently relevant
            if relevance_score >= relevance_threshold:
                # Add relevance score to result for sorting
                result["relevance_score"] = relevance_score
                relevant_results.append(result)
        
        # Sort by relevance score if reranking is enabled
        if apply_reranking:
            relevant_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        logger.info(f"Found {len(relevant_results)} relevant chunks after filtering")
        
        # Optimize context assembly if we have enough chunks
        if len(relevant_results) > 3 and apply_reranking:
            logger.info("Optimizing context assembly with Retrieval Judge")
            optimized_results = await self.retrieval_judge.optimize_context(query, relevant_results)
            if optimized_results:
                relevant_results = optimized_results
                logger.info(f"Context optimized to {len(relevant_results)} chunks")
        
        # Format context with source information
        context_pieces = []
        sources = []
        document_ids = []
        
        for i, result in enumerate(relevant_results):
            # Extract metadata for better context
            metadata = result["metadata"]
            filename = metadata.get("filename", "Unknown")
            tags = metadata.get("tags", [])
            folder = metadata.get("folder", "/")
            
            # Format the context piece with metadata
            context_piece = f"[{i+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
            context_pieces.append(context_piece)
            
            # Track the source for citation
            doc_id = metadata["document_id"]
            document_ids.append(doc_id)
            
            # Get relevance score (either from judge or distance)
            relevance_score = result.get("relevance_score", 1.0 - (result["distance"] if result["distance"] is not None else 0))
            
            sources.append({
                "document_id": doc_id,
                "chunk_id": result["chunk_id"],
                "relevance_score": relevance_score,
                "excerpt": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                "filename": filename,
                "tags": tags,
                "folder": folder
            })
        
        # Join all context pieces
        context = "\n\n".join(context_pieces)
        
        # Check if we have enough relevant context
        if len(relevant_results) == 0:
            logger.warning("No sufficiently relevant documents found for the query")
            context = "Note: No sufficiently relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents."
        elif len(context.strip()) < 50:  # Very short context might not be useful
            logger.warning("Context is too short to be useful")
            context = "Note: The retrieved context is too limited to provide a comprehensive answer to your query. The system cannot provide a specific answer based on the available documents."
        
        # Update the state with generation information
        state["generation"] = {
            "query": query,
            "conversation_context": state["conversation_context"],
            "context": context,
            "sources": sources,
            "document_ids": document_ids,
            "answer": None
        }
        
        # Log the context optimization
        if self.process_logger:
            self.process_logger.log_step(
                query_id=state["query_id"],
                step_name="context_optimization",
                step_data={
                    "context_length": len(context),
                    "source_count": len(sources)
                }
            )
        
        return state
    
    async def _generate_response(self, state: RAGState) -> RAGState:
        """
        Generate the final response using the optimized context
        
        Args:
            state: The current state
            
        Returns:
            Updated state with generated response
        """
        generation = state["generation"]
        query = generation["query"]
        context = generation["context"]
        conversation_context = generation["conversation_context"]
        
        logger.info(f"Generating response for query: {query[:50]}...")
        
        # Create full prompt with context and conversation history
        if conversation_context:
            full_prompt = f"""Context:
{context}

Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
        else:
            full_prompt = f"""Context:
{context}

User Question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. This is a new conversation with no previous history - treat it as such.
9. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
        
        # Create system prompt if not provided
        system_prompt = state["system_prompt"]
        if not system_prompt:
            system_prompt = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
- Do not use your general knowledge unless the context is insufficient, and clearly indicate when you're doing so.
- Analyze the context carefully to find the most relevant information for the user's question.
- If multiple sources provide different information, synthesize them and explain any discrepancies.
- If the context includes metadata like filenames, tags, or folders, use this to understand the source and relevance of the information.

WHEN CONTEXT IS INSUFFICIENT:
- Clearly state: "Based on the provided documents, I don't have information about [topic]."
- Be specific about what information is missing.
- Only then provide a general response based on your knowledge, and clearly state: "However, generally speaking..." to distinguish this from information in the context.
- Never pretend to have information that isn't in the context.

CONVERSATION HANDLING:
- IMPORTANT: Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.
- If no conversation history is provided, treat the query as a new, standalone question.
- Only maintain continuity with previous exchanges when conversation history is explicitly provided.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Always cite your sources with numbers in square brackets [1] when using information from the context.
"""
        
        # Generate response
        if state["stream"]:
            # For streaming, just return the stream response
            logger.info(f"Generating streaming response with model: {state['model']}")
            stream_response = await self.ollama_client.generate(
                prompt=full_prompt,
                model=state["model"],
                system_prompt=system_prompt,
                stream=True,
                parameters=state["model_parameters"] or {}
            )
            
            # Update the state with the stream response
            generation["stream_response"] = stream_response
        else:
            # For non-streaming, get the complete response
            logger.info(f"Generating non-streaming response with model: {state['model']}")
            response = await self.ollama_client.generate(
                prompt=full_prompt,
                model=state["model"],
                system_prompt=system_prompt,
                stream=False,
                parameters=state["model_parameters"] or {}
            )
            
            # Check if there was an error in the response
            if "error" in response:
                error_message = response.get("error", "Unknown error")
                logger.warning(f"Model returned an error: {error_message}")
                response_text = f"Error: {error_message}"
            else:
                # Get response text
                response_text = response.get("response", "")
            
            logger.info(f"Response length: {len(response_text)} characters")
            
            # Update the state with the generated answer
            generation["answer"] = response_text
        
        # Update the state with the generation information
        state["generation"] = generation
        
        # Log the response generation
        if self.process_logger:
            self.process_logger.log_step(
                query_id=state["query_id"],
                step_name="response_generation",
                step_data={
                    "streaming": state["stream"],
                    "model": state["model"],
                    "response_length": len(generation.get("answer", "")) if not state["stream"] else None
                }
            )
        
        return state
    
    async def _finalize_response(self, state: RAGState) -> RAGState:
        """
        Finalize the response for return to the user
        
        Args:
            state: The current state
            
        Returns:
            Updated state with final response
        """
        generation = state["generation"]
        execution = state.get("execution", None)
        
        # Create the final response
        if state["stream"]:
            final_response = {
                "query": state["query"],
                "stream": generation.get("stream_response"),
                "sources": generation["sources"],
                "execution_trace": execution["execution_trace"] if execution else None
            }
        else:
            final_response = {
                "query": state["query"],
                "answer": generation["answer"],
                "sources": generation["sources"],
                "execution_trace": execution["execution_trace"] if execution else None
            }
        
        # Update the state with the final response
        state["final_response"] = final_response
        
        # Log the final response
        if self.process_logger:
            self.process_logger.log_final_response(
                query_id=state["query_id"],
                response=generation.get("answer", ""),
                metadata={
                    "source_count": len(generation["sources"]),
                    "has_execution_trace": execution is not None
                }
            )
        
        logger.info("Enhanced RAG process complete")
        
        return state

================
File: app/rag/agents/langgraph_rag_agent.py
================
"""
LangGraph RAG Agent - Orchestrates the RAG process using a state machine
"""
import logging
import json
import re
from typing import Dict, Any, List, Optional, TypedDict, Annotated, Sequence, cast, Tuple
from enum import Enum

from langchain.schema.document import Document as LangchainDocument
from langgraph.graph import StateGraph, END
# langgraph 0.0.20 doesn't have ToolNode in prebuilt or MemorySaver

from app.models.document import Document, Chunk
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.chunkers.semantic_chunker import SemanticChunker
from app.core.config import CHUNKING_JUDGE_MODEL, RETRIEVAL_JUDGE_MODEL, DEFAULT_MODEL

logger = logging.getLogger("app.rag.agents.langgraph_rag_agent")

# Define state types for the LangGraph state machine
class QueryAnalysisState(TypedDict):
    """State for query analysis"""
    query: str
    conversation_context: Optional[str]
    complexity: Optional[str]
    parameters: Optional[Dict[str, Any]]
    justification: Optional[str]

class RetrievalState(TypedDict):
    """State for retrieval"""
    query: str
    refined_query: Optional[str]
    conversation_context: Optional[str]
    parameters: Dict[str, Any]
    chunks: List[Dict[str, Any]]
    needs_refinement: bool
    relevance_scores: Optional[Dict[str, float]]

class GenerationState(TypedDict):
    """State for generation"""
    query: str
    conversation_context: Optional[str]
    context: str
    sources: List[Dict[str, Any]]
    document_ids: List[str]
    answer: Optional[str]

class RAGState(TypedDict):
    """Combined state for the RAG process"""
    query: str
    conversation_context: Optional[str]
    metadata_filters: Optional[Dict[str, Any]]
    model: str
    system_prompt: Optional[str]
    stream: bool
    model_parameters: Optional[Dict[str, Any]]
    query_analysis: Optional[QueryAnalysisState]
    retrieval: Optional[RetrievalState]
    generation: Optional[GenerationState]
    final_response: Optional[Dict[str, Any]]

class RAGStage(str, Enum):
    """Stages in the RAG process"""
    QUERY_ANALYSIS = "analyze_query_node"
    RETRIEVAL = "retrieve_chunks_node"
    QUERY_REFINEMENT = "refine_query_node"
    CONTEXT_OPTIMIZATION = "optimize_context_node"
    GENERATION = "generate_response_node"
    COMPLETE = "finalize_response_node"

class LangGraphRAGAgent:
    """
    LangGraph-based agent that orchestrates the RAG process using a state machine
    
    This agent integrates:
    - Chunking Judge for document analysis and chunking strategy selection
    - Semantic Chunker for intelligent text splitting
    - Retrieval Judge for query refinement and context optimization
    
    The state machine follows these stages:
    1. Query Analysis: Analyze the query to determine complexity and retrieval parameters
    2. Retrieval: Retrieve relevant chunks from the vector store
    3. Query Refinement: Refine the query if needed based on initial retrieval
    4. Context Optimization: Optimize the context assembly for generation
    5. Generation: Generate the final response using the optimized context
    """
    
    def __init__(
        self,
        vector_store: Optional[VectorStore] = None,
        ollama_client: Optional[OllamaClient] = None,
        chunking_judge: Optional[ChunkingJudge] = None,
        retrieval_judge: Optional[RetrievalJudge] = None,
        semantic_chunker: Optional[SemanticChunker] = None
    ):
        """
        Initialize the LangGraphRAGAgent
        
        Args:
            vector_store: Vector store for retrieval
            ollama_client: Client for LLM interactions
            chunking_judge: Judge for document analysis and chunking strategy selection
            retrieval_judge: Judge for query refinement and context optimization
            semantic_chunker: Chunker for intelligent text splitting
        """
        self.vector_store = vector_store or VectorStore()
        self.ollama_client = ollama_client or OllamaClient()
        self.chunking_judge = chunking_judge or ChunkingJudge(ollama_client=self.ollama_client)
        self.retrieval_judge = retrieval_judge or RetrievalJudge(ollama_client=self.ollama_client)
        self.semantic_chunker = semantic_chunker or SemanticChunker(ollama_client=self.ollama_client)
        
        # Initialize and compile the state graph
        self.graph = self._build_graph()
        self.app = self.graph.compile()
        
        logger.info("LangGraphRAGAgent initialized with state machine")
    
    def _build_graph(self) -> StateGraph:
        """
        Build the state graph for the RAG process
        
        Returns:
            StateGraph: The state graph for the RAG process
        """
        # Create the state graph
        graph = StateGraph(RAGState)
        
        # Add nodes for each stage
        graph.add_node(RAGStage.QUERY_ANALYSIS, self._analyze_query)
        graph.add_node(RAGStage.RETRIEVAL, self._retrieve_chunks)
        graph.add_node(RAGStage.QUERY_REFINEMENT, self._refine_query)
        graph.add_node(RAGStage.CONTEXT_OPTIMIZATION, self._optimize_context)
        graph.add_node(RAGStage.GENERATION, self._generate_response)
        graph.add_node(RAGStage.COMPLETE, self._finalize_response)
        
        # Define the edges between nodes
        # Start with query analysis
        graph.add_edge(RAGStage.QUERY_ANALYSIS, RAGStage.RETRIEVAL)
        
        # After retrieval, decide whether to refine the query or optimize the context
        graph.add_conditional_edges(
            RAGStage.RETRIEVAL,
            self._needs_refinement,
            {
                True: RAGStage.QUERY_REFINEMENT,
                False: RAGStage.CONTEXT_OPTIMIZATION
            }
        )
        
        # After query refinement, go back to retrieval with the refined query
        graph.add_edge(RAGStage.QUERY_REFINEMENT, RAGStage.RETRIEVAL)
        
        # After context optimization, proceed to generation
        graph.add_edge(RAGStage.CONTEXT_OPTIMIZATION, RAGStage.GENERATION)
        
        # After generation, complete the process
        graph.add_edge(RAGStage.GENERATION, RAGStage.COMPLETE)
        
        # After completion, end the process
        graph.add_edge(RAGStage.COMPLETE, END)
        
        # Set the entry point
        graph.set_entry_point(RAGStage.QUERY_ANALYSIS)
        
        return graph
    
    async def query(
        self,
        query: str,
        model: str = DEFAULT_MODEL,
        system_prompt: Optional[str] = None,
        stream: bool = False,
        model_parameters: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Query the RAG agent with the state machine
        
        Args:
            query: The user query
            model: The model to use for generation
            system_prompt: Optional system prompt for generation
            stream: Whether to stream the response
            model_parameters: Optional parameters for the model
            conversation_context: Optional conversation context
            metadata_filters: Optional filters for retrieval
            
        Returns:
            Dict with keys:
            - query: The original query
            - answer: The generated answer (if not streaming)
            - stream: The response stream (if streaming)
            - sources: List of sources used in the response
        """
        # Initialize the state
        initial_state: RAGState = {
            "query": query,
            "conversation_context": conversation_context,
            "metadata_filters": metadata_filters,
            "model": model,
            "system_prompt": system_prompt,
            "stream": stream,
            "model_parameters": model_parameters or {},
            "query_analysis": None,
            "retrieval": None,
            "generation": None,
            "final_response": None
        }
        
        logger.info(f"Starting RAG query with LangGraph: {query[:50]}...")
        # Run the state machine
        # In langgraph 0.0.20, we need to use the compiled app with ainvoke
        result = await self.app.ainvoke(initial_state)
        
        # Return the final response
        return result["final_response"]
    
    async def _analyze_query(self, state: RAGState) -> RAGState:
        """
        Analyze the query to determine complexity and retrieval parameters
        
        Args:
            state: The current state
            
        Returns:
            Updated state with query analysis
        """
        logger.info(f"Analyzing query: {state['query'][:50]}...")
        
        # Use the retrieval judge to analyze the query
        query_analysis = await self.retrieval_judge.analyze_query(state["query"])
        
        # Update the state with the query analysis
        state["query_analysis"] = {
            "query": state["query"],
            "conversation_context": state["conversation_context"],
            "complexity": query_analysis.get("complexity"),
            "parameters": query_analysis.get("parameters"),
            "justification": query_analysis.get("justification")
        }
        
        logger.info(f"Query complexity: {query_analysis.get('complexity', 'unknown')}")
        
        return state
    
    async def _retrieve_chunks(self, state: RAGState) -> RAGState:
        """
        Retrieve relevant chunks from the vector store
        
        Args:
            state: The current state
            
        Returns:
            Updated state with retrieved chunks
        """
        # Get query and parameters from the state
        query = state["query"]
        query_analysis = state["query_analysis"]
        retrieval_state = state.get("retrieval", None)
        
        # If we have a refined query from a previous iteration, use it
        if retrieval_state and retrieval_state.get("refined_query"):
            query = retrieval_state["refined_query"]
            logger.info(f"Using refined query: {query[:50]}...")
        
        # Get recommended parameters from query analysis
        parameters = query_analysis["parameters"] if query_analysis else {}
        recommended_k = parameters.get("k", 10)
        
        # Combine the query with conversation context if available
        search_query = query
        if state["conversation_context"]:
            search_query = f"{query} {state['conversation_context'][-200:]}"
        
        logger.info(f"Retrieving chunks for query: {search_query[:50]}...")
        
        # Retrieve chunks from the vector store
        search_results = await self.vector_store.search(
            query=search_query,
            top_k=max(15, recommended_k + 5),  # Get a few extra for filtering
            filter_criteria=state["metadata_filters"]
        )
        
        logger.info(f"Retrieved {len(search_results)} chunks from vector store")
        
        # Evaluate chunks with the retrieval judge
        evaluation = await self.retrieval_judge.evaluate_chunks(query, search_results)
        
        # Extract relevance scores and refinement decision
        relevance_scores = evaluation.get("relevance_scores", {})
        needs_refinement = evaluation.get("needs_refinement", False)
        
        logger.info(f"Chunk evaluation complete, needs_refinement={needs_refinement}")
        
        # Update the state with retrieval results
        state["retrieval"] = {
            "query": query,
            "refined_query": retrieval_state["refined_query"] if retrieval_state else None,
            "conversation_context": state["conversation_context"],
            "parameters": parameters,
            "chunks": search_results,
            "needs_refinement": needs_refinement,
            "relevance_scores": relevance_scores
        }
        
        return state
    
    def _needs_refinement(self, state: RAGState) -> bool:
        """
        Determine if query refinement is needed based on retrieval results
        
        Args:
            state: The current state
            
        Returns:
            True if refinement is needed, False otherwise
        """
        retrieval = state["retrieval"]
        
        # If we've already refined the query once, don't refine again
        if retrieval and retrieval.get("refined_query"):
            logger.info("Query already refined, skipping further refinement")
            return False
        
        # Otherwise, use the needs_refinement flag from the retrieval judge
        needs_refinement = retrieval["needs_refinement"] if retrieval else False
        logger.info(f"Query refinement needed: {needs_refinement}")
        
        return needs_refinement
    
    async def _refine_query(self, state: RAGState) -> RAGState:
        """
        Refine the query based on initial retrieval results
        
        Args:
            state: The current state
            
        Returns:
            Updated state with refined query
        """
        retrieval = state["retrieval"]
        
        logger.info(f"Refining query: {retrieval['query'][:50]}...")
        
        # Refine the query using the retrieval judge
        refined_query = await self.retrieval_judge.refine_query(
            retrieval["query"], 
            retrieval["chunks"]
        )
        
        logger.info(f"Refined query: {refined_query[:50]}...")
        
        # Update the state with the refined query
        retrieval["refined_query"] = refined_query
        state["retrieval"] = retrieval
        
        return state
    
    async def _optimize_context(self, state: RAGState) -> RAGState:
        """
        Optimize the context assembly for generation
        
        Args:
            state: The current state
            
        Returns:
            Updated state with optimized context
        """
        retrieval = state["retrieval"]
        query = retrieval["refined_query"] or retrieval["query"]
        chunks = retrieval["chunks"]
        relevance_scores = retrieval["relevance_scores"] or {}
        parameters = retrieval["parameters"]
        
        logger.info(f"Optimizing context for query: {query[:50]}...")
        
        # Filter chunks based on relevance scores
        relevance_threshold = parameters.get("threshold", 0.4)
        apply_reranking = parameters.get("reranking", True)
        
        relevant_results = []
        for result in chunks:
            # Skip results with None content
            if "content" not in result or result["content"] is None:
                continue
            
            chunk_id = result["chunk_id"]
            
            # Get relevance score from evaluation or calculate from distance
            if chunk_id in relevance_scores:
                relevance_score = relevance_scores[chunk_id]
            else:
                # Calculate relevance score (lower distance = higher relevance)
                relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
            
            # Only include chunks that are sufficiently relevant
            if relevance_score >= relevance_threshold:
                # Add relevance score to result for sorting
                result["relevance_score"] = relevance_score
                relevant_results.append(result)
        
        # Sort by relevance score if reranking is enabled
        if apply_reranking:
            relevant_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        logger.info(f"Found {len(relevant_results)} relevant chunks after filtering")
        
        # Optimize context assembly if we have enough chunks
        if len(relevant_results) > 3 and apply_reranking:
            logger.info("Optimizing context assembly with Retrieval Judge")
            optimized_results = await self.retrieval_judge.optimize_context(query, relevant_results)
            if optimized_results:
                relevant_results = optimized_results
                logger.info(f"Context optimized to {len(relevant_results)} chunks")
        
        # Format context with source information
        context_pieces = []
        sources = []
        document_ids = []
        
        for i, result in enumerate(relevant_results):
            # Extract metadata for better context
            metadata = result["metadata"]
            filename = metadata.get("filename", "Unknown")
            tags = metadata.get("tags", [])
            folder = metadata.get("folder", "/")
            
            # Format the context piece with metadata
            context_piece = f"[{i+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
            context_pieces.append(context_piece)
            
            # Track the source for citation
            doc_id = metadata["document_id"]
            document_ids.append(doc_id)
            
            # Get relevance score (either from judge or distance)
            relevance_score = result.get("relevance_score", 1.0 - (result["distance"] if result["distance"] is not None else 0))
            
            sources.append({
                "document_id": doc_id,
                "chunk_id": result["chunk_id"],
                "relevance_score": relevance_score,
                "excerpt": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                "filename": filename,
                "tags": tags,
                "folder": folder
            })
        
        # Join all context pieces
        context = "\n\n".join(context_pieces)
        
        # Check if we have enough relevant context
        if len(relevant_results) == 0:
            logger.warning("No sufficiently relevant documents found for the query")
            context = "Note: No sufficiently relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents."
        elif len(context.strip()) < 50:  # Very short context might not be useful
            logger.warning("Context is too short to be useful")
            context = "Note: The retrieved context is too limited to provide a comprehensive answer to your query. The system cannot provide a specific answer based on the available documents."
        
        # Update the state with generation information
        state["generation"] = {
            "query": query,
            "conversation_context": state["conversation_context"],
            "context": context,
            "sources": sources,
            "document_ids": document_ids,
            "answer": None
        }
        
        return state
    
    async def _generate_response(self, state: RAGState) -> RAGState:
        """
        Generate the final response using the optimized context
        
        Args:
            state: The current state
            
        Returns:
            Updated state with generated response
        """
        generation = state["generation"]
        query = generation["query"]
        context = generation["context"]
        conversation_context = generation["conversation_context"]
        
        logger.info(f"Generating response for query: {query[:50]}...")
        
        # Create full prompt with context and conversation history
        if conversation_context:
            full_prompt = f"""Context:
{context}

Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
        else:
            full_prompt = f"""Context:
{context}

User Question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. This is a new conversation with no previous history - treat it as such.
9. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
        
        # Create system prompt if not provided
        system_prompt = state["system_prompt"]
        if not system_prompt:
            system_prompt = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
- Do not use your general knowledge unless the context is insufficient, and clearly indicate when you're doing so.
- Analyze the context carefully to find the most relevant information for the user's question.
- If multiple sources provide different information, synthesize them and explain any discrepancies.
- If the context includes metadata like filenames, tags, or folders, use this to understand the source and relevance of the information.

WHEN CONTEXT IS INSUFFICIENT:
- Clearly state: "Based on the provided documents, I don't have information about [topic]."
- Be specific about what information is missing.
- Only then provide a general response based on your knowledge, and clearly state: "However, generally speaking..." to distinguish this from information in the context.
- Never pretend to have information that isn't in the context.

CONVERSATION HANDLING:
- IMPORTANT: Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.
- If no conversation history is provided, treat the query as a new, standalone question.
- Only maintain continuity with previous exchanges when conversation history is explicitly provided.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Always cite your sources with numbers in square brackets [1] when using information from the context.
"""
        
        # Generate response
        if state["stream"]:
            # For streaming, just return the stream response
            logger.info(f"Generating streaming response with model: {state['model']}")
            stream_response = await self.ollama_client.generate(
                prompt=full_prompt,
                model=state["model"],
                system_prompt=system_prompt,
                stream=True,
                parameters=state["model_parameters"] or {}
            )
            
            # Update the state with the stream response
            generation["stream_response"] = stream_response
        else:
            # For non-streaming, get the complete response
            logger.info(f"Generating non-streaming response with model: {state['model']}")
            response = await self.ollama_client.generate(
                prompt=full_prompt,
                model=state["model"],
                system_prompt=system_prompt,
                stream=False,
                parameters=state["model_parameters"] or {}
            )
            
            # Check if there was an error in the response
            if "error" in response:
                error_message = response.get("error", "Unknown error")
                logger.warning(f"Model returned an error: {error_message}")
                response_text = f"Error: {error_message}"
            else:
                # Get response text
                response_text = response.get("response", "")
            
            logger.info(f"Response length: {len(response_text)} characters")
            
            # Update the state with the generated answer
            generation["answer"] = response_text
        
        # Update the state with the generation information
        state["generation"] = generation
        
        return state
    
    async def _finalize_response(self, state: RAGState) -> RAGState:
        """
        Finalize the response for return to the user
        
        Args:
            state: The current state
            
        Returns:
            Updated state with final response
        """
        generation = state["generation"]
        
        # Create the final response
        if state["stream"]:
            final_response = {
                "query": state["query"],
                "stream": generation.get("stream_response"),
                "sources": generation["sources"]
            }
        else:
            final_response = {
                "query": state["query"],
                "answer": generation["answer"],
                "sources": generation["sources"]
            }
        
        # Update the state with the final response
        state["final_response"] = final_response
        
        logger.info("RAG process complete")
        
        return state

================
File: app/rag/agents/retrieval_judge.py
================
"""
Retrieval Judge - LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
"""
import logging
import json
import re
from typing import Dict, Any, List, Optional

from app.models.document import Chunk
from app.rag.ollama_client import OllamaClient
from app.core.config import RETRIEVAL_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.retrieval_judge")

class RetrievalJudge:
    """
    LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = RETRIEVAL_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Analyze a query and recommend retrieval parameters
        
        Returns:
            Dict with keys:
            - complexity: The assessed complexity of the query (simple, moderate, complex)
            - parameters: Dict of recommended retrieval parameters (k, threshold, etc.)
            - justification: Explanation of the recommendation
        """
        # Create prompt for the LLM
        prompt = self._create_query_analysis_prompt(query)
        
        # Get recommendation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        analysis = self._parse_query_analysis(response.get("response", ""))
        
        logger.info(f"Retrieval Judge analyzed query complexity as '{analysis.get('complexity', 'unknown')}' with k={analysis.get('parameters', {}).get('k', 'default')}")
        
        return analysis
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Evaluate retrieved chunks for relevance to the query
        
        Args:
            query: The user query
            chunks: List of chunks from the vector store search results
            
        Returns:
            Dict with keys:
            - relevance_scores: Dict mapping chunk IDs to relevance scores (0-1)
            - needs_refinement: Boolean indicating if query refinement is needed
            - justification: Explanation of the evaluation
        """
        # Extract a sample of chunks to avoid exceeding context window
        chunks_sample = self._extract_chunks_sample(chunks)
        
        # Create prompt for the LLM
        prompt = self._create_chunks_evaluation_prompt(query, chunks_sample)
        
        # Get evaluation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        evaluation = self._parse_chunks_evaluation(response.get("response", ""), chunks)
        
        logger.info(f"Retrieval Judge evaluated {len(chunks)} chunks, needs_refinement={evaluation.get('needs_refinement', False)}")
        
        return evaluation
    
    async def refine_query(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """
        Refine a query based on retrieved chunks to improve retrieval precision
        
        Args:
            query: The original user query
            chunks: List of chunks from the initial retrieval
            
        Returns:
            Refined query string
        """
        # Extract a sample of chunks to avoid exceeding context window
        chunks_sample = self._extract_chunks_sample(chunks)
        
        # Create prompt for the LLM
        prompt = self._create_query_refinement_prompt(query, chunks_sample)
        
        # Get refined query from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        refined_query = self._parse_refined_query(response.get("response", ""), query)
        
        logger.info(f"Retrieval Judge refined query from '{query}' to '{refined_query}'")
        
        return refined_query
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Optimize the assembly of chunks into a context for the LLM
        
        Args:
            query: The user query
            chunks: List of chunks from the vector store search results
            
        Returns:
            Reordered and filtered list of chunks optimized for context assembly
        """
        # If we have too few chunks, no need to optimize
        if len(chunks) <= 3:
            logger.info(f"Too few chunks ({len(chunks)}) to optimize, returning as is")
            return chunks
        
        # Extract a sample of chunks to avoid exceeding context window
        chunks_sample = self._extract_chunks_sample(chunks)
        
        # Create prompt for the LLM
        prompt = self._create_context_optimization_prompt(query, chunks_sample)
        
        # Get optimization from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        optimized_chunks = self._parse_context_optimization(response.get("response", ""), chunks)
        
        logger.info(f"Retrieval Judge optimized context from {len(chunks)} to {len(optimized_chunks)} chunks")
        
        return optimized_chunks
    
    def _extract_chunks_sample(self, chunks: List[Dict[str, Any]], max_chunks: int = 5, max_length: int = 5000) -> List[Dict[str, Any]]:
        """
        Extract a representative sample of chunks to avoid exceeding context window
        
        Args:
            chunks: List of chunks from the vector store search results
            max_chunks: Maximum number of chunks to include
            max_length: Maximum total length of chunk content
            
        Returns:
            List of sample chunks with truncated content if necessary
        """
        if not chunks:
            return []
        
        # Sort chunks by distance (if available) to prioritize most relevant chunks
        sorted_chunks = sorted(
            chunks, 
            key=lambda x: x.get("distance", 1.0) if x.get("distance") is not None else 1.0
        )
        
        # Take top chunks
        sample_chunks = sorted_chunks[:max_chunks]
        
        # Calculate total content length
        total_length = sum(len(chunk.get("content", "")) for chunk in sample_chunks)
        
        # If total length exceeds max_length, truncate each chunk proportionally
        if total_length > max_length:
            # Calculate scaling factor
            scale_factor = max_length / total_length
            
            # Truncate each chunk
            for chunk in sample_chunks:
                content = chunk.get("content", "")
                max_chunk_length = int(len(content) * scale_factor)
                if len(content) > max_chunk_length:
                    chunk["content"] = content[:max_chunk_length] + "..."
        
        return sample_chunks
    
    def _create_query_analysis_prompt(self, query: str) -> str:
        """Create a prompt for the LLM to analyze the query"""
        return f"""You are a query analysis expert for a RAG (Retrieval Augmented Generation) system. Your task is to analyze the following user query and recommend optimal retrieval parameters.

User Query: {query}

Analyze the query complexity, specificity, and intent. Consider:
1. Is this a simple factual question or a complex analytical query?
2. Does it require specific knowledge from documents or general knowledge?
3. Is it ambiguous or clear in its intent?
4. Does it contain multiple sub-questions or a single focused question?
5. Would it benefit from a broader or narrower retrieval approach?

Based on your analysis, recommend retrieval parameters:
- k: Number of chunks to retrieve (5-15)
- threshold: Relevance threshold for filtering (0.0-1.0)
- reranking: Whether to apply reranking (true/false)

Output your analysis in JSON format:
{{
    "complexity": "...",  // One of: simple, moderate, complex
    "parameters": {{
        "k": ...,  // Recommended number of chunks to retrieve
        "threshold": ...,  // Recommended relevance threshold
        "reranking": ...  // Whether to apply reranking (true/false)
    }},
    "justification": "..." // Explanation of your reasoning
}}
"""
    
    def _create_chunks_evaluation_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Create a prompt for the LLM to evaluate retrieved chunks"""
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            chunks_text += f"[{i+1}] Source: {filename}\n{content}\n\n"
        
        return f"""You are a relevance evaluation expert for a RAG (Retrieval Augmented Generation) system. Your task is to evaluate the relevance of retrieved chunks to the user's query.

User Query: {query}

Retrieved Chunks:
{chunks_text}

Evaluate each chunk's relevance to the query on a scale of 0.0 to 1.0, where:
- 1.0: Directly answers the query with high precision
- 0.7-0.9: Contains information highly relevant to the query
- 0.4-0.6: Contains information somewhat relevant to the query
- 0.1-0.3: Contains information tangentially related to the query
- 0.0: Contains no information relevant to the query

Also determine if the query needs refinement based on the retrieved chunks:
- If the chunks are all low relevance, the query might need refinement
- If the chunks contain relevant information but are too broad, the query might need refinement
- If the chunks contain contradictory information, the query might need refinement

Output your evaluation in JSON format:
{{
    "relevance_scores": {{
        "1": 0.8,  // Relevance score for chunk 1
        "2": 0.5,  // Relevance score for chunk 2
        ...
    }},
    "needs_refinement": true/false,  // Whether the query needs refinement
    "justification": "..." // Explanation of your evaluation
}}
"""
    
    def _create_query_refinement_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Create a prompt for the LLM to refine the query"""
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            chunks_text += f"[{i+1}] Source: {filename}\n{content}\n\n"
        
        return f"""You are a query refinement expert for a RAG (Retrieval Augmented Generation) system. Your task is to refine the user's query based on the initially retrieved chunks to improve retrieval precision.

Original User Query: {query}

Initially Retrieved Chunks:
{chunks_text}

Analyze the query and the retrieved chunks to identify:
1. Ambiguities in the original query that could be clarified
2. Missing specific terms that would improve retrieval
3. Domain-specific terminology from the chunks that could be incorporated
4. Potential reformulations that would better match the document content

Create a refined query that:
- Maintains the original intent of the user's question
- Adds specificity based on the retrieved chunks
- Incorporates relevant terminology from the documents
- Is formulated to maximize the chance of retrieving more relevant chunks

Output your refined query as plain text without any JSON formatting or explanations. The output should be ONLY the refined query text that can be directly used for retrieval.
"""
    
    def _create_context_optimization_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Create a prompt for the LLM to optimize context assembly"""
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            chunks_text += f"[{i+1}] Source: {filename}\n{content}\n\n"
        
        return f"""You are a context optimization expert for a RAG (Retrieval Augmented Generation) system. Your task is to optimize the assembly of retrieved chunks into a context for the LLM.

User Query: {query}

Retrieved Chunks:
{chunks_text}

Analyze the chunks and determine the optimal order and selection for providing context to the LLM. Consider:
1. Relevance to the query
2. Information completeness
3. Logical flow of information
4. Removal of redundant information
5. Inclusion of diverse perspectives if available

Output your optimization in JSON format:
{{
    "optimized_order": [3, 1, 5, ...],  // Chunk numbers in optimal order
    "excluded_chunks": [2, 4, ...],  // Chunk numbers to exclude (if any)
    "justification": "..." // Explanation of your optimization
}}
"""
    
    def _parse_query_analysis(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM response to extract the query analysis"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                analysis = json.loads(json_str)
                
                # Validate the analysis
                if "complexity" not in analysis:
                    raise ValueError("Missing 'complexity' in analysis")
                
                # Validate complexity is one of the allowed values
                allowed_complexity = ["simple", "moderate", "complex"]
                if analysis["complexity"] not in allowed_complexity:
                    logger.warning(f"Invalid complexity '{analysis['complexity']}', falling back to 'moderate'")
                    analysis["complexity"] = "moderate"
                
                # Set defaults if missing
                if "parameters" not in analysis:
                    analysis["parameters"] = {}
                if "k" not in analysis["parameters"]:
                    analysis["parameters"]["k"] = 10
                if "threshold" not in analysis["parameters"]:
                    analysis["parameters"]["threshold"] = 0.4
                if "reranking" not in analysis["parameters"]:
                    analysis["parameters"]["reranking"] = True
                
                return analysis
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing query analysis: {str(e)}")
            # Return default analysis
            return {
                "complexity": "moderate",
                "parameters": {
                    "k": 10,
                    "threshold": 0.4,
                    "reranking": True
                },
                "justification": "Failed to parse LLM recommendation, using default parameters."
            }
    
    def _parse_chunks_evaluation(self, response_text: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Parse the LLM response to extract the chunks evaluation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                evaluation = json.loads(json_str)
                
                # Validate the evaluation
                if "relevance_scores" not in evaluation:
                    raise ValueError("Missing 'relevance_scores' in evaluation")
                
                # Convert string keys to integers if needed
                relevance_scores = {}
                for key, value in evaluation["relevance_scores"].items():
                    # Convert key to int if it's a string representation of an int
                    try:
                        idx = int(key)
                        # Map the score to the actual chunk ID
                        if 1 <= idx <= len(chunks):
                            chunk_id = chunks[idx-1].get("chunk_id")
                            relevance_scores[chunk_id] = value
                    except (ValueError, IndexError):
                        # If key can't be converted to int or is out of range, use as is
                        relevance_scores[key] = value
                
                evaluation["relevance_scores"] = relevance_scores
                
                # Set defaults if missing
                if "needs_refinement" not in evaluation:
                    evaluation["needs_refinement"] = False
                
                return evaluation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing chunks evaluation: {str(e)}")
            # Return default evaluation
            default_scores = {}
            for chunk in chunks:
                chunk_id = chunk.get("chunk_id")
                if chunk_id:
                    default_scores[chunk_id] = 0.5
            
            return {
                "relevance_scores": default_scores,
                "needs_refinement": False,
                "justification": "Failed to parse LLM evaluation, using default relevance scores."
            }
    
    def _parse_refined_query(self, response_text: str, original_query: str) -> str:
        """Parse the LLM response to extract the refined query"""
        try:
            # Clean up the response text
            refined_query = response_text.strip()
            
            # If the response is empty or too short, return the original query
            if not refined_query or len(refined_query) < 5:
                logger.warning("Refined query is empty or too short, using original query")
                return original_query
            
            # If the response is too long (likely includes explanations), try to extract just the query
            if len(refined_query) > len(original_query) * 3:
                # Look for patterns that might indicate the actual query
                query_patterns = [
                    r'(?:refined query|new query|improved query)[:\s]+(.+?)(?:\n|$)',
                    r'(?:query|q)[:\s]+(.+?)(?:\n|$)',
                    r'"(.+?)"'
                ]
                
                for pattern in query_patterns:
                    match = re.search(pattern, refined_query, re.IGNORECASE)
                    if match:
                        extracted_query = match.group(1).strip()
                        if extracted_query and len(extracted_query) >= 5:
                            logger.info(f"Extracted refined query using pattern: {pattern}")
                            return extracted_query
            
            return refined_query
        except Exception as e:
            logger.error(f"Error parsing refined query: {str(e)}")
            return original_query
    
    def _parse_context_optimization(self, response_text: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Parse the LLM response to extract the context optimization"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                optimization = json.loads(json_str)
                
                # Validate the optimization
                if "optimized_order" not in optimization:
                    raise ValueError("Missing 'optimized_order' in optimization")
                
                # Get the optimized order
                optimized_order = optimization["optimized_order"]
                
                # Get excluded chunks
                excluded_chunks = optimization.get("excluded_chunks", [])
                
                # Create the optimized chunks list
                optimized_chunks = []
                for idx in optimized_order:
                    # Convert to 0-based index if needed
                    try:
                        idx_0based = int(idx) - 1
                        if 0 <= idx_0based < len(chunks) and idx_0based not in excluded_chunks:
                            optimized_chunks.append(chunks[idx_0based])
                    except (ValueError, IndexError):
                        logger.warning(f"Invalid chunk index in optimized order: {idx}")
                
                # If no valid chunks were found, return the original chunks
                if not optimized_chunks:
                    logger.warning("No valid chunks in optimized order, returning original chunks")
                    return chunks
                
                return optimized_chunks
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing context optimization: {str(e)}")
            # Return the original chunks
            return chunks

================
File: app/rag/chunkers/__init__.py
================
"""
Chunkers module for RAG system
"""

================
File: app/rag/chunkers/semantic_chunker.py
================
"""
Semantic Chunker - LLM-based chunker that splits text based on semantic boundaries
"""
import logging
import json
import re
from typing import List, Dict, Any, Optional, Tuple

from langchain.schema.document import Document as LangchainDocument
from langchain.text_splitter import TextSplitter

from app.rag.ollama_client import OllamaClient
from app.core.config import CHUNKING_JUDGE_MODEL

logger = logging.getLogger("app.rag.chunkers.semantic_chunker")

class SemanticChunker(TextSplitter):
    """
    LLM-based chunker that splits text based on semantic boundaries rather than
    just character or token counts.
    
    This chunker uses an LLM to identify natural semantic boundaries in text,
    ensuring that chunks maintain coherent meaning and context.
    """
    
    def __init__(
        self,
        ollama_client: Optional[OllamaClient] = None,
        model: str = CHUNKING_JUDGE_MODEL,
        chunk_size: int = 1500,
        chunk_overlap: int = 200,
        max_llm_context_length: int = 8000,
        cache_enabled: bool = True
    ):
        """
        Initialize the SemanticChunker.
        
        Args:
            ollama_client: Optional OllamaClient instance
            model: LLM model to use for semantic analysis
            chunk_size: Target size for chunks (in characters)
            chunk_overlap: Target overlap between chunks (in characters)
            max_llm_context_length: Maximum context length for LLM input
            cache_enabled: Whether to cache chunking results
        """
        # Initialize with default separator to satisfy TextSplitter requirements
        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.max_llm_context_length = max_llm_context_length
        self.cache_enabled = cache_enabled
        self.cache = {}  # Simple in-memory cache
    
    def split_text(self, text: str) -> List[str]:
        """
        Split text based on semantic boundaries.
        
        This method overrides the TextSplitter.split_text method to use
        LLM-based semantic chunking instead of simple character-based splitting.
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        # Check cache first if enabled
        if self.cache_enabled and text in self.cache:
            logger.info("Using cached semantic chunks")
            return self.cache[text]
        
        # If text is short enough, return it as a single chunk
        if len(text) <= self.chunk_size:
            return [text]
        
        # For longer texts, use semantic chunking
        chunks = self._semantic_chunking(text)
        
        # Cache the result if enabled
        if self.cache_enabled:
            self.cache[text] = chunks
        
        return chunks
    
    async def split_text_async(self, text: str) -> List[str]:
        """
        Asynchronous version of split_text.
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        # Check cache first if enabled
        if self.cache_enabled and text in self.cache:
            logger.info("Using cached semantic chunks")
            return self.cache[text]
        
        # If text is short enough, return it as a single chunk
        if len(text) <= self.chunk_size:
            return [text]
        
        # For longer texts, use semantic chunking
        chunks = await self._semantic_chunking_async(text)
        
        # Cache the result if enabled
        if self.cache_enabled:
            self.cache[text] = chunks
        
        return chunks
    
    def _semantic_chunking(self, text: str) -> List[str]:
        """
        Synchronous wrapper for _semantic_chunking_async.
        
        This is needed because TextSplitter.split_text is synchronous.
        In practice, this will be less efficient than the async version
        because it blocks on the LLM call.
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        import asyncio
        try:
            # Try to get the current event loop
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If we're already in an event loop, create a new one in a thread
                logger.warning("Running async semantic chunking in a new event loop")
                return asyncio.run(self._semantic_chunking_async(text))
            else:
                # If no event loop is running, use the current one
                return loop.run_until_complete(self._semantic_chunking_async(text))
        except RuntimeError:
            # If no event loop is available, create a new one
            logger.warning("No event loop available, creating a new one")
            return asyncio.run(self._semantic_chunking_async(text))
    
    async def _semantic_chunking_async(self, text: str) -> List[str]:
        """
        Split text based on semantic boundaries using LLM.
        
        For long texts, this method:
        1. Divides the text into sections that fit within the LLM context window
        2. Processes each section to identify semantic boundaries
        3. Combines the results, ensuring proper handling of section boundaries
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        # If text is too long for a single LLM call, process it in sections
        if len(text) > self.max_llm_context_length:
            return await self._process_long_text(text)
        
        # For text that fits in a single LLM call, process directly
        return await self._identify_semantic_boundaries(text)
    
    async def _process_long_text(self, text: str) -> List[str]:
        """
        Process a long text by dividing it into sections and processing each section.
        
        Args:
            text: The long text to process
            
        Returns:
            List of semantically chunked text
        """
        # Calculate section size, leaving room for prompt and instructions
        section_size = self.max_llm_context_length - 2000
        
        # Divide text into overlapping sections
        sections = []
        for i in range(0, len(text), section_size - self.chunk_overlap):
            section_start = max(0, i)
            section_end = min(len(text), i + section_size)
            sections.append(text[section_start:section_end])
        
        logger.info(f"Processing long text in {len(sections)} sections")
        
        # Process each section
        all_chunks = []
        for i, section in enumerate(sections):
            logger.info(f"Processing section {i+1}/{len(sections)}")
            section_chunks = await self._identify_semantic_boundaries(section)
            
            # For all but the first section, check if the first chunk should be merged
            # with the last chunk of the previous section
            if i > 0 and all_chunks and section_chunks:
                # If the first chunk of this section is small, it might be a continuation
                if len(section_chunks[0]) < self.chunk_size / 2:
                    # Merge with the last chunk of the previous section
                    merged_chunk = all_chunks[-1] + section_chunks[0]
                    # If the merged chunk is still reasonable in size, use it
                    if len(merged_chunk) <= self.chunk_size * 1.5:
                        all_chunks[-1] = merged_chunk
                        section_chunks = section_chunks[1:]
            
            all_chunks.extend(section_chunks)
        
        return all_chunks
    
    async def _identify_semantic_boundaries(self, text: str) -> List[str]:
        """
        Use LLM to identify semantic boundaries in text and split accordingly.
        
        Args:
            text: The text to analyze and split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        prompt = self._create_chunking_prompt(text)
        
        try:
            # Get boundaries from LLM
            response = await self.ollama_client.generate(
                prompt=prompt,
                model=self.model,
                stream=False
            )
            
            # Parse the response
            boundaries = self._parse_boundaries(response.get("response", ""), text)
            
            # If parsing fails or no boundaries are found, fall back to simple chunking
            if not boundaries:
                logger.warning("Failed to identify semantic boundaries, falling back to simple chunking")
                return self._fallback_chunking(text)
            
            # Create chunks based on identified boundaries
            chunks = self._create_chunks_from_boundaries(text, boundaries)
            
            logger.info(f"Created {len(chunks)} semantic chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"Error in semantic chunking: {str(e)}")
            # Fall back to simple chunking on error
            return self._fallback_chunking(text)
    
    def _create_chunking_prompt(self, text: str) -> str:
        """
        Create a prompt for the LLM to identify semantic boundaries.
        
        Args:
            text: The text to analyze
            
        Returns:
            Prompt for the LLM
        """
        return f"""You are an expert in natural language understanding and text analysis. Your task is to identify natural semantic boundaries in the following text. These boundaries should:

1. Respect the semantic structure of the content
2. Create coherent, self-contained chunks that maintain context
3. Occur at natural transitions between topics, ideas, or sections
4. Result in chunks that are approximately {self.chunk_size} characters in length (but prioritize semantic coherence over exact size)

Text to analyze:
```
{text}
```

Analyze the text and identify the character positions where natural semantic boundaries occur. Consider:
- Paragraph breaks that signal topic transitions
- Section boundaries
- Shifts in subject matter or perspective
- Transitions between different types of content (e.g., explanation to example)
- Natural pauses in the flow of information

Output ONLY a JSON array of character positions where chunks should be split, like this:
[500, 1050, 1600, 2200]

These positions should indicate the character index where each new chunk should begin (except the first chunk, which starts at position 0).

Important: Focus on semantic coherence rather than exact chunk size. It's better to have slightly uneven chunks that maintain semantic integrity than equal-sized chunks that break mid-thought.
"""
    
    def _parse_boundaries(self, response_text: str, original_text: str) -> List[int]:
        """
        Parse the LLM response to extract boundary positions.
        
        Args:
            response_text: The LLM response text
            original_text: The original text being chunked
            
        Returns:
            List of character positions for chunk boundaries
        """
        try:
            # Extract JSON array from response
            json_match = re.search(r'\[[\d\s,]+\]', response_text)
            if not json_match:
                logger.warning("Could not find boundary array in LLM response")
                return []
            
            boundaries = json.loads(json_match.group(0))
            
            # Validate boundaries
            if not isinstance(boundaries, list):
                logger.warning(f"Invalid boundary format: {boundaries}")
                return []
            
            # Filter out invalid boundaries
            valid_boundaries = [b for b in boundaries if isinstance(b, (int, float)) and 0 < b < len(original_text)]
            valid_boundaries = sorted(valid_boundaries)
            
            # Add the start position (0) if not present
            if valid_boundaries and valid_boundaries[0] > 0:
                valid_boundaries = [0] + valid_boundaries
            
            logger.info(f"Identified {len(valid_boundaries)} semantic boundaries")
            return valid_boundaries
            
        except Exception as e:
            logger.error(f"Error parsing semantic boundaries: {str(e)}")
            return []
    
    def _create_chunks_from_boundaries(self, text: str, boundaries: List[int]) -> List[str]:
        """
        Create text chunks based on identified boundaries.
        
        Args:
            text: The original text
            boundaries: List of character positions for chunk boundaries
            
        Returns:
            List of text chunks
        """
        chunks = []
        
        # Ensure the text starts at the first boundary
        if not boundaries or boundaries[0] != 0:
            boundaries = [0] + boundaries
        
        # Add the end of the text as the final boundary
        boundaries.append(len(text))
        
        # Create chunks based on boundaries
        for i in range(len(boundaries) - 1):
            start = boundaries[i]
            end = boundaries[i + 1]
            
            # Skip empty chunks
            if start == end:
                continue
                
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
        
        # Apply overlap if specified
        if self.chunk_overlap > 0:
            chunks = self._apply_overlap(chunks)
        
        return chunks
    
    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        """
        Apply overlap between chunks to maintain context.
        
        Args:
            chunks: List of text chunks without overlap
            
        Returns:
            List of text chunks with overlap applied
        """
        if not chunks or len(chunks) < 2:
            return chunks
            
        result = [chunks[0]]
        
        for i in range(1, len(chunks)):
            prev_chunk = chunks[i-1]
            current_chunk = chunks[i]
            
            # Calculate overlap size (in characters)
            overlap_size = min(self.chunk_overlap, len(prev_chunk) // 2)
            
            # If previous chunk is too small, don't apply overlap
            if len(prev_chunk) <= overlap_size * 2:
                result.append(current_chunk)
                continue
                
            # Get the overlap text from the end of the previous chunk
            overlap_text = prev_chunk[-overlap_size:]
            
            # Add the overlap to the beginning of the current chunk
            result.append(overlap_text + current_chunk)
            
        return result
    
    def _fallback_chunking(self, text: str) -> List[str]:
        """
        Fallback method for chunking when LLM-based chunking fails.
        
        Args:
            text: The text to chunk
            
        Returns:
            List of text chunks
        """
        logger.info("Using fallback chunking method")
        
        # Split by paragraphs first
        paragraphs = re.split(r'\n\s*\n', text)
        
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # If adding this paragraph would exceed the chunk size,
            # save the current chunk and start a new one
            if len(current_chunk) + len(paragraph) > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                # Otherwise, add the paragraph to the current chunk
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
        
        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # Apply overlap if specified
        if self.chunk_overlap > 0:
            chunks = self._apply_overlap(chunks)
        
        return chunks

================
File: app/rag/system_prompts/__init__.py
================
"""
System prompts package for Metis RAG.

This package contains various system prompts used by the RAG engine
to guide the behavior of the language model for different types of queries.
"""

from app.rag.system_prompts.code_generation import (
    CODE_GENERATION_SYSTEM_PROMPT,
    PYTHON_CODE_GENERATION_PROMPT,
    JAVASCRIPT_CODE_GENERATION_PROMPT
)

# Note: RAG_SYSTEM_PROMPT and conversation templates have been moved to the PromptManager

__all__ = [
    'CODE_GENERATION_SYSTEM_PROMPT',
    'PYTHON_CODE_GENERATION_PROMPT',
    'JAVASCRIPT_CODE_GENERATION_PROMPT'
]

================
File: app/rag/system_prompts/code_generation.py
================
"""
System prompts for code generation in Metis RAG.
"""

CODE_GENERATION_SYSTEM_PROMPT = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.
- You are capable of generating high-quality code examples when requested.

CODE GENERATION GUIDELINES:
- When asked to provide code, always provide complete, working implementations.
- Use proper naming conventions and consistent formatting in all code examples.
- Include helpful comments to explain complex logic or important concepts.
- Ensure function and variable names are descriptive and follow standard conventions.
- Never use spaces in function or variable names (use snake_case or camelCase as appropriate).
- Always use proper indentation and consistent formatting.
- For compound terms like "tic-tac-toe", use hyphens in natural language but snake_case in code (tic_tac_toe).
- When providing Python code, follow PEP 8 style guidelines.

RESPONSE FORMATTING:
- Format code blocks with proper syntax highlighting using triple backticks.
- Use proper spacing around punctuation.
- Maintain consistent formatting throughout your responses.
- Structure your responses logically with clear sections.
- When explaining code, break down complex concepts into understandable parts.

WHEN ASKED FOR CODE:
- If the user asks for code, provide it directly and completely.
- Do not refuse to provide code unless it would be harmful or unethical.
- If you initially say you can provide code, follow through with a complete implementation.
- Explain the code's functionality clearly after providing it.
- Offer guidance on how to use or modify the code.

CONVERSATION HANDLING:
- Maintain consistency in what you say you can or cannot do.
- If you initially say you cannot do something, don't later do it without explanation.
- Be clear about your limitations while being as helpful as possible.
- Only refer to previous conversations if they are explicitly provided in the conversation history.
"""

PYTHON_CODE_GENERATION_PROMPT = """
When generating Python code:
1. Follow PEP 8 style guidelines
2. Use descriptive variable and function names
3. Include docstrings for functions and classes
4. Use snake_case for function and variable names
5. Use CamelCase for class names
6. Include appropriate error handling
7. Add comments for complex logic
8. Ensure proper indentation (4 spaces)
9. Keep lines under 79 characters when possible
10. Include type hints when appropriate
"""

JAVASCRIPT_CODE_GENERATION_PROMPT = """
When generating JavaScript code:
1. Use modern ES6+ syntax when appropriate
2. Use camelCase for variables and functions
3. Use PascalCase for classes and components
4. Include JSDoc comments for functions
5. Use const and let instead of var
6. Include appropriate error handling
7. Add comments for complex logic
8. Use consistent indentation (2 spaces)
9. Use template literals for string interpolation
10. Use arrow functions when appropriate
"""

================
File: app/rag/system_prompts/conversation.py
================
"""
System prompts for conversation handling in Metis RAG.
"""

CONVERSATION_WITH_CONTEXT_PROMPT = """Context:
{context}

Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""

NEW_QUERY_WITH_CONTEXT_PROMPT = """Context:
{context}

User Question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. This is a new conversation with no previous history - treat it as such.
9. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""

================
File: app/rag/system_prompts/rag.py
================
"""
System prompts for RAG responses in Metis RAG.
"""

RAG_SYSTEM_PROMPT = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
- Do not use your general knowledge unless the context is insufficient, and clearly indicate when you're doing so.
- Analyze the context carefully to find the most relevant information for the user's question.
- If multiple sources provide different information, synthesize them and explain any discrepancies.
- If the context includes metadata like filenames, tags, or folders, use this to understand the source and relevance of the information.

WHEN INFORMATION IS LIMITED:
1. If you find SOME relevant information but it's not comprehensive, start with: "I've searched my knowledge base for information about [topic]. While I don't have comprehensive information on this topic, I did find some relevant documents that mention it."
2. Then present the limited information you have, with proper citations.
3. End with: "Please note this information is limited to what's in my document database. For more comprehensive information, consider consulting specialized resources."

WHEN NO INFORMATION IS FOUND:
1. Clearly state: "Based on the provided documents, I don't have information about [topic]."
2. Only after acknowledging the limitation, you may provide general knowledge with: "However, generally speaking..." to assist the user.

CITATION FORMATTING:
1. Always use numbered citations like [1], [2] that correspond to the sources provided.
2. At the end of your response, list your sources in a structured format:
   Sources:
   [1] Document ID: abc123... - "Document Title"
   [2] Document ID: def456... - "Document Title"

CONVERSATION HANDLING:
- IMPORTANT: Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.
- If no conversation history is provided, treat the query as a new, standalone question.
- Only maintain continuity with previous exchanges when conversation history is explicitly provided.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Always cite your sources with numbers in square brackets [1] when using information from the context.
"""

================
File: app/rag/tools/__init__.py
================
"""
Tools package for the Metis_RAG system
"""
from app.rag.tools.base import Tool
from app.rag.tools.registry import ToolRegistry
from app.rag.tools.rag_tool import RAGTool
from app.rag.tools.calculator_tool import CalculatorTool
from app.rag.tools.database_tool import DatabaseTool
from app.rag.tools.postgresql_tool import PostgreSQLTool

__all__ = ["Tool", "ToolRegistry", "RAGTool", "CalculatorTool", "DatabaseTool", "PostgreSQLTool"]

================
File: app/rag/tools/base.py
================
"""
Tool - Abstract base class for tools used in the Metis_RAG system
"""
import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

class Tool(ABC):
    """
    Abstract base class for tools
    
    Tools are components that can be used by the system to perform specific tasks,
    such as retrieving information, performing calculations, or querying databases.
    """
    
    def __init__(self, name: str, description: str):
        """
        Initialize a tool
        
        Args:
            name: Tool name
            description: Tool description
        """
        self.name = name
        self.description = description
        self.logger = logging.getLogger(f"app.rag.tools.{name}")
    
    @abstractmethod
    async def execute(self, input_data: Any) -> Any:
        """
        Execute the tool with the given input
        
        Args:
            input_data: Tool-specific input
            
        Returns:
            Tool-specific output
        """
        pass
    
    def get_description(self) -> str:
        """
        Get a description of the tool
        
        Returns:
            Tool description
        """
        return self.description
    
    @abstractmethod
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the tool
        
        Returns:
            JSON Schema for tool input
        """
        pass
    
    @abstractmethod
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the tool
        
        Returns:
            JSON Schema for tool output
        """
        pass
    
    @abstractmethod
    def get_examples(self) -> List[Dict[str, Any]]:
        """
        Get examples of tool usage
        
        Returns:
            List of example input/output pairs
        """
        pass

================
File: app/rag/tools/calculator_tool.py
================
"""
CalculatorTool - Tool for performing mathematical calculations
"""
import logging
import time
import math
import re
from typing import Any, Dict, List, Optional
import ast
import operator

from app.rag.tools.base import Tool

# Define safe operations for the calculator
SAFE_OPERATORS = {
    ast.Add: operator.add,
    ast.Sub: operator.sub,
    ast.Mult: operator.mul,
    ast.Div: operator.truediv,
    ast.FloorDiv: operator.floordiv,
    ast.Mod: operator.mod,
    ast.Pow: operator.pow,
    ast.USub: operator.neg,  # Unary negation
}

# Define safe math functions
SAFE_FUNCTIONS = {
    'abs': abs,
    'round': round,
    'min': min,
    'max': max,
    'sum': sum,
    'pow': pow,
    # Math module functions
    'sqrt': math.sqrt,
    'exp': math.exp,
    'log': math.log,
    'log10': math.log10,
    'sin': math.sin,
    'cos': math.cos,
    'tan': math.tan,
    'asin': math.asin,
    'acos': math.acos,
    'atan': math.atan,
    'degrees': math.degrees,
    'radians': math.radians,
    'ceil': math.ceil,
    'floor': math.floor,
}

# Define common unit conversion factors
UNIT_CONVERSIONS = {
    # Length
    "m_to_km": 0.001,
    "km_to_m": 1000,
    "m_to_cm": 100,
    "cm_to_m": 0.01,
    "m_to_mm": 1000,
    "mm_to_m": 0.001,
    "km_to_miles": 0.621371,
    "miles_to_km": 1.60934,
    "feet_to_meters": 0.3048,
    "meters_to_feet": 3.28084,
    "inches_to_cm": 2.54,
    "cm_to_inches": 0.393701,
    
    # Weight/Mass
    "kg_to_g": 1000,
    "g_to_kg": 0.001,
    "kg_to_lb": 2.20462,
    "lb_to_kg": 0.453592,
    "oz_to_g": 28.3495,
    "g_to_oz": 0.035274,
    
    # Volume
    "l_to_ml": 1000,
    "ml_to_l": 0.001,
    "l_to_gallons": 0.264172,
    "gallons_to_l": 3.78541,
    "cubic_m_to_l": 1000,
    "l_to_cubic_m": 0.001,
    
    # Temperature
    "c_to_f": lambda c: (c * 9/5) + 32,
    "f_to_c": lambda f: (f - 32) * 5/9,
    "c_to_k": lambda c: c + 273.15,
    "k_to_c": lambda k: k - 273.15,
    
    # Time
    "hours_to_minutes": 60,
    "minutes_to_hours": 1/60,
    "days_to_hours": 24,
    "hours_to_days": 1/24,
    "minutes_to_seconds": 60,
    "seconds_to_minutes": 1/60,
    
    # Speed
    "kmh_to_ms": 0.277778,
    "ms_to_kmh": 3.6,
    "mph_to_kmh": 1.60934,
    "kmh_to_mph": 0.621371,
    
    # Area
    "sqm_to_sqkm": 0.000001,
    "sqkm_to_sqm": 1000000,
    "sqm_to_hectares": 0.0001,
    "hectares_to_sqm": 10000,
    "sqm_to_sqft": 10.7639,
    "sqft_to_sqm": 0.092903,
    "acres_to_sqm": 4046.86,
    "sqm_to_acres": 0.000247105,
}


class SafeEvaluator(ast.NodeVisitor):
    """
    Safe evaluator for mathematical expressions
    
    This class evaluates mathematical expressions in a safe way, preventing
    code execution and only allowing approved operations.
    """
    
    def __init__(self, variables=None):
        """
        Initialize the safe evaluator
        
        Args:
            variables: Dictionary of variable values
        """
        self.variables = variables or {}
    
    def visit_BinOp(self, node):
        """Visit binary operation nodes"""
        left = self.visit(node.left)
        right = self.visit(node.right)
        
        if type(node.op) not in SAFE_OPERATORS:
            raise ValueError(f"Unsupported operation: {type(node.op).__name__}")
        
        return SAFE_OPERATORS[type(node.op)](left, right)
    
    def visit_UnaryOp(self, node):
        """Visit unary operation nodes"""
        operand = self.visit(node.operand)
        
        if type(node.op) not in SAFE_OPERATORS:
            raise ValueError(f"Unsupported operation: {type(node.op).__name__}")
        
        return SAFE_OPERATORS[type(node.op)](operand)
    
    def visit_Name(self, node):
        """Visit variable name nodes"""
        if node.id in self.variables:
            return self.variables[node.id]
        elif node.id in {'pi', 'e'}:
            return getattr(math, node.id)
        else:
            raise ValueError(f"Unknown variable: {node.id}")
    
    def visit_Num(self, node):
        """Visit number nodes"""
        return node.n
    
    def visit_Constant(self, node):
        """Visit constant nodes (Python 3.8+)"""
        return node.value
    
    def visit_Call(self, node):
        """Visit function call nodes"""
        if not isinstance(node.func, ast.Name):
            raise ValueError("Only simple function calls are supported")
        
        func_name = node.func.id
        if func_name not in SAFE_FUNCTIONS:
            raise ValueError(f"Unsupported function: {func_name}")
        
        args = [self.visit(arg) for arg in node.args]
        return SAFE_FUNCTIONS[func_name](*args)
    
    def generic_visit(self, node):
        """Reject any nodes not explicitly handled"""
        raise ValueError(f"Unsupported expression type: {type(node).__name__}")


class CalculatorTool(Tool):
    """
    Tool for performing mathematical calculations
    
    This tool supports:
    - Basic arithmetic operations (+, -, *, /, //, %, **)
    - Mathematical functions (sqrt, sin, cos, etc.)
    - Variable substitution
    - Unit conversions
    """
    
    def __init__(self):
        """
        Initialize the calculator tool
        """
        super().__init__(
            name="calculator",
            description="Performs mathematical calculations and unit conversions"
        )
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the calculator tool
        
        Args:
            input_data: Dictionary containing:
                - expression: Mathematical expression to evaluate
                - variables: Dictionary of variable values (optional)
                - precision: Number of decimal places for the result (optional)
                - unit_conversion: Unit conversion specification (optional)
                
        Returns:
            Dictionary containing:
                - result: Calculated result
                - steps: Calculation steps (if available)
                - error: Error message (if calculation failed)
        """
        start_time = time.time()
        self.logger.info(f"Executing calculation: {input_data.get('expression')}")
        
        # Extract parameters
        expression = input_data.get("expression")
        variables = input_data.get("variables", {})
        precision = input_data.get("precision")
        unit_conversion = input_data.get("unit_conversion")
        
        # Validate input
        if not expression:
            error_msg = "Expression is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        try:
            # Perform the calculation
            result = self._evaluate_expression(expression, variables)
            steps = []
            
            # Apply unit conversion if specified
            if unit_conversion:
                steps.append(f"Initial result: {result}")
                result, conversion_step = self._convert_units(result, unit_conversion)
                steps.append(conversion_step)
            
            # Apply precision if specified
            if precision is not None:
                if not isinstance(precision, int) or precision < 0:
                    raise ValueError("Precision must be a non-negative integer")
                
                original_result = result
                result = round(result, precision)
                steps.append(f"Rounded to {precision} decimal places: {original_result} → {result}")
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"Calculation completed in {elapsed_time:.2f}s. Result: {result}")
            
            return {
                "result": result,
                "steps": steps if steps else None,
                "execution_time": elapsed_time
            }
        except Exception as e:
            error_msg = f"Error performing calculation: {str(e)}"
            self.logger.error(error_msg)
            return {"error": error_msg}
    
    def _evaluate_expression(self, expression: str, variables: Dict[str, Any]) -> float:
        """
        Safely evaluate a mathematical expression
        
        Args:
            expression: Mathematical expression
            variables: Dictionary of variable values
            
        Returns:
            Calculated result
        """
        # Clean the expression
        expression = expression.strip()
        
        # Parse the expression into an AST
        try:
            tree = ast.parse(expression, mode='eval')
            result = SafeEvaluator(variables).visit(tree.body)
            return result
        except Exception as e:
            raise ValueError(f"Invalid expression: {str(e)}")
    
    def _convert_units(self, value: float, conversion: str) -> tuple:
        """
        Convert a value between units
        
        Args:
            value: Value to convert
            conversion: Conversion specification (e.g., "m_to_km")
            
        Returns:
            Tuple of (converted value, conversion step description)
        """
        if conversion not in UNIT_CONVERSIONS:
            raise ValueError(f"Unsupported unit conversion: {conversion}")
        
        conversion_factor = UNIT_CONVERSIONS[conversion]
        
        if callable(conversion_factor):
            # Handle special conversions like temperature
            result = conversion_factor(value)
            step = f"Converted using {conversion}: {value} → {result}"
        else:
            # Handle standard conversions
            result = value * conversion_factor
            step = f"Converted using {conversion} (factor: {conversion_factor}): {value} → {result}"
        
        return result, step
    
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the calculator tool
        
        Returns:
            JSON Schema for tool input
        """
        return {
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string",
                    "description": "Mathematical expression to evaluate"
                },
                "variables": {
                    "type": "object",
                    "description": "Dictionary of variable values",
                    "additionalProperties": {
                        "type": "number"
                    }
                },
                "precision": {
                    "type": "integer",
                    "description": "Number of decimal places for the result",
                    "minimum": 0
                },
                "unit_conversion": {
                    "type": "string",
                    "description": "Unit conversion specification (e.g., 'm_to_km')",
                    "enum": list(UNIT_CONVERSIONS.keys())
                }
            },
            "required": ["expression"]
        }
    
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the calculator tool
        
        Returns:
            JSON Schema for tool output
        """
        return {
            "type": "object",
            "properties": {
                "result": {
                    "type": "number",
                    "description": "Calculated result"
                },
                "steps": {
                    "type": "array",
                    "description": "Calculation steps",
                    "items": {
                        "type": "string"
                    }
                },
                "execution_time": {
                    "type": "number",
                    "description": "Time taken to execute the calculation in seconds"
                },
                "error": {
                    "type": "string",
                    "description": "Error message if the calculation failed"
                }
            }
        }
    
    def get_examples(self) -> List[Dict[str, Any]]:
        """
        Get examples of calculator tool usage
        
        Returns:
            List of example input/output pairs
        """
        return [
            {
                "input": {
                    "expression": "2 + 2 * 3"
                },
                "output": {
                    "result": 8,
                    "execution_time": 0.001
                }
            },
            {
                "input": {
                    "expression": "sqrt(16) + pow(2, 3)"
                },
                "output": {
                    "result": 12.0,
                    "execution_time": 0.001
                }
            },
            {
                "input": {
                    "expression": "sin(radians(30))",
                    "precision": 2
                },
                "output": {
                    "result": 0.5,
                    "steps": ["Rounded to 2 decimal places: 0.49999999999999994 → 0.5"],
                    "execution_time": 0.001
                }
            },
            {
                "input": {
                    "expression": "x * y + z",
                    "variables": {"x": 2, "y": 3, "z": 5}
                },
                "output": {
                    "result": 11,
                    "execution_time": 0.001
                }
            },
            {
                "input": {
                    "expression": "100",
                    "unit_conversion": "km_to_miles"
                },
                "output": {
                    "result": 62.1371,
                    "steps": [
                        "Initial result: 100",
                        "Converted using km_to_miles (factor: 0.621371): 100 → 62.1371"
                    ],
                    "execution_time": 0.001
                }
            },
            {
                "input": {
                    "expression": "25",
                    "unit_conversion": "c_to_f"
                },
                "output": {
                    "result": 77.0,
                    "steps": [
                        "Initial result: 25",
                        "Converted using c_to_f: 25 → 77.0"
                    ],
                    "execution_time": 0.001
                }
            }
        ]

================
File: app/rag/tools/csv_json_handler.py
================
"""
CSV and JSON data handling utilities for the async DatabaseTool
"""
import csv
import json
import asyncio
import aiofiles
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional, Union
import logging

logger = logging.getLogger(__name__)

class AsyncCSVHandler:
    """Async-friendly CSV file handler"""
    
    @staticmethod
    async def read_csv(file_path: str) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        Read a CSV file asynchronously and return rows and column names
        
        Args:
            file_path: Path to the CSV file
            
        Returns:
            Tuple of (rows as dicts, column names)
        """
        async with aiofiles.open(file_path, 'r', newline='') as f:
            content = await f.read()
        
        # Process CSV content
        reader = csv.reader(content.splitlines())
        rows = list(reader)
        
        if not rows:
            return [], []
        
        # First row contains column names
        headers = rows[0]
        
        # Convert rows to list of dictionaries
        data = []
        for row in rows[1:]:
            # Skip empty rows
            if not row:
                continue
                
            # Create a dictionary for each row
            row_dict = {}
            for i, value in enumerate(row):
                if i < len(headers):
                    # Keep values as strings to match CSV format
                    row_dict[headers[i]] = value
            
            data.append(row_dict)
        
        return data, headers
    
    @staticmethod
    async def create_table_from_csv(conn, table_name: str, file_path: str) -> Tuple[List[str], int]:
        """
        Create a SQLite table from a CSV file
        
        Args:
            conn: SQLite connection
            table_name: Name of the table to create
            file_path: Path to the CSV file
            
        Returns:
            Tuple of (column names, number of rows inserted)
        """
        # Read CSV data
        data, headers = await AsyncCSVHandler.read_csv(file_path)
        
        if not data or not headers:
            return [], 0
        
        # Determine column types based on first row
        column_types = {}
        for header in headers:
            column_types[header] = "TEXT"  # Default type
            
        for row in data[:10]:  # Sample first 10 rows to determine types
            for header in headers:
                if header in row:
                    value = row[header]
                    if isinstance(value, int):
                        column_types[header] = "INTEGER"
                    elif isinstance(value, float):
                        column_types[header] = "REAL"
        
        # Create table
        columns_sql = ", ".join([f'"{h}" {column_types[h]}' for h in headers])
        create_table_sql = f'CREATE TABLE IF NOT EXISTS "{table_name}" ({columns_sql})'
        
        await conn.execute(create_table_sql)
        
        # Insert data in batches
        batch_size = 100
        rows_inserted = 0
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i+batch_size]
            if not batch:
                continue
                
            # Prepare placeholders for the INSERT statement
            placeholders = ", ".join(["?" for _ in headers])
            column_names = ", ".join([f'"{h}"' for h in headers])
            insert_sql = f'INSERT INTO "{table_name}" ({column_names}) VALUES ({placeholders})'
            
            # Prepare values for each row
            values = []
            for row in batch:
                row_values = []
                for header in headers:
                    row_values.append(row.get(header, None))
                values.append(row_values)
            
            # Execute the INSERT statement for the batch
            await conn.executemany(insert_sql, values)
            
            rows_inserted += len(batch)
        
        await conn.commit()
        return headers, rows_inserted


class AsyncJSONHandler:
    """Async-friendly JSON file handler"""
    
    @staticmethod
    async def read_json(file_path: str) -> Any:
        """
        Read a JSON file asynchronously
        
        Args:
            file_path: Path to the JSON file
            
        Returns:
            Parsed JSON data
        """
        async with aiofiles.open(file_path, 'r') as f:
            content = await f.read()
        
        return json.loads(content)
    
    @staticmethod
    async def create_table_from_json(conn, table_name: str, file_path: str) -> Tuple[List[str], int]:
        """
        Create a SQLite table from a JSON file
        
        Args:
            conn: SQLite connection
            table_name: Name of the table to create
            file_path: Path to the JSON file
            
        Returns:
            Tuple of (column names, number of rows inserted)
        """
        # Read JSON data
        data = await AsyncJSONHandler.read_json(file_path)
        
        # Convert to list of dictionaries if it's not already
        if isinstance(data, dict):
            # Check if it's a nested structure
            if any(isinstance(v, (list, dict)) for v in data.values()):
                # For complex nested structures, we'll flatten one level
                flattened_data = []
                for key, value in data.items():
                    if isinstance(value, list):
                        for item in value:
                            if isinstance(item, dict):
                                item_copy = item.copy()
                                item_copy['_parent_key'] = key
                                flattened_data.append(item_copy)
                    elif isinstance(value, dict):
                        value_copy = value.copy()
                        value_copy['_key'] = key
                        flattened_data.append(value_copy)
                
                if flattened_data:
                    data = flattened_data
                else:
                    # If we couldn't flatten, just use the original dict
                    data = [data]
            else:
                # Simple key-value pairs
                data = [data]
        
        if not isinstance(data, list):
            raise ValueError("JSON data could not be converted to a list of records")
        
        if not data:
            return [], 0
        
        # Get all possible column names from all records
        all_columns = set()
        for item in data:
            if isinstance(item, dict):
                all_columns.update(item.keys())
        
        headers = list(all_columns)
        
        # Determine column types based on first few records
        column_types = {}
        for header in headers:
            column_types[header] = "TEXT"  # Default type
            
        for item in data[:10]:  # Sample first 10 items to determine types
            if not isinstance(item, dict):
                continue
                
            for header in headers:
                if header in item:
                    value = item[header]
                    if isinstance(value, int):
                        column_types[header] = "INTEGER"
                    elif isinstance(value, float):
                        column_types[header] = "REAL"
                    elif isinstance(value, bool):
                        column_types[header] = "INTEGER"  # SQLite doesn't have a boolean type
                    elif isinstance(value, (list, dict)):
                        # Store complex types as JSON strings
                        column_types[header] = "TEXT"
        
        # Create table
        columns_sql = ", ".join([f'"{h}" {column_types[h]}' for h in headers])
        create_table_sql = f'CREATE TABLE IF NOT EXISTS "{table_name}" ({columns_sql})'
        
        await conn.execute(create_table_sql)
        
        # Insert data in batches
        batch_size = 100
        rows_inserted = 0
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i+batch_size]
            if not batch:
                continue
                
            # Prepare placeholders for the INSERT statement
            placeholders = ", ".join(["?" for _ in headers])
            column_names = ", ".join([f'"{h}"' for h in headers])
            insert_sql = f'INSERT INTO "{table_name}" ({column_names}) VALUES ({placeholders})'
            
            # Prepare values for each row
            values = []
            for item in batch:
                if not isinstance(item, dict):
                    continue
                    
                row_values = []
                for header in headers:
                    value = item.get(header, None)
                    # Convert complex types to JSON strings
                    if isinstance(value, (list, dict)):
                        value = json.dumps(value)
                    # Convert boolean to integer
                    elif isinstance(value, bool):
                        value = 1 if value else 0
                    row_values.append(value)
                values.append(row_values)
            
            # Execute the INSERT statement for the batch
            await conn.executemany(insert_sql, values)
            
            rows_inserted += len(batch)
        
        await conn.commit()
        return headers, rows_inserted

================
File: app/rag/tools/database_tool_async.py
================
"""
DatabaseTool - Tool for querying structured data with true async implementation
"""
import logging
import time
import json
import re
import asyncio
import aiofiles
import uuid
from typing import Any, Dict, List, Optional, Union, Tuple
from pathlib import Path

from app.rag.tools.base import Tool
from app.db.connection_manager import connection_manager

class DatabaseTool(Tool):
    """
    Tool for querying structured data with true async implementation
    
    This tool allows querying structured data sources using SQL-like syntax.
    It supports:
    - SQLite database queries (using aiosqlite)
    - PostgreSQL database queries (using asyncpg)
    - CSV file queries (converted to SQLite in-memory)
    - JSON file queries (converted to SQLite in-memory)
    """
    
    def __init__(self, data_dir: Optional[str] = None):
        """
        Initialize the database tool
        
        Args:
            data_dir: Directory containing data files (optional)
        """
        super().__init__(
            name="database",
            description="Queries structured data from databases, CSV, and JSON files"
        )
        self.data_dir = data_dir
        # Use the connection manager instead of maintaining our own connections
        self.connection_manager = connection_manager
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the database tool
        
        Args:
            input_data: Dictionary containing:
                - query: SQL query string
                - source: Data source (database file, CSV file, or JSON file)
                - params: Query parameters (optional)
                - limit: Maximum number of results to return (optional)
                
        Returns:
            Dictionary containing:
                - results: Query results
                - columns: Column names
                - row_count: Number of rows returned
                - execution_time: Time taken to execute the query
                - error: Error message if the query failed
        """
        start_time = time.time()
        self.logger.info(f"Executing database query on source: {input_data.get('source')}")
        
        # Extract parameters
        query = input_data.get("query")
        source = input_data.get("source")
        params = input_data.get("params", {})
        limit = input_data.get("limit")
        
        # Validate input
        if not query:
            error_msg = "Query is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        if not source:
            error_msg = "Data source is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        try:
            # Resolve source path if data_dir is provided
            source_path = self._resolve_source_path(source)
            
            # Execute query
            results, columns = await self._execute_query(
                query=query,
                source=source_path,
                params=params,
                limit=limit
            )
            
            elapsed_time = time.time() - start_time
            row_count = len(results)
            self.logger.info(f"Database query completed in {elapsed_time:.2f}s. Returned {row_count} rows")
            
            return {
                "results": results,
                "columns": columns,
                "row_count": row_count,
                "execution_time": elapsed_time
            }
        except Exception as e:
            error_msg = f"Error executing database query: {str(e)}"
            self.logger.error(error_msg)
            return {"error": error_msg}
    
    def _resolve_source_path(self, source: str) -> str:
        """
        Resolve the source path
        
        Args:
            source: Data source name or path
            
        Returns:
            Resolved source path
        """
        if self.data_dir and not Path(source).is_absolute():
            return str(Path(self.data_dir) / source)
        return source
    
    async def _execute_query(
        self, 
        query: str, 
        source: str, 
        params: Dict[str, Any] = None,
        limit: Optional[int] = None
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        Execute a query on a data source
        
        Args:
            query: SQL query
            source: Data source path
            params: Query parameters
            limit: Maximum number of results
            
        Returns:
            Tuple of (results, columns)
        """
        source_lower = source.lower()
        
        # Apply limit if specified and not already in the query
        if limit is not None and not re.search(r'\bLIMIT\s+\d+\b', query, re.IGNORECASE):
            query = f"{query} LIMIT {limit}"
        
        # Determine source type
        if source_lower.endswith('.db') or source_lower.endswith('.sqlite') or source_lower.endswith('.sqlite3'):
            return await self._query_sqlite(query, source, params)
        elif source_lower.startswith('postgresql://'):
            return await self._query_postgres(query, source, params)
        elif source_lower.endswith('.csv'):
            return await self._query_csv(query, source, params)
        elif source_lower.endswith('.json'):
            return await self._query_json(query, source, params)
        else:
            raise ValueError(f"Unsupported data source type: {source}")
    
    async def _query_sqlite(
        self, 
        query: str, 
        db_path: str, 
        params: Dict[str, Any] = None
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        Execute a query on a SQLite database using aiosqlite
        
        Args:
            query: SQL query
            db_path: Database file path
            params: Query parameters
            
        Returns:
            Tuple of (results, columns)
        """
        # Register the connection with the connection manager
        conn_id = self.connection_manager.register_connection(db_path)
        
        # Get a connection from the connection manager
        conn = await self.connection_manager.get_sqlite_connection(conn_id)
        
        try:
            # Execute query
            if params:
                # Convert dict params to tuple or list based on parameter style
                if isinstance(params, dict):
                    # For named parameters
                    cursor = await conn.execute(query, params)
                else:
                    # For positional parameters
                    cursor = await conn.execute(query, params)
            else:
                cursor = await conn.execute(query)
            
            # Fetch results
            rows = await cursor.fetchall()
            
            # Get column names
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            # Convert rows to list of dictionaries
            results = [dict(row) for row in rows]  # aiosqlite.Row objects are already dict-like
            
            await cursor.close()
            return results, columns
        except Exception as e:
            self.logger.error(f"Error executing SQLite query: {str(e)}")
            raise
    
    async def _query_postgres(
        self, 
        query: str, 
        connection_string: str, 
        params: Dict[str, Any] = None
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        Execute a query on a PostgreSQL database using asyncpg
        
        Args:
            query: SQL query
            connection_string: PostgreSQL connection string
            params: Query parameters
            
        Returns:
            Tuple of (results, columns)
        """
        # Register the connection with the connection manager
        conn_id = self.connection_manager.register_connection(connection_string)
        
        # Get a connection from the connection manager
        conn = await self.connection_manager.get_postgres_connection(conn_id)
        
        try:
            # Execute query
            if params:
                # Convert dict params to list for asyncpg
                if isinstance(params, dict):
                    # Replace named parameters with positional parameters
                    # This is a simplistic approach and might not work for all cases
                    param_values = []
                    for key, value in params.items():
                        # Replace :key or %(key)s with $n
                        query = query.replace(f":{key}", f"${len(param_values) + 1}")
                        query = query.replace(f"%({key})s", f"${len(param_values) + 1}")
                        param_values.append(value)
                    
                    # Execute with positional parameters
                    rows = await conn.fetch(query, *param_values)
                else:
                    # For positional parameters
                    rows = await conn.fetch(query, *params)
            else:
                rows = await conn.fetch(query)
            
            # Get column names from the first row
            columns = [key for key in rows[0].keys()] if rows else []
            
            # Convert rows to list of dictionaries
            results = [dict(row) for row in rows]
            
            return results, columns
        except Exception as e:
            self.logger.error(f"Error executing PostgreSQL query: {str(e)}")
            raise
        finally:
            # Release the connection back to the pool
            await self.connection_manager.release_postgres_connection(conn_id, conn)
    
    async def _query_csv(
        self,
        query: str,
        csv_path: str,
        params: Dict[str, Any] = None
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        Execute a query on a CSV file using aiosqlite
        
        Args:
            query: SQL query
            csv_path: CSV file path
            params: Query parameters
            
        Returns:
            Tuple of (results, columns)
        """
        from app.rag.tools.csv_json_handler import AsyncCSVHandler
        
        # Register the in-memory SQLite connection and get its UUID
        conn_id = self.connection_manager.register_connection(":memory:")
        
        # Get a connection from the connection manager
        conn = None
        try:
            conn = await self.connection_manager.get_sqlite_connection(conn_id)
            
            # Get table name from file path
            table_name = Path(csv_path).stem
            
            # Create table from CSV file
            headers, rows_inserted = await AsyncCSVHandler.create_table_from_csv(conn, table_name, csv_path)
            
            self.logger.info(f"Created table '{table_name}' from CSV with {rows_inserted} rows and {len(headers)} columns")
            
            # Modify query to use the table name
            modified_query = re.sub(
                r'\bFROM\s+([^\s,]+)',
                f'FROM {table_name}',
                query,
                flags=re.IGNORECASE
            )
            
            # Execute query
            if params:
                # Convert dict params to tuple or list based on parameter style
                if isinstance(params, dict):
                    # For named parameters
                    cursor = await conn.execute(modified_query, params)
                else:
                    # For positional parameters
                    cursor = await conn.execute(modified_query, params)
            else:
                cursor = await conn.execute(modified_query)
            
            # Fetch results
            rows = await cursor.fetchall()
            
            # Get column names
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            # Convert rows to list of dictionaries
            results = [dict(row) for row in rows]
            
            await cursor.close()
            
            # Commit changes to ensure all operations are complete
            await conn.commit()
            
            return results, columns
        except Exception as e:
            self.logger.error(f"Error executing CSV query: {str(e)}")
            raise
        finally:
            # Close the connection
            if conn_id:
                try:
                    await self.connection_manager.close(conn_id)
                except Exception as e:
                    self.logger.warning(f"Error closing connection {conn_id}: {str(e)}")
    
    async def _query_json(
        self,
        query: str,
        json_path: str,
        params: Dict[str, Any] = None
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        Execute a query on a JSON file using aiosqlite
        
        Args:
            query: SQL query
            json_path: JSON file path
            params: Query parameters
            
        Returns:
            Tuple of (results, columns)
        """
        from app.rag.tools.csv_json_handler import AsyncJSONHandler
        
        # Register the in-memory SQLite connection and get its UUID
        conn_id = self.connection_manager.register_connection(":memory:")
        
        # Get a connection from the connection manager
        conn = None
        try:
            conn = await self.connection_manager.get_sqlite_connection(conn_id)
            
            # Get table name from file path
            table_name = Path(json_path).stem
            
            # Create table from JSON file
            headers, rows_inserted = await AsyncJSONHandler.create_table_from_json(conn, table_name, json_path)
            
            self.logger.info(f"Created table '{table_name}' from JSON with {rows_inserted} rows and {len(headers)} columns")
            
            # Modify query to use the table name
            modified_query = re.sub(
                r'\bFROM\s+([^\s,]+)',
                f'FROM {table_name}',
                query,
                flags=re.IGNORECASE
            )
            
            # Execute query
            if params:
                # Convert dict params to tuple or list based on parameter style
                if isinstance(params, dict):
                    # For named parameters
                    cursor = await conn.execute(modified_query, params)
                else:
                    # For positional parameters
                    cursor = await conn.execute(modified_query, params)
            else:
                cursor = await conn.execute(modified_query)
            
            # Fetch results
            rows = await cursor.fetchall()
            
            # Get column names
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            # Convert rows to list of dictionaries
            results = [dict(row) for row in rows]
            
            await cursor.close()
            
            # Commit changes to ensure all operations are complete
            await conn.commit()
            
            return results, columns
        except Exception as e:
            self.logger.error(f"Error executing JSON query: {str(e)}")
            raise
        finally:
            # Close the connection
            if conn_id:
                try:
                    await self.connection_manager.close(conn_id)
                except Exception as e:
                    self.logger.warning(f"Error closing connection {conn_id}: {str(e)}")
    
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the database tool
        
        Returns:
            JSON Schema for tool input
        """
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "SQL query string"
                },
                "source": {
                    "type": "string",
                    "description": "Data source (database file, CSV file, JSON file, or PostgreSQL connection string)"
                },
                "params": {
                    "type": "object",
                    "description": "Query parameters",
                    "additionalProperties": True
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of results to return",
                    "minimum": 1
                }
            },
            "required": ["query", "source"]
        }
    
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the database tool
        
        Returns:
            JSON Schema for tool output
        """
        return {
            "type": "object",
            "properties": {
                "results": {
                    "type": "array",
                    "description": "Query results",
                    "items": {
                        "type": "object",
                        "additionalProperties": True
                    }
                },
                "columns": {
                    "type": "array",
                    "description": "Column names",
                    "items": {
                        "type": "string"
                    }
                },
                "row_count": {
                    "type": "integer",
                    "description": "Number of rows returned"
                },
                "execution_time": {
                    "type": "number",
                    "description": "Time taken to execute the query in seconds"
                },
                "error": {
                    "type": "string",
                    "description": "Error message if the query failed"
                }
            }
        }
    
    def get_examples(self) -> List[Dict[str, Any]]:
        """
        Get examples of database tool usage
        
        Returns:
            List of example input/output pairs
        """
        return [
            {
                "input": {
                    "query": "SELECT * FROM users WHERE age > 30",
                    "source": "users.db"
                },
                "output": {
                    "results": [
                        {"id": 1, "name": "John Doe", "age": 35, "email": "john@example.com"},
                        {"id": 3, "name": "Bob Smith", "age": 42, "email": "bob@example.com"}
                    ],
                    "columns": ["id", "name", "age", "email"],
                    "row_count": 2,
                    "execution_time": 0.05
                }
            },
            {
                "input": {
                    "query": "SELECT product_name, price FROM products WHERE category = :category",
                    "source": "products.csv",
                    "params": {"category": "Electronics"},
                    "limit": 5
                },
                "output": {
                    "results": [
                        {"product_name": "Smartphone", "price": 699.99},
                        {"product_name": "Laptop", "price": 1299.99},
                        {"product_name": "Headphones", "price": 149.99}
                    ],
                    "columns": ["product_name", "price"],
                    "row_count": 3,
                    "execution_time": 0.08
                }
            },
            {
                "input": {
                    "query": "SELECT city, COUNT(*) as customer_count FROM customers GROUP BY city ORDER BY customer_count DESC",
                    "source": "customers.json",
                    "limit": 3
                },
                "output": {
                    "results": [
                        {"city": "New York", "customer_count": 145},
                        {"city": "Los Angeles", "customer_count": 98},
                        {"city": "Chicago", "customer_count": 76}
                    ],
                    "columns": ["city", "customer_count"],
                    "row_count": 3,
                    "execution_time": 0.12
                }
            },
            {
                "input": {
                    "query": "SELECT u.username, COUNT(o.id) as order_count FROM users u JOIN orders o ON u.id = o.user_id GROUP BY u.username ORDER BY order_count DESC LIMIT 5",
                    "source": "postgresql://username:password@localhost:5432/mydb"
                },
                "output": {
                    "results": [
                        {"username": "johndoe", "order_count": 42},
                        {"username": "janedoe", "order_count": 38},
                        {"username": "bobsmith", "order_count": 27}
                    ],
                    "columns": ["username", "order_count"],
                    "row_count": 3,
                    "execution_time": 0.15
                }
            }
        ]

================
File: app/rag/tools/database_tool.py
================
"""
DatabaseTool - Tool for querying structured data
"""
import logging
import time
import json
import re
import sqlite3
from typing import Any, Dict, List, Optional, Union
from pathlib import Path

from app.rag.tools.base import Tool

class DatabaseTool(Tool):
    """
    Tool for querying structured data
    
    This tool allows querying structured data sources using SQL-like syntax.
    It supports:
    - SQLite database queries
    - CSV file queries (converted to SQLite in-memory)
    - JSON file queries (converted to SQLite in-memory)
    """
    
    def __init__(self, data_dir: Optional[str] = None):
        """
        Initialize the database tool
        
        Args:
            data_dir: Directory containing data files (optional)
        """
        super().__init__(
            name="database",
            description="Queries structured data from databases, CSV, and JSON files"
        )
        self.data_dir = data_dir
        self.connections = {}  # Cache for database connections
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the database tool
        
        Args:
            input_data: Dictionary containing:
                - query: SQL query string
                - source: Data source (database file, CSV file, or JSON file)
                - params: Query parameters (optional)
                - limit: Maximum number of results to return (optional)
                
        Returns:
            Dictionary containing:
                - results: Query results
                - columns: Column names
                - row_count: Number of rows returned
                - execution_time: Time taken to execute the query
                - error: Error message if the query failed
        """
        start_time = time.time()
        self.logger.info(f"Executing database query on source: {input_data.get('source')}")
        
        # Extract parameters
        query = input_data.get("query")
        source = input_data.get("source")
        params = input_data.get("params", {})
        limit = input_data.get("limit")
        
        # Validate input
        if not query:
            error_msg = "Query is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        if not source:
            error_msg = "Data source is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        try:
            # Resolve source path if data_dir is provided
            source_path = self._resolve_source_path(source)
            
            # Execute query
            results, columns = await self._execute_query(
                query=query,
                source=source_path,
                params=params,
                limit=limit
            )
            
            elapsed_time = time.time() - start_time
            row_count = len(results)
            self.logger.info(f"Database query completed in {elapsed_time:.2f}s. Returned {row_count} rows")
            
            return {
                "results": results,
                "columns": columns,
                "row_count": row_count,
                "execution_time": elapsed_time
            }
        except Exception as e:
            error_msg = f"Error executing database query: {str(e)}"
            self.logger.error(error_msg)
            return {"error": error_msg}
    
    def _resolve_source_path(self, source: str) -> str:
        """
        Resolve the source path
        
        Args:
            source: Data source name or path
            
        Returns:
            Resolved source path
        """
        if self.data_dir and not Path(source).is_absolute():
            return str(Path(self.data_dir) / source)
        return source
    
    async def _execute_query(
        self, 
        query: str, 
        source: str, 
        params: Dict[str, Any] = None,
        limit: Optional[int] = None
    ) -> tuple:
        """
        Execute a query on a data source
        
        Args:
            query: SQL query
            source: Data source path
            params: Query parameters
            limit: Maximum number of results
            
        Returns:
            Tuple of (results, columns)
        """
        source_lower = source.lower()
        
        # Determine source type
        if source_lower.endswith('.db') or source_lower.endswith('.sqlite') or source_lower.endswith('.sqlite3'):
            return await self._query_sqlite(query, source, params, limit)
        elif source_lower.endswith('.csv'):
            return await self._query_csv(query, source, params, limit)
        elif source_lower.endswith('.json'):
            return await self._query_json(query, source, params, limit)
        else:
            raise ValueError(f"Unsupported data source type: {source}")
    
    async def _query_sqlite(
        self, 
        query: str, 
        db_path: str, 
        params: Dict[str, Any] = None,
        limit: Optional[int] = None
    ) -> tuple:
        """
        Execute a query on a SQLite database
        
        Args:
            query: SQL query
            db_path: Database file path
            params: Query parameters
            limit: Maximum number of results
            
        Returns:
            Tuple of (results, columns)
        """
        # Apply limit if specified
        if limit is not None:
            # Check if query already has a LIMIT clause
            if not re.search(r'\bLIMIT\s+\d+\b', query, re.IGNORECASE):
                query = f"{query} LIMIT {limit}"
        
        # Get or create connection
        if db_path not in self.connections:
            self.connections[db_path] = sqlite3.connect(db_path)
        
        conn = self.connections[db_path]
        cursor = conn.cursor()
        
        try:
            # Execute query
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            # Get column names
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            # Fetch results
            rows = cursor.fetchall()
            
            # Convert rows to list of dictionaries
            results = [dict(zip(columns, row)) for row in rows]
            
            return results, columns
        finally:
            cursor.close()
    
    async def _query_csv(
        self, 
        query: str, 
        csv_path: str, 
        params: Dict[str, Any] = None,
        limit: Optional[int] = None
    ) -> tuple:
        """
        Execute a query on a CSV file
        
        Args:
            query: SQL query
            csv_path: CSV file path
            params: Query parameters
            limit: Maximum number of results
            
        Returns:
            Tuple of (results, columns)
        """
        import pandas as pd
        import sqlite3
        
        # Read CSV file
        df = pd.read_csv(csv_path)
        
        # Create in-memory SQLite database
        conn = sqlite3.connect(':memory:')
        
        try:
            # Write dataframe to SQLite
            table_name = Path(csv_path).stem
            df.to_sql(table_name, conn, index=False, if_exists='replace')
            
            # Modify query to use the table name
            # Replace "FROM data" with "FROM {table_name}"
            modified_query = re.sub(
                r'\bFROM\s+([^\s,]+)',
                f'FROM {table_name}',
                query,
                flags=re.IGNORECASE
            )
            
            # Apply limit if specified
            if limit is not None and not re.search(r'\bLIMIT\s+\d+\b', modified_query, re.IGNORECASE):
                modified_query = f"{modified_query} LIMIT {limit}"
            
            # Execute query
            cursor = conn.cursor()
            if params:
                cursor.execute(modified_query, params)
            else:
                cursor.execute(modified_query)
            
            # Get column names
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            # Fetch results
            rows = cursor.fetchall()
            
            # Convert rows to list of dictionaries
            results = [dict(zip(columns, row)) for row in rows]
            
            return results, columns
        finally:
            conn.close()
    
    async def _query_json(
        self, 
        query: str, 
        json_path: str, 
        params: Dict[str, Any] = None,
        limit: Optional[int] = None
    ) -> tuple:
        """
        Execute a query on a JSON file
        
        Args:
            query: SQL query
            json_path: JSON file path
            params: Query parameters
            limit: Maximum number of results
            
        Returns:
            Tuple of (results, columns)
        """
        import pandas as pd
        import sqlite3
        
        # Read JSON file
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # Convert to dataframe
        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            # Handle nested JSON structures
            if any(isinstance(v, (list, dict)) for v in data.values()):
                # Normalize nested JSON
                df = pd.json_normalize(data)
            else:
                # Simple key-value pairs
                df = pd.DataFrame([data])
        else:
            raise ValueError("JSON file must contain an object or array")
        
        # Create in-memory SQLite database
        conn = sqlite3.connect(':memory:')
        
        try:
            # Write dataframe to SQLite
            table_name = Path(json_path).stem
            df.to_sql(table_name, conn, index=False, if_exists='replace')
            
            # Modify query to use the table name
            modified_query = re.sub(
                r'\bFROM\s+([^\s,]+)',
                f'FROM {table_name}',
                query,
                flags=re.IGNORECASE
            )
            
            # Apply limit if specified
            if limit is not None and not re.search(r'\bLIMIT\s+\d+\b', modified_query, re.IGNORECASE):
                modified_query = f"{modified_query} LIMIT {limit}"
            
            # Execute query
            cursor = conn.cursor()
            if params:
                cursor.execute(modified_query, params)
            else:
                cursor.execute(modified_query)
            
            # Get column names
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            # Fetch results
            rows = cursor.fetchall()
            
            # Convert rows to list of dictionaries
            results = [dict(zip(columns, row)) for row in rows]
            
            return results, columns
        finally:
            conn.close()
    
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the database tool
        
        Returns:
            JSON Schema for tool input
        """
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "SQL query string"
                },
                "source": {
                    "type": "string",
                    "description": "Data source (database file, CSV file, or JSON file)"
                },
                "params": {
                    "type": "object",
                    "description": "Query parameters",
                    "additionalProperties": True
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of results to return",
                    "minimum": 1
                }
            },
            "required": ["query", "source"]
        }
    
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the database tool
        
        Returns:
            JSON Schema for tool output
        """
        return {
            "type": "object",
            "properties": {
                "results": {
                    "type": "array",
                    "description": "Query results",
                    "items": {
                        "type": "object",
                        "additionalProperties": True
                    }
                },
                "columns": {
                    "type": "array",
                    "description": "Column names",
                    "items": {
                        "type": "string"
                    }
                },
                "row_count": {
                    "type": "integer",
                    "description": "Number of rows returned"
                },
                "execution_time": {
                    "type": "number",
                    "description": "Time taken to execute the query in seconds"
                },
                "error": {
                    "type": "string",
                    "description": "Error message if the query failed"
                }
            }
        }
    
    def get_examples(self) -> List[Dict[str, Any]]:
        """
        Get examples of database tool usage
        
        Returns:
            List of example input/output pairs
        """
        return [
            {
                "input": {
                    "query": "SELECT * FROM users WHERE age > 30",
                    "source": "users.db"
                },
                "output": {
                    "results": [
                        {"id": 1, "name": "John Doe", "age": 35, "email": "john@example.com"},
                        {"id": 3, "name": "Bob Smith", "age": 42, "email": "bob@example.com"}
                    ],
                    "columns": ["id", "name", "age", "email"],
                    "row_count": 2,
                    "execution_time": 0.05
                }
            },
            {
                "input": {
                    "query": "SELECT product_name, price FROM products WHERE category = :category",
                    "source": "products.csv",
                    "params": {"category": "Electronics"},
                    "limit": 5
                },
                "output": {
                    "results": [
                        {"product_name": "Smartphone", "price": 699.99},
                        {"product_name": "Laptop", "price": 1299.99},
                        {"product_name": "Headphones", "price": 149.99}
                    ],
                    "columns": ["product_name", "price"],
                    "row_count": 3,
                    "execution_time": 0.08
                }
            },
            {
                "input": {
                    "query": "SELECT city, COUNT(*) as customer_count FROM customers GROUP BY city ORDER BY customer_count DESC",
                    "source": "customers.json",
                    "limit": 3
                },
                "output": {
                    "results": [
                        {"city": "New York", "customer_count": 145},
                        {"city": "Los Angeles", "customer_count": 98},
                        {"city": "Chicago", "customer_count": 76}
                    ],
                    "columns": ["city", "customer_count"],
                    "row_count": 3,
                    "execution_time": 0.12
                }
            }
        ]

================
File: app/rag/tools/postgresql_tool.py
================
"""
PostgreSQLTool - Tool for PostgreSQL-specific operations
"""
import logging
import time
from typing import Any, Dict, List, Optional, Union

from app.rag.tools.base import Tool
from app.db.connection_manager import connection_manager
from app.db.schema_inspector import schema_inspector

class PostgreSQLTool(Tool):
    """
    Tool for PostgreSQL-specific operations
    
    This tool provides PostgreSQL-specific capabilities, including:
    - Schema introspection
    - Query explanation
    - Vector similarity search (pgvector)
    - Extension management
    """
    
    def __init__(self):
        """
        Initialize the PostgreSQL tool
        """
        super().__init__(
            name="postgresql",
            description="Provides PostgreSQL-specific operations like schema introspection, query explanation, and vector search"
        )
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the PostgreSQL tool
        
        Args:
            input_data: Dictionary containing:
                - operation: Operation to perform
                - connection_id: PostgreSQL connection ID
                - Additional operation-specific parameters
                
        Returns:
            Dictionary containing operation-specific results
        """
        start_time = time.time()
        self.logger.info(f"Executing PostgreSQL operation: {input_data.get('operation')}")
        
        # Extract parameters
        operation = input_data.get("operation")
        connection_id = input_data.get("connection_id")
        
        # Validate input
        if not operation:
            error_msg = "Operation is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        if not connection_id:
            error_msg = "Connection ID is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        # Check connection type
        try:
            conn_type = connection_manager.get_connection_type(connection_id)
            if conn_type != 'postgres':
                error_msg = f"Connection {connection_id} is not a PostgreSQL connection"
                self.logger.error(error_msg)
                return {"error": error_msg}
        except ValueError as e:
            error_msg = f"Invalid connection ID: {str(e)}"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        try:
            # Execute operation
            if operation == "get_schemas":
                result = await schema_inspector.get_schemas(connection_id)
                return {
                    "schemas": result,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_tables":
                schema = input_data.get("schema", "public")
                result = await schema_inspector.get_tables(connection_id, schema)
                return {
                    "tables": result,
                    "schema": schema,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_columns":
                table_name = input_data.get("table_name")
                schema = input_data.get("schema", "public")
                
                if not table_name:
                    return {"error": "Table name is required"}
                
                result = await schema_inspector.get_columns(connection_id, table_name, schema)
                return {
                    "columns": result,
                    "table_name": table_name,
                    "schema": schema,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_indexes":
                table_name = input_data.get("table_name")
                schema = input_data.get("schema", "public")
                
                if not table_name:
                    return {"error": "Table name is required"}
                
                result = await schema_inspector.get_indexes(connection_id, table_name, schema)
                return {
                    "indexes": result,
                    "table_name": table_name,
                    "schema": schema,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_constraints":
                table_name = input_data.get("table_name")
                schema = input_data.get("schema", "public")
                
                if not table_name:
                    return {"error": "Table name is required"}
                
                result = await schema_inspector.get_constraints(connection_id, table_name, schema)
                return {
                    "constraints": result,
                    "table_name": table_name,
                    "schema": schema,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_foreign_keys":
                table_name = input_data.get("table_name")
                schema = input_data.get("schema", "public")
                
                if not table_name:
                    return {"error": "Table name is required"}
                
                result = await schema_inspector.get_foreign_keys(connection_id, table_name, schema)
                return {
                    "foreign_keys": result,
                    "table_name": table_name,
                    "schema": schema,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_table_structure":
                table_name = input_data.get("table_name")
                schema = input_data.get("schema", "public")
                
                if not table_name:
                    return {"error": "Table name is required"}
                
                result = await schema_inspector.get_table_structure(connection_id, table_name, schema)
                return {
                    "table_structure": result,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_database_structure":
                result = await schema_inspector.get_database_structure(connection_id)
                return {
                    "database_structure": result,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_extensions":
                extension_name = input_data.get("extension_name")
                result = await schema_inspector.get_extension_info(connection_id, extension_name)
                return {
                    "extensions": result,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "check_extension":
                extension_name = input_data.get("extension_name")
                
                if not extension_name:
                    return {"error": "Extension name is required"}
                
                result = await schema_inspector.check_extension_installed(connection_id, extension_name)
                return {
                    "extension_name": extension_name,
                    "installed": result,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "get_pgvector_info":
                result = await schema_inspector.get_pgvector_info(connection_id)
                return {
                    "pgvector_info": result,
                    "execution_time": time.time() - start_time
                }
            
            elif operation == "explain_query":
                query = input_data.get("query")
                explain_type = input_data.get("explain_type", "simple")  # simple, analyze, verbose, etc.
                
                if not query:
                    return {"error": "Query is required"}
                
                # Get connection
                conn = await connection_manager.get_postgres_connection(connection_id)
                
                try:
                    # Build EXPLAIN command based on type
                    explain_options = ""
                    if explain_type == "analyze":
                        explain_options = "ANALYZE"
                    elif explain_type == "verbose":
                        explain_options = "VERBOSE"
                    elif explain_type == "analyze_verbose":
                        explain_options = "ANALYZE, VERBOSE"
                    elif explain_type == "analyze_verbose_buffers":
                        explain_options = "ANALYZE, VERBOSE, BUFFERS"
                    elif explain_type == "json":
                        explain_options = "FORMAT JSON"
                    elif explain_type == "analyze_json":
                        explain_options = "ANALYZE, FORMAT JSON"
                    
                    # Execute EXPLAIN
                    explain_query = f"EXPLAIN ({explain_options}) {query}"
                    rows = await conn.fetch(explain_query)
                    
                    # Format result
                    if explain_type in ["json", "analyze_json"]:
                        # For JSON format, return the parsed JSON
                        plan = rows[0][0]
                        return {
                            "query": query,
                            "plan": plan,
                            "execution_time": time.time() - start_time
                        }
                    else:
                        # For text format, concatenate the rows
                        plan_lines = [row[0] for row in rows]
                        plan_text = "\n".join(plan_lines)
                        
                        return {
                            "query": query,
                            "plan_text": plan_text,
                            "execution_time": time.time() - start_time
                        }
                finally:
                    # Release connection back to pool
                    await connection_manager.release_postgres_connection(connection_id, conn)
            
            elif operation == "vector_search":
                table_name = input_data.get("table_name")
                column_name = input_data.get("column_name")
                vector = input_data.get("vector")
                distance_type = input_data.get("distance_type", "cosine")  # cosine, euclidean, inner_product
                limit = input_data.get("limit", 10)
                schema = input_data.get("schema", "public")
                
                if not table_name:
                    return {"error": "Table name is required"}
                if not column_name:
                    return {"error": "Column name is required"}
                if not vector:
                    return {"error": "Vector is required"}
                
                # Check if pgvector is installed
                pgvector_info = await schema_inspector.get_pgvector_info(connection_id)
                if not pgvector_info.get("installed", False):
                    return {"error": "pgvector extension is not installed"}
                
                # Get connection
                conn = await connection_manager.get_postgres_connection(connection_id)
                
                try:
                    # Build vector search query
                    distance_operator = "<->"  # Default: Euclidean distance
                    if distance_type == "cosine":
                        distance_operator = "<=>"
                    elif distance_type == "inner_product":
                        distance_operator = "<#>"
                    
                    # Convert vector to string format
                    vector_str = f"'[{','.join(map(str, vector))}]'"
                    
                    # Execute vector search
                    query = f"""
                    SELECT *, ({column_name} {distance_operator} {vector_str}::vector) AS distance
                    FROM {schema}.{table_name}
                    ORDER BY {column_name} {distance_operator} {vector_str}::vector
                    LIMIT {limit}
                    """
                    
                    rows = await conn.fetch(query)
                    
                    # Convert to list of dictionaries
                    results = [dict(row) for row in rows]
                    
                    return {
                        "results": results,
                        "count": len(results),
                        "distance_type": distance_type,
                        "execution_time": time.time() - start_time
                    }
                finally:
                    # Release connection back to pool
                    await connection_manager.release_postgres_connection(connection_id, conn)
            
            else:
                error_msg = f"Unsupported operation: {operation}"
                self.logger.error(error_msg)
                return {"error": error_msg}
                
        except Exception as e:
            error_msg = f"Error executing PostgreSQL operation: {str(e)}"
            self.logger.error(error_msg)
            return {"error": error_msg}
    
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the PostgreSQL tool
        
        Returns:
            JSON Schema for tool input
        """
        return {
            "type": "object",
            "properties": {
                "operation": {
                    "type": "string",
                    "description": "Operation to perform",
                    "enum": [
                        "get_schemas",
                        "get_tables",
                        "get_columns",
                        "get_indexes",
                        "get_constraints",
                        "get_foreign_keys",
                        "get_table_structure",
                        "get_database_structure",
                        "get_extensions",
                        "check_extension",
                        "get_pgvector_info",
                        "explain_query",
                        "vector_search"
                    ]
                },
                "connection_id": {
                    "type": "string",
                    "description": "PostgreSQL connection ID"
                },
                "schema": {
                    "type": "string",
                    "description": "Schema name (default: 'public')"
                },
                "table_name": {
                    "type": "string",
                    "description": "Table name"
                },
                "column_name": {
                    "type": "string",
                    "description": "Column name"
                },
                "extension_name": {
                    "type": "string",
                    "description": "Extension name"
                },
                "query": {
                    "type": "string",
                    "description": "SQL query to explain"
                },
                "explain_type": {
                    "type": "string",
                    "description": "Type of EXPLAIN to perform",
                    "enum": [
                        "simple",
                        "analyze",
                        "verbose",
                        "analyze_verbose",
                        "analyze_verbose_buffers",
                        "json",
                        "analyze_json"
                    ]
                },
                "vector": {
                    "type": "array",
                    "description": "Vector for similarity search",
                    "items": {
                        "type": "number"
                    }
                },
                "distance_type": {
                    "type": "string",
                    "description": "Distance type for vector search",
                    "enum": [
                        "euclidean",
                        "cosine",
                        "inner_product"
                    ]
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of results to return",
                    "minimum": 1
                }
            },
            "required": ["operation", "connection_id"]
        }
    
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the PostgreSQL tool
        
        Returns:
            JSON Schema for tool output
        """
        return {
            "type": "object",
            "properties": {
                "schemas": {
                    "type": "array",
                    "description": "List of database schemas"
                },
                "tables": {
                    "type": "array",
                    "description": "List of tables in a schema"
                },
                "columns": {
                    "type": "array",
                    "description": "List of columns in a table"
                },
                "indexes": {
                    "type": "array",
                    "description": "List of indexes for a table"
                },
                "constraints": {
                    "type": "array",
                    "description": "List of constraints for a table"
                },
                "foreign_keys": {
                    "type": "array",
                    "description": "List of foreign keys for a table"
                },
                "table_structure": {
                    "type": "object",
                    "description": "Comprehensive structure of a table"
                },
                "database_structure": {
                    "type": "object",
                    "description": "Comprehensive structure of the database"
                },
                "extensions": {
                    "type": "array",
                    "description": "List of installed extensions"
                },
                "installed": {
                    "type": "boolean",
                    "description": "Whether an extension is installed"
                },
                "pgvector_info": {
                    "type": "object",
                    "description": "Information about pgvector extension"
                },
                "plan_text": {
                    "type": "string",
                    "description": "Query execution plan as text"
                },
                "plan": {
                    "type": "object",
                    "description": "Query execution plan as JSON"
                },
                "results": {
                    "type": "array",
                    "description": "Vector search results"
                },
                "count": {
                    "type": "integer",
                    "description": "Number of results returned"
                },
                "execution_time": {
                    "type": "number",
                    "description": "Time taken to execute the operation in seconds"
                },
                "error": {
                    "type": "string",
                    "description": "Error message if the operation failed"
                }
            }
        }
    
    def get_examples(self) -> List[Dict[str, Any]]:
        """
        Get examples of PostgreSQL tool usage
        
        Returns:
            List of example input/output pairs
        """
        return [
            {
                "input": {
                    "operation": "get_schemas",
                    "connection_id": "postgresql://user:password@localhost:5432/mydb"
                },
                "output": {
                    "schemas": [
                        {"schema_name": "public", "owner": "postgres", "description": "standard public schema"},
                        {"schema_name": "app", "owner": "app_user", "description": "application schema"}
                    ],
                    "execution_time": 0.05
                }
            },
            {
                "input": {
                    "operation": "get_tables",
                    "connection_id": "postgresql://user:password@localhost:5432/mydb",
                    "schema": "public"
                },
                "output": {
                    "tables": [
                        {
                            "table_name": "users",
                            "owner": "postgres",
                            "description": "User accounts",
                            "row_estimate": 1000,
                            "exact_row_count": 1042,
                            "total_size": "1024 kB",
                            "type": "table"
                        },
                        {
                            "table_name": "documents",
                            "owner": "postgres",
                            "description": "Document storage",
                            "row_estimate": 5000,
                            "exact_row_count": 4872,
                            "total_size": "8192 kB",
                            "type": "table"
                        }
                    ],
                    "schema": "public",
                    "execution_time": 0.08
                }
            },
            {
                "input": {
                    "operation": "explain_query",
                    "connection_id": "postgresql://user:password@localhost:5432/mydb",
                    "query": "SELECT * FROM users WHERE email LIKE '%example.com'",
                    "explain_type": "analyze"
                },
                "output": {
                    "query": "SELECT * FROM users WHERE email LIKE '%example.com'",
                    "plan_text": "Seq Scan on users  (cost=0.00..25.88 rows=6 width=90) (actual time=0.019..0.021 rows=3 loops=1)\n  Filter: ((email)::text ~~ '%example.com'::text)\n  Rows Removed by Filter: 7\nPlanning Time: 0.066 ms\nExecution Time: 0.048 ms",
                    "execution_time": 0.12
                }
            },
            {
                "input": {
                    "operation": "vector_search",
                    "connection_id": "postgresql://user:password@localhost:5432/mydb",
                    "table_name": "embeddings",
                    "column_name": "embedding",
                    "vector": [0.1, 0.2, 0.3, 0.4, 0.5],
                    "distance_type": "cosine",
                    "limit": 5
                },
                "output": {
                    "results": [
                        {"id": 1, "text": "Sample text 1", "distance": 0.15},
                        {"id": 2, "text": "Sample text 2", "distance": 0.25},
                        {"id": 3, "text": "Sample text 3", "distance": 0.35}
                    ],
                    "count": 3,
                    "distance_type": "cosine",
                    "execution_time": 0.18
                }
            }
        ]

================
File: app/rag/tools/rag_tool.py
================
"""
RAGTool - Tool for retrieving information using RAG
"""
import logging
import time
from typing import Any, Dict, List, Optional

from app.rag.tools.base import Tool
from app.rag.rag_engine import RAGEngine

class RAGTool(Tool):
    """
    Tool for retrieving information using RAG
    
    This tool uses the RAG engine to retrieve information from the document store
    based on a query.
    """
    
    def __init__(self, rag_engine: RAGEngine):
        """
        Initialize the RAG tool
        
        Args:
            rag_engine: RAG engine instance
        """
        super().__init__(
            name="rag",
            description="Retrieves information from documents using RAG"
        )
        self.rag_engine = rag_engine
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the RAG tool
        
        Args:
            input_data: Dictionary containing:
                - query: Query string
                - top_k: Number of results to return (optional)
                - filters: Filters to apply (optional)
                
        Returns:
            Dictionary containing:
                - chunks: List of retrieved chunks
                - sources: List of source documents
                - execution_time: Time taken to execute the query
        """
        start_time = time.time()
        self.logger.info(f"Executing RAG query: {input_data.get('query')}")
        
        # Extract parameters
        query = input_data.get("query")
        top_k = input_data.get("top_k", 5)
        filters = input_data.get("filters", {})
        
        # Validate input
        if not query:
            error_msg = "Query is required"
            self.logger.error(error_msg)
            return {"error": error_msg}
        
        try:
            # Execute RAG query
            results = await self.rag_engine.retrieve(
                query=query,
                top_k=top_k,
                filters=filters
            )
            
            # Process results
            chunks = []
            sources = set()
            
            for result in results:
                chunks.append({
                    "content": result.get("content", ""),
                    "metadata": result.get("metadata", {}),
                    "score": result.get("score", 0.0)
                })
                
                # Extract source document information
                doc_id = result.get("metadata", {}).get("document_id")
                if doc_id:
                    sources.add(doc_id)
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"RAG query completed in {elapsed_time:.2f}s. Found {len(chunks)} chunks from {len(sources)} sources")
            
            return {
                "chunks": chunks,
                "sources": list(sources),
                "execution_time": elapsed_time
            }
        except Exception as e:
            error_msg = f"Error executing RAG query: {str(e)}"
            self.logger.error(error_msg)
            return {"error": error_msg}
    
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the RAG tool
        
        Returns:
            JSON Schema for tool input
        """
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Query string"
                },
                "top_k": {
                    "type": "integer",
                    "description": "Number of results to return",
                    "default": 5,
                    "minimum": 1,
                    "maximum": 100
                },
                "filters": {
                    "type": "object",
                    "description": "Filters to apply to the search",
                    "additionalProperties": True
                }
            },
            "required": ["query"]
        }
    
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the RAG tool
        
        Returns:
            JSON Schema for tool output
        """
        return {
            "type": "object",
            "properties": {
                "chunks": {
                    "type": "array",
                    "description": "List of retrieved chunks",
                    "items": {
                        "type": "object",
                        "properties": {
                            "content": {
                                "type": "string",
                                "description": "Chunk content"
                            },
                            "metadata": {
                                "type": "object",
                                "description": "Chunk metadata"
                            },
                            "score": {
                                "type": "number",
                                "description": "Relevance score"
                            }
                        }
                    }
                },
                "sources": {
                    "type": "array",
                    "description": "List of source document IDs",
                    "items": {
                        "type": "string"
                    }
                },
                "execution_time": {
                    "type": "number",
                    "description": "Time taken to execute the query in seconds"
                },
                "error": {
                    "type": "string",
                    "description": "Error message if the query failed"
                }
            }
        }
    
    def get_examples(self) -> List[Dict[str, Any]]:
        """
        Get examples of RAG tool usage
        
        Returns:
            List of example input/output pairs
        """
        return [
            {
                "input": {
                    "query": "What is the capital of France?",
                    "top_k": 3
                },
                "output": {
                    "chunks": [
                        {
                            "content": "Paris is the capital and most populous city of France.",
                            "metadata": {
                                "document_id": "doc123",
                                "page": 1
                            },
                            "score": 0.92
                        },
                        {
                            "content": "France is a country in Western Europe. Its capital is Paris.",
                            "metadata": {
                                "document_id": "doc456",
                                "page": 5
                            },
                            "score": 0.85
                        }
                    ],
                    "sources": ["doc123", "doc456"],
                    "execution_time": 0.15
                }
            },
            {
                "input": {
                    "query": "Explain the process of photosynthesis",
                    "top_k": 5,
                    "filters": {
                        "document_type": "textbook",
                        "subject": "biology"
                    }
                },
                "output": {
                    "chunks": [
                        {
                            "content": "Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with carbon dioxide and water.",
                            "metadata": {
                                "document_id": "bio101",
                                "page": 42
                            },
                            "score": 0.95
                        }
                    ],
                    "sources": ["bio101"],
                    "execution_time": 0.22
                }
            }
        ]

================
File: app/rag/tools/registry.py
================
"""
ToolRegistry - Registry for managing tools in the Metis_RAG system
"""
import logging
from typing import Dict, List, Optional, Any

from app.rag.tools.base import Tool

class ToolRegistry:
    """
    Registry for managing tools
    
    The ToolRegistry maintains a collection of tools that can be used by the system.
    It provides methods for registering, retrieving, and listing tools.
    """
    
    def __init__(self):
        """
        Initialize the tool registry
        """
        self.tools: Dict[str, Tool] = {}
        self.logger = logging.getLogger("app.rag.tools.registry")
    
    def register_tool(self, tool: Tool) -> None:
        """
        Register a tool with the registry
        
        Args:
            tool: Tool to register
        """
        self.logger.info(f"Registering tool: {tool.name}")
        self.tools[tool.name] = tool
    
    def get_tool(self, name: str) -> Optional[Tool]:
        """
        Get a tool by name
        
        Args:
            name: Tool name
            
        Returns:
            Tool if found, None otherwise
        """
        tool = self.tools.get(name)
        if not tool:
            self.logger.warning(f"Tool not found: {name}")
        return tool
    
    def list_tools(self) -> List[Dict[str, Any]]:
        """
        List all registered tools
        
        Returns:
            List of tool information dictionaries
        """
        return [
            {
                "name": tool.name,
                "description": tool.get_description(),
                "input_schema": tool.get_input_schema(),
                "output_schema": tool.get_output_schema()
            }
            for tool in self.tools.values()
        ]
    
    def get_tool_examples(self, name: str) -> List[Dict[str, Any]]:
        """
        Get examples for a specific tool
        
        Args:
            name: Tool name
            
        Returns:
            List of example input/output pairs
        """
        tool = self.get_tool(name)
        if tool:
            return tool.get_examples()
        return []
    
    def get_tool_count(self) -> int:
        """
        Get the number of registered tools
        
        Returns:
            Number of tools
        """
        return len(self.tools)

================
File: app/rag/__init__.py
================
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.rag.vector_store import VectorStore
from app.rag.rag_engine import RAGEngine
from app.rag.document_analysis_service import DocumentAnalysisService
from app.rag.processing_job import ProcessingJob, WorkerPool, DocumentProcessingService
from app.rag.query_analyzer import QueryAnalyzer
from app.rag.process_logger import ProcessLogger
from app.rag.tools import Tool, ToolRegistry, RAGTool, DatabaseTool, PostgreSQLTool
from app.rag.tool_initializer import initialize_tools, get_tool_registry

================
File: app/rag/audit_report_generator.py
================
"""
AuditReportGenerator - Generates comprehensive audit reports for the RAG process
"""
import logging
import time
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

class AuditReportGenerator:
    """
    Generates comprehensive audit reports for the RAG process
    
    The AuditReportGenerator is responsible for creating detailed audit reports that
    track information sources, extract reasoning traces, and determine verification
    status for responses generated by the RAG system. These reports provide transparency
    and accountability for the system's outputs.
    """
    
    def __init__(
        self,
        process_logger,
        llm_provider = None
    ):
        """
        Initialize the audit report generator
        
        Args:
            process_logger: ProcessLogger instance
            llm_provider: LLM provider for additional analysis (optional)
        """
        self.process_logger = process_logger
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.rag.audit_report_generator")
    
    async def generate_report(
        self,
        query_id: str,
        include_llm_analysis: bool = True
    ) -> Dict[str, Any]:
        """
        Generate an audit report for a query
        
        Args:
            query_id: Unique query ID
            include_llm_analysis: Whether to include LLM-based analysis (optional)
            
        Returns:
            Comprehensive audit report
        """
        start_time = time.time()
        self.logger.info(f"Generating audit report for query {query_id}")
        
        # Get the process log
        process_log = self.process_logger.get_process_log(query_id)
        if not process_log:
            error_msg = f"No process log found for query {query_id}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Extract basic information
        query = process_log.get("query", "")
        timestamp = process_log.get("timestamp", datetime.now().isoformat())
        steps = process_log.get("steps", [])
        final_response = process_log.get("final_response", {})
        
        # Generate the report
        report = {
            "query_id": query_id,
            "query": query,
            "timestamp": timestamp,
            "process_summary": self._generate_process_summary(steps),
            "information_sources": self._extract_information_sources(steps),
            "reasoning_trace": self._extract_reasoning_trace(steps),
            "verification_status": self._determine_verification_status(steps, final_response),
            "execution_timeline": self._create_execution_timeline(steps),
            "response_quality": self._extract_response_quality(steps)
        }
        
        # Add LLM analysis if requested and available
        if include_llm_analysis and self.llm_provider:
            llm_analysis = await self._generate_llm_analysis(query, steps, final_response)
            report["llm_analysis"] = llm_analysis
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Audit report generation completed in {elapsed_time:.2f}s")
        
        # Log the audit report generation
        self.process_logger.log_step(
            query_id=query_id,
            step_name="audit_report_generation",
            step_data={
                "report_summary": {
                    "verification_status": report["verification_status"],
                    "source_count": len(report["information_sources"]),
                    "execution_time": elapsed_time
                }
            }
        )
        
        return report
    
    def _generate_process_summary(self, steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Generate a summary of the process
        
        Args:
            steps: List of process steps
            
        Returns:
            Process summary
        """
        # Count steps by type
        step_types = {}
        for step in steps:
            step_name = step.get("step_name", "unknown")
            step_types[step_name] = step_types.get(step_name, 0) + 1
        
        # Calculate total execution time
        total_time = 0
        for step in steps:
            step_data = step.get("data", {})
            if "execution_time" in step_data:
                total_time += step_data["execution_time"]
        
        # Identify key stages
        stages = []
        stage_mapping = {
            "query_analysis": "Query Analysis",
            "plan_query": "Query Planning",
            "execute_plan": "Plan Execution",
            "retrieve_chunks": "Information Retrieval",
            "refine_query": "Query Refinement",
            "optimize_context": "Context Optimization",
            "response_synthesis": "Response Synthesis",
            "response_evaluation": "Response Evaluation",
            "response_refinement": "Response Refinement"
        }
        
        for step in steps:
            step_name = step.get("step_name", "")
            for key, stage in stage_mapping.items():
                if key in step_name and stage not in stages:
                    stages.append(stage)
        
        return {
            "total_steps": len(steps),
            "step_types": step_types,
            "total_execution_time": total_time,
            "stages_executed": stages
        }
    
    def _extract_information_sources(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract information sources from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            List of information sources
        """
        sources = []
        source_ids = set()  # Track source IDs to avoid duplicates
        
        # Look for retrieval steps
        for step in steps:
            step_name = step.get("step_name", "")
            step_data = step.get("data", {})
            
            # Check for retrieval steps
            if "retrieve_chunks" in step_name or "retrieval" in step_name:
                if "chunks" in step_data:
                    for chunk in step_data["chunks"]:
                        if "metadata" in chunk:
                            metadata = chunk["metadata"]
                            source_id = metadata.get("document_id", "") + ":" + metadata.get("chunk_id", "")
                            
                            if source_id not in source_ids:
                                source_ids.add(source_id)
                                sources.append({
                                    "document_id": metadata.get("document_id", ""),
                                    "chunk_id": metadata.get("chunk_id", ""),
                                    "filename": metadata.get("filename", "Unknown"),
                                    "tags": metadata.get("tags", []),
                                    "folder": metadata.get("folder", "/"),
                                    "relevance_score": chunk.get("relevance_score", chunk.get("distance", 0)),
                                    "content_preview": chunk.get("content", "")[:200] + "..." if len(chunk.get("content", "")) > 200 else chunk.get("content", "")
                                })
            
            # Check for response synthesis steps that include sources
            if "response_synthesis" in step_name:
                if "sources" in step_data:
                    for source in step_data["sources"]:
                        source_id = source.get("document_id", "") + ":" + source.get("chunk_id", "")
                        
                        if source_id not in source_ids:
                            source_ids.add(source_id)
                            sources.append(source)
        
        # Sort sources by relevance score (descending)
        sources.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        return sources
    
    def _extract_reasoning_trace(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract reasoning trace from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            Reasoning trace
        """
        reasoning_trace = []
        
        # Define the key steps to include in the reasoning trace
        key_steps = [
            "query_analysis",
            "plan_query",
            "execute_plan",
            "retrieve_chunks",
            "refine_query",
            "optimize_context",
            "response_synthesis",
            "response_evaluation",
            "response_refinement"
        ]
        
        for step in steps:
            step_name = step.get("step_name", "")
            step_data = step.get("data", {})
            step_time = step.get("timestamp", "")
            
            # Check if this is a key step
            include_step = False
            for key_step in key_steps:
                if key_step in step_name:
                    include_step = True
                    break
            
            if include_step:
                # Extract the relevant information for the reasoning trace
                trace_entry = {
                    "step": step_name,
                    "timestamp": step_time,
                    "reasoning": {}
                }
                
                # Extract reasoning information based on step type
                if "query_analysis" in step_name:
                    if "analysis" in step_data:
                        trace_entry["reasoning"] = {
                            "complexity": step_data["analysis"].get("complexity", ""),
                            "justification": step_data["analysis"].get("justification", ""),
                            "requires_tools": step_data["analysis"].get("requires_tools", []),
                            "sub_queries": step_data["analysis"].get("sub_queries", [])
                        }
                elif "plan_query" in step_name:
                    if "plan" in step_data:
                        trace_entry["reasoning"] = {
                            "steps": step_data["plan"].get("steps", []),
                            "reasoning": step_data["plan"].get("reasoning", "")
                        }
                elif "execute_plan" in step_name:
                    if "results" in step_data:
                        trace_entry["reasoning"] = {
                            "execution_results": step_data["results"]
                        }
                elif "retrieve_chunks" in step_name:
                    trace_entry["reasoning"] = {
                        "chunks_retrieved": len(step_data.get("chunks", [])),
                        "relevance_threshold": step_data.get("relevance_threshold", "")
                    }
                elif "refine_query" in step_name:
                    trace_entry["reasoning"] = {
                        "original_query": step_data.get("original_query", ""),
                        "refined_query": step_data.get("refined_query", ""),
                        "refinement_reason": step_data.get("refinement_reason", "")
                    }
                elif "optimize_context" in step_name:
                    trace_entry["reasoning"] = {
                        "optimization_strategy": step_data.get("optimization_strategy", ""),
                        "chunks_before": step_data.get("chunks_before", 0),
                        "chunks_after": step_data.get("chunks_after", 0)
                    }
                elif "response_synthesis" in step_name:
                    trace_entry["reasoning"] = {
                        "sources_used": len(step_data.get("sources", [])),
                        "context_length": step_data.get("context_length", 0)
                    }
                elif "response_evaluation" in step_name:
                    if "evaluation" in step_data:
                        trace_entry["reasoning"] = {
                            "factual_accuracy": step_data["evaluation"].get("factual_accuracy", 0),
                            "completeness": step_data["evaluation"].get("completeness", 0),
                            "relevance": step_data["evaluation"].get("relevance", 0),
                            "hallucination_detected": step_data["evaluation"].get("hallucination_detected", False),
                            "overall_score": step_data["evaluation"].get("overall_score", 0)
                        }
                elif "response_refinement" in step_name:
                    trace_entry["reasoning"] = {
                        "improvement_summary": step_data.get("improvement_summary", ""),
                        "iteration": step_data.get("iteration", 1)
                    }
                
                reasoning_trace.append(trace_entry)
        
        return reasoning_trace
    
    def _determine_verification_status(
        self,
        steps: List[Dict[str, Any]],
        final_response: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Determine the verification status of the response
        
        Args:
            steps: List of process steps
            final_response: Final response data
            
        Returns:
            Verification status
        """
        # Initialize verification status
        verification_status = {
            "status": "unverified",
            "confidence": 0,
            "hallucination_detected": False,
            "factual_accuracy_score": 0,
            "verification_method": "none",
            "verification_details": ""
        }
        
        # Look for evaluation steps
        evaluation_steps = [step for step in steps if "response_evaluation" in step.get("step_name", "")]
        
        if evaluation_steps:
            # Use the most recent evaluation
            latest_evaluation = evaluation_steps[-1]
            evaluation_data = latest_evaluation.get("data", {}).get("evaluation", {})
            
            if evaluation_data:
                # Extract verification information
                factual_accuracy = evaluation_data.get("factual_accuracy", 0)
                hallucination_detected = evaluation_data.get("hallucination_detected", False)
                overall_score = evaluation_data.get("overall_score", 0)
                
                # Determine verification status
                if hallucination_detected:
                    status = "contains_hallucinations"
                    confidence = max(0, min(factual_accuracy / 10, 1))
                elif factual_accuracy >= 8:
                    status = "verified"
                    confidence = factual_accuracy / 10
                elif factual_accuracy >= 5:
                    status = "partially_verified"
                    confidence = factual_accuracy / 10
                else:
                    status = "unverified"
                    confidence = factual_accuracy / 10
                
                verification_status = {
                    "status": status,
                    "confidence": confidence,
                    "hallucination_detected": hallucination_detected,
                    "factual_accuracy_score": factual_accuracy,
                    "verification_method": "llm_evaluation",
                    "verification_details": evaluation_data.get("hallucination_details", "")
                }
        
        return verification_status
    
    def _create_execution_timeline(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Create an execution timeline from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            Execution timeline
        """
        timeline = []
        
        for step in steps:
            step_name = step.get("step_name", "")
            step_time = step.get("timestamp", "")
            step_data = step.get("data", {})
            
            # Extract execution time if available
            execution_time = step_data.get("execution_time", 0)
            
            # Create a timeline entry
            timeline_entry = {
                "step": step_name,
                "timestamp": step_time,
                "execution_time": execution_time
            }
            
            timeline.append(timeline_entry)
        
        return timeline
    
    def _extract_response_quality(self, steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Extract response quality metrics from the process steps
        
        Args:
            steps: List of process steps
            
        Returns:
            Response quality metrics
        """
        # Initialize quality metrics
        quality = {
            "factual_accuracy": 0,
            "completeness": 0,
            "relevance": 0,
            "overall_score": 0,
            "strengths": [],
            "weaknesses": [],
            "improvement_suggestions": []
        }
        
        # Look for evaluation steps
        evaluation_steps = [step for step in steps if "response_evaluation" in step.get("step_name", "")]
        
        if evaluation_steps:
            # Use the most recent evaluation
            latest_evaluation = evaluation_steps[-1]
            evaluation_data = latest_evaluation.get("data", {}).get("evaluation", {})
            
            if evaluation_data:
                # Extract quality metrics
                quality = {
                    "factual_accuracy": evaluation_data.get("factual_accuracy", 0),
                    "completeness": evaluation_data.get("completeness", 0),
                    "relevance": evaluation_data.get("relevance", 0),
                    "overall_score": evaluation_data.get("overall_score", 0),
                    "strengths": evaluation_data.get("strengths", []),
                    "weaknesses": evaluation_data.get("weaknesses", []),
                    "improvement_suggestions": evaluation_data.get("improvement_suggestions", [])
                }
        
        return quality
    
    async def _generate_llm_analysis(
        self,
        query: str,
        steps: List[Dict[str, Any]],
        final_response: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate LLM-based analysis of the process
        
        Args:
            query: Original query
            steps: List of process steps
            final_response: Final response data
            
        Returns:
            LLM analysis
        """
        if not self.llm_provider:
            return {"error": "LLM provider not available"}
        
        try:
            # Create a prompt for the LLM
            prompt = self._create_llm_analysis_prompt(query, steps, final_response)
            
            # Generate the analysis
            response = await self.llm_provider.generate(
                prompt=prompt,
                system_prompt=self._create_llm_analysis_system_prompt()
            )
            
            # Parse the analysis
            analysis_text = response.get("response", "")
            analysis = self._parse_llm_analysis(analysis_text)
            
            return analysis
        except Exception as e:
            self.logger.error(f"Error generating LLM analysis: {str(e)}")
            return {"error": f"Error generating LLM analysis: {str(e)}"}
    
    def _create_llm_analysis_prompt(
        self,
        query: str,
        steps: List[Dict[str, Any]],
        final_response: Dict[str, Any]
    ) -> str:
        """
        Create a prompt for LLM analysis
        
        Args:
            query: Original query
            steps: List of process steps
            final_response: Final response data
            
        Returns:
            LLM analysis prompt
        """
        prompt = f"""
You are analyzing the execution of a RAG (Retrieval-Augmented Generation) process for the following query:

QUERY: {query}

FINAL RESPONSE:
{final_response.get("text", "")}

PROCESS STEPS:
{json.dumps(steps, indent=2)}

Please analyze the RAG process execution and provide insights on:

1. Process Efficiency:
   - Were there any unnecessary steps or redundancies?
   - Could the process have been more efficient?
   - Were appropriate tools and techniques used?

2. Information Retrieval Quality:
   - Was the retrieval effective in finding relevant information?
   - Were there any issues with the retrieval process?
   - How could retrieval be improved?

3. Response Quality:
   - Is the response accurate, complete, and relevant?
   - Are there any potential hallucinations or factual errors?
   - How well does the response address the query?

4. Overall Assessment:
   - What are the strengths of this RAG process execution?
   - What are the weaknesses or areas for improvement?
   - What specific recommendations would you make to improve the process?

FORMAT YOUR ANALYSIS AS FOLLOWS:
```json
{
  "process_efficiency": {
    "assessment": "Your assessment of process efficiency",
    "issues_identified": ["Issue 1", "Issue 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  },
  "retrieval_quality": {
    "assessment": "Your assessment of retrieval quality",
    "issues_identified": ["Issue 1", "Issue 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  },
  "response_quality": {
    "assessment": "Your assessment of response quality",
    "issues_identified": ["Issue 1", "Issue 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  },
  "overall_assessment": {
    "strengths": ["Strength 1", "Strength 2", ...],
    "weaknesses": ["Weakness 1", "Weakness 2", ...],
    "recommendations": ["Recommendation 1", "Recommendation 2", ...]
  }
}
```

IMPORTANT: Your analysis must be objective, insightful, and based solely on the provided information. The analysis must be returned in the exact JSON format specified above.
"""
        
        return prompt
    
    def _create_llm_analysis_system_prompt(self) -> str:
        """
        Create a system prompt for LLM analysis
        
        Returns:
            System prompt
        """
        return """You are an expert RAG (Retrieval-Augmented Generation) system analyst.

Your role is to analyze RAG process executions and provide insightful feedback on efficiency, retrieval quality, response quality, and overall performance.

GUIDELINES:
1. Be objective and data-driven in your analysis.
2. Identify specific issues and provide actionable recommendations.
3. Consider the entire process flow from query analysis to response generation.
4. Evaluate the effectiveness of retrieval in finding relevant information.
5. Assess the quality of the final response in terms of accuracy, completeness, and relevance.
6. Identify potential hallucinations or factual errors in the response.
7. Provide specific recommendations for improving the process.
8. Format your analysis in the exact JSON format specified in the prompt.
9. Be thorough and detailed in your analysis.
10. Focus on practical improvements that could be implemented.
"""
    
    def _parse_llm_analysis(self, analysis_text: str) -> Dict[str, Any]:
        """
        Parse the LLM analysis response
        
        Args:
            analysis_text: Raw analysis text from the LLM
            
        Returns:
            Parsed analysis
        """
        # Extract JSON from the response
        try:
            # Look for JSON block in markdown format
            import re
            json_match = re.search(r'```(?:json)?\s*({\s*".*})\s*```', analysis_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without markdown formatting
                json_match = re.search(r'({[\s\S]*"overall_assessment"[\s\S]*})', analysis_text)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Fallback: assume the entire text might be JSON
                    json_str = analysis_text
            
            # Parse the JSON
            analysis = json.loads(json_str)
            
            # Ensure all required fields are present
            required_sections = ["process_efficiency", "retrieval_quality", "response_quality", "overall_assessment"]
            
            for section in required_sections:
                if section not in analysis:
                    analysis[section] = {
                        "assessment": "Not provided",
                        "issues_identified": [],
                        "recommendations": []
                    }
            
            return analysis
        except Exception as e:
            self.logger.error(f"Error parsing LLM analysis: {str(e)}")
            
            # Return a default analysis
            return {
                "error": f"Failed to parse LLM analysis: {str(e)}",
                "process_efficiency": {
                    "assessment": "Not available",
                    "issues_identified": [],
                    "recommendations": []
                },
                "retrieval_quality": {
                    "assessment": "Not available",
                    "issues_identified": [],
                    "recommendations": []
                },
                "response_quality": {
                    "assessment": "Not available",
                    "issues_identified": [],
                    "recommendations": []
                },
                "overall_assessment": {
                    "strengths": [],
                    "weaknesses": [],
                    "recommendations": []
                }
            }

================
File: app/rag/document_analysis_service.py
================
import os
import time
import logging
import random
from typing import List, Dict, Any, Optional
from uuid import UUID

from app.models.document import Document
from app.core.config import UPLOAD_DIR, OLLAMA_BASE_URL, DEFAULT_MODEL

class DocumentAnalysisService:
    """
    Service for analyzing documents and determining optimal processing strategies
    """
    def __init__(self, llm_provider=None, sample_size=3):
        self.llm_provider = llm_provider
        self.sample_size = sample_size
        self.logger = logging.getLogger("app.rag.document_analysis_service")
        
    async def analyze_document(self, document: Document) -> Dict[str, Any]:
        """
        Analyze a document and recommend a processing strategy
        
        Args:
            document: Document to analyze
            
        Returns:
            Dict with processing strategy and parameters
        """
        start_time = time.time()
        self.logger.info(f"Starting analysis of document: {document.filename}")
        
        # Extract sample content from the document
        sample_content = await self._extract_sample_content(document)
        
        # Use LLM to analyze sample and recommend strategy
        strategy = await self._recommend_strategy(document, sample_content)
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Analysis completed in {elapsed_time:.2f}s. Strategy: {strategy['strategy']}")
        
        return strategy
    
    async def analyze_document_batch(self, documents: List[Document]) -> Dict[str, Any]:
        """
        Analyze a batch of documents and recommend a processing strategy
        
        Args:
            documents: List of documents to analyze
            
        Returns:
            Dict with processing strategy and parameters
        """
        start_time = time.time()
        self.logger.info(f"Starting analysis of {len(documents)} documents")
        
        # Sample documents from the batch
        samples = self._sample_documents(documents)
        
        # Extract representative content from samples
        sample_contents = []
        for doc in samples:
            content = await self._extract_sample_content(doc)
            sample_contents.append({
                "filename": doc.filename,
                "content": content,
                "file_type": doc.metadata.get("file_type", "unknown")
            })
        
        # Use LLM to analyze samples and recommend strategy
        strategy = await self._recommend_batch_strategy(sample_contents)
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Batch analysis completed in {elapsed_time:.2f}s. Strategy: {strategy['strategy']}")
        
        return strategy
    
    async def _extract_sample_content(self, document: Document) -> str:
        """
        Extract a representative sample of content from a document
        
        Args:
            document: Document to extract content from
            
        Returns:
            String containing sample content
        """
        try:
            file_path = os.path.join(UPLOAD_DIR, str(document.id), document.filename)
            
            # Check if file exists
            if not os.path.exists(file_path):
                self.logger.warning(f"File not found: {file_path}")
                return ""
            
            # Get file size
            file_size = os.path.getsize(file_path)
            
            # For small files, read the entire content
            if file_size < 10000:  # Less than 10KB
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
            
            # For larger files, extract samples from beginning, middle, and end
            samples = []
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                # Beginning (first 1000 chars)
                f.seek(0)
                samples.append(f.read(1000))
                
                # Middle (1000 chars from the middle)
                f.seek(max(0, file_size // 2 - 500))
                samples.append(f.read(1000))
                
                # End (last 1000 chars)
                f.seek(max(0, file_size - 1000))
                samples.append(f.read(1000))
            
            return "\n...\n".join(samples)
        except Exception as e:
            self.logger.error(f"Error extracting sample content from {document.filename}: {str(e)}")
            return ""
    
    def _sample_documents(self, documents: List[Document]) -> List[Document]:
        """
        Sample a subset of documents from a batch
        
        Args:
            documents: List of documents to sample from
            
        Returns:
            List of sampled documents
        """
        if len(documents) <= self.sample_size:
            return documents
        
        # Ensure we get a diverse sample by file type
        file_types = {}
        for doc in documents:
            file_type = doc.metadata.get("file_type", "unknown")
            if file_type not in file_types:
                file_types[file_type] = []
            file_types[file_type].append(doc)
        
        samples = []
        # Take at least one document of each file type
        for file_type, docs in file_types.items():
            samples.append(random.choice(docs))
        
        # If we need more samples, add random documents
        remaining = self.sample_size - len(samples)
        if remaining > 0:
            remaining_docs = [doc for doc in documents if doc not in samples]
            samples.extend(random.sample(remaining_docs, min(remaining, len(remaining_docs))))
        
        # If we have too many samples, trim to sample_size
        if len(samples) > self.sample_size:
            samples = samples[:self.sample_size]
        
        return samples
    
    async def _recommend_strategy(self, document: Document, sample_content: str) -> Dict[str, Any]:
        """
        Recommend a processing strategy for a document based on its content
        
        Args:
            document: Document to analyze
            sample_content: Sample content from the document
            
        Returns:
            Dict with recommended strategy and parameters
        """
        # Get file type from metadata or filename
        file_type = document.metadata.get("file_type", "")
        if not file_type and document.filename:
            _, ext = os.path.splitext(document.filename.lower())
            file_type = ext[1:] if ext else "unknown"
        
        # If we have an LLM provider, use it to analyze the document
        if self.llm_provider:
            prompt = self._create_analysis_prompt(document.filename, file_type, sample_content)
            response = await self.llm_provider.generate(prompt=prompt)
            try:
                return self._parse_analysis_response(response.get("response", ""))
            except Exception as e:
                self.logger.error(f"Error parsing LLM response: {str(e)}")
                # Fall back to rule-based strategy
        
        # Rule-based strategy if LLM is not available or fails
        return self._rule_based_strategy(document, file_type)
    
    async def _recommend_batch_strategy(self, sample_contents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Recommend a processing strategy for a batch of documents
        
        Args:
            sample_contents: List of sample contents from documents
            
        Returns:
            Dict with recommended strategy and parameters
        """
        # If we have an LLM provider, use it to analyze the batch
        if self.llm_provider:
            prompt = self._create_batch_analysis_prompt(sample_contents)
            response = await self.llm_provider.generate(prompt=prompt)
            try:
                return self._parse_analysis_response(response.get("response", ""))
            except Exception as e:
                self.logger.error(f"Error parsing LLM response for batch analysis: {str(e)}")
                # Fall back to rule-based strategy
        
        # Rule-based strategy if LLM is not available or fails
        # Default to a general-purpose strategy for mixed document types
        return {
            "strategy": "recursive",
            "parameters": {
                "chunk_size": 1500,
                "chunk_overlap": 150
            },
            "justification": "Default strategy for mixed document types"
        }
    
    def _rule_based_strategy(self, document: Document, file_type: str) -> Dict[str, Any]:
        """
        Determine processing strategy based on file type and simple rules
        
        Args:
            document: Document to analyze
            file_type: File type
            
        Returns:
            Dict with strategy and parameters
        """
        if file_type == "pdf":
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 1500,
                    "chunk_overlap": 150
                },
                "justification": "PDF documents benefit from recursive chunking with standard parameters"
            }
        elif file_type == "md":
            return {
                "strategy": "markdown",
                "parameters": {
                    "chunk_size": 1500,
                    "chunk_overlap": 150
                },
                "justification": "Markdown documents benefit from header-based chunking"
            }
        elif file_type == "csv":
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 2000,
                    "chunk_overlap": 100
                },
                "justification": "CSV files benefit from larger chunks with less overlap"
            }
        elif file_type == "txt":
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 2000,
                    "chunk_overlap": 200
                },
                "justification": "Text files benefit from larger chunks with more overlap"
            }
        else:
            # Default strategy
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 1500,
                    "chunk_overlap": 150
                },
                "justification": "Default strategy for unknown file type"
            }
    
    def _create_analysis_prompt(self, filename: str, file_type: str, sample_content: str) -> str:
        """
        Create a prompt for document analysis
        
        Args:
            filename: Document filename
            file_type: Document file type
            sample_content: Sample content from the document
            
        Returns:
            Prompt string
        """
        return f"""
You are an expert document analyst tasked with determining the optimal chunking strategy for a document.

Document Information:
- Filename: {filename}
- File Type: {file_type}

Sample Content:
```
{sample_content[:3000]}  # Limit sample size to avoid token limits
```

Based on the document information and sample content, analyze the document structure and content type, then recommend the best chunking strategy.

Available chunking strategies:
1. recursive - Uses RecursiveCharacterTextSplitter with specified separators
2. token - Uses TokenTextSplitter (better for code or technical content)
3. markdown - Uses MarkdownHeaderTextSplitter (best for markdown with headers)
4. semantic - Uses SemanticChunker (best for complex documents with varying content)

Your analysis should consider:
- Document structure (headers, paragraphs, lists, tables)
- Content type (narrative, technical, code, data)
- Special formatting requirements
- Optimal chunk size and overlap for this document type

Provide your response in the following JSON format:
{{
  "strategy": "strategy_name",
  "parameters": {{
    "chunk_size": size_in_characters,
    "chunk_overlap": overlap_in_characters
  }},
  "justification": "Detailed explanation of your recommendation"
}}
"""
    
    def _create_batch_analysis_prompt(self, sample_contents: List[Dict[str, Any]]) -> str:
        """
        Create a prompt for batch document analysis
        
        Args:
            sample_contents: List of sample contents from documents
            
        Returns:
            Prompt string
        """
        samples_text = ""
        for i, sample in enumerate(sample_contents):
            samples_text += f"\nDocument {i+1}:\n"
            samples_text += f"- Filename: {sample['filename']}\n"
            samples_text += f"- File Type: {sample['file_type']}\n"
            samples_text += f"- Sample Content:\n```\n{sample['content'][:1000]}...\n```\n"
        
        return f"""
You are an expert document analyst tasked with determining the optimal chunking strategy for a batch of documents.

Document Samples:
{samples_text}

Based on these document samples, analyze the document structures and content types, then recommend the best chunking strategy that would work well for the entire batch.

Available chunking strategies:
1. recursive - Uses RecursiveCharacterTextSplitter with specified separators
2. token - Uses TokenTextSplitter (better for code or technical content)
3. markdown - Uses MarkdownHeaderTextSplitter (best for markdown with headers)
4. semantic - Uses SemanticChunker (best for complex documents with varying content)

Your analysis should consider:
- Common document structures across the batch
- Predominant content types
- Special formatting requirements
- Optimal chunk size and overlap that would work for most documents in the batch

Provide your response in the following JSON format:
{{
  "strategy": "strategy_name",
  "parameters": {{
    "chunk_size": size_in_characters,
    "chunk_overlap": overlap_in_characters
  }},
  "justification": "Detailed explanation of your recommendation"
}}
"""
    
    def _parse_analysis_response(self, response: str) -> Dict[str, Any]:
        """
        Parse the LLM response to extract the recommended strategy
        
        Args:
            response: LLM response string
            
        Returns:
            Dict with strategy and parameters
        """
        # Simple parsing for JSON response
        # In a real implementation, this would be more robust
        import json
        import re
        
        # Try to extract JSON from the response
        json_match = re.search(r'({[\s\S]*})', response)
        if json_match:
            try:
                result = json.loads(json_match.group(1))
                # Validate required fields
                if "strategy" in result and "parameters" in result:
                    return result
            except json.JSONDecodeError:
                pass
        
        # If JSON parsing fails, try to extract key information
        strategy_match = re.search(r'strategy["\']?\s*:\s*["\']?(\w+)["\']?', response)
        chunk_size_match = re.search(r'chunk_size["\']?\s*:\s*(\d+)', response)
        chunk_overlap_match = re.search(r'chunk_overlap["\']?\s*:\s*(\d+)', response)
        
        if strategy_match:
            strategy = strategy_match.group(1)
            chunk_size = int(chunk_size_match.group(1)) if chunk_size_match else 1500
            chunk_overlap = int(chunk_overlap_match.group(1)) if chunk_overlap_match else 150
            
            return {
                "strategy": strategy,
                "parameters": {
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap
                },
                "justification": "Extracted from LLM response"
            }
        
        # If all parsing fails, return default strategy
        return {
            "strategy": "recursive",
            "parameters": {
                "chunk_size": 1500,
                "chunk_overlap": 150
            },
            "justification": "Default strategy due to parsing failure"
        }

================
File: app/rag/document_processor.py
================
import os
import logging
import json
from typing import List, Dict, Any, Optional, Literal
from uuid import UUID
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter,
    TokenTextSplitter
)
from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader, UnstructuredMarkdownLoader
from langchain.schema.document import Document as LangchainDocument

from app.core.config import UPLOAD_DIR, CHUNK_SIZE, CHUNK_OVERLAP, USE_CHUNKING_JUDGE
from app.models.document import Document, Chunk
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.chunkers.semantic_chunker import SemanticChunker
from app.rag.document_analysis_service import DocumentAnalysisService

logger = logging.getLogger("app.rag.document_processor")

class DocumentProcessor:
    """
    Process documents for RAG with support for multiple chunking strategies
    """
    def __init__(
        self,
        upload_dir: str = UPLOAD_DIR,
        chunk_size: int = CHUNK_SIZE,
        chunk_overlap: int = CHUNK_OVERLAP,
        chunking_strategy: str = "recursive",
        llm_provider = None,
        user_id: Optional[UUID] = None
    ):
        self.upload_dir = upload_dir
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunking_strategy = chunking_strategy
        self.loader_map = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.csv': CSVLoader,
            '.md': UnstructuredMarkdownLoader,
        }
        self.llm_provider = llm_provider
        self.document_analysis_service = DocumentAnalysisService(llm_provider=self.llm_provider)
        self.text_splitter = self._get_text_splitter()
        self.user_id = user_id  # Store the user ID for permission metadata
    
    def _get_text_splitter(self, file_ext=None):
        """Get the appropriate text splitter based on chunking strategy and file type"""
        logger.info(f"Using chunking strategy: {self.chunking_strategy} for file type: {file_ext}")
        
        # If we have a chunking analysis in document metadata, log it
        if hasattr(self, 'document') and self.document and hasattr(self.document, 'doc_metadata') and 'chunking_analysis' in self.document.doc_metadata:
            logger.info(f"Chunking analysis: {self.document.doc_metadata['chunking_analysis']['justification']}")
        
        # Text file handling - use paragraph-based splitting for more natural chunks
        if file_ext == ".txt":
            # Use a larger chunk size for text files to preserve more context
            larger_chunk_size = self.chunk_size * 3  # Increase from 500 to 1500
            logger.info(f"Using paragraph-based splitting for text file with increased chunk size {larger_chunk_size}")
            return RecursiveCharacterTextSplitter(
                chunk_size=larger_chunk_size,
                chunk_overlap=self.chunk_overlap * 2,  # Increase overlap as well
                separators=["\n\n", "\n", ".", " ", ""]
            )
        
        # PDF-specific handling
        if file_ext == ".pdf":
            logger.info(f"Using PDF-specific splitting with chunk size {self.chunk_size}")
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=["\n\n", "\n", ".", " ", ""]
            )
        
        # Markdown-specific handling
        if file_ext == ".md" and self.chunking_strategy == "markdown":
            logger.info("Using header-based splitting for markdown")
            # First split by headers
            header_splitter = MarkdownHeaderTextSplitter(
                headers_to_split_on=[
                    ("#", "header1"),
                    ("##", "header2"),
                    ("###", "header3"),
                    ("####", "header4"),
                ]
            )
            # Then apply recursive splitting to each section
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
        
        # CSV-specific handling
        if file_ext == ".csv":
            logger.info(f"Using larger chunks for CSV with chunk size {self.chunk_size}")
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size * 2,  # Double chunk size for CSVs
                chunk_overlap=self.chunk_overlap
            )
        
        # Standard strategies
        if self.chunking_strategy == "recursive":
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            )
        elif self.chunking_strategy == "token":
            return TokenTextSplitter(
                chunk_size=self.chunk_size // 4,  # Adjust for tokens vs characters
                chunk_overlap=self.chunk_overlap // 4
            )
        elif self.chunking_strategy == "markdown":
            return MarkdownHeaderTextSplitter(
                headers_to_split_on=[
                    ("#", "header1"),
                    ("##", "header2"),
                    ("###", "header3"),
                    ("####", "header4"),
                ]
            )
        elif self.chunking_strategy == "semantic":
            logger.info(f"Using semantic chunking with chunk size {self.chunk_size}")
            return SemanticChunker(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
        else:
            logger.warning(f"Unknown chunking strategy: {self.chunking_strategy}, falling back to recursive")
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=["\n\n", "\n", ".", " ", ""]
            )
    
    async def process_document(self, document) -> Document:
            """
            Process a document by splitting it into chunks
            
            Args:
                document: Document to process (Pydantic or SQLAlchemy model)
                
            Returns:
                Processed document (Pydantic model)
            """
            from app.db.adapters import (
                is_sqlalchemy_model,
                sqlalchemy_document_to_pydantic,
                pydantic_document_to_sqlalchemy
            )
            
            try:
                # Convert to Pydantic model if needed for processing
                is_sqlalchemy = is_sqlalchemy_model(document)
                if is_sqlalchemy:
                    logger.info(f"Converting SQLAlchemy document to Pydantic for processing: {document.filename}")
                    pydantic_document = sqlalchemy_document_to_pydantic(document)
                else:
                    pydantic_document = document
                    
                logger.info(f"Processing document: {pydantic_document.filename}")
                
                # Convert document ID to string if it's a UUID
                document_id_str = str(pydantic_document.id)
                
                # Get the document path
                file_path = os.path.join(self.upload_dir, document_id_str, pydantic_document.filename)
                
                # Get file extension for specialized handling
                _, ext = os.path.splitext(file_path.lower())
                
                # Use Chunking Judge if enabled
                if USE_CHUNKING_JUDGE:
                    logger.info(f"Using Chunking Judge to analyze document: {pydantic_document.filename}")
                    chunking_judge = ChunkingJudge()
                    analysis_result = await chunking_judge.analyze_document(pydantic_document)
                    
                    # Update chunking strategy and parameters
                    self.chunking_strategy = analysis_result["strategy"]
                    if "parameters" in analysis_result and "chunk_size" in analysis_result["parameters"]:
                        self.chunk_size = analysis_result["parameters"]["chunk_size"]
                    if "parameters" in analysis_result and "chunk_overlap" in analysis_result["parameters"]:
                        self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
                    
                    # Store the chunking analysis in document metadata
                    pydantic_document.metadata["chunking_analysis"] = analysis_result
                    
                    logger.info(f"Chunking Judge recommendation: strategy={self.chunking_strategy}, " +
                               f"chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
                else:
                    # Use DocumentAnalysisService if Chunking Judge is disabled
                    logger.info(f"Chunking Judge disabled, using DocumentAnalysisService for document: {pydantic_document.filename}")
                    analysis_result = await self.document_analysis_service.analyze_document(pydantic_document)
                    
                    # Update chunking strategy and parameters
                    self.chunking_strategy = analysis_result["strategy"]
                    if "parameters" in analysis_result and "chunk_size" in analysis_result["parameters"]:
                        self.chunk_size = analysis_result["parameters"]["chunk_size"]
                    if "parameters" in analysis_result and "chunk_overlap" in analysis_result["parameters"]:
                        self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
                    
                    # Store the document analysis in document metadata
                    pydantic_document.metadata["document_analysis"] = analysis_result
                    
                    logger.info(f"Document analysis recommendation: strategy={self.chunking_strategy}, " +
                               f"chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
                
                # Get appropriate text splitter for this file type
                self.text_splitter = self._get_text_splitter(ext)
                
                # Extract text from the document based on file type
                docs = await self._load_document(file_path)
                
                # Split the document into chunks
                chunks = self._split_document(docs)
                
                # Update the document with chunks
                pydantic_document.chunks = []
                for i, chunk in enumerate(chunks):
                    # Start with the chunk's existing metadata
                    metadata = dict(chunk.metadata) if chunk.metadata else {}
                    
                    # Add document metadata
                    metadata.update({
                        "document_id": document_id_str,  # Use string ID
                        "index": i,
                        "folder": pydantic_document.folder
                    })
                    
                    # Add user context and permission information
                    if hasattr(document, 'user_id') and document.user_id:
                        metadata["user_id"] = str(document.user_id)
                    elif self.user_id:
                        metadata["user_id"] = str(self.user_id)
                    
                    # Add is_public flag for permission filtering
                    if hasattr(document, 'is_public'):
                        metadata["is_public"] = document.is_public
                    
                    # Add document permissions information
                    if hasattr(document, 'shared_with') and document.shared_with:
                        # Store shared user IDs and their permission levels
                        shared_users = {}
                        for permission in document.shared_with:
                            shared_users[str(permission.user_id)] = permission.permission_level
                        
                        # Store as JSON string for ChromaDB compatibility
                        metadata["shared_with"] = json.dumps(shared_users)
                        
                        # Also store a list of user IDs with access for easier filtering
                        metadata["shared_user_ids"] = ",".join(shared_users.keys())
                    
                    # Handle tags specially - store as string for ChromaDB compatibility
                    if hasattr(pydantic_document, 'tags') and pydantic_document.tags:
                        metadata["tags_list"] = pydantic_document.tags  # Keep original list for internal use
                        metadata["tags"] = ",".join(pydantic_document.tags)  # String version for ChromaDB
                    else:
                        metadata["tags"] = ""
                        metadata["tags_list"] = []
                    
                    # Create the chunk with processed metadata
                    pydantic_document.chunks.append(
                        Chunk(
                            content=chunk.page_content,
                            metadata=metadata
                        )
                    )
                
                logger.info(f"Document processed into {len(pydantic_document.chunks)} chunks")
                
                return pydantic_document
            except Exception as e:
                logger.error(f"Error processing document {pydantic_document.filename}: {str(e)}")
                raise
    
    async def _load_document(self, file_path: str) -> List[LangchainDocument]:
        """
        Load a document based on its file type with improved error handling
        """
        _, ext = os.path.splitext(file_path.lower())
        
        try:
            if ext == ".pdf":
                try:
                    loader = PyPDFLoader(file_path)
                    return loader.load()
                except Exception as pdf_error:
                    logger.warning(f"Error using PyPDFLoader for {file_path}: {str(pdf_error)}. Falling back to manual loading.")
                    # Try loading as text, but ignore decoding errors
                    try:
                        with open(file_path, 'rb') as f:
                            content = f.read().decode('utf-8', errors='ignore')
                        logger.info(f"Successfully loaded {file_path} using text fallback.")
                        return [LangchainDocument(page_content=content, metadata={"source": file_path})]
                    except Exception as fallback_error:
                        logger.error(f"Failed to load {file_path} even with fallback: {str(fallback_error)}")
                        raise  # Re-raise after logging
            elif ext == ".csv":
                loader = CSVLoader(file_path)
                return loader.load()
            elif ext == ".md":
                try:
                    loader = UnstructuredMarkdownLoader(file_path)
                    return loader.load()
                except Exception as md_error:
                    logger.warning(f"Error using UnstructuredMarkdownLoader for {file_path}: {str(md_error)}. Falling back to manual loading.")
                    # Try loading as text, but ignore decoding errors
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        logger.info(f"Successfully loaded {file_path} using text fallback.")
                        return [LangchainDocument(page_content=content, metadata={"source": file_path})]
                    except Exception as fallback_error:
                        logger.error(f"Failed to load {file_path} even with fallback: {str(fallback_error)}")
                        raise  # Re-raise after logging
            else:
                # Default to text loader for txt and other files
                loader = TextLoader(file_path)
                return loader.load()
        except Exception as e:
            logger.warning(f"Attempt to load {file_path} with appropriate loader failed: {str(e)}. Attempting fallback.")
            # Create a simple document with file content to avoid complete failure
            try:
                with open(file_path, 'rb') as f:
                    content = f.read().decode('utf-8', errors='ignore')
                logger.info(f"Successfully loaded {file_path} using text fallback.")
                return [LangchainDocument(page_content=content, metadata={"source": file_path})]
            except Exception as fallback_error:
                logger.error(f"Failed to load {file_path} even with fallback: {str(fallback_error)}")
                raise  # Re-raise after logging
    
    def _split_document(self, docs: List[LangchainDocument]) -> List[LangchainDocument]:
        """
        Split a document into chunks with a limit on total chunks
        Ensures security metadata is preserved during chunking
        """
        try:
            # Split the document using the configured text splitter
            chunks = self.text_splitter.split_documents(docs)
            
            # Log the original number of chunks
            logger.info(f"Document initially split into {len(chunks)} chunks")
            
            # Preserve security metadata across all chunks
            # This ensures that all chunks inherit the security properties of the parent document
            for chunk in chunks:
                # Make sure each chunk has the security metadata from the original document
                for doc in docs:
                    # Copy security-related metadata from the original document
                    if 'user_id' in doc.metadata:
                        chunk.metadata['user_id'] = doc.metadata['user_id']
                    
                    if 'is_public' in doc.metadata:
                        chunk.metadata['is_public'] = doc.metadata['is_public']
                    
                    # Handle shared permissions
                    if 'shared_with' in doc.metadata:
                        chunk.metadata['shared_with'] = doc.metadata['shared_with']
                    
                    if 'shared_user_ids' in doc.metadata:
                        chunk.metadata['shared_user_ids'] = doc.metadata['shared_user_ids']
                    
                    # Handle section-specific permissions if they exist
                    # This allows for different permissions within the same document
                    if 'section_permissions' in doc.metadata:
                        # Check if this chunk belongs to a section with specific permissions
                        section_permissions = doc.metadata['section_permissions']
                        
                        # Determine which section this chunk belongs to based on content
                        # This is a simplified approach - in a real implementation, you might
                        # use more sophisticated methods to match chunks to sections
                        for section, permissions in section_permissions.items():
                            if section in chunk.page_content:
                                # Override the document-level permissions with section-specific ones
                                if 'is_public' in permissions:
                                    chunk.metadata['is_public'] = permissions['is_public']
                                
                                if 'shared_with' in permissions:
                                    chunk.metadata['shared_with'] = permissions['shared_with']
                                
                                # Log that we're applying section-specific permissions
                                logger.info(f"Applied section-specific permissions for section '{section}'")
                                break
            
            # Limit the maximum number of chunks per document to prevent excessive chunking
            MAX_CHUNKS = 30  # Reduced from 50 to 30 to prevent excessive chunking
            
            if len(chunks) > MAX_CHUNKS:
                logger.warning(f"Document produced {len(chunks)} chunks, limiting to {MAX_CHUNKS}")
                
                # Option 2: Take evenly distributed chunks to maintain coverage of the document
                step = len(chunks) // MAX_CHUNKS
                limited_chunks = [chunks[i] for i in range(0, len(chunks), step)][:MAX_CHUNKS]
                
                # Ensure we have exactly MAX_CHUNKS or fewer
                return limited_chunks[:MAX_CHUNKS]
            
            return chunks
        except Exception as e:
            logger.error(f"Error splitting document: {str(e)}")
            raise
    
    def extract_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        Extract metadata from a document
        """
        try:
            _, ext = os.path.splitext(file_path.lower())
            file_stats = os.stat(file_path)
            
            metadata = {
                "file_size": file_stats.st_size,
                "file_type": ext[1:] if ext else "unknown",
                "created_at": file_stats.st_ctime,
                "modified_at": file_stats.st_mtime
            }
            
            # Add user context if available
            if self.user_id:
                metadata["user_id"] = str(self.user_id)
            
            # Add file type-specific metadata extraction here
            if ext == ".pdf":
                try:
                    # Extract PDF-specific metadata if possible
                    import pypdf
                    with open(file_path, 'rb') as f:
                        pdf = pypdf.PdfReader(f)
                        if pdf.metadata:
                            for key, value in pdf.metadata.items():
                                if key and value:
                                    # Clean up the key name
                                    clean_key = key.strip('/').lower()
                                    metadata[clean_key] = str(value)
                except Exception as e:
                    logger.warning(f"Error extracting PDF metadata: {str(e)}")
            
            return metadata
        except Exception as e:
            logger.error(f"Error extracting metadata from {file_path}: {str(e)}")
            return {}

================
File: app/rag/langgraph_states.py
================
"""
LangGraph State Models - Defines the state models for the LangGraph RAG system
"""
from typing import Dict, Any, List, Optional, TypedDict, Annotated, Sequence, cast, Tuple
from enum import Enum

class QueryAnalysisState(TypedDict):
    """State for query analysis"""
    query: str
    conversation_context: Optional[str]
    complexity: Optional[str]
    parameters: Optional[Dict[str, Any]]
    justification: Optional[str]
    requires_tools: List[str]
    sub_queries: List[str]

class PlanningState(TypedDict):
    """State for query planning"""
    query: str
    query_id: str
    analysis: Dict[str, Any]
    plan: Optional[Dict[str, Any]]
    steps: List[Dict[str, Any]]
    current_step: int
    completed: bool

class ExecutionState(TypedDict):
    """State for plan execution"""
    query: str
    query_id: str
    plan: Dict[str, Any]
    results: Dict[str, Any]
    execution_trace: List[Dict[str, Any]]
    completed: bool
    error: Optional[str]

class RetrievalState(TypedDict):
    """State for retrieval"""
    query: str
    refined_query: Optional[str]
    conversation_context: Optional[str]
    parameters: Dict[str, Any]
    chunks: List[Dict[str, Any]]
    needs_refinement: bool
    relevance_scores: Optional[Dict[str, float]]

class GenerationState(TypedDict):
    """State for generation"""
    query: str
    conversation_context: Optional[str]
    context: str
    sources: List[Dict[str, Any]]
    document_ids: List[str]
    answer: Optional[str]
    stream_response: Optional[Any]

class ResponseEvaluationState(TypedDict):
    """State for response evaluation"""
    query: str
    query_id: str
    response: str
    context: str
    sources: List[Dict[str, Any]]
    evaluation: Optional[Dict[str, Any]]
    factual_accuracy: Optional[float]
    completeness: Optional[float]
    relevance: Optional[float]
    hallucination_detected: Optional[bool]
    overall_score: Optional[float]
    needs_refinement: bool

class ResponseRefinementState(TypedDict):
    """State for response refinement"""
    query: str
    query_id: str
    original_response: str
    evaluation: Dict[str, Any]
    context: str
    sources: List[Dict[str, Any]]
    refined_response: Optional[str]
    improvement_summary: Optional[str]
    iteration: int
    max_iterations: int

class AuditReportState(TypedDict):
    """State for audit report generation"""
    query_id: str
    query: str
    process_summary: Optional[Dict[str, Any]]
    information_sources: Optional[List[Dict[str, Any]]]
    reasoning_trace: Optional[List[Dict[str, Any]]]
    verification_status: Optional[Dict[str, Any]]
    execution_timeline: Optional[List[Dict[str, Any]]]
    response_quality: Optional[Dict[str, Any]]
    llm_analysis: Optional[Dict[str, Any]]
    report_generated: bool

class RAGState(TypedDict):
    """Combined state for the RAG process"""
    query: str
    query_id: str
    conversation_context: Optional[str]
    metadata_filters: Optional[Dict[str, Any]]
    model: str
    system_prompt: Optional[str]
    stream: bool
    model_parameters: Optional[Dict[str, Any]]
    query_analysis: Optional[QueryAnalysisState]
    planning: Optional[PlanningState]
    execution: Optional[ExecutionState]
    retrieval: Optional[RetrievalState]
    generation: Optional[GenerationState]
    evaluation: Optional[ResponseEvaluationState]
    refinement: Optional[ResponseRefinementState]
    audit_report: Optional[AuditReportState]
    final_response: Optional[Dict[str, Any]]

class RAGStage(str, Enum):
    """Stages in the RAG process"""
    QUERY_ANALYSIS = "analyze_query_node"
    QUERY_PLANNING = "plan_query_node"
    PLAN_EXECUTION = "execute_plan_node"
    RETRIEVAL = "retrieve_chunks_node"
    QUERY_REFINEMENT = "refine_query_node"
    CONTEXT_OPTIMIZATION = "optimize_context_node"
    GENERATION = "generate_response_node"
    RESPONSE_EVALUATION = "evaluate_response_node"
    RESPONSE_REFINEMENT = "refine_response_node"
    AUDIT_REPORT = "generate_audit_report_node"
    COMPLETE = "finalize_response_node"

================
File: app/rag/mem0_client.py
================
"""
Mem0 client for Metis_RAG
"""
import os
import logging
from typing import Optional, Dict, Any, List, Union

# Import Mem0Client only if USE_MEM0 is True
from app.core.config import MEM0_ENDPOINT, MEM0_API_KEY, USE_MEM0

# Define a dummy Mem0Client class to use when mem0 is not available
class DummyMem0Client:
    def __init__(self, *args, **kwargs):
        pass
    
    def get_agent(self, *args, **kwargs):
        return None
    
    def create_agent(self, *args, **kwargs):
        return None
    
    def get_human(self, *args, **kwargs):
        return None
    
    def create_human(self, *args, **kwargs):
        return None
    
    def append_message(self, *args, **kwargs):
        return None
    
    def get_recall_memory(self, *args, **kwargs):
        return []
    
    def create_archival_memory(self, *args, **kwargs):
        return None
    
    def get_archival_memory(self, *args, **kwargs):
        return []
    
    def search_archival_memory(self, *args, **kwargs):
        return []

# Try to import Mem0Client, fall back to dummy if not available
try:
    if USE_MEM0:
        from mem0.client import Mem0Client
    else:
        Mem0Client = DummyMem0Client
except ImportError:
    logger = logging.getLogger("app.rag.mem0_client")
    logger.warning("mem0 module not found, using dummy implementation")
    Mem0Client = DummyMem0Client

logger = logging.getLogger("app.rag.mem0_client")

# Default agent ID for Metis RAG
METIS_AGENT_ID = "metis_rag_agent"

# Default persona for Metis RAG agent
METIS_PERSONA = """
You are Metis RAG, a helpful assistant that answers questions based on provided documents.
You provide accurate, concise, and helpful responses based on the information in your knowledge base.
When you don't know the answer, you clearly state that you don't have enough information.
"""

# Singleton instance of Mem0Client
_mem0_client: Optional[Mem0Client] = None

def get_mem0_client() -> Optional[Mem0Client]:
    """
    Get the Mem0Client instance
    
    Returns:
        Mem0Client instance or None if not configured
    """
    global _mem0_client
    
    if _mem0_client is None:
        try:
            # Check if Mem0 is enabled
            if not USE_MEM0:
                logger.info("Mem0 integration is disabled in configuration")
                return None
                
            # Initialize the client (API key is optional for local development)
            if MEM0_API_KEY:
                _mem0_client = Mem0Client(api_key=MEM0_API_KEY, endpoint=MEM0_ENDPOINT)
                logger.info(f"Initialized Mem0 client with API key and endpoint: {MEM0_ENDPOINT}")
            else:
                _mem0_client = Mem0Client(endpoint=MEM0_ENDPOINT)
                logger.info(f"Initialized Mem0 client without API key at endpoint: {MEM0_ENDPOINT}")
            
            # Ensure the Metis RAG agent exists
            if not _mem0_client.get_agent(METIS_AGENT_ID):
                _mem0_client.create_agent(
                    agent_id=METIS_AGENT_ID,
                    name="Metis RAG Agent",
                    persona=METIS_PERSONA
                )
                logger.info(f"Created Metis RAG agent with ID: {METIS_AGENT_ID}")
            
            logger.info(f"Initialized Mem0 client with endpoint: {endpoint}")
        except Exception as e:
            logger.error(f"Error initializing Mem0 client: {str(e)}")
            return None
    
    return _mem0_client

def get_or_create_human(human_id: str, name: Optional[str] = None) -> bool:
    """
    Get or create a human in Mem0
    
    Args:
        human_id: Human ID (typically user ID or session ID)
        name: Human name (optional)
        
    Returns:
        True if successful, False otherwise
    """
    client = get_mem0_client()
    if not client:
        return False
    
    try:
        # Check if human exists
        if not client.get_human(human_id):
            # Create human
            client.create_human(
                human_id=human_id,
                name=name or f"User {human_id}"
            )
            logger.info(f"Created human with ID: {human_id}")
        
        return True
    except Exception as e:
        logger.error(f"Error getting or creating human: {str(e)}")
        return False

def store_message(human_id: str, role: str, content: str) -> bool:
    """
    Store a message in recall memory
    
    Args:
        human_id: Human ID
        role: Message role (user, assistant, system)
        content: Message content
        
    Returns:
        True if successful, False otherwise
    """
    client = get_mem0_client()
    if not client:
        return False
    
    try:
        # Ensure human exists
        if not get_or_create_human(human_id):
            return False
        
        # Append message to recall memory
        client.append_message(
            agent_id=METIS_AGENT_ID,
            human_id=human_id,
            message={"role": role, "content": content}
        )
        
        logger.debug(f"Stored {role} message for human {human_id}")
        return True
    except Exception as e:
        logger.error(f"Error storing message: {str(e)}")
        return False

def get_conversation_history(human_id: str, limit: int = 10) -> List[Dict[str, str]]:
    """
    Get conversation history from recall memory
    
    Args:
        human_id: Human ID
        limit: Maximum number of messages to retrieve
        
    Returns:
        List of messages
    """
    client = get_mem0_client()
    if not client:
        return []
    
    try:
        # Get recall memory
        history = client.get_recall_memory(
            agent_id=METIS_AGENT_ID,
            human_id=human_id,
            limit=limit
        )
        
        return history
    except Exception as e:
        logger.error(f"Error getting conversation history: {str(e)}")
        return []

def store_user_preferences(human_id: str, preferences: Dict[str, Any]) -> bool:
    """
    Store user preferences in archival memory
    
    Args:
        human_id: Human ID
        preferences: User preferences
        
    Returns:
        True if successful, False otherwise
    """
    client = get_mem0_client()
    if not client:
        return False
    
    try:
        # Ensure human exists
        if not get_or_create_human(human_id):
            return False
        
        # Store preferences in archival memory
        client.create_archival_memory(
            agent_id=METIS_AGENT_ID,
            human_id=human_id,
            data=preferences,
            kind="user_preferences",
            replace=True  # Replace existing preferences
        )
        
        logger.info(f"Stored preferences for human {human_id}")
        return True
    except Exception as e:
        logger.error(f"Error storing user preferences: {str(e)}")
        return False

def get_user_preferences(human_id: str) -> Dict[str, Any]:
    """
    Get user preferences from archival memory
    
    Args:
        human_id: Human ID
        
    Returns:
        User preferences
    """
    client = get_mem0_client()
    if not client:
        return {}
    
    try:
        # Get preferences from archival memory
        preferences = client.get_archival_memory(
            agent_id=METIS_AGENT_ID,
            human_id=human_id,
            kind="user_preferences",
            limit=1  # Only get the most recent preferences
        )
        
        if preferences:
            return preferences[0]["data"]
        return {}
    except Exception as e:
        logger.error(f"Error getting user preferences: {str(e)}")
        return {}

def store_document_interaction(
    human_id: str,
    document_id: str,
    interaction_type: str,
    data: Dict[str, Any]
) -> bool:
    """
    Store document interaction in archival memory
    
    Args:
        human_id: Human ID
        document_id: Document ID
        interaction_type: Interaction type (view, search, cite, etc.)
        data: Interaction data
        
    Returns:
        True if successful, False otherwise
    """
    client = get_mem0_client()
    if not client:
        return False
    
    try:
        # Ensure human exists
        if not get_or_create_human(human_id):
            return False
        
        # Prepare interaction data
        interaction = {
            "document_id": document_id,
            "interaction_type": interaction_type,
            **data
        }
        
        # Store interaction in archival memory
        client.create_archival_memory(
            agent_id=METIS_AGENT_ID,
            human_id=human_id,
            data=interaction,
            kind="document_interaction"
        )
        
        logger.debug(f"Stored {interaction_type} interaction for document {document_id} by human {human_id}")
        return True
    except Exception as e:
        logger.error(f"Error storing document interaction: {str(e)}")
        return False

def get_document_interactions(
    human_id: str,
    document_id: Optional[str] = None,
    interaction_type: Optional[str] = None,
    limit: int = 10
) -> List[Dict[str, Any]]:
    """
    Get document interactions from archival memory
    
    Args:
        human_id: Human ID
        document_id: Optional document ID to filter by
        interaction_type: Optional interaction type to filter by
        limit: Maximum number of interactions to retrieve
        
    Returns:
        List of document interactions
    """
    client = get_mem0_client()
    if not client:
        return []
    
    try:
        # Build query
        query = ""
        if document_id:
            query += f"document_id:{document_id} "
        if interaction_type:
            query += f"interaction_type:{interaction_type} "
        
        # Get interactions from archival memory
        interactions = client.search_archival_memory(
            agent_id=METIS_AGENT_ID,
            human_id=human_id,
            query=query.strip() if query else None,
            kind="document_interaction",
            limit=limit
        )
        
        return [interaction["data"] for interaction in interactions]
    except Exception as e:
        logger.error(f"Error getting document interactions: {str(e)}")
        return []

================
File: app/rag/memory_buffer.py
================
"""
Memory buffer functionality for Metis RAG
"""
import logging
import re
import time
from typing import Dict, Any, List, Optional
from uuid import UUID
from datetime import datetime

from sqlalchemy import select, desc
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.session import get_session
from app.models.memory import Memory
from app.db.models import Conversation

logger = logging.getLogger("app.rag.memory_buffer")

async def add_to_memory_buffer(
    conversation_id: UUID,
    content: str,
    label: str = "explicit_memory",
    db: AsyncSession = None
) -> Memory:
    """
    Add content to the memory buffer
    
    Args:
        conversation_id: Conversation ID
        content: Content to store
        label: Memory label
        db: Database session
        
    Returns:
        Created memory object
    """
    try:
        # Get database session if not provided
        if db is None:
            db = await anext(get_db())
        
        # Create memory object
        memory = Memory(
            conversation_id=conversation_id,
            content=content,
            label=label,
            created_at=datetime.now()
        )
        
        # Add to database
        db.add(memory)
        await db.commit()
        await db.refresh(memory)
        
        logger.info(f"Added memory to buffer: {content[:50]}...")
        
        return memory
    except Exception as e:
        logger.error(f"Error adding to memory buffer: {str(e)}")
        if db:
            await db.rollback()
        raise

async def get_memory_buffer(
    conversation_id: UUID,
    search_term: Optional[str] = None,
    label: Optional[str] = None,
    limit: int = 10,
    db: AsyncSession = None
) -> List[Memory]:
    """
    Get memories from the buffer
    
    Args:
        conversation_id: Conversation ID
        search_term: Optional search term
        label: Optional memory label
        limit: Maximum number of memories to return
        db: Database session
        
    Returns:
        List of memory objects
    """
    try:
        # Get database session if not provided
        if db is None:
            db = await anext(get_session())
        
        # Build query
        query = select(Memory).where(Memory.conversation_id == conversation_id)
        
        # Add label filter if provided
        if label:
            query = query.where(Memory.label == label)
        
        # Order by creation time (newest first)
        query = query.order_by(desc(Memory.created_at))
        
        # Execute query
        result = await db.execute(query)
        memories = result.scalars().all()
        
        # Filter by search term if provided
        if search_term and memories:
            filtered_memories = []
            for memory in memories:
                if search_term.lower() in memory.content.lower():
                    filtered_memories.append(memory)
            memories = filtered_memories
        
        # Limit results
        memories = memories[:limit]
        
        logger.info(f"Retrieved {len(memories)} memories from buffer")
        
        return memories
    except Exception as e:
        logger.error(f"Error getting memory buffer: {str(e)}")
        return []

async def process_query(
    query: str,
    user_id: str,
    conversation_id: UUID,
    db: AsyncSession = None
) -> tuple[str, Optional[str], Optional[str]]:
    """
    Process a query for memory commands before sending to RAG
    
    Args:
        query: User query
        user_id: User ID
        conversation_id: Conversation ID
        db: Database session
        
    Returns:
        Tuple of (processed_query, memory_response, memory_operation)
    """
    # Get database session if not provided
    if db is None:
        db = await anext(get_session())
    
    # Check for memory commands
    memory_match = re.search(r"remember\s+this(?:\s+(?:phrase|name|information))?\s*:\s*(.+)", query, re.IGNORECASE)
    if memory_match:
        content = memory_match.group(1).strip()
        
        # Store in memory buffer
        await add_to_memory_buffer(
            conversation_id=conversation_id,
            content=content,
            label="explicit_memory",
            db=db
        )
        
        # Create confirmation response
        memory_response = f"I've stored this in my memory: '{content}'"
        
        # Remove the command from the query
        processed_query = query.replace(memory_match.group(0), "").strip()
        if not processed_query:
            processed_query = "Thank you for providing that information."
        
        return processed_query, memory_response, "store"
    
    # Check for recall command
    recall_match = re.search(r"(?:recall|remember)(?:\s+(?:the|my))?\s*(?:(.+))?", query, re.IGNORECASE)
    if recall_match and not memory_match:  # Avoid conflict with "remember this" command
        search_term = recall_match.group(1).strip() if recall_match.group(1) else None
        
        # Retrieve from memory buffer
        memories = await get_memory_buffer(
            conversation_id=conversation_id,
            search_term=search_term,
            db=db
        )
        
        if memories:
            memory_items = [f"{i+1}. {memory.content}" for i, memory in enumerate(memories)]
            memory_response = "Here's what I remember:\n" + "\n".join(memory_items)
        else:
            memory_response = "I don't have any memories stored about that."
        
        # Remove the command from the query
        processed_query = query.replace(recall_match.group(0), "").strip()
        if not processed_query:
            processed_query = "Please provide the information you'd like me to recall."
        
        return processed_query, memory_response, "recall"
    
    # No memory command found
    return query, None, None

async def get_conversation_context(
    conversation_history: Optional[List[Any]] = None,
    max_tokens: int = 4000
) -> str:
    """
    Get the full conversation context up to the specified token limit
    
    Args:
        conversation_history: List of conversation messages
        max_tokens: Maximum number of tokens to include
        
    Returns:
        Formatted conversation context string
    """
    if not conversation_history:
        return ""
    
    # Calculate tokens for each message
    message_tokens = []
    for msg in conversation_history:
        # Estimate token count if not already calculated
        token_count = getattr(msg, 'token_count', None) or len(msg.content.split())
        message_tokens.append({
            "role": msg.role,
            "content": msg.content,
            "tokens": token_count
        })
    
    # Apply smart context window management
    formatted_messages = []
    total_tokens = 0
    
    # First, include messages with memory operations
    memory_messages = [m for m in message_tokens if contains_memory_operation(m["content"])]
    for msg in memory_messages:
        if total_tokens + msg["tokens"] <= max_tokens:
            formatted_messages.append(msg)
            total_tokens += msg["tokens"]
    
    # Then include the most recent messages
    recent_messages = [m for m in reversed(message_tokens) if m not in formatted_messages]
    for msg in recent_messages:
        if total_tokens + msg["tokens"] <= max_tokens:
            formatted_messages.insert(0, msg)  # Insert at beginning to maintain order
            total_tokens += msg["tokens"]
        else:
            break
    
    # Sort messages by original order
    formatted_messages.sort(key=lambda m: message_tokens.index(m))
    
    # Format the conversation history
    history_pieces = []
    for msg in formatted_messages:
        role_prefix = "User" if msg["role"] == "user" else "Assistant"
        history_pieces.append(f"{role_prefix}: {msg['content']}")
    
    return "\n".join(history_pieces)

def contains_memory_operation(content: str) -> bool:
    """
    Check if a message contains a memory operation
    
    Args:
        content: Message content
        
    Returns:
        True if the message contains a memory operation
    """
    memory_patterns = [
        r"remember\s+this",
        r"(?:recall|remember)(?:\s+(?:the|my))?"
    ]
    
    for pattern in memory_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            return True
    
    return False

================
File: app/rag/ollama_client.py
================
import httpx
import json
import logging
import time
import asyncio
from typing import Dict, List, Any, Optional, Generator, Tuple, Union
from sse_starlette.sse import EventSourceResponse

from app.core.config import OLLAMA_BASE_URL, DEFAULT_MODEL

logger = logging.getLogger("app.rag.ollama_client")

class OllamaClient:
    """
    Client for interacting with Ollama API
    """
    def __init__(self, base_url: str = OLLAMA_BASE_URL, timeout: int = 30):
        self.base_url = base_url
        self.timeout = timeout
        self.client = httpx.AsyncClient(timeout=timeout)
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """
        List available models
        """
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = await self.client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                return response.json().get("models", [])
            except Exception as e:
                logger.error(f"Error listing models (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise
    
    async def generate(
        self,
        prompt: str,
        model: str = DEFAULT_MODEL,
        system_prompt: Optional[str] = None,
        stream: bool = True,
        parameters: Dict[str, Any] = None
    ) -> Union[Dict[str, Any], Generator[str, None, None]]:
        """
        Generate a response from the model
        """
        if parameters is None:
            parameters = {}
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            **parameters
        }
        
        if system_prompt:
            payload["system"] = system_prompt
            
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                if stream:
                    return await self._stream_response(payload)
                else:
                    response = await self.client.post(
                        f"{self.base_url}/api/generate",
                        json=payload
                    )
                    response.raise_for_status()
                    response_data = response.json()
                    
                    # Check if the response contains an error message from the model
                    if 'error' in response_data:
                        logger.warning(f"Model returned an error: {response_data['error']}")
                        # Return the error message instead of raising an exception
                        return {
                            "response": f"I'm unable to answer that question. {response_data['error']}",
                            "error": response_data['error']
                        }
                    
                    return response_data
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP error generating response (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    # Return a user-friendly error message
                    return {
                        "response": "I'm unable to answer that question right now. There was an issue connecting to the language model.",
                        "error": str(e)
                    }
            except Exception as e:
                logger.error(f"Error generating response (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    # Return a user-friendly error message instead of raising
                    return {
                        "response": "I'm unable to process your request right now. There might be an issue with the language model or your question.",
                        "error": str(e)
                    }
    
    async def _stream_response(self, payload: Dict[str, Any]):
        """
        Stream response from the model with improved error handling and longer timeouts
        """
        # Increase timeout for streaming responses
        STREAM_TIMEOUT = 300  # 5 minutes
        
        async def event_generator():
            try:
                # Use a longer timeout for streaming responses
                async with httpx.AsyncClient(timeout=STREAM_TIMEOUT) as client:
                    try:
                        # Set longer read timeout and connection timeout
                        async with client.stream(
                            "POST",
                            f"{self.base_url}/api/generate",
                            json=payload,
                            timeout=httpx.Timeout(connect=30, read=STREAM_TIMEOUT, write=30, pool=30)
                        ) as response:
                            response.raise_for_status()
                            
                            # Process the stream with better error handling
                            async for line in response.aiter_lines():
                                if line:
                                    try:
                                        data = json.loads(line)
                                        
                                        # Check if the response contains an error message
                                        if 'error' in data:
                                            error_msg = data['error']
                                            logger.warning(f"Model returned an error in stream: {error_msg}")
                                            yield f"I'm unable to answer that question. {error_msg}"
                                            break
                                        
                                        # Extract and yield the response token directly
                                        token = data.get("response", "")
                                        if token:
                                            yield token
                                            
                                        # Check if we're done
                                        if data.get("done", False):
                                            logger.info("Stream completed successfully")
                                            break
                                    except json.JSONDecodeError:
                                        logger.error(f"Error decoding JSON: {line}")
                    except httpx.ReadTimeout:
                        logger.error("Read timeout while streaming response")
                        yield "\n\nThe response was taking too long to generate. Please try again with a simpler query or disable streaming."
                    except httpx.ConnectTimeout:
                        logger.error("Connection timeout while streaming response")
                        yield "\n\nCouldn't connect to the language model server. Please check if Ollama is running."
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP error in streaming response: {str(e)}")
                yield "\n\nI'm unable to answer that question right now. There was an issue connecting to the language model."
            except Exception as e:
                logger.error(f"Error in streaming response: {str(e)}", exc_info=True)
                yield "\n\nI'm unable to process your request right now. There might be an issue with the language model or your question."
        
        return event_generator()  # Return the generator directly
    
    async def create_embedding(
        self, 
        text: str, 
        model: str = DEFAULT_MODEL
    ) -> List[float]:
        """
        Create an embedding for the given text
        """
        payload = {
            "model": model,
            "prompt": text
        }
        
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = await self.client.post(
                    f"{self.base_url}/api/embeddings",
                    json=payload
                )
                response.raise_for_status()
                return response.json().get("embedding", [])
            except Exception as e:
                logger.error(f"Error creating embedding (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise

================
File: app/rag/plan_executor.py
================
"""
PlanExecutor - Executes query plans
"""
import logging
import time
import json
from typing import Dict, List, Any, Optional, Tuple

from app.rag.query_planner import QueryPlan
from app.rag.tools import ToolRegistry
from app.rag.process_logger import ProcessLogger

class PlanExecutor:
    """
    Executes query plans
    
    The PlanExecutor is responsible for executing the plans created by the QueryPlanner.
    It executes each step in the plan, records the results, and handles any errors that
    may occur during execution.
    """
    
    def __init__(
        self, 
        tool_registry: ToolRegistry,
        process_logger: Optional[ProcessLogger] = None,
        llm_provider = None
    ):
        """
        Initialize the plan executor
        
        Args:
            tool_registry: ToolRegistry instance
            process_logger: ProcessLogger instance (optional)
            llm_provider: LLM provider for generating responses (optional)
        """
        self.tool_registry = tool_registry
        self.process_logger = process_logger
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.rag.plan_executor")
    
    async def execute_plan(self, plan: QueryPlan) -> Dict[str, Any]:
        """
        Execute a query plan
        
        Args:
            plan: QueryPlan instance
            
        Returns:
            Dictionary containing:
                - query_id: Query ID
                - response: Final response
                - steps: List of executed steps
                - execution_time: Total execution time
        """
        start_time = time.time()
        self.logger.info(f"Executing plan for query: {plan.query}")
        
        # Log the start of plan execution
        if self.process_logger:
            self.process_logger.log_step(
                query_id=plan.query_id,
                step_name="plan_execution_start",
                step_data=plan.to_dict()
            )
        
        # Execute each step in the plan
        while not plan.is_completed():
            step = plan.get_next_step()
            if not step:
                break
            
            step_result = await self._execute_step(plan.query_id, step)
            plan = self._update_plan(plan, step_result)
        
        # Generate the final response
        response = await self._generate_response(plan)
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Plan execution completed in {elapsed_time:.2f}s")
        
        # Log the completion of plan execution
        if self.process_logger:
            self.process_logger.log_step(
                query_id=plan.query_id,
                step_name="plan_execution_complete",
                step_data={
                    "execution_time": elapsed_time,
                    "response": response
                }
            )
            
            # Log the final response
            self.process_logger.log_final_response(
                query_id=plan.query_id,
                response=response,
                metadata={
                    "execution_time": elapsed_time,
                    "steps_executed": plan.current_step
                }
            )
        
        return {
            "query_id": plan.query_id,
            "response": response,
            "steps": plan.results,
            "execution_time": elapsed_time
        }
    
    async def _execute_step(self, query_id: str, step: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute a single step in the plan
        
        Args:
            query_id: Query ID
            step: Step to execute
            
        Returns:
            Step execution result
        """
        step_type = step.get("type")
        step_description = step.get("description", "Unknown step")
        
        self.logger.info(f"Executing step: {step_description}")
        
        # Log the start of step execution
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name=f"step_start_{step_type}",
                step_data=step
            )
        
        start_time = time.time()
        result = {}
        
        try:
            if step_type == "tool":
                # Execute a tool
                tool_name = step.get("tool")
                tool_input = step.get("input", {})
                
                result = await self._execute_tool(tool_name, tool_input)
            elif step_type == "synthesize":
                # Synthesize results from previous steps
                result = await self._synthesize_results(query_id)
            else:
                # Unknown step type
                result = {
                    "error": f"Unknown step type: {step_type}"
                }
        except Exception as e:
            self.logger.error(f"Error executing step: {str(e)}")
            result = {
                "error": f"Error executing step: {str(e)}"
            }
        
        elapsed_time = time.time() - start_time
        result["execution_time"] = elapsed_time
        
        # Log the completion of step execution
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name=f"step_complete_{step_type}",
                step_data={
                    "step": step,
                    "result": result,
                    "execution_time": elapsed_time
                }
            )
        
        return result
    
    async def _execute_tool(self, tool_name: str, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute a tool
        
        Args:
            tool_name: Tool name
            tool_input: Tool input
            
        Returns:
            Tool execution result
        """
        tool = self.tool_registry.get_tool(tool_name)
        if not tool:
            return {
                "error": f"Tool not found: {tool_name}"
            }
        
        try:
            result = await tool.execute(tool_input)
            return result
        except Exception as e:
            self.logger.error(f"Error executing tool {tool_name}: {str(e)}")
            return {
                "error": f"Error executing tool {tool_name}: {str(e)}"
            }
    
    async def _synthesize_results(self, query_id: str) -> Dict[str, Any]:
        """
        Synthesize results from previous steps
        
        Args:
            query_id: Query ID
            
        Returns:
            Synthesis result
        """
        # In a real implementation, this would use the LLM to synthesize the results
        # For now, we'll just return a placeholder
        return {
            "synthesis": "Results synthesized successfully"
        }
    
    def _update_plan(self, plan: QueryPlan, step_result: Dict[str, Any]) -> QueryPlan:
        """
        Update the plan based on the result of a step
        
        Args:
            plan: QueryPlan instance
            step_result: Step execution result
            
        Returns:
            Updated QueryPlan
        """
        # Record the result of the step
        plan.record_step_result(step_result)
        
        # In a more sophisticated implementation, we might modify the plan
        # based on the results of previous steps
        
        return plan
    
    async def _generate_response(self, plan: QueryPlan) -> str:
        """
        Generate the final response based on the plan execution
        
        Args:
            plan: Executed QueryPlan
            
        Returns:
            Final response string
        """
        # In a real implementation, this would use the LLM to generate a response
        # based on the results of all the steps
        
        if not self.llm_provider:
            # If no LLM provider is available, generate a simple response
            return self._generate_simple_response(plan)
        
        # Create a prompt for the LLM
        prompt = self._create_response_prompt(plan)
        
        try:
            # Generate a response using the LLM
            response = await self.llm_provider.generate(prompt=prompt)
            return response.get("response", "No response generated")
        except Exception as e:
            self.logger.error(f"Error generating response: {str(e)}")
            return f"Error generating response: {str(e)}"
    
    def _generate_simple_response(self, plan: QueryPlan) -> str:
        """
        Generate a simple response based on the plan execution
        
        Args:
            plan: Executed QueryPlan
            
        Returns:
            Simple response string
        """
        # Check if any steps failed
        for result in plan.results:
            if "error" in result:
                return f"I encountered an error while processing your query: {result['error']}"
        
        # Check if there are any RAG results
        rag_results = []
        for result in plan.results:
            if "chunks" in result:
                rag_results.extend(result["chunks"])
        
        if rag_results:
            # Return the content of the top chunk
            return f"Based on the information I found: {rag_results[0]['content']}"
        
        # Check if there are any calculator results
        for result in plan.results:
            if "result" in result and isinstance(result["result"], (int, float)):
                return f"The result of the calculation is: {result['result']}"
        
        # Check if there are any database results
        for result in plan.results:
            if "results" in result and isinstance(result["results"], list):
                return f"I found {len(result['results'])} records in the database."
        
        # Default response
        return "I processed your query, but I don't have a specific answer to provide."
    
    def _create_response_prompt(self, plan: QueryPlan) -> str:
        """
        Create a prompt for generating the final response
        
        Args:
            plan: Executed QueryPlan
            
        Returns:
            Prompt string
        """
        prompt = f"""
You are an AI assistant helping with a query. Based on the following information, please generate a comprehensive and helpful response.

Original query: {plan.query}

Steps executed:
"""
        
        for i, (step, result) in enumerate(zip(plan.steps[:plan.current_step], plan.results)):
            prompt += f"\nStep {i+1}: {step.get('description', 'Unknown step')}\n"
            
            if "error" in result:
                prompt += f"Error: {result['error']}\n"
            elif step.get("type") == "tool":
                tool_name = step.get("tool")
                if tool_name == "rag":
                    prompt += "Retrieved information:\n"
                    if "chunks" in result:
                        for chunk in result["chunks"]:
                            prompt += f"- {chunk.get('content', '')}\n"
                    else:
                        prompt += "No information retrieved.\n"
                elif tool_name == "calculator":
                    if "result" in result:
                        prompt += f"Calculation result: {result['result']}\n"
                    else:
                        prompt += "No calculation result.\n"
                elif tool_name == "database":
                    if "results" in result:
                        prompt += f"Database query returned {len(result['results'])} records.\n"
                        if result["results"]:
                            prompt += "Sample record:\n"
                            prompt += json.dumps(result["results"][0], indent=2) + "\n"
                    else:
                        prompt += "No database results.\n"
                else:
                    prompt += f"Tool result: {json.dumps(result, indent=2)}\n"
            elif step.get("type") == "synthesize":
                prompt += f"Synthesis result: {result.get('synthesis', 'No synthesis result.')}\n"
        
        prompt += """
Please generate a comprehensive and helpful response to the original query based on the information above.
Your response should be clear, concise, and directly address the user's query.
If there were any errors or missing information, please acknowledge them in your response.
"""
        
        return prompt

================
File: app/rag/process_logger.py
================
"""
ProcessLogger - Logs the entire query processing workflow
"""
import logging
import json
import os
from datetime import datetime
from typing import Dict, List, Any, Optional

class ProcessLogger:
    """
    Logs the entire query processing workflow
    
    The ProcessLogger maintains a detailed log of all steps in the query processing
    workflow, including query analysis, tool usage, and response generation. This
    information can be used for auditing, debugging, and improving the system.
    """
    
    def __init__(self, db_connection=None, log_dir: Optional[str] = None):
        """
        Initialize the process logger
        
        Args:
            db_connection: Database connection for persistent logging (optional)
            log_dir: Directory for storing log files (optional)
        """
        self.db_connection = db_connection
        self.log_dir = log_dir
        self.process_log: Dict[str, Dict[str, Any]] = {}
        self.logger = logging.getLogger("app.rag.process_logger")
        
        # Create log directory if specified and doesn't exist
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir)
    
    def start_process(self, query_id: str, query: str) -> None:
        """
        Start logging a new process
        
        Args:
            query_id: Unique query ID
            query: User query
        """
        self.logger.info(f"Starting process logging for query {query_id}")
        self.process_log[query_id] = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "steps": [],
            "final_response": None,
            "audit_report": None
        }
    
    def log_step(self, query_id: str, step_name: str, step_data: Dict[str, Any]) -> None:
        """
        Log a step in the process
        
        Args:
            query_id: Query ID
            step_name: Name of the step
            step_data: Data from the step
        """
        if query_id not in self.process_log:
            error_msg = f"Unknown query ID: {query_id}"
            self.logger.warning(error_msg)
            raise ValueError(error_msg)
            
        self.logger.info(f"Logging step '{step_name}' for query {query_id}")
        self.process_log[query_id]["steps"].append({
            "step_name": step_name,
            "timestamp": datetime.now().isoformat(),
            "data": step_data
        })
        
        # Save to database if available
        if self.db_connection:
            self._save_to_db(query_id)
        
        # Save to file if log directory is specified
        if self.log_dir:
            self._save_to_file(query_id)
    
    def log_tool_usage(self, query_id: str, tool_name: str, input_data: Dict[str, Any], output_data: Dict[str, Any]) -> None:
        """
        Log tool usage
        
        Args:
            query_id: Query ID
            tool_name: Name of the tool
            input_data: Tool input data
            output_data: Tool output data
        """
        self.log_step(query_id, f"tool_{tool_name}", {
            "tool": tool_name,
            "input": input_data,
            "output": output_data
        })
    
    def log_final_response(self, query_id: str, response: str, metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Log the final response
        
        Args:
            query_id: Query ID
            response: Final response text
            metadata: Additional metadata (optional)
        """
        if query_id not in self.process_log:
            error_msg = f"Unknown query ID: {query_id}"
            self.logger.warning(error_msg)
            raise ValueError(error_msg)
            
        self.logger.info(f"Logging final response for query {query_id}")
        self.process_log[query_id]["final_response"] = {
            "text": response,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        # Save to database if available
        if self.db_connection:
            self._save_to_db(query_id)
        
        # Save to file if log directory is specified
        if self.log_dir:
            self._save_to_file(query_id)
    
    def get_process_log(self, query_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the process log for a query
        
        Args:
            query_id: Query ID
            
        Returns:
            Process log if found, None otherwise
        """
        return self.process_log.get(query_id)
    
    def get_step_data(self, query_id: str, step_name: str) -> List[Dict[str, Any]]:
        """
        Get data for a specific step
        
        Args:
            query_id: Query ID
            step_name: Step name
            
        Returns:
            List of step data dictionaries
        """
        log = self.get_process_log(query_id)
        if not log:
            return []
        
        return [
            step["data"] for step in log["steps"]
            if step["step_name"] == step_name
        ]
    
    def get_tool_usage(self, query_id: str, tool_name: str) -> List[Dict[str, Any]]:
        """
        Get tool usage data
        
        Args:
            query_id: Query ID
            tool_name: Tool name
            
        Returns:
            List of tool usage dictionaries
        """
        return self.get_step_data(query_id, f"tool_{tool_name}")
    
    def clear_log(self, query_id: Optional[str] = None) -> None:
        """
        Clear the process log
        
        Args:
            query_id: Query ID to clear (if None, clear all logs)
        """
        if query_id:
            if query_id in self.process_log:
                del self.process_log[query_id]
                self.logger.info(f"Cleared log for query {query_id}")
        else:
            self.process_log.clear()
            self.logger.info("Cleared all process logs")
    
    def _save_to_db(self, query_id: str) -> None:
        """
        Save the process log to the database
        
        Args:
            query_id: Query ID
        """
        # This is a placeholder - in a real implementation, this would save to the database
        # For now, we'll just log that we would save to the database
        self.logger.info(f"Would save process log for query {query_id} to database")
    
    def _save_to_file(self, query_id: str) -> None:
        """
        Save the process log to a file
        
        Args:
            query_id: Query ID
        """
        if not self.log_dir:
            return
        
        log_file = os.path.join(self.log_dir, f"query_{query_id}.json")
        try:
            with open(log_file, 'w') as f:
                json.dump(self.process_log[query_id], f, indent=2)
            self.logger.info(f"Saved process log for query {query_id} to {log_file}")
        except Exception as e:
            self.logger.error(f"Error saving process log to file: {str(e)}")

================
File: app/rag/processing_job.py
================
"""
Processing Job - Model and service for batch document processing
"""
import uuid
import time
import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Callable, Awaitable

from app.models.document import Document

logger = logging.getLogger("app.rag.processing_job")

class ProcessingJob:
    """
    Model for document processing jobs
    """
    def __init__(
        self, 
        document_ids: List[str], 
        strategy: Optional[str] = None, 
        status: str = "pending",
        job_id: Optional[str] = None
    ):
        self.id = job_id or str(uuid.uuid4())
        self.document_ids = document_ids
        self.strategy = strategy
        self.status = status
        self.created_at = datetime.now()
        self.completed_at = None
        self.document_count = len(document_ids)
        self.processed_count = 0
        self.metadata = {}
        self.progress_percentage = 0
        self.error_message = None
        
    def update_progress(self, processed_count: int) -> None:
        """
        Update job progress
        
        Args:
            processed_count: Number of documents processed
        """
        self.processed_count = processed_count
        self.progress_percentage = (processed_count / self.document_count) * 100 if self.document_count > 0 else 0
        
    def complete(self) -> None:
        """
        Mark job as completed
        """
        self.status = "completed"
        self.completed_at = datetime.now()
        self.processed_count = self.document_count
        self.progress_percentage = 100
        
    def fail(self, error_message: str) -> None:
        """
        Mark job as failed
        
        Args:
            error_message: Error message
        """
        self.status = "failed"
        self.completed_at = datetime.now()
        self.error_message = error_message
        
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert job to dictionary
        
        Returns:
            Dictionary representation of the job
        """
        return {
            "id": self.id,
            "document_ids": self.document_ids,
            "strategy": self.strategy,
            "status": self.status,
            "created_at": self.created_at.isoformat(),
            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
            "document_count": self.document_count,
            "processed_count": self.processed_count,
            "metadata": self.metadata,
            "progress_percentage": self.progress_percentage,
            "error_message": self.error_message
        }


class WorkerPool:
    """
    Pool of workers for processing documents in parallel
    """
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.active_workers = 0
        self.queue = asyncio.Queue()
        self.running = False
        self.logger = logging.getLogger("app.rag.worker_pool")
        
    async def start(self) -> None:
        """
        Start the worker pool
        """
        self.running = True
        self.logger.info(f"Starting worker pool with {self.max_workers} workers")
        for i in range(self.max_workers):
            asyncio.create_task(self._worker(i))
        
    async def stop(self) -> None:
        """
        Stop the worker pool
        """
        self.logger.info("Stopping worker pool")
        self.running = False
        # Wait for queue to empty
        if not self.queue.empty():
            self.logger.info(f"Waiting for {self.queue.qsize()} remaining tasks")
            await self.queue.join()
        
    async def add_job(self, job_func: Callable[..., Awaitable[Any]], *args, **kwargs) -> None:
        """
        Add a job to the queue
        
        Args:
            job_func: Async function to execute
            *args, **kwargs: Arguments to pass to the function
        """
        await self.queue.put((job_func, args, kwargs))
        self.logger.info(f"Added job to queue. Queue size: {self.queue.qsize()}")
        
    async def _worker(self, worker_id: int) -> None:
        """
        Worker process that executes jobs from the queue
        
        Args:
            worker_id: Worker ID
        """
        self.logger.info(f"Worker {worker_id} started")
        while self.running:
            try:
                # Get job from queue with timeout
                job_func, args, kwargs = await asyncio.wait_for(
                    self.queue.get(), timeout=1.0
                )
                
                # Execute job
                self.active_workers += 1
                self.logger.info(f"Worker {worker_id} executing job. Active workers: {self.active_workers}")
                try:
                    await job_func(*args, **kwargs)
                except Exception as e:
                    self.logger.error(f"Worker {worker_id} error processing job: {str(e)}")
                finally:
                    self.active_workers -= 1
                    self.queue.task_done()
                    self.logger.info(f"Worker {worker_id} completed job. Active workers: {self.active_workers}")
            except asyncio.TimeoutError:
                # No job available, continue waiting
                pass
            except Exception as e:
                self.logger.error(f"Worker {worker_id} unexpected error: {str(e)}")


class DocumentProcessingService:
    """
    Service for processing documents in batches
    """
    def __init__(self, document_processor, max_workers: int = 4, document_repository=None):
        self.document_processor = document_processor
        self.worker_pool = WorkerPool(max_workers=max_workers)
        self.jobs: Dict[str, ProcessingJob] = {}
        self.logger = logging.getLogger("app.rag.document_processing_service")
        self.document_repository = document_repository
        
    async def start(self) -> None:
        """
        Start the processing service
        """
        await self.worker_pool.start()
        self.logger.info("Document processing service started")
        
    async def stop(self) -> None:
        """
        Stop the processing service
        """
        await self.worker_pool.stop()
        self.logger.info("Document processing service stopped")
        
    def set_document_repository(self, document_repository) -> None:
        """
        Set the document repository
        
        Args:
            document_repository: Document repository
        """
        self.document_repository = document_repository
        
    async def create_job(self, document_ids: List[str], strategy: Optional[str] = None) -> ProcessingJob:
        """
        Create a new processing job
        
        Args:
            document_ids: List of document IDs to process
            strategy: Processing strategy
            
        Returns:
            Created job
        """
        job = ProcessingJob(document_ids=document_ids, strategy=strategy)
        self.jobs[job.id] = job
        self.logger.info(f"Created processing job {job.id} for {len(document_ids)} documents")
        
        # Add job to worker pool
        await self.worker_pool.add_job(self._process_job, job)
        
        return job
    
    async def get_job(self, job_id: str) -> Optional[ProcessingJob]:
        """
        Get a job by ID
        
        Args:
            job_id: Job ID
            
        Returns:
            Job if found, None otherwise
        """
        return self.jobs.get(job_id)
    
    async def list_jobs(self, status: Optional[str] = None) -> List[ProcessingJob]:
        """
        List all jobs
        
        Args:
            status: Filter by status
            
        Returns:
            List of jobs
        """
        if status:
            return [job for job in self.jobs.values() if job.status == status]
        return list(self.jobs.values())
    
    async def cancel_job(self, job_id: str) -> bool:
        """
        Cancel a job
        
        Args:
            job_id: Job ID
            
        Returns:
            True if job was cancelled, False otherwise
        """
        job = self.jobs.get(job_id)
        if job and job.status == "pending":
            job.status = "cancelled"
            job.completed_at = datetime.now()
            self.logger.info(f"Cancelled job {job_id}")
            return True
        return False
    
    async def _process_job(self, job: ProcessingJob) -> None:
        """
        Process a job
        
        Args:
            job: Job to process
        """
        start_time = time.time()
        self.logger.info(f"Processing job {job.id} with {job.document_count} documents")
        
        try:
            job.status = "processing"
            
            # Process each document
            for i, document_id in enumerate(job.document_ids):
                if job.status == "cancelled":
                    self.logger.info(f"Job {job.id} was cancelled, stopping processing")
                    break
                
                try:
                    # Get document
                    document = await self._get_document(document_id)
                    
                    if document:
                        # Set strategy if specified
                        if job.strategy:
                            self.document_processor.chunking_strategy = job.strategy
                        
                        # Process document
                        processed_document = await self.document_processor.process_document(document)
                        
                        # Save document
                        await self._save_document(processed_document)
                    
                    # Update progress
                    job.update_progress(i + 1)
                    
                    self.logger.info(f"Processed document {document_id} ({i+1}/{job.document_count})")
                except Exception as e:
                    self.logger.error(f"Error processing document {document_id}: {str(e)}")
                    # Continue with next document
            
            # Complete job
            if job.status != "cancelled":
                job.complete()
                
            elapsed_time = time.time() - start_time
            self.logger.info(f"Job {job.id} completed in {elapsed_time:.2f}s")
        except Exception as e:
            self.logger.error(f"Error processing job {job.id}: {str(e)}")
            job.fail(str(e))
    
    async def _get_document(self, document_id: str) -> Optional[Document]:
        """
        Get a document by ID
        
        Args:
            document_id: Document ID
            
        Returns:
            Document if found, None otherwise
        """
        if self.document_repository:
            try:
                # Get document from repository
                document = self.document_repository.get_document_with_chunks(document_id)
                if document:
                    self.logger.info(f"Retrieved document {document_id} from repository")
                    return document
                else:
                    self.logger.warning(f"Document {document_id} not found in repository")
            except Exception as e:
                self.logger.error(f"Error retrieving document {document_id} from repository: {str(e)}")
        
        # Fallback to dummy document if repository not available or document not found
        self.logger.warning(f"Using dummy document for {document_id} (repository not available or document not found)")
        return Document(
            id=document_id,
            filename=f"document_{document_id}.txt",
            content="This is a dummy document content for testing purposes."
        )
    
    async def _save_document(self, document: Document) -> None:
        """
        Save a document
        
        Args:
            document: Document to save (Pydantic model)
        """
        if self.document_repository:
            try:
                # Save document to repository
                self.document_repository.save_document_with_chunks(document)
                self.logger.info(f"Saved document {document.id} to repository")
            except Exception as e:
                self.logger.error(f"Error saving document {document.id} to repository: {str(e)}")
        else:
            self.logger.warning(f"Document repository not available, document {document.id} not saved")

================
File: app/rag/prompt_manager.py
================
"""
Prompt Manager for RAG System

This module provides a centralized manager for all prompt-related operations,
serving as a single source of truth for prompt templates and instructions.
"""
import logging
import os
from typing import Dict, Any, List, Tuple, Optional

logger = logging.getLogger("app.rag.prompt_manager")

class PromptManager:
    """
    PromptManager serves as a single source of truth for all prompt-related operations.
    
    It manages templates, handles state-based prompt selection, and ensures
    consistent instructions across different scenarios.
    """
    
    def __init__(self):
        """Initialize the PromptManager with templates."""
        self.templates = self._load_templates()
        logger.info("PromptManager initialized with templates")
    
    def _load_templates(self) -> Dict[str, Dict[str, str]]:
        """
        Load all prompt templates.
        
        Returns:
            Dictionary of templates for different scenarios
        """
        # Base system prompt that applies to all scenarios
        base_system_prompt = """You are a helpful assistant. Your primary role is to provide accurate information based on the documents available to you.

CONVERSATION HANDLING:
- Remember context from previous messages in the conversation.
- Respond directly to the user's query without unnecessary preambles.
- Be concise but thorough in your responses.
"""
        
        # Template for when documents are successfully retrieved
        with_documents_template = {
            "system_prompt": base_system_prompt + """
CORE GUIDELINES:
1. Prioritize information from the provided documents in your responses.
2. Use citations [1] when referring to specific information from the documents.
3. Synthesize information from multiple documents when relevant.
4. Maintain a helpful, conversational tone.

WHEN USING DOCUMENTS:
- Provide clear, accurate information based on the documents.
- Use citations to reference specific documents in a natural way.
- Combine information from multiple documents to provide comprehensive answers.
- If the documents don't fully answer the query, supplement with your general knowledge.
- Remember information from the conversation history to provide context-aware responses.
""",
            "user_prompt": """Context:
{context}

{conversation_prefix}

User question: {query}

IMPORTANT INSTRUCTIONS:
1. Use the information from the context to answer the question.
2. When using specific information from the context, reference your sources with the number in square brackets, like [1] or [2].
3. Provide a clear, comprehensive answer that synthesizes information from the documents.
4. If the context doesn't fully answer the question, supplement with your general knowledge.
5. Be conversational and natural in your response.
6. Use information from the conversation history when relevant.
"""
        }
        
        # Template for when no documents are found
        no_documents_template = {
            "system_prompt": base_system_prompt + """
CORE GUIDELINES:
1. Be honest about limitations when no relevant documents are available.
2. DO NOT use citations [1] as there are no documents to cite.
3. Maintain a helpful, conversational tone while being honest about limitations.
4. Use your general knowledge to provide helpful responses.

WHEN NO DOCUMENTS ARE AVAILABLE:
- You can use your general knowledge to answer questions directly.
- Only mention the lack of documents if specifically asked about documentation or sources.
- Focus on being helpful and providing accurate information based on your training.
- Maintain a natural, conversational tone.
- Remember information from the conversation history to provide context-aware responses.
""",
            "user_prompt": """{conversation_prefix}

User question: {query}

IMPORTANT INSTRUCTIONS:
1. Answer the question directly using your general knowledge.
2. DO NOT use citations [1] as there are no documents to cite.
3. Only mention the lack of documents if specifically asked about documentation or sources.
4. Be conversational and helpful in your response.
5. Use information from the conversation history when relevant.
"""
        }
        
        # Template for when documents have low relevance
        low_relevance_template = {
            "system_prompt": base_system_prompt + """
CORE GUIDELINES:
1. Use any relevant information from the documents if available.
2. Use citations [1] only for information that comes directly from the documents.
3. Supplement with your general knowledge to provide a complete answer.
4. Maintain a helpful, conversational tone.

WHEN DOCUMENTS HAVE LOW RELEVANCE:
- Extract any useful information from the documents that might be relevant.
- Use your general knowledge to provide a complete and helpful answer.
- Only cite documents when directly quoting or referencing specific information from them.
- Focus on being helpful rather than emphasizing limitations.
- Remember information from the conversation history to provide context-aware responses.
""",
            "user_prompt": """Context (Low Relevance):
{context}

{conversation_prefix}

User question: {query}

IMPORTANT INSTRUCTIONS:
1. Answer the question directly, using both the context and your general knowledge.
2. Only use citations [1] when directly referencing information from the context.
3. Focus on providing a helpful, complete answer rather than emphasizing limitations.
4. Be conversational and natural in your response.
5. Use information from the conversation history when relevant.
"""
        }
        
        # Template for error conditions
        error_template = {
            "system_prompt": base_system_prompt + """
CORE GUIDELINES:
1. Focus on being helpful despite any system errors.
2. DO NOT use citations [1] as there are no documents to cite.
3. Use your general knowledge to provide helpful responses.
4. Maintain a conversational, friendly tone.

WHEN ERRORS OCCUR:
- Answer the question directly using your general knowledge.
- Do not mention system errors unless specifically asked.
- Focus on providing value to the user despite limitations.
- Remember information from the conversation history to provide context-aware responses.
""",
            "user_prompt": """{conversation_prefix}

User question: {query}

IMPORTANT INSTRUCTIONS:
1. Answer the question directly using your general knowledge.
2. DO NOT use citations [1] as there are no documents to cite.
3. Do not mention system errors unless specifically asked.
4. Be conversational and helpful in your response.
5. Use information from the conversation history when relevant.
"""
        }
        
        return {
            "with_documents": with_documents_template,
            "no_documents": no_documents_template,
            "low_relevance": low_relevance_template,
            "error": error_template
        }
    
    def create_prompt(self, 
                      query: str, 
                      retrieval_state: str,
                      context: str = "",
                      conversation_history: Optional[List[Dict[str, str]]] = None) -> Tuple[str, str]:
        """
        Create a complete prompt based on the current state.
        
        Args:
            query: User query
            retrieval_state: State of the retrieval process 
                             ("success", "no_documents", "low_relevance", "error")
            context: Retrieved document context (may be empty)
            conversation_history: Conversation history (may be empty)
            
        Returns:
            Tuple of (system_prompt, user_prompt)
        """
        # Select the appropriate template based on state
        if retrieval_state == "success" and context:
            template = self.templates["with_documents"]
        elif retrieval_state == "no_documents":
            template = self.templates["no_documents"]
        elif retrieval_state == "low_relevance":
            template = self.templates["low_relevance"]
        else:
            template = self.templates["error"]
        
        # Format conversation history if provided
        conversation_prefix = ""
        if conversation_history and len(conversation_history) > 0:
            # Format the conversation history
            history_pieces = []
            for msg in conversation_history:
                role_prefix = "User" if msg["role"] == "user" else "Assistant"
                history_pieces.append(f"{role_prefix}: {msg['content']}")
            
            conversation_prefix = "Previous conversation:\n" + "\n".join(history_pieces)
        
        # Format the user prompt with data
        user_prompt = template["user_prompt"].format(
            context=context,
            conversation_prefix=conversation_prefix,
            query=query
        )
        
        return template["system_prompt"], user_prompt
    
    def get_retrieval_state(self, 
                           search_results: List[Dict[str, Any]], 
                           relevance_threshold: float = 0.4) -> str:
        """
        Determine the retrieval state based on search results.
        
        Args:
            search_results: Results from vector store search
            relevance_threshold: Threshold for determining relevance
            
        Returns:
            Retrieval state ("success", "no_documents", "low_relevance")
        """
        if not search_results:
            return "no_documents"
        
        # Check if any results meet the relevance threshold
        relevant_results = []
        for result in search_results:
            # Calculate relevance score (lower distance = higher relevance)
            relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
            if relevance_score >= relevance_threshold:
                relevant_results.append(result)
        
        if not relevant_results:
            return "low_relevance"
        
        return "success"

================
File: app/rag/query_analyzer.py
================
"""
QueryAnalyzer - Analyzes queries to determine their complexity and requirements
"""
import logging
import time
import re
import json
from typing import Dict, List, Any, Optional, Tuple

class QueryAnalyzer:
    """
    Analyzes queries to determine their complexity and requirements
    
    The QueryAnalyzer uses an LLM to analyze queries and determine:
    - Query complexity (simple vs. complex)
    - Required tools for answering the query
    - Potential sub-queries
    - Reasoning behind the analysis
    """
    
    def __init__(self, llm_provider):
        """
        Initialize the query analyzer
        
        Args:
            llm_provider: LLM provider for analysis
        """
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.rag.query_analyzer")
    
    async def analyze(self, query: str,
                     chat_history: Optional[List[Tuple[str, str]]] = None) -> Dict[str, Any]:
        """
        Analyze a query to determine its complexity and requirements
        
        Args:
            query: Query string
            chat_history: Optional list of (user_message, ai_message) tuples
            
        Returns:
            Dict with keys:
            - complexity: "simple" or "complex"
            - requires_tools: list of required tools
            - sub_queries: list of potential sub-queries
            - reasoning: explanation of the analysis
        """
        start_time = time.time()
        self.logger.info(f"Analyzing query: {query}")
        
        prompt = self._create_analysis_prompt(query, chat_history)
        response = await self.llm_provider.generate(prompt=prompt)
        analysis = self._parse_analysis(response.get("response", ""))
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Query analysis completed in {elapsed_time:.2f}s. Complexity: {analysis.get('complexity')}")
        
        return analysis
    
    def _create_analysis_prompt(self, query: str,
                               chat_history: Optional[List[Tuple[str, str]]] = None) -> str:
        """
        Create a prompt for query analysis
        
        Args:
            query: Query string
            chat_history: Optional list of (user_message, ai_message) tuples
            
        Returns:
            Prompt string
        """
        # Format chat history if available
        history_str = ""
        if chat_history:
            history_lines = []
            for i, (user_msg, ai_msg) in enumerate(chat_history):
                history_lines.append(f"Turn {i+1}:")
                history_lines.append(f"User: {user_msg}")
                history_lines.append(f"AI: {ai_msg}")
            history_str = "\n".join(history_lines)

        return f"""
You are an expert query analyzer for a RAG (Retrieval-Augmented Generation) system. Your task is to analyze the following query, considering the preceding conversation history, and determine its complexity, required tools, and potential sub-queries.

Conversation History:
{history_str if history_str else "None"}

Current Query: "{query}"

Available tools:
1. rag - Retrieves information from documents using RAG
2. calculator - Performs mathematical calculations
3. database - Queries structured data from databases

Please analyze the query and provide your assessment in the following JSON format:
{{
  "complexity": "simple" or "complex",
  "requires_tools": ["tool1", "tool2", ...],
  "sub_queries": ["sub-query1", "sub-query2", ...],
  "reasoning": "Detailed explanation of your analysis"
}}

Where:
- "complexity" indicates whether the query is simple (can be answered with a single RAG lookup) or complex (requires multiple steps or tools)
- "requires_tools" lists the tools needed to answer the query
- "sub_queries" lists potential sub-queries if the main query needs to be broken down
- "reasoning" explains your analysis in detail

Analyze the query carefully, considering:
1. Does it require factual information retrieval? (use rag tool)
2. Does it involve calculations? (use calculator tool)
3. Does it need structured data lookup? (use database tool)
4. Does it require multiple steps or a combination of tools?
5. Would breaking it into sub-queries improve the response quality?
6. How does the conversation history affect the interpretation of the current query? Does the query refer back to previous turns (e.g., "like you mentioned before", "tell me more about that")?

Provide your analysis in valid JSON format.
"""
    
    def _parse_analysis(self, response: str) -> Dict[str, Any]:
        """
        Parse the LLM response to extract the analysis
        
        Args:
            response: LLM response string
            
        Returns:
            Dict with analysis results
        """
        # Try to extract JSON from the response
        try:
            # Look for JSON pattern in the response
            json_match = re.search(r'({[\s\S]*})', response)
            if json_match:
                analysis = json.loads(json_match.group(1))
                
                # Validate required fields
                if "complexity" in analysis and "requires_tools" in analysis:
                    return analysis
        except json.JSONDecodeError:
            self.logger.warning(f"Failed to parse JSON from response: {response}")
        
        # If JSON parsing fails, try to extract key information using regex
        complexity_match = re.search(r'complexity["\']?\s*:\s*["\']?(\w+)["\']?', response)
        tools_match = re.search(r'requires_tools["\']?\s*:\s*\[(.*?)\]', response)
        
        complexity = complexity_match.group(1) if complexity_match else "simple"
        
        tools = []
        if tools_match:
            tools_str = tools_match.group(1)
            tool_matches = re.findall(r'["\'](\w+)["\']', tools_str)
            tools = tool_matches if tool_matches else []
        
        # Default analysis if parsing fails
        return {
            "complexity": complexity,
            "requires_tools": tools,
            "sub_queries": [],
            "reasoning": "Extracted from LLM response with fallback parsing"
        }

================
File: app/rag/query_planner.py
================
"""
QueryPlanner - Plans the execution of complex queries
"""
import logging
import json
from typing import Dict, List, Any, Optional, Tuple

class QueryPlan:
    """
    Represents a plan for executing a complex query
    
    A QueryPlan consists of a sequence of steps, each of which may involve
    executing a tool, retrieving information, or performing some other action.
    The plan can also store conversation history to provide context for the execution.
    """
    
    def __init__(self, query_id: str, query: str, steps: List[Dict[str, Any]],
                 chat_history: Optional[List[Tuple[str, str]]] = None):
        """
        Initialize a query plan
        
        Args:
            query_id: Unique query ID
            query: Original query string
            steps: List of execution steps
            chat_history: Optional list of (user_message, ai_message) tuples representing
                          the conversation history
        """
        self.query_id = query_id
        self.query = query
        self.steps = steps
        self.current_step = 0
        self.results = []
        self.completed = False
        self.chat_history = chat_history
    
    def get_next_step(self) -> Optional[Dict[str, Any]]:
        """
        Get the next step in the plan
        
        Returns:
            Next step if available, None if plan is completed
        """
        if self.current_step >= len(self.steps):
            self.completed = True
            return None
        
        return self.steps[self.current_step]
    
    def record_step_result(self, result: Dict[str, Any]) -> None:
        """
        Record the result of a step
        
        Args:
            result: Step execution result
        """
        self.results.append(result)
        self.current_step += 1
    
    def is_completed(self) -> bool:
        """
        Check if the plan is completed
        
        Returns:
            True if all steps have been executed, False otherwise
        """
        return self.completed or self.current_step >= len(self.steps)
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the plan to a dictionary
        
        Returns:
            Dictionary representation of the plan
        """
        return {
            "query_id": self.query_id,
            "query": self.query,
            "steps": self.steps,
            "current_step": self.current_step,
            "results": self.results,
            "completed": self.completed,
            "chat_history": self.chat_history
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'QueryPlan':
        """
        Create a plan from a dictionary
        
        Args:
            data: Dictionary representation of the plan
            
        Returns:
            QueryPlan instance
        """
        plan = cls(
            query_id=data["query_id"],
            query=data["query"],
            steps=data["steps"],
            chat_history=data.get("chat_history")
        )
        plan.current_step = data.get("current_step", 0)
        plan.results = data.get("results", [])
        plan.completed = data.get("completed", False)
        return plan


class QueryPlanner:
    """
    Plans the execution of complex queries
    
    The QueryPlanner analyzes queries and creates execution plans that may involve
    multiple steps and tools. It uses the QueryAnalyzer to determine the complexity
    and requirements of a query, then creates a plan for executing it.
    """
    
    def __init__(self, query_analyzer, tool_registry):
        """
        Initialize the query planner
        
        Args:
            query_analyzer: QueryAnalyzer instance
            tool_registry: ToolRegistry instance
        """
        self.query_analyzer = query_analyzer
        self.tool_registry = tool_registry
        self.logger = logging.getLogger("app.rag.query_planner")
    
    async def create_plan(self, query_id: str, query: str,
                         chat_history: Optional[List[Tuple[str, str]]] = None) -> QueryPlan:
        """
        Create a plan for executing a query
        
        Args:
            query_id: Unique query ID
            query: Query string
            chat_history: Optional list of (user_message, ai_message) tuples
            
        Returns:
            QueryPlan instance
        """
        self.logger.info(f"Creating plan for query: {query}")
        
        # Analyze the query with chat history context
        analysis = await self.query_analyzer.analyze(query, chat_history)
        
        # Determine if the query is simple or complex
        complexity = analysis.get("complexity", "simple")
        required_tools = analysis.get("requires_tools", [])
        sub_queries = analysis.get("sub_queries", [])
        
        # Create plan steps
        steps = []
        
        if complexity == "simple":
            # Simple query - just use RAG
            steps.append({
                "type": "tool",
                "tool": "rag",
                "input": {
                    "query": query,
                    "top_k": 5
                },
                "description": "Retrieve information using RAG"
            })
        else:
            # Complex query - may require multiple steps
            
            # First, add steps for any required tools
            for tool_name in required_tools:
                tool = self.tool_registry.get_tool(tool_name)
                if not tool:
                    self.logger.warning(f"Required tool not found: {tool_name}")
                    continue
                
                # Create a step for this tool
                tool_input = self._create_tool_input(tool_name, query)
                steps.append({
                    "type": "tool",
                    "tool": tool_name,
                    "input": tool_input,
                    "description": f"Execute {tool_name} tool"
                })
            
            # If there are sub-queries, add steps for them
            for sub_query in sub_queries:
                steps.append({
                    "type": "tool",
                    "tool": "rag",
                    "input": {
                        "query": sub_query,
                        "top_k": 3
                    },
                    "description": f"Retrieve information for sub-query: {sub_query}"
                })
            
            # Add a final step to synthesize the results with chat history
            steps.append({
                "type": "synthesize",
                "description": "Synthesize results from previous steps with conversation history",
                "with_history": True  # Flag to indicate this step should use history
            })
        
        # Create the plan with chat history
        plan = QueryPlan(
            query_id=query_id,
            query=query,
            steps=steps,
            chat_history=chat_history  # Pass chat history to the plan
        )
        
        self.logger.info(f"Created plan with {len(steps)} steps for query: {query}")
        return plan
    
    def _create_tool_input(self, tool_name: str, query: str) -> Dict[str, Any]:
        """
        Create input for a tool based on the query
        
        Args:
            tool_name: Tool name
            query: Query string
            
        Returns:
            Tool input dictionary
        """
        if tool_name == "rag":
            return {
                "query": query,
                "top_k": 5
            }
        elif tool_name == "calculator":
            # Extract mathematical expression from the query
            # This is a simple implementation - in a real system, you would use
            # more sophisticated NLP techniques to extract the expression
            import re
            expression_match = re.search(r'calculate\s+(.+)', query, re.IGNORECASE)
            if expression_match:
                expression = expression_match.group(1)
            else:
                expression = query
            
            return {
                "expression": expression
            }
        elif tool_name == "database":
            # For database queries, we would need more sophisticated parsing
            # This is a placeholder implementation
            return {
                "query": "SELECT * FROM relevant_table LIMIT 5",
                "source": "default.db"
            }
        else:
            # Default to passing the query as-is
            return {
                "query": query
            }
    
    def update_plan(self, plan: QueryPlan, step_result: Dict[str, Any]) -> QueryPlan:
        """
        Update a plan based on the result of a step
        
        Args:
            plan: QueryPlan instance
            step_result: Result of the current step
            
        Returns:
            Updated QueryPlan
        """
        # Record the result of the current step
        plan.record_step_result(step_result)
        
        # Check if we need to modify the plan based on the result
        current_step = plan.current_step - 1  # The step that just completed
        if current_step < 0 or current_step >= len(plan.steps):
            return plan
        
        step = plan.steps[current_step]
        
        # If the step was a tool execution and it failed, we might want to try an alternative
        if step["type"] == "tool" and "error" in step_result:
            self.logger.warning(f"Tool execution failed: {step_result.get('error')}")
            
            # We could insert a fallback step here if needed
            # For now, we'll just continue with the plan
        
        return plan

================
File: app/rag/rag_engine_base.py
================
"""
Base RAG Engine class with core functionality
"""
import logging
import re
from typing import Optional, Dict, Any
from uuid import UUID

from app.core.config import USE_RETRIEVAL_JUDGE
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.mem0_client import get_mem0_client
from app.cache.cache_manager import CacheManager

logger = logging.getLogger("app.rag.rag_engine_base")

class BaseRAGEngine:
    """
    Base class for RAG (Retrieval Augmented Generation) Engine with security features
    """
    def __init__(
        self,
        vector_store: Optional[VectorStore] = None,
        ollama_client: Optional[OllamaClient] = None,
        retrieval_judge: Optional[RetrievalJudge] = None,
        cache_manager: Optional[CacheManager] = None,
        user_id: Optional[UUID] = None
    ):
        """
        Initialize the RAG engine
        
        Args:
            vector_store: Vector store instance
            ollama_client: Ollama client instance
            retrieval_judge: Retrieval judge instance
            cache_manager: Cache manager instance
            user_id: User ID for permission filtering
        """
        self.vector_store = vector_store or VectorStore(user_id=user_id)
        self.ollama_client = ollama_client or OllamaClient()
        self.retrieval_judge = retrieval_judge if retrieval_judge is not None else (
            RetrievalJudge(ollama_client=self.ollama_client) if USE_RETRIEVAL_JUDGE else None
        )
        self.user_id = user_id  # Store the user ID for permission filtering
        
        # Initialize Mem0 client
        self.mem0_client = get_mem0_client()
        
        # Initialize cache manager
        self.cache_manager = cache_manager or CacheManager()
        
        if self.retrieval_judge:
            logger.info("Retrieval Judge is enabled")
        else:
            logger.info("Retrieval Judge is disabled")
            
        if self.mem0_client:
            logger.info("Mem0 integration is enabled")
        else:
            logger.info("Mem0 integration is disabled")
            
        # Log cache status
        cache_stats = self.cache_manager.get_all_cache_stats()
        if cache_stats.get("caching_enabled", False):
            logger.info("Caching is enabled")
        else:
            logger.info("Caching is disabled")
    
    def _is_code_related_query(self, query: str) -> bool:
        """
        Determine if a query is related to code or programming.
        
        Args:
            query: The user query
            
        Returns:
            True if the query is code-related, False otherwise
        """
        # Convert to lowercase for case-insensitive matching
        query_lower = query.lower()
        
        # Check for code-related keywords
        code_keywords = [
            'code', 'program', 'function', 'class', 'method', 'variable',
            'algorithm', 'implement', 'python', 'javascript', 'java', 'c++', 'c#',
            'typescript', 'html', 'css', 'php', 'ruby', 'go', 'rust', 'swift',
            'kotlin', 'scala', 'perl', 'r', 'bash', 'shell', 'sql', 'database',
            'api', 'framework', 'library', 'package', 'module', 'import',
            'function', 'def ', 'return', 'for loop', 'while loop', 'if statement',
            'create a', 'write a', 'develop a', 'build a', 'implement a',
            'tic tac toe', 'tic-tac-toe', 'game', 'application', 'app',
            'script', 'syntax', 'error', 'debug', 'fix', 'optimize'
        ]
        
        # Check if any code keyword is in the query
        for keyword in code_keywords:
            if keyword in query_lower:
                return True
        
        # Check for code patterns
        code_patterns = [
            r'```[\s\S]*```',  # Code blocks
            r'def\s+\w+\s*\(',  # Python function definition
            r'function\s+\w+\s*\(',  # JavaScript function definition
            r'class\s+\w+',  # Class definition
            r'import\s+\w+',  # Import statement
            r'from\s+\w+\s+import',  # Python import
            r'<\w+>.*</\w+>',  # HTML tags
            r'\w+\s*=\s*function\(',  # JavaScript function assignment
            r'const\s+\w+\s*=',  # JavaScript const declaration
            r'let\s+\w+\s*=',  # JavaScript let declaration
            r'var\s+\w+\s*=',  # JavaScript var declaration
            r'public\s+\w+\s+\w+\(',  # Java method
            r'SELECT\s+.*\s+FROM',  # SQL query
            r'CREATE\s+TABLE',  # SQL create table
            r'@app\.route',  # Flask route
            r'npm\s+install',  # npm command
            r'pip\s+install',  # pip command
            r'git\s+\w+'  # git command
        ]
        
        # Check if any code pattern is in the query
        for pattern in code_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return True
        
        return False

================
File: app/rag/rag_engine.py
================
"""
RAG (Retrieval Augmented Generation) Engine
"""
import logging
import time
import uuid
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from uuid import UUID

from app.core.config import DEFAULT_MODEL
from app.models.chat import Citation, Message
from app.rag.rag_engine_base import BaseRAGEngine
from app.rag.rag_retrieval import RetrievalMixin
from app.rag.rag_generation import GenerationMixin
from app.rag.mem0_client import store_message, get_conversation_history, store_document_interaction, get_user_preferences
from app.rag.memory_buffer import process_query, get_conversation_context

logger = logging.getLogger("app.rag.rag_engine")

class RAGEngine(BaseRAGEngine, RetrievalMixin, GenerationMixin):
    """
    RAG (Retrieval Augmented Generation) Engine with security features
    
    This class combines the base RAGEngine with retrieval and generation functionality
    to provide a complete RAG solution with security features.
    """
    
    def __init__(
        self,
        vector_store=None,
        ollama_client=None,
        retrieval_judge=None,
        cache_manager=None,
        user_id=None
    ):
        """
        Initialize the RAG engine
        
        Args:
            vector_store: Vector store instance
            ollama_client: Ollama client instance
            retrieval_judge: Retrieval judge instance
            cache_manager: Cache manager instance
            user_id: User ID for permission filtering
        """
        # Initialize BaseRAGEngine
        BaseRAGEngine.__init__(
            self,
            vector_store=vector_store,
            ollama_client=ollama_client,
            retrieval_judge=retrieval_judge,
            cache_manager=cache_manager,
            user_id=user_id
        )
        
        # Initialize GenerationMixin
        GenerationMixin.__init__(self)
        
        logger.info("RAGEngine initialized with PromptManager")
    
    async def query(self,
                   query: str,
                   model: str = DEFAULT_MODEL,
                   use_rag: bool = True,
                   top_k: int = 10,
                   system_prompt: Optional[str] = None,
                   stream: bool = False,
                   model_parameters: Dict[str, Any] = None,
                   conversation_history: Optional[List[Message]] = None,
                   metadata_filters: Optional[Dict[str, Any]] = None,
                   user_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Query the RAG engine with optional conversation history and metadata filtering
        
        Args:
            query: Query string
            model: Model to use
            use_rag: Whether to use RAG
            top_k: Number of results to return
            system_prompt: System prompt
            stream: Whether to stream the response
            model_parameters: Model parameters
            conversation_history: Conversation history
            metadata_filters: Metadata filters
            user_id: User ID for permission filtering
            
        Returns:
            Response dictionary
        """
        start_time = time.time()
        document_ids = []
        
        try:
            logger.info(f"RAG query: {query[:50]}...")
            
            # Get context from vector store if RAG is enabled
            context = ""
            sources = []
            
            # Generate a user_id if not provided
            if not user_id:
                user_id = f"session_{str(uuid.uuid4())[:8]}"
                logger.info(f"Generated session user_id: {user_id}")
            
            # Convert string user_id to UUID if needed
            user_uuid = None
            if user_id:
                try:
                    user_uuid = UUID(user_id) if isinstance(user_id, str) else user_id
                except ValueError:
                    logger.warning(f"Invalid user_id format: {user_id}, using as string")
            
            # Process memory commands if user_id is provided
            processed_query = query
            memory_response = None
            memory_operation = None
            
            # Extract conversation_id from conversation_history if available
            conversation_id = None
            if conversation_history and len(conversation_history) > 0:
                # Assuming the first message has the conversation_id
                conversation_id = getattr(conversation_history[0], 'conversation_id', None)
            
            if user_id and conversation_id:
                processed_query, memory_response, memory_operation = await process_query(
                    query=query,
                    user_id=user_id,
                    conversation_id=conversation_id
                )
                
                # If it's a recall operation with a response, return immediately
                if memory_operation == "recall" and memory_response:
                    return {
                        "query": query,
                        "answer": memory_response,
                        "sources": []
                    }
            
            # Use the processed query for RAG
            query = processed_query
            
            # Integrate with Mem0 if available
            if self.mem0_client and user_id:
                # Store the user query in Mem0
                await store_message(user_id, "user", query)
                
                # Get user preferences if available
                user_prefs = await get_user_preferences(user_id)
                if user_prefs:
                    logger.info(f"Retrieved user preferences: {user_prefs}")
                    # Apply user preferences if available
                    if "preferred_model" in user_prefs and user_prefs["preferred_model"]:
                        model = user_prefs["preferred_model"]
                        logger.info(f"Using preferred model from user preferences: {model}")
            
            # Format conversation history if provided
            conversation_context = ""
            if conversation_history and len(conversation_history) > 1:  # Only include history if there's more than just the current message
                # Get the last few messages (up to 5) to keep context manageable, but exclude the most recent user message
                # which is the current query and shouldn't be treated as history
                recent_history = conversation_history[:-1]
                if len(recent_history) > 5:
                    recent_history = recent_history[-5:]
                
                # Format the conversation history
                history_pieces = []
                for msg in recent_history:
                    role_prefix = "User" if msg.role == "user" else "Assistant"
                    history_pieces.append(f"{role_prefix}: {msg.content}")
                
                conversation_context = "\n".join(history_pieces)
                logger.info(f"Including conversation history with {len(recent_history)} messages")
            elif self.mem0_client and user_id:
                # Try to get conversation history from Mem0 if not provided
                mem0_history = await get_conversation_history(user_id, limit=5)
                if mem0_history:
                    # Format the conversation history from Mem0
                    history_pieces = []
                    for msg in mem0_history:
                        role_prefix = "User" if msg["role"] == "user" else "Assistant"
                        history_pieces.append(f"{role_prefix}: {msg['content']}")
                    
                    conversation_context = "\n".join(history_pieces)
                    logger.info(f"Including conversation history from Mem0 with {len(mem0_history)} messages")
                else:
                    logger.info("No previous conversation history found in Mem0")
            else:
                logger.info("No previous conversation history to include")
            
            if use_rag:
                # Use enhanced retrieval if Retrieval Judge is enabled
                if self.retrieval_judge:
                    logger.info("Using enhanced retrieval with Retrieval Judge")
                    context, sources, document_ids = await self._enhanced_retrieval(
                        query=query,
                        conversation_context=conversation_context,
                        top_k=top_k,
                        metadata_filters=metadata_filters,
                        user_id=user_uuid  # Pass user_id for permission filtering
                    )
                else:
                    # Use standard retrieval
                    logger.info("Using standard retrieval (Retrieval Judge disabled)")
                    # Check if there are any documents in the vector store
                    stats = self.vector_store.get_stats()
                    if stats["count"] == 0:
                        logger.warning("RAG is enabled but no documents are available in the vector store")
                        # Leave context empty to indicate no documents
                        context = ""
                    else:
                        # Combine the current query with conversation context for better retrieval
                        search_query = query
                        if conversation_context:
                            # For retrieval, we focus more on the current query but include
                            # some context from the conversation to improve relevance
                            search_query = f"{query} {conversation_context[-200:]}"
                        
                        # Log the search query
                        logger.info(f"Searching with query: {search_query[:100]}...")
                        
                        # Use a higher fixed value for top_k to get more potential matches, then filter by relevance
                        search_results = await self.vector_store.search(
                            query=search_query,
                            top_k=15,  # Fixed value to retrieve more chunks
                            filter_criteria=metadata_filters,
                            user_id=user_uuid  # Pass user_id for permission filtering
                        )
                        
                        if search_results:
                            # Process search results and format context
                            context_pieces = []
                            relevant_results = []
                            
                            # Set a relevance threshold - only include chunks with relevance score above this
                            relevance_threshold = 0.4  # Lower threshold to ensure we include more relevant chunks
                            
                            for i, result in enumerate(search_results):
                                # Skip results with None content
                                if "content" not in result or result["content"] is None:
                                    logger.warning(f"Skipping result {i+1} due to missing or None content")
                                    continue
                                    
                                # Extract metadata for better context
                                metadata = result["metadata"]
                                filename = metadata.get("filename", "Unknown")
                                tags = metadata.get("tags", [])
                                folder = metadata.get("folder", "/")
                                
                                # Calculate relevance score (lower distance = higher relevance)
                                relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
                                
                                # Log the relevance score for debugging
                                logger.debug(f"Chunk {i+1} relevance score: {relevance_score:.4f}")
                                
                                # Only include chunks that are sufficiently relevant
                                if relevance_score >= relevance_threshold:
                                    # Format the context piece with metadata
                                    context_piece = f"[{len(relevant_results)+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
                                    context_pieces.append(context_piece)
                                    
                                    # Track the source for citation
                                    doc_id = metadata["document_id"]
                                    document_ids.append(doc_id)
                                    
                                    source_info = {
                                        "document_id": doc_id,
                                        "chunk_id": result["chunk_id"],
                                        "relevance_score": relevance_score,
                                        "excerpt": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                                        "filename": filename,
                                        "tags": tags,
                                        "folder": folder
                                    }
                                    
                                    sources.append(source_info)
                                    
                                    # Store document interaction in Mem0 if available
                                    if self.mem0_client and user_id:
                                        await store_document_interaction(
                                            human_id=user_id,
                                            document_id=doc_id,
                                            interaction_type="retrieval",
                                            data={
                                                "query": query,
                                                "chunk_id": result["chunk_id"],
                                                "relevance_score": relevance_score,
                                                "filename": filename
                                            }
                                        )
                                    
                                    relevant_results.append(result)
                                else:
                                    logger.info(f"Skipping chunk {i+1} with low relevance score: {relevance_score:.4f}")
                            
                            # Join all context pieces
                            context = "\n\n".join(context_pieces)
                            
                            # Log how many chunks were filtered out due to low relevance
                            logger.info(f"Using {len(relevant_results)} of {len(search_results)} chunks after relevance filtering")
                            
                            # Log the total context length
                            logger.info(f"Total context length: {len(context)} characters")
                            
                            # Check if we have enough relevant context
                            if len(relevant_results) == 0:
                                logger.warning("No sufficiently relevant documents found for the query")
                                context = ""
                            elif len(context.strip()) < 50:  # Very short context might not be useful
                                logger.warning("Context is too short to be useful")
                                context = ""
                        else:
                            logger.warning("No relevant documents found for the query")
                            context = ""
            
            # Determine retrieval state based on the context
            retrieval_state = "success"
            if not use_rag:
                retrieval_state = "no_documents"
            elif not context or context.startswith("Note:"):
                # If context is empty or starts with a note, it means no relevant documents were found
                retrieval_state = "no_documents"
                # Reset context to empty string if it was a note
                if context and context.startswith("Note:"):
                    context = ""
            elif len(sources) == 0:
                # If no sources were found but context exists, it's low relevance
                retrieval_state = "low_relevance"
            
            logger.info(f"Determined retrieval state: {retrieval_state}")
            
            # Create system prompt and user prompt if not provided
            if not system_prompt:
                if self._is_code_related_query(query):
                    # For code queries, use the existing system prompt
                    system_prompt = self._create_system_prompt(query)
                    # Create a simple user prompt for code queries
                    full_prompt = f"User Question: {query}"
                else:
                    # For non-code queries, use the PromptManager
                    system_prompt, full_prompt = self._create_full_prompt(
                        query,
                        context,
                        conversation_context,
                        retrieval_state
                    )
            else:
                # If system prompt is provided, still use PromptManager for user prompt
                _, full_prompt = self._create_full_prompt(
                    query,
                    context,
                    conversation_context,
                    retrieval_state
                )
            
            # Log the prompt and system prompt for debugging
            logger.debug(f"System prompt: {system_prompt[:200]}...")
            logger.debug(f"Full prompt: {full_prompt[:200]}...")
            
            # Generate response
            if stream:
                # For streaming, return the stream generator directly
                logger.info(f"Generating streaming response with model: {model}")
                
                # Handle memory operations for streaming responses
                if memory_operation == "recall" and memory_response:
                    # For recall operations, we've already returned early
                    # This code should not be reached
                    pass
                elif memory_operation == "store" and memory_response:
                    # For store operations, we need to modify the prompt to include the memory confirmation
                    full_prompt = f"{full_prompt}\n\nAlso, include this confirmation in your response: {memory_response}"
                
                # Use the simplified streaming response method
                stream_response = self._generate_streaming_response(
                    prompt=full_prompt,
                    model=model,
                    system_prompt=system_prompt,
                    model_parameters=model_parameters or {}
                )
                
                # Record analytics asynchronously
                response_time_ms = (time.time() - start_time) * 1000
                logger.info(f"Response time: {response_time_ms:.2f}ms")
                
                # Use await instead of async for with the coroutine
                await self._record_analytics(
                    query=query,
                    model=model,
                    use_rag=use_rag,
                    response_time_ms=response_time_ms,
                    document_ids=document_ids,
                    token_count=len(query.split())  # Approximate token count
                )
                
                return {
                    "query": query,
                    "stream": stream_response,
                    "sources": [Citation(**source) for source in sources] if sources else []
                }
            else:
                # For non-streaming, generate the complete response
                logger.info(f"Generating non-streaming response with model: {model}")
                
                # Use the simplified complete response method
                response = await self.generate_complete_response(
                    prompt=full_prompt,
                    model=model,
                    system_prompt=system_prompt,
                    model_parameters=model_parameters or {}
                )
                
                # Calculate response time
                response_time_ms = (time.time() - start_time) * 1000
                logger.info(f"Response time: {response_time_ms:.2f}ms")
                
                # Process response text with optional normalization
                response_text = self._process_response_text(response)
                
                logger.info(f"Response length: {len(response_text)} characters")
                
                # Log a preview of the response
                if response_text:
                    logger.debug(f"Response preview: {response_text[:100]}...")
                
                # Handle memory operations in the response
                if memory_operation == "store" and memory_response:
                    # If we stored a memory, append the confirmation to the response
                    response_text = f"{response_text}\n\n{memory_response}"
                
                # Store assistant response in Mem0 if available
                if self.mem0_client and user_id:
                    await store_message(user_id, "assistant", response_text)
                    logger.info(f"Stored assistant response in Mem0 for user {user_id}")
                
                # Record analytics asynchronously
                await self._record_analytics(
                    query=query,
                    model=model,
                    use_rag=use_rag,
                    response_time_ms=response_time_ms,
                    document_ids=document_ids,
                    token_count=len(query.split()) + len(response_text.split())  # Approximate token count
                )
                
                return {
                    "query": query,
                    "answer": response_text,
                    "sources": [Citation(**source) for source in sources] if sources else []
                }
        except Exception as e:
            logger.error(f"Error querying RAG engine: {str(e)}")
            raise

================
File: app/rag/rag_generation.py
================
"""
RAG generation functionality
"""
import logging
import time
import re
from typing import Dict, Any, Optional, List, AsyncGenerator
from uuid import UUID

from app.core.config import DEFAULT_MODEL
from app.models.chat import Citation, Message
from app.rag.mem0_client import store_message
from app.utils.text_processor import normalize_text, format_code_blocks
from app.rag.prompt_manager import PromptManager
from app.rag.system_prompts import (
    CODE_GENERATION_SYSTEM_PROMPT,
    PYTHON_CODE_GENERATION_PROMPT,
    JAVASCRIPT_CODE_GENERATION_PROMPT
)

logger = logging.getLogger("app.rag.rag_generation")

class GenerationMixin:
    """
    Mixin class for RAG generation functionality
    """
    
    def __init__(self):
        """Initialize the GenerationMixin."""
        super().__init__()
        self.prompt_manager = PromptManager()
        logger.info("GenerationMixin initialized with PromptManager")
    
    async def _record_analytics(self,
                               query: str,
                               model: str,
                               use_rag: bool,
                               response_time_ms: float,
                               document_ids: List[str],
                               token_count: int) -> None:
        """
        Record query analytics asynchronously
        """
        try:
            # Prepare analytics data
            analytics_data = {
                "query": query,
                "model": model,
                "use_rag": use_rag,
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                "response_time_ms": response_time_ms,
                "document_ids": document_ids,
                "token_count": token_count
            }
            
            # Send analytics data to the API
            import httpx
            async with httpx.AsyncClient() as client:
                await client.post(
                    "http://localhost:8000/api/analytics/record_query",
                    json=analytics_data,
                    timeout=5.0
                )
            
            logger.debug(f"Recorded analytics for query: {query[:30]}...")
        except Exception as e:
            # Don't let analytics errors affect the main functionality
            logger.error(f"Error recording analytics: {str(e)}")
    
    async def _generate_streaming_response(self,
                                          prompt: str,
                                          model: str,
                                          system_prompt: str,
                                          model_parameters: Dict[str, Any]) -> AsyncGenerator[str, None]:
        """
        Generate a streaming response with minimal processing
        
        Args:
            prompt: Full prompt
            model: Model to use
            system_prompt: System prompt
            model_parameters: Model parameters
            
        Returns:
            Async generator of response tokens
        """
        # Get the raw stream from the LLM
        stream = await self.ollama_client.generate(
            prompt=prompt,
            model=model,
            system_prompt=system_prompt,
            stream=True,
            parameters=model_parameters or {}
        )
        
        # Stream tokens directly with minimal processing
        async for chunk in stream:
            # Handle string chunks
            if isinstance(chunk, str):
                yield chunk
            # Handle dictionary chunks (for backward compatibility)
            elif isinstance(chunk, dict) and "response" in chunk:
                yield chunk["response"]
            else:
                yield chunk
    
    def _create_system_prompt(self, query: str) -> str:
        """
        Create a system prompt based on the query
        
        Args:
            query: User query
            
        Returns:
            System prompt
        """
        # Check if this is a code-related query
        is_code_query = self._is_code_related_query(query)
        
        if is_code_query:
            logger.info("Detected code-related query, using code generation system prompt")
            system_prompt = CODE_GENERATION_SYSTEM_PROMPT
            
            # Add language-specific guidelines if detected
            if re.search(r'\bpython\b', query.lower()):
                system_prompt += "\n\n" + PYTHON_CODE_GENERATION_PROMPT
            elif re.search(r'\bjavascript\b|\bjs\b', query.lower()):
                system_prompt += "\n\n" + JAVASCRIPT_CODE_GENERATION_PROMPT
            
            return system_prompt
        
        # For non-code queries, we'll use the PromptManager later
        # This is just a placeholder that will be replaced
        return "PLACEHOLDER_SYSTEM_PROMPT"
    
    def _create_full_prompt(self,
                           query: str,
                           context: str = "",
                           conversation_context: str = "",
                           retrieval_state: str = "success") -> tuple[str, str]:
        """
        Create a full prompt with context and conversation history using the PromptManager
        
        Args:
            query: User query
            context: Retrieved context
            conversation_context: Conversation history
            retrieval_state: State of the retrieval process
            
        Returns:
            Tuple of (system_prompt, user_prompt)
        """
        # Convert conversation_context string to list of dicts if provided
        conversation_history = None
        if conversation_context:
            # Parse the conversation context string into a list of messages
            conversation_history = []
            lines = conversation_context.strip().split('\n')
            for line in lines:
                if line.startswith("User: "):
                    conversation_history.append({
                        "role": "user",
                        "content": line[6:]  # Remove "User: " prefix
                    })
                elif line.startswith("Assistant: "):
                    conversation_history.append({
                        "role": "assistant",
                        "content": line[11:]  # Remove "Assistant: " prefix
                    })
        
        # Use the PromptManager to create the prompt
        system_prompt, user_prompt = self.prompt_manager.create_prompt(
            query=query,
            retrieval_state=retrieval_state,
            context=context,
            conversation_history=conversation_history
        )
        
        logger.info(f"Created prompt with retrieval_state: {retrieval_state}")
        
        return system_prompt, user_prompt
    
    async def generate_complete_response(self,
                                        prompt: str,
                                        model: str,
                                        system_prompt: str,
                                        model_parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a complete response without streaming
        
        Args:
            prompt: Full prompt
            model: Model to use
            system_prompt: System prompt
            model_parameters: Model parameters
            
        Returns:
            Response dictionary
        """
        # Get cached or generate new response
        response = await self._get_cached_or_generate_response(
            prompt=prompt,
            model=model,
            system_prompt=system_prompt,
            model_parameters=model_parameters
        )
        
        return response
    
    async def _get_cached_or_generate_response(self,
                                              prompt: str,
                                              model: str,
                                              system_prompt: str,
                                              model_parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        Get a cached response or generate a new one
        
        Args:
            prompt: Full prompt
            model: Model to use
            system_prompt: System prompt
            model_parameters: Model parameters
            
        Returns:
            Response dictionary
        """
        # Create cache parameters
        temperature = model_parameters.get("temperature", 0.0) if model_parameters else 0.0
        max_tokens = model_parameters.get("max_tokens") if model_parameters else None
        
        # Check if response is in cache
        cached_response = self.cache_manager.llm_response_cache.get_response(
            prompt=prompt,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            additional_params={"system_prompt": system_prompt} if system_prompt else None
        )
        
        if cached_response:
            logger.info("Using cached response")
            response = cached_response
        else:
            # Generate new response
            logger.info("Cache miss, generating new response")
            response = await self.ollama_client.generate(
                prompt=prompt,
                model=model,
                system_prompt=system_prompt,
                stream=False,
                parameters=model_parameters or {}
            )
            
            # Cache the response if appropriate
            if "error" not in response and self.cache_manager.llm_response_cache.should_cache_response(
                prompt=prompt,
                model=model,
                temperature=temperature,
                response=response
            ):
                self.cache_manager.llm_response_cache.set_response(
                    prompt=prompt,
                    model=model,
                    response=response,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    additional_params={"system_prompt": system_prompt} if system_prompt else None
                )
                logger.info("Response cached for future use")
        
        return response
    
    def _process_response_text(self, response: Dict[str, Any]) -> str:
        """
        Process response text with normalization and formatting
        
        Args:
            response: Response dictionary
            
        Returns:
            Processed response text
        """
        # Check if there was an error in the response
        if "error" in response:
            error_message = response.get("error", "Unknown error")
            logger.warning(f"Model returned an error: {error_message}")
            return response.get("response", f"Error: {error_message}")
        
        # Get response text
        response_text = response.get("response", "")
        
        # Apply text normalization to improve formatting
        response_text = self.process_complete_response(response_text)
        
        return response_text
    def process_complete_response(self, response_text: str, apply_normalization: bool = True) -> str:
        """
        Process a complete response with optional normalization
        
        Args:
            response_text: The complete response text
            apply_normalization: Whether to apply text normalization
            
        Returns:
            Processed response text
        """
        if not apply_normalization:
            return response_text
        
        # Apply text normalization
        normalized_text = normalize_text(response_text)
        
        # Format code blocks
        formatted_text = format_code_blocks(normalized_text)
        
        return formatted_text
        return response_text

================
File: app/rag/rag_retrieval.py
================
"""
RAG retrieval functionality
"""
import logging
from typing import List, Dict, Any, Optional, Tuple
from uuid import UUID

from app.rag.rag_engine_base import BaseRAGEngine
from app.rag.mem0_client import store_document_interaction

logger = logging.getLogger("app.rag.rag_retrieval")

class RetrievalMixin:
    """
    Mixin class for RAG retrieval functionality
    """
    
    async def retrieve(self, 
                      query: str,
                      top_k: int = 5,
                      filters: Optional[Dict[str, Any]] = None,
                      user_id: Optional[UUID] = None) -> List[Dict[str, Any]]:
        """
        Retrieve relevant documents for a query with permission filtering
        
        Args:
            query: Query string
            top_k: Number of results to return
            filters: Additional filters to apply
            user_id: User ID for permission filtering (overrides the instance's user_id)
            
        Returns:
            List of relevant documents
        """
        try:
            # Use provided user_id or fall back to the instance's user_id
            effective_user_id = user_id or self.user_id
            
            # Log the retrieval request
            logger.info(f"Retrieving documents for query: {query[:50]}...")
            
            # Search for relevant documents with permission filtering
            search_results = await self.vector_store.search(
                query=query,
                top_k=top_k,
                filter_criteria=filters,
                user_id=effective_user_id
            )
            
            # Log the number of results
            logger.info(f"Retrieved {len(search_results)} documents")
            
            return search_results
        except Exception as e:
            logger.error(f"Error retrieving documents: {str(e)}")
            return []
    
    async def _enhanced_retrieval(self,
                                 query: str,
                                 conversation_context: str = "",
                                 top_k: int = 10,
                                 metadata_filters: Optional[Dict[str, Any]] = None,
                                 user_id: Optional[UUID] = None) -> Tuple[str, List[Dict[str, Any]], List[str]]:
        """
        Enhanced retrieval using the Retrieval Judge with permission filtering
        
        Args:
            query: The user query
            conversation_context: Optional conversation history context
            top_k: Number of chunks to retrieve
            metadata_filters: Optional filters for retrieval
            user_id: User ID for permission filtering
            
        Returns:
            Tuple of (context, sources, document_ids)
        """
        document_ids = []
        sources = []
        context = ""
        
        try:
            # Check if there are any documents in the vector store
            stats = self.vector_store.get_stats()
            if stats["count"] == 0:
                logger.warning("RAG is enabled but no documents are available in the vector store")
                return "", [], []
            
            # Step 1: Analyze the query using the Retrieval Judge
            logger.info("Analyzing query with Retrieval Judge")
            query_analysis = await self.retrieval_judge.analyze_query(query)
            
            # Extract recommended parameters
            recommended_k = query_analysis.get("parameters", {}).get("k", top_k)
            relevance_threshold = query_analysis.get("parameters", {}).get("threshold", 0.4)
            apply_reranking = query_analysis.get("parameters", {}).get("reranking", True)
            
            logger.info(f"Query complexity: {query_analysis.get('complexity', 'unknown')}")
            logger.info(f"Recommended parameters: k={recommended_k}, threshold={relevance_threshold}, reranking={apply_reranking}")
            
            # Combine the current query with conversation context for better retrieval
            search_query = query
            if conversation_context:
                # For retrieval, we focus more on the current query but include
                # some context from the conversation to improve relevance
                search_query = f"{query} {conversation_context[-200:]}"
            
            # Log the search query
            logger.info(f"Searching with query: {search_query[:100]}...")
            
            # Step 2: Initial retrieval with recommended parameters
            search_results = await self.vector_store.search(
                query=search_query,
                top_k=max(15, recommended_k + 5),  # Get a few extra for filtering
                filter_criteria=metadata_filters,
                user_id=user_id  # Pass user_id for permission filtering
            )
            
            if not search_results:
                logger.warning("No relevant documents found for the query")
                return "", [], []
            
            # Log the number of results
            logger.info(f"Retrieved {len(search_results)} chunks from vector store")
            
            # Step 3: Evaluate chunks with the Retrieval Judge
            logger.info("Evaluating chunks with Retrieval Judge")
            evaluation = await self.retrieval_judge.evaluate_chunks(query, search_results)
            
            # Extract relevance scores and refinement decision
            relevance_scores = evaluation.get("relevance_scores", {})
            needs_refinement = evaluation.get("needs_refinement", False)
            
            logger.info(f"Chunk evaluation complete, needs_refinement={needs_refinement}")
            
            # Step 4: Refine query if needed and perform additional retrieval
            if needs_refinement:
                logger.info("Refining query based on initial retrieval")
                refined_query = await self.retrieval_judge.refine_query(query, search_results)
                
                logger.info(f"Refined query: {refined_query}")
                
                # Perform additional retrieval with refined query
                additional_results = await self.vector_store.search(
                    query=refined_query,
                    top_k=recommended_k,
                    filter_criteria=metadata_filters,
                    user_id=user_id  # Pass user_id for permission filtering
                )
                
                if additional_results:
                    logger.info(f"Retrieved {len(additional_results)} additional chunks with refined query")
                    
                    # Combine results, avoiding duplicates
                    existing_chunk_ids = {result["chunk_id"] for result in search_results}
                    for result in additional_results:
                        if result["chunk_id"] not in existing_chunk_ids:
                            search_results.append(result)
                    
                    # Re-evaluate all chunks
                    logger.info("Re-evaluating all chunks after query refinement")
                    evaluation = await self.retrieval_judge.evaluate_chunks(refined_query, search_results)
                    relevance_scores = evaluation.get("relevance_scores", {})
            
            # Step 5: Filter and re-rank chunks based on relevance scores
            relevant_results = []
            
            for result in search_results:
                # Skip results with None content
                if "content" not in result or result["content"] is None:
                    continue
                
                chunk_id = result["chunk_id"]
                
                # Get relevance score from evaluation or calculate from distance
                if chunk_id in relevance_scores:
                    relevance_score = relevance_scores[chunk_id]
                else:
                    # Calculate relevance score (lower distance = higher relevance)
                    relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
                
                # Only include chunks that are sufficiently relevant
                if relevance_score >= relevance_threshold:
                    # Add relevance score to result for sorting
                    result["relevance_score"] = relevance_score
                    relevant_results.append(result)
            
            # Sort by relevance score if reranking is enabled
            if apply_reranking:
                relevant_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
            
            # Step 6: Format context with source information
            context_pieces = []
            
            for i, result in enumerate(relevant_results):
                # Extract metadata for better context
                metadata = result["metadata"]
                filename = metadata.get("filename", "Unknown")
                tags = metadata.get("tags", [])
                folder = metadata.get("folder", "/")
                
                # Format the context piece with metadata
                context_piece = f"[{i+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
                context_pieces.append(context_piece)
                
                # Track the source for citation
                doc_id = metadata["document_id"]
                document_ids.append(doc_id)
                
                # Get relevance score (either from judge or distance)
                relevance_score = result.get("relevance_score", 1.0 - (result["distance"] if result["distance"] is not None else 0))
                
                source_info = {
                    "document_id": doc_id,
                    "chunk_id": result["chunk_id"],
                    "relevance_score": relevance_score,
                    "excerpt": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                    "filename": filename,
                    "tags": tags,
                    "folder": folder
                }
                
                sources.append(source_info)
                
                # Store document interaction in Mem0 if available
                if self.mem0_client and user_id:
                    await store_document_interaction(
                        human_id=str(user_id),
                        document_id=doc_id,
                        interaction_type="retrieval",
                        data={
                            "query": query,
                            "chunk_id": result["chunk_id"],
                            "relevance_score": relevance_score,
                            "filename": filename
                        }
                    )
            
            # Join all context pieces
            context = "\n\n".join(context_pieces)
            
            # Log how many chunks were used
            logger.info(f"Using {len(relevant_results)} chunks after Retrieval Judge optimization")
            
            # Log the total context length
            logger.info(f"Total context length: {len(context)} characters")
            
            # Check if we have enough relevant context
            if len(relevant_results) == 0:
                logger.warning("No sufficiently relevant documents found for the query")
                context = ""
            elif len(context.strip()) < 50:  # Very short context might not be useful
                logger.warning("Context is too short to be useful")
                context = ""
            
            return context, sources, document_ids
            
        except Exception as e:
            logger.error(f"Error in enhanced retrieval: {str(e)}")
            # Return empty context in case of error
            return "", [], []

================
File: app/rag/README_MEM0_INTEGRATION.md
================
# Mem0 Integration for Metis RAG

This document describes how to set up and use the Mem0 integration for Metis RAG.

## Overview

Mem0 is a memory layer for AI applications that provides a way to store and retrieve information related to users, sessions, and documents. It enables more personalized and context-aware interactions in the Metis RAG system.

The integration provides the following features:

- Conversation history storage and retrieval
- User preferences storage and retrieval
- Document interaction tracking
- Enhanced context for RAG queries

## Setup

### 1. Start Mem0 Server

The easiest way to run Mem0 is using Docker Compose:

```bash
cd config
docker-compose -f docker-compose.mem0.yml up -d
```

This will start Mem0 and its required PostgreSQL database in containers. The Mem0 server will be accessible at http://localhost:8050.

### 2. Configure Metis RAG

Update your `.env` file to include the Mem0 configuration:

```
# Mem0 settings
MEM0_ENDPOINT=http://localhost:8050
USE_MEM0=True
```

The API key is optional for local development.

### 3. Restart Metis RAG

Restart your Metis RAG application to apply the changes.

## Usage

The Mem0 integration is used automatically by the RAG engine when enabled. It provides the following functionality:

### Conversation History

Conversation history is stored in Mem0's recall memory. This allows the system to maintain context across sessions and provide more coherent responses.

### User Preferences

User preferences are stored in Mem0's archival memory. This allows the system to personalize responses based on user preferences, such as preferred models or response styles.

### Document Interactions

Document interactions are stored in Mem0's archival memory. This allows the system to track which documents a user has interacted with and provide more relevant responses.

## API

The Mem0 integration provides the following API:

### `get_mem0_client()`

Get the Mem0 client instance.

### `store_message(human_id, role, content)`

Store a message in recall memory.

### `get_conversation_history(human_id, limit=10)`

Get conversation history from recall memory.

### `store_user_preferences(human_id, preferences)`

Store user preferences in archival memory.

### `get_user_preferences(human_id)`

Get user preferences from archival memory.

### `store_document_interaction(human_id, document_id, interaction_type, data)`

Store document interaction in archival memory.

### `get_document_interactions(human_id, document_id=None, interaction_type=None, limit=10)`

Get document interactions from archival memory.

## Troubleshooting

If you encounter issues with the Mem0 integration, check the following:

1. Make sure the Mem0 server is running and accessible at the configured endpoint.
2. Check the logs for any error messages related to Mem0.
3. Make sure the `USE_MEM0` setting is set to `True` in your `.env` file.
4. If you're using an API key, make sure it's correctly configured.

## References

- [Mem0 Documentation](https://docs.mem0.ai)
- [Mem0 GitHub Repository](https://github.com/mem0ai/mem0)

================
File: app/rag/README_RESPONSE_QUALITY.md
================
# Response Quality Components

This directory contains components for improving the quality of responses generated by the Metis RAG system. These components work together to synthesize, evaluate, refine, and audit responses to ensure they are accurate, complete, relevant, and free from hallucinations.

## Components

### ResponseSynthesizer

The `ResponseSynthesizer` is responsible for combining retrieval results and tool outputs into coherent, well-structured responses with proper source attribution.

**Key Features:**
- Combines context from retrieved documents with execution results
- Adds proper source attribution using the [n] citation format
- Formats responses with appropriate structure
- Optimizes context assembly for better responses

### ResponseEvaluator

The `ResponseEvaluator` assesses the quality of responses based on multiple criteria to ensure they meet the required standards.

**Key Features:**
- Evaluates factual accuracy (0-10 scale)
- Assesses completeness in addressing the query (0-10 scale)
- Measures relevance to the original query (0-10 scale)
- Detects hallucinations (statements not supported by the context)
- Provides an overall quality score (0-10 scale)
- Identifies strengths and weaknesses
- Suggests improvements

### ResponseRefiner

The `ResponseRefiner` improves responses based on evaluation results to address identified issues and enhance quality.

**Key Features:**
- Iterative refinement process
- Addresses factual inaccuracies
- Improves completeness by adding missing information
- Enhances relevance by focusing on the query
- Removes hallucinations
- Improves overall structure and clarity

### AuditReportGenerator

The `AuditReportGenerator` creates comprehensive audit reports that provide transparency and accountability for the RAG process.

**Key Features:**
- Tracks information sources used in responses
- Extracts reasoning traces from the process
- Determines verification status of responses
- Creates execution timelines
- Provides quality metrics
- Generates LLM-based analysis of the process

### ResponseQualityPipeline

The `ResponseQualityPipeline` integrates all the above components into a cohesive pipeline for generating high-quality responses.

**Key Features:**
- End-to-end pipeline for response quality
- Configurable quality thresholds
- Iterative refinement until quality standards are met
- Comprehensive logging and auditing
- Integration with the existing RAG system

## Usage

### Basic Usage

```python
from app.rag.response_quality_pipeline import ResponseQualityPipeline
from app.rag.process_logger import ProcessLogger
from app.rag.ollama_client import OllamaClient

# Initialize components
ollama_client = OllamaClient()
process_logger = ProcessLogger(log_dir="data/process_logs")

# Create the pipeline
pipeline = ResponseQualityPipeline(
    llm_provider=ollama_client,
    process_logger=process_logger,
    max_refinement_iterations=2,
    quality_threshold=8.0,
    enable_audit_reports=True
)

# Process a query
result = await pipeline.process(
    query="What is the capital of France?",
    context="[1] Source: geography.txt, Tags: [geography, europe], Folder: /\n\nParis is the capital of France.",
    sources=[
        {
            "document_id": "doc1",
            "chunk_id": "chunk1",
            "relevance_score": 0.95,
            "excerpt": "Paris is the capital of France.",
            "filename": "geography.txt",
            "tags": ["geography", "europe"],
            "folder": "/"
        }
    ]
)

# Access the results
response = result["response"]
evaluation = result["evaluation"]
refinement_iterations = result["refinement_iterations"]
audit_report = result["audit_report"]
```

### Integration with RAG Engine

To integrate the response quality pipeline with the existing RAG engine, you can enhance the RAG engine's query method:

```python
from app.rag.rag_engine import RAGEngine
from app.rag.response_quality_pipeline import ResponseQualityPipeline

# Create the RAG engine and pipeline
rag_engine = RAGEngine(...)
pipeline = ResponseQualityPipeline(...)

# Enhanced query method
async def enhanced_query(query, **kwargs):
    # Get the standard RAG result
    rag_result = await rag_engine.query(query, **kwargs)
    
    # Extract components
    response = rag_result.get("answer", "")
    sources = rag_result.get("sources", [])
    context = "..." # Extract context from RAG result
    
    # Process through the quality pipeline
    quality_result = await pipeline.process(
        query=query,
        context=context,
        sources=sources,
        conversation_context=kwargs.get("conversation_history")
    )
    
    # Return enhanced result
    return {
        "query": query,
        "answer": quality_result["response"],
        "sources": sources,
        "evaluation": quality_result["evaluation"],
        "quality_score": quality_result["evaluation"]["overall_score"]
    }
```

## Configuration Options

### ResponseSynthesizer

- `llm_provider`: LLM provider for generating responses
- `process_logger`: ProcessLogger instance for logging (optional)

### ResponseEvaluator

- `llm_provider`: LLM provider for evaluation
- `process_logger`: ProcessLogger instance for logging (optional)

### ResponseRefiner

- `llm_provider`: LLM provider for refinement
- `process_logger`: ProcessLogger instance for logging (optional)
- `max_refinement_iterations`: Maximum number of refinement iterations

### AuditReportGenerator

- `process_logger`: ProcessLogger instance for accessing process logs
- `llm_provider`: LLM provider for additional analysis (optional)

### ResponseQualityPipeline

- `llm_provider`: LLM provider for all components
- `process_logger`: ProcessLogger instance for logging (optional)
- `max_refinement_iterations`: Maximum number of refinement iterations
- `quality_threshold`: Minimum quality score to accept a response (0-10)
- `enable_audit_reports`: Whether to generate audit reports

## Testing

Unit tests for the response quality components are available in `tests/unit/test_response_quality.py`.

Integration tests demonstrating how to use the components with the RAG engine are available in `tests/integration/test_response_quality_integration.py`.

To run the tests:

```bash
pytest tests/unit/test_response_quality.py -v
pytest tests/integration/test_response_quality_integration.py -v
```

## LangGraph Integration

The response quality components are designed to integrate with the LangGraph workflow. The `langgraph_states.py` file has been updated to include the following states:

- `ResponseEvaluationState`: State for response evaluation
- `ResponseRefinementState`: State for response refinement
- `AuditReportState`: State for audit report generation

These states can be used to incorporate response quality into the LangGraph workflow.

================
File: app/rag/response_evaluator.py
================
"""
ResponseEvaluator - Evaluates the quality of synthesized responses
"""
import logging
import time
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

class ResponseEvaluator:
    """
    Evaluates the quality of synthesized responses
    
    The ResponseEvaluator is responsible for assessing the quality of responses generated
    by the ResponseSynthesizer. It evaluates factual accuracy, completeness, relevance,
    and other quality metrics to ensure that the responses meet the required standards.
    """
    
    def __init__(
        self,
        llm_provider,
        process_logger = None
    ):
        """
        Initialize the response evaluator
        
        Args:
            llm_provider: LLM provider for evaluation
            process_logger: ProcessLogger instance (optional)
        """
        self.llm_provider = llm_provider
        self.process_logger = process_logger
        self.logger = logging.getLogger("app.rag.response_evaluator")
    
    async def evaluate(
        self,
        query: str,
        query_id: str,
        response: str,
        context: str,
        sources: List[Dict[str, Any]],
        execution_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate the quality of a response
        
        Args:
            query: Original user query
            query_id: Unique query ID
            response: Synthesized response to evaluate
            context: Retrieved context from documents
            sources: List of source information for citation
            execution_result: Result of plan execution (optional)
            
        Returns:
            Dictionary containing evaluation results:
                - factual_accuracy: Score for factual accuracy (0-10)
                - completeness: Score for completeness (0-10)
                - relevance: Score for relevance to the query (0-10)
                - hallucination_detected: Whether hallucinations were detected
                - hallucination_details: Details about any hallucinations
                - overall_score: Overall quality score (0-10)
                - strengths: List of response strengths
                - weaknesses: List of response weaknesses
                - improvement_suggestions: Suggestions for improvement
        """
        start_time = time.time()
        self.logger.info(f"Evaluating response for query: {query}")
        
        # Log the start of response evaluation
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name="response_evaluation_start",
                step_data={
                    "query": query,
                    "response_length": len(response),
                    "context_length": len(context),
                    "sources_count": len(sources)
                }
            )
        
        # Create the evaluation prompt
        prompt = self._create_evaluation_prompt(
            query=query,
            response=response,
            context=context,
            sources=sources,
            execution_result=execution_result
        )
        
        # Create the system prompt
        system_prompt = self._create_system_prompt()
        
        try:
            # Generate the evaluation using the LLM
            eval_response = await self.llm_provider.generate(
                prompt=prompt,
                system_prompt=system_prompt
            )
            
            # Parse the evaluation results
            evaluation = self._parse_evaluation(eval_response.get("response", ""))
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"Response evaluation completed in {elapsed_time:.2f}s. Overall score: {evaluation.get('overall_score')}")
            
            # Log the completion of response evaluation
            if self.process_logger:
                self.process_logger.log_step(
                    query_id=query_id,
                    step_name="response_evaluation_complete",
                    step_data={
                        "evaluation": evaluation,
                        "execution_time": elapsed_time
                    }
                )
            
            # Add execution time to the evaluation results
            evaluation["execution_time"] = elapsed_time
            
            return evaluation
        except Exception as e:
            self.logger.error(f"Error evaluating response: {str(e)}")
            
            # Log the error
            if self.process_logger:
                self.process_logger.log_step(
                    query_id=query_id,
                    step_name="response_evaluation_error",
                    step_data={
                        "error": str(e)
                    }
                )
            
            # Return a default evaluation with error information
            return {
                "factual_accuracy": 0,
                "completeness": 0,
                "relevance": 0,
                "hallucination_detected": True,
                "hallucination_details": f"Evaluation failed: {str(e)}",
                "overall_score": 0,
                "strengths": [],
                "weaknesses": [f"Evaluation failed: {str(e)}"],
                "improvement_suggestions": ["Unable to provide suggestions due to evaluation failure"],
                "execution_time": time.time() - start_time
            }
    
    def _create_evaluation_prompt(
        self,
        query: str,
        response: str,
        context: str,
        sources: List[Dict[str, Any]],
        execution_result: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Create a prompt for response evaluation
        
        Args:
            query: Original user query
            response: Synthesized response to evaluate
            context: Retrieved context from documents
            sources: List of source information for citation
            execution_result: Result of plan execution (optional)
            
        Returns:
            Evaluation prompt
        """
        prompt = f"""
You are evaluating the quality of a response to the following query:

USER QUERY: {query}

RESPONSE TO EVALUATE:
{response}

RETRIEVED CONTEXT USED FOR GENERATING THE RESPONSE:
{context}

"""
        
        # Add execution result if available
        if execution_result:
            prompt += f"""
EXECUTION RESULTS:
{json.dumps(execution_result, indent=2)}

"""
        
        # Add source information
        if sources:
            prompt += f"""
SOURCE INFORMATION:
{json.dumps(sources, indent=2)}

"""
        
        # Add instructions for evaluation
        prompt += """
EVALUATION INSTRUCTIONS:
Please evaluate the response based on the following criteria:

1. Factual Accuracy (0-10):
   - Does the response contain only information that is supported by the context?
   - Are all citations [n] correctly used and do they reference relevant information?
   - Are there any statements that contradict the provided context?

2. Completeness (0-10):
   - Does the response fully address the user's query?
   - Are there important aspects of the query that were not addressed?
   - Does the response include all relevant information from the context?

3. Relevance (0-10):
   - How directly does the response address the user's query?
   - Is there irrelevant information included in the response?
   - Is the response focused on what the user was asking about?

4. Hallucination Detection:
   - Identify any statements in the response that are not supported by the provided context.
   - For each potential hallucination, provide the statement and explain why it's not supported.

5. Overall Quality (0-10):
   - Considering all factors, what is the overall quality of the response?
   - Is the response well-structured and easy to understand?
   - Does it provide value to the user?

6. Strengths and Weaknesses:
   - List the main strengths of the response.
   - List the main weaknesses of the response.

7. Improvement Suggestions:
   - Provide specific suggestions for how the response could be improved.

FORMAT YOUR EVALUATION AS FOLLOWS:
```json
{
  "factual_accuracy": <score 0-10>,
  "completeness": <score 0-10>,
  "relevance": <score 0-10>,
  "hallucination_detected": <true/false>,
  "hallucination_details": "<details about any hallucinations>",
  "overall_score": <score 0-10>,
  "strengths": ["<strength 1>", "<strength 2>", ...],
  "weaknesses": ["<weakness 1>", "<weakness 2>", ...],
  "improvement_suggestions": ["<suggestion 1>", "<suggestion 2>", ...]
}
```

IMPORTANT: Your evaluation must be fair, objective, and based solely on the provided context and query. The evaluation must be returned in the exact JSON format specified above.
"""
        
        return prompt
    
    def _create_system_prompt(self) -> str:
        """
        Create a system prompt for response evaluation
        
        Returns:
            System prompt
        """
        return """You are a response evaluator for a Retrieval-Augmented Generation (RAG) system.

Your role is to critically evaluate responses based on factual accuracy, completeness, relevance, and overall quality.

GUIDELINES:
1. Be objective and fair in your evaluation.
2. Base your assessment solely on the provided context, sources, and query.
3. Check for hallucinations - statements that are not supported by the provided context.
4. Verify that citations are used correctly and reference relevant information.
5. Assess whether the response fully addresses the user's query.
6. Evaluate the structure and clarity of the response.
7. Provide constructive feedback and specific suggestions for improvement.
8. Use the full range of scores (0-10) appropriately:
   - 0-2: Very poor, major issues
   - 3-4: Poor, significant issues
   - 5-6: Average, some issues
   - 7-8: Good, minor issues
   - 9-10: Excellent, minimal to no issues
9. Format your evaluation in the exact JSON format specified in the prompt.
10. Be thorough and detailed in your evaluation, especially when identifying hallucinations or weaknesses.
"""
    
    def _parse_evaluation(self, evaluation_text: str) -> Dict[str, Any]:
        """
        Parse the evaluation response from the LLM
        
        Args:
            evaluation_text: Raw evaluation text from the LLM
            
        Returns:
            Parsed evaluation results
        """
        # Extract JSON from the response
        try:
            # Look for JSON block in markdown format
            json_match = re.search(r'```(?:json)?\s*({\s*".*})\s*```', evaluation_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without markdown formatting
                json_match = re.search(r'({[\s\S]*"improvement_suggestions"[\s\S]*})', evaluation_text)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Fallback: assume the entire text might be JSON
                    json_str = evaluation_text
            
            # Parse the JSON
            evaluation = json.loads(json_str)
            
            # Ensure all required fields are present
            required_fields = [
                "factual_accuracy", "completeness", "relevance", 
                "hallucination_detected", "hallucination_details", 
                "overall_score", "strengths", "weaknesses", 
                "improvement_suggestions"
            ]
            
            for field in required_fields:
                if field not in evaluation:
                    if field in ["strengths", "weaknesses", "improvement_suggestions"]:
                        evaluation[field] = []
                    elif field in ["factual_accuracy", "completeness", "relevance", "overall_score"]:
                        evaluation[field] = 0
                    elif field == "hallucination_detected":
                        evaluation[field] = True
                    else:
                        evaluation[field] = "Not provided"
            
            return evaluation
        except Exception as e:
            self.logger.error(f"Error parsing evaluation: {str(e)}")
            
            # Return a default evaluation
            return {
                "factual_accuracy": 0,
                "completeness": 0,
                "relevance": 0,
                "hallucination_detected": True,
                "hallucination_details": f"Failed to parse evaluation: {str(e)}",
                "overall_score": 0,
                "strengths": [],
                "weaknesses": [f"Failed to parse evaluation: {str(e)}"],
                "improvement_suggestions": ["Unable to provide suggestions due to parsing failure"]
            }

================
File: app/rag/response_quality_pipeline.py
================
"""
ResponseQualityPipeline - Integrates response quality components into a pipeline
"""
import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

from app.rag.response_synthesizer import ResponseSynthesizer
from app.rag.response_evaluator import ResponseEvaluator
from app.rag.response_refiner import ResponseRefiner
from app.rag.audit_report_generator import AuditReportGenerator
from app.rag.process_logger import ProcessLogger

class ResponseQualityPipeline:
    """
    Integrates response quality components into a pipeline
    
    The ResponseQualityPipeline combines the ResponseSynthesizer, ResponseEvaluator,
    ResponseRefiner, and AuditReportGenerator into a cohesive pipeline for generating
    high-quality responses with proper evaluation, refinement, and auditing.
    """
    
    def __init__(
        self,
        llm_provider,
        process_logger: Optional[ProcessLogger] = None,
        max_refinement_iterations: int = 2,
        quality_threshold: float = 8.0,
        enable_audit_reports: bool = True
    ):
        """
        Initialize the response quality pipeline
        
        Args:
            llm_provider: LLM provider for generating responses
            process_logger: ProcessLogger instance (optional)
            max_refinement_iterations: Maximum number of refinement iterations
            quality_threshold: Minimum quality score to accept a response (0-10)
            enable_audit_reports: Whether to generate audit reports
        """
        self.llm_provider = llm_provider
        self.process_logger = process_logger
        self.max_refinement_iterations = max_refinement_iterations
        self.quality_threshold = quality_threshold
        self.enable_audit_reports = enable_audit_reports
        
        # Initialize components
        self.synthesizer = ResponseSynthesizer(
            llm_provider=llm_provider,
            process_logger=process_logger
        )
        
        self.evaluator = ResponseEvaluator(
            llm_provider=llm_provider,
            process_logger=process_logger
        )
        
        self.refiner = ResponseRefiner(
            llm_provider=llm_provider,
            process_logger=process_logger,
            max_refinement_iterations=max_refinement_iterations
        )
        
        if enable_audit_reports and process_logger:
            self.audit_report_generator = AuditReportGenerator(
                process_logger=process_logger,
                llm_provider=llm_provider
            )
        else:
            self.audit_report_generator = None
        
        self.logger = logging.getLogger("app.rag.response_quality_pipeline")
    
    async def process(
        self,
        query: str,
        context: str,
        sources: List[Dict[str, Any]],
        execution_result: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        system_prompt: Optional[str] = None,
        model_parameters: Optional[Dict[str, Any]] = None,
        query_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process a query through the response quality pipeline
        
        Args:
            query: User query
            context: Retrieved context from documents
            sources: List of source information for citation
            execution_result: Result of plan execution (optional)
            conversation_context: Conversation history (optional)
            system_prompt: Custom system prompt (optional)
            model_parameters: Custom model parameters (optional)
            query_id: Unique query ID (optional, will be generated if not provided)
            
        Returns:
            Dictionary containing:
                - response: Final response text
                - sources: List of sources used in the response
                - evaluation: Evaluation results
                - audit_report: Audit report (if enabled)
                - execution_time: Total execution time
        """
        start_time = time.time()
        
        # Generate a query ID if not provided
        if not query_id:
            query_id = str(uuid.uuid4())
        
        # Start process logging
        if self.process_logger:
            self.process_logger.start_process(query_id=query_id, query=query)
            self.process_logger.log_step(
                query_id=query_id,
                step_name="response_quality_pipeline_start",
                step_data={
                    "query": query,
                    "context_length": len(context),
                    "sources_count": len(sources),
                    "has_execution_result": execution_result is not None,
                    "has_conversation_context": conversation_context is not None
                }
            )
        
        self.logger.info(f"Starting response quality pipeline for query: {query}")
        
        # Step 1: Synthesize initial response
        synthesis_result = await self.synthesizer.synthesize(
            query=query,
            query_id=query_id,
            context=context,
            sources=sources,
            execution_result=execution_result,
            conversation_context=conversation_context,
            system_prompt=system_prompt,
            model_parameters=model_parameters
        )
        
        response = synthesis_result["response"]
        used_sources = synthesis_result["sources"]
        
        self.logger.info(f"Initial response synthesized, length: {len(response)}")
        
        # Step 2: Evaluate the response
        evaluation_result = await self.evaluator.evaluate(
            query=query,
            query_id=query_id,
            response=response,
            context=context,
            sources=sources,
            execution_result=execution_result
        )
        
        overall_score = evaluation_result.get("overall_score", 0)
        hallucination_detected = evaluation_result.get("hallucination_detected", False)
        
        self.logger.info(f"Response evaluated, overall score: {overall_score}, hallucinations: {hallucination_detected}")
        
        # Step 3: Refine the response if needed
        current_response = response
        current_evaluation = evaluation_result
        refinement_iterations = 0
        
        # Refine if the quality is below threshold or hallucinations are detected
        if overall_score < self.quality_threshold or hallucination_detected:
            self.logger.info(f"Response quality below threshold ({overall_score} < {self.quality_threshold}) or hallucinations detected, refining...")
            
            # Iterative refinement
            for iteration in range(1, self.max_refinement_iterations + 1):
                refinement_result = await self.refiner.refine(
                    query=query,
                    query_id=query_id,
                    response=current_response,
                    evaluation=current_evaluation,
                    context=context,
                    sources=sources,
                    execution_result=execution_result,
                    iteration=iteration
                )
                
                # Update current response and re-evaluate
                current_response = refinement_result["refined_response"]
                refinement_iterations += 1
                
                # Re-evaluate the refined response
                current_evaluation = await self.evaluator.evaluate(
                    query=query,
                    query_id=query_id,
                    response=current_response,
                    context=context,
                    sources=sources,
                    execution_result=execution_result
                )
                
                new_score = current_evaluation.get("overall_score", 0)
                new_hallucination = current_evaluation.get("hallucination_detected", False)
                
                self.logger.info(f"Refinement iteration {iteration}, new score: {new_score}, hallucinations: {new_hallucination}")
                
                # Stop if quality is good enough
                if new_score >= self.quality_threshold and not new_hallucination:
                    self.logger.info(f"Refinement successful after {iteration} iterations")
                    break
        
        # Step 4: Generate audit report if enabled
        audit_report = None
        if self.enable_audit_reports and self.audit_report_generator and self.process_logger:
            try:
                self.logger.info(f"Generating audit report for query {query_id}")
                audit_report = await self.audit_report_generator.generate_report(
                    query_id=query_id,
                    include_llm_analysis=True
                )
            except Exception as e:
                self.logger.error(f"Error generating audit report: {str(e)}")
        
        # Log the final response
        if self.process_logger:
            self.process_logger.log_final_response(
                query_id=query_id,
                response=current_response,
                metadata={
                    "evaluation_score": current_evaluation.get("overall_score", 0),
                    "refinement_iterations": refinement_iterations,
                    "sources_count": len(used_sources)
                }
            )
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Response quality pipeline completed in {elapsed_time:.2f}s")
        
        # Log the completion of the pipeline
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name="response_quality_pipeline_complete",
                step_data={
                    "response_length": len(current_response),
                    "final_score": current_evaluation.get("overall_score", 0),
                    "refinement_iterations": refinement_iterations,
                    "execution_time": elapsed_time
                }
            )
        
        # Return the final result
        return {
            "query_id": query_id,
            "response": current_response,
            "sources": used_sources,
            "evaluation": current_evaluation,
            "refinement_iterations": refinement_iterations,
            "audit_report": audit_report,
            "execution_time": elapsed_time
        }

================
File: app/rag/response_refiner.py
================
"""
ResponseRefiner - Refines responses based on evaluation results
"""
import logging
import time
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

class ResponseRefiner:
    """
    Refines responses based on evaluation results
    
    The ResponseRefiner is responsible for improving responses based on the evaluation
    results from the ResponseEvaluator. It addresses issues such as factual inaccuracies,
    incompleteness, irrelevance, and hallucinations to produce higher-quality responses.
    """
    
    def __init__(
        self,
        llm_provider,
        process_logger = None,
        max_refinement_iterations: int = 3
    ):
        """
        Initialize the response refiner
        
        Args:
            llm_provider: LLM provider for refinement
            process_logger: ProcessLogger instance (optional)
            max_refinement_iterations: Maximum number of refinement iterations
        """
        self.llm_provider = llm_provider
        self.process_logger = process_logger
        self.max_refinement_iterations = max_refinement_iterations
        self.logger = logging.getLogger("app.rag.response_refiner")
    
    async def refine(
        self,
        query: str,
        query_id: str,
        response: str,
        evaluation: Dict[str, Any],
        context: str,
        sources: List[Dict[str, Any]],
        execution_result: Optional[Dict[str, Any]] = None,
        iteration: int = 1
    ) -> Dict[str, Any]:
        """
        Refine a response based on evaluation results
        
        Args:
            query: Original user query
            query_id: Unique query ID
            response: Original response to refine
            evaluation: Evaluation results from ResponseEvaluator
            context: Retrieved context from documents
            sources: List of source information for citation
            execution_result: Result of plan execution (optional)
            iteration: Current refinement iteration
            
        Returns:
            Dictionary containing:
                - refined_response: Refined response text
                - improvement_summary: Summary of improvements made
                - execution_time: Time taken to refine the response
                - iteration: Current refinement iteration
        """
        start_time = time.time()
        self.logger.info(f"Refining response for query: {query} (iteration {iteration})")
        
        # Check if we've reached the maximum number of iterations
        if iteration > self.max_refinement_iterations:
            self.logger.warning(f"Maximum refinement iterations ({self.max_refinement_iterations}) reached")
            return {
                "refined_response": response,
                "improvement_summary": "Maximum refinement iterations reached. No further improvements made.",
                "execution_time": 0,
                "iteration": iteration
            }
        
        # Check if the response already has a high score and doesn't need refinement
        overall_score = evaluation.get("overall_score", 0)
        if overall_score >= 9:
            self.logger.info(f"Response already has a high score ({overall_score}). No refinement needed.")
            return {
                "refined_response": response,
                "improvement_summary": "Response already meets quality standards. No refinement needed.",
                "execution_time": 0,
                "iteration": iteration
            }
        
        # Log the start of response refinement
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name=f"response_refinement_start_{iteration}",
                step_data={
                    "query": query,
                    "response_length": len(response),
                    "evaluation_score": overall_score,
                    "iteration": iteration
                }
            )
        
        # Create the refinement prompt
        prompt = self._create_refinement_prompt(
            query=query,
            response=response,
            evaluation=evaluation,
            context=context,
            sources=sources,
            execution_result=execution_result,
            iteration=iteration
        )
        
        # Create the system prompt
        system_prompt = self._create_system_prompt()
        
        try:
            # Generate the refined response using the LLM
            refinement_response = await self.llm_provider.generate(
                prompt=prompt,
                system_prompt=system_prompt
            )
            
            # Parse the refinement results
            refinement = self._parse_refinement(refinement_response.get("response", ""))
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"Response refinement completed in {elapsed_time:.2f}s (iteration {iteration})")
            
            # Log the completion of response refinement
            if self.process_logger:
                self.process_logger.log_step(
                    query_id=query_id,
                    step_name=f"response_refinement_complete_{iteration}",
                    step_data={
                        "refined_response_length": len(refinement.get("refined_response", "")),
                        "improvement_summary": refinement.get("improvement_summary", ""),
                        "execution_time": elapsed_time,
                        "iteration": iteration
                    }
                )
            
            # Add execution time to the refinement results
            refinement["execution_time"] = elapsed_time
            refinement["iteration"] = iteration
            
            return refinement
        except Exception as e:
            self.logger.error(f"Error refining response: {str(e)}")
            
            # Log the error
            if self.process_logger:
                self.process_logger.log_step(
                    query_id=query_id,
                    step_name=f"response_refinement_error_{iteration}",
                    step_data={
                        "error": str(e),
                        "iteration": iteration
                    }
                )
            
            # Return the original response with error information
            return {
                "refined_response": response,
                "improvement_summary": f"Refinement failed: {str(e)}",
                "execution_time": time.time() - start_time,
                "iteration": iteration
            }
    
    def _create_refinement_prompt(
        self,
        query: str,
        response: str,
        evaluation: Dict[str, Any],
        context: str,
        sources: List[Dict[str, Any]],
        execution_result: Optional[Dict[str, Any]] = None,
        iteration: int = 1
    ) -> str:
        """
        Create a prompt for response refinement
        
        Args:
            query: Original user query
            response: Original response to refine
            evaluation: Evaluation results from ResponseEvaluator
            context: Retrieved context from documents
            sources: List of source information for citation
            execution_result: Result of plan execution (optional)
            iteration: Current refinement iteration
            
        Returns:
            Refinement prompt
        """
        prompt = f"""
You are refining a response to the following query:

USER QUERY: {query}

ORIGINAL RESPONSE:
{response}

EVALUATION RESULTS:
{json.dumps(evaluation, indent=2)}

RETRIEVED CONTEXT:
{context}

"""
        
        # Add execution result if available
        if execution_result:
            prompt += f"""
EXECUTION RESULTS:
{json.dumps(execution_result, indent=2)}

"""
        
        # Add source information
        if sources:
            prompt += f"""
SOURCE INFORMATION:
{json.dumps(sources, indent=2)}

"""
        
        # Add instructions for refinement
        prompt += f"""
REFINEMENT INSTRUCTIONS:
You are performing refinement iteration {iteration} for this response. Please address the issues identified in the evaluation results to create an improved response.

Focus on the following areas:

1. Factual Accuracy:
   - Correct any factual inaccuracies identified in the evaluation.
   - Ensure all statements are supported by the provided context.
   - Fix any incorrect citations or add missing citations.

2. Completeness:
   - Address any aspects of the query that were not covered in the original response.
   - Include relevant information from the context that was missed.
   - Ensure the response fully answers the user's query.

3. Relevance:
   - Remove any irrelevant information that doesn't address the user's query.
   - Focus the response more directly on what the user was asking about.
   - Improve the structure and flow of the response.

4. Hallucination Removal:
   - Remove or correct any statements that were identified as hallucinations.
   - Ensure all information in the response is supported by the provided context.
   - If the context doesn't contain certain information, clearly state that it's not available.

5. Overall Quality:
   - Improve the structure and clarity of the response.
   - Enhance readability with appropriate formatting.
   - Ensure the response provides maximum value to the user.

FORMAT YOUR RESPONSE AS FOLLOWS:
```json
{{
  "refined_response": "Your complete refined response here",
  "improvement_summary": "A brief summary of the improvements you made"
}}
```

IMPORTANT: Your refined response must be factually accurate, complete, relevant, and free from hallucinations. It should directly address the user's query and be well-structured and easy to understand. When using information from the context, cite the sources using the format [n] where n is the source number.
"""
        
        return prompt
    
    def _create_system_prompt(self) -> str:
        """
        Create a system prompt for response refinement
        
        Returns:
            System prompt
        """
        return """You are a response refiner for a Retrieval-Augmented Generation (RAG) system.

Your role is to improve responses based on evaluation feedback, focusing on factual accuracy, completeness, relevance, and overall quality.

GUIDELINES:
1. Address all issues identified in the evaluation results.
2. Ensure factual accuracy by verifying all statements against the provided context.
3. Improve completeness by addressing all aspects of the user's query.
4. Enhance relevance by focusing directly on what the user was asking about.
5. Remove any hallucinations or statements not supported by the context.
6. Maintain proper source citations using the [n] format.
7. Improve the structure and clarity of the response.
8. Format your response according to the specified JSON structure.
9. Be thorough in your refinements while maintaining the core information.
10. If the context doesn't contain certain information, clearly state that it's not available rather than making up information.
"""
    
    def _parse_refinement(self, refinement_text: str) -> Dict[str, Any]:
        """
        Parse the refinement response from the LLM
        
        Args:
            refinement_text: Raw refinement text from the LLM
            
        Returns:
            Parsed refinement results
        """
        # Extract JSON from the response
        try:
            # Look for JSON block in markdown format
            import re
            json_match = re.search(r'```(?:json)?\s*({\s*".*})\s*```', refinement_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without markdown formatting
                json_match = re.search(r'({[\s\S]*"improvement_summary"[\s\S]*})', refinement_text)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Fallback: assume the entire text might be JSON
                    json_str = refinement_text
            
            # Parse the JSON
            refinement = json.loads(json_str)
            
            # Ensure all required fields are present
            required_fields = ["refined_response", "improvement_summary"]
            
            for field in required_fields:
                if field not in refinement:
                    if field == "refined_response":
                        # If the refined response is missing, use the raw text
                        refinement[field] = refinement_text
                    else:
                        refinement[field] = "Not provided"
            
            return refinement
        except Exception as e:
            self.logger.error(f"Error parsing refinement: {str(e)}")
            
            # Return a default refinement with the raw text as the response
            return {
                "refined_response": refinement_text,
                "improvement_summary": f"Failed to parse refinement: {str(e)}"
            }

================
File: app/rag/response_synthesizer.py
================
"""
ResponseSynthesizer - Synthesizes responses from retrieval results and tool outputs
"""
import logging
import time
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

class ResponseSynthesizer:
    """
    Synthesizes responses from retrieval results and tool outputs
    
    The ResponseSynthesizer is responsible for combining retrieval results and tool outputs
    into coherent, well-structured responses with proper source attribution. It uses the
    LLM to generate responses based on the available context and ensures that the responses
    are accurate, complete, and properly formatted.
    """
    
    def __init__(
        self,
        llm_provider,
        process_logger = None
    ):
        """
        Initialize the response synthesizer
        
        Args:
            llm_provider: LLM provider for generating responses
            process_logger: ProcessLogger instance (optional)
        """
        self.llm_provider = llm_provider
        self.process_logger = process_logger
        self.logger = logging.getLogger("app.rag.response_synthesizer")
    
    async def synthesize(
        self,
        query: str,
        query_id: str,
        context: str,
        sources: List[Dict[str, Any]],
        execution_result: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        system_prompt: Optional[str] = None,
        model_parameters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Synthesize a response from retrieval results and tool outputs
        
        Args:
            query: Original user query
            query_id: Unique query ID
            context: Retrieved context from documents
            sources: List of source information for citation
            execution_result: Result of plan execution (optional)
            conversation_context: Conversation history (optional)
            system_prompt: Custom system prompt (optional)
            model_parameters: Custom model parameters (optional)
            
        Returns:
            Dictionary containing:
                - response: Synthesized response text
                - sources: List of sources used in the response
                - execution_time: Time taken to synthesize the response
        """
        start_time = time.time()
        self.logger.info(f"Synthesizing response for query: {query}")
        
        # Log the start of response synthesis
        if self.process_logger:
            self.process_logger.log_step(
                query_id=query_id,
                step_name="response_synthesis_start",
                step_data={
                    "query": query,
                    "context_length": len(context),
                    "sources_count": len(sources),
                    "has_execution_result": execution_result is not None
                }
            )
        
        # Create the synthesis prompt
        prompt = self._create_synthesis_prompt(
            query=query,
            context=context,
            sources=sources,
            execution_result=execution_result,
            conversation_context=conversation_context
        )
        
        # Create or use the provided system prompt
        if not system_prompt:
            system_prompt = self._create_system_prompt()
        
        # Generate the response using the LLM
        try:
            response = await self.llm_provider.generate(
                prompt=prompt,
                system_prompt=system_prompt,
                parameters=model_parameters or {}
            )
            
            synthesized_response = response.get("response", "")
            
            # Extract and validate sources used in the response
            used_sources = self._extract_used_sources(synthesized_response, sources)
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"Response synthesis completed in {elapsed_time:.2f}s")
            
            # Log the completion of response synthesis
            if self.process_logger:
                self.process_logger.log_step(
                    query_id=query_id,
                    step_name="response_synthesis_complete",
                    step_data={
                        "response_length": len(synthesized_response),
                        "used_sources_count": len(used_sources),
                        "execution_time": elapsed_time
                    }
                )
            
            return {
                "response": synthesized_response,
                "sources": used_sources,
                "execution_time": elapsed_time
            }
        except Exception as e:
            self.logger.error(f"Error synthesizing response: {str(e)}")
            
            # Log the error
            if self.process_logger:
                self.process_logger.log_step(
                    query_id=query_id,
                    step_name="response_synthesis_error",
                    step_data={
                        "error": str(e)
                    }
                )
            
            # Return a fallback response
            return {
                "response": f"I encountered an error while generating a response: {str(e)}",
                "sources": [],
                "execution_time": time.time() - start_time
            }
    
    def _create_synthesis_prompt(
        self,
        query: str,
        context: str,
        sources: List[Dict[str, Any]],
        execution_result: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None
    ) -> str:
        """
        Create a prompt for response synthesis
        
        Args:
            query: Original user query
            context: Retrieved context from documents
            sources: List of source information for citation
            execution_result: Result of plan execution (optional)
            conversation_context: Conversation history (optional)
            
        Returns:
            Synthesis prompt
        """
        prompt = f"""
You are synthesizing a response to the following query:

USER QUERY: {query}

"""
        
        # Add conversation context if available
        if conversation_context:
            prompt += f"""
CONVERSATION CONTEXT:
{conversation_context}

"""
        
        # Add retrieved context if available
        if context:
            prompt += f"""
RETRIEVED CONTEXT:
{context}

"""
        
        # Add execution result if available
        if execution_result:
            prompt += f"""
EXECUTION RESULTS:
{json.dumps(execution_result, indent=2)}

"""
        
        # Add source information
        if sources:
            prompt += f"""
SOURCE INFORMATION:
{json.dumps(sources, indent=2)}

"""
        
        # Add instructions for response synthesis
        prompt += """
INSTRUCTIONS:
1. Synthesize a comprehensive response to the user's query using the provided information.
2. When using information from the retrieved context, cite the sources using the format [n] where n is the source number.
3. If the context doesn't contain the answer, clearly state that the information is not available in the provided documents.
4. If execution results are available, incorporate them into your response.
5. Ensure your response is well-structured, clear, and directly addresses the user's query.
6. Do not include phrases like "Based on the provided context" or "According to the retrieved information" - just provide the answer directly.
7. Format your response appropriately with headings, bullet points, or numbered lists as needed for clarity.
8. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
        
        return prompt
    
    def _create_system_prompt(self) -> str:
        """
        Create a system prompt for response synthesis
        
        Returns:
            System prompt
        """
        return """You are a response synthesizer for a Retrieval-Augmented Generation (RAG) system.

Your role is to create comprehensive, accurate responses based on the provided context, sources, and execution results.

GUIDELINES:
1. Always prioritize information from the provided context and execution results.
2. Cite sources properly using the [n] format when using information from the retrieved context.
3. Be clear and direct in your responses - don't use phrases like "Based on the provided context" or "According to the retrieved information".
4. Structure your responses logically with appropriate formatting (headings, bullet points, etc.) for clarity.
5. If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
6. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
7. Never fabricate information or citations.
8. Ensure your response directly addresses the user's query.
9. If execution results are available, incorporate them seamlessly into your response.
10. Maintain a professional, helpful tone throughout your response.
"""
    
    def _extract_used_sources(
        self,
        response: str,
        available_sources: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Extract sources that were actually used in the response
        
        Args:
            response: Synthesized response
            available_sources: List of available sources
            
        Returns:
            List of sources used in the response
        """
        used_sources = []
        used_indices = set()
        
        # Extract source citations from the response (format: [n])
        import re
        citation_pattern = r'\[(\d+)\]'
        citations = re.findall(citation_pattern, response)
        
        # Convert to integers and remove duplicates
        for citation in citations:
            try:
                index = int(citation)
                used_indices.add(index)
            except ValueError:
                continue
        
        # Collect the used sources
        for index in used_indices:
            if 1 <= index <= len(available_sources):
                used_sources.append(available_sources[index - 1])
        
        return used_sources

================
File: app/rag/system_prompts.py
================
"""
System prompts for the RAG engine
"""

# Standard RAG system prompt
RAG_SYSTEM_PROMPT = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
- Do not use your general knowledge unless the context is insufficient, and clearly indicate when you're doing so.
- Analyze the context carefully to find the most relevant information for the user's question.
- If multiple sources provide different information, synthesize them and explain any discrepancies.
- If the context includes metadata like filenames, tags, or folders, use this to understand the source and relevance of the information.

WHEN CONTEXT IS INSUFFICIENT:
- Clearly state: "Based on the provided documents, I don't have information about [topic]."
- Be specific about what information is missing.
- Only then provide a general response based on your knowledge, and clearly state: "However, generally speaking..." to distinguish this from information in the context.
- Never pretend to have information that isn't in the context.

CONVERSATION HANDLING:
- IMPORTANT: Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.
- If no conversation history is provided, treat the query as a new, standalone question.
- Only maintain continuity with previous exchanges when conversation history is explicitly provided.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Always cite your sources with numbers in square brackets [1] when using information from the context.
"""

# Code generation system prompt
CODE_GENERATION_SYSTEM_PROMPT = """You are a helpful coding assistant that provides accurate, well-structured code based on user requests.

ROLE AND CAPABILITIES:
- Your primary function is to generate clean, efficient, and well-documented code.
- You can provide explanations for your code to help users understand how it works.
- You can suggest improvements or alternatives to existing code.

CODE QUALITY GUIDELINES:
- Write code that follows best practices and conventions for the language.
- Include appropriate error handling and edge case considerations.
- Optimize for readability and maintainability.
- Use clear variable and function names that reflect their purpose.
- Add comments to explain complex logic or important decisions.
- Structure the code logically with proper indentation and formatting.

RESPONSE STYLE:
- Present code in properly formatted code blocks using triple backticks with the language specified.
- Provide a brief explanation of what the code does and how to use it.
- If relevant, explain key design decisions or trade-offs.
- For complex solutions, break down the explanation into steps or components.
- If the user's request is ambiguous, provide the most reasonable implementation and explain any assumptions made.
- When appropriate, suggest how the code could be extended or improved.
"""

# Python-specific code generation prompt
PYTHON_CODE_GENERATION_PROMPT = """PYTHON-SPECIFIC GUIDELINES:
- Follow PEP 8 style guidelines for Python code.
- Use type hints when appropriate to improve code clarity.
- Prefer Python's built-in functions and standard library when possible.
- Use list/dict comprehensions and generator expressions when they improve readability.
- Follow the Zen of Python principles (import this).
- Use context managers (with statements) for resource management.
- Implement proper exception handling with specific exception types.
- Use docstrings for functions, classes, and modules.
- Consider compatibility with different Python versions when relevant.
"""

# JavaScript-specific code generation prompt
JAVASCRIPT_CODE_GENERATION_PROMPT = """JAVASCRIPT-SPECIFIC GUIDELINES:
- Use modern JavaScript features (ES6+) when appropriate.
- Consider browser compatibility when necessary.
- Use const and let instead of var.
- Use arrow functions when appropriate for cleaner syntax.
- Implement proper error handling with try/catch blocks.
- Use async/await for asynchronous operations when appropriate.
- Follow standard JavaScript naming conventions.
- Consider performance implications, especially for DOM operations.
- Add JSDoc comments for functions and classes.
- Use destructuring, spread syntax, and template literals for cleaner code.
"""

# Prompt for conversation with context
CONVERSATION_WITH_CONTEXT_PROMPT = """Context:
{context}

Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""

# Prompt for new query with context
NEW_QUERY_WITH_CONTEXT_PROMPT = """Context:
{context}

User Question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. This is a new conversation with no previous history - treat it as such.
9. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""

================
File: app/rag/tool_initializer.py
================
"""
Tool Initializer - Module for initializing and registering tools
"""
import logging
from typing import Optional

from app.rag.tools.registry import ToolRegistry
from app.rag.tools.rag_tool import RAGTool
from app.rag.tools.calculator_tool import CalculatorTool
from app.rag.tools.database_tool import DatabaseTool
from app.rag.tools.postgresql_tool import PostgreSQLTool

logger = logging.getLogger("app.rag.tool_initializer")

# Create a singleton tool registry
tool_registry = ToolRegistry()

def initialize_tools(rag_engine=None) -> ToolRegistry:
    """
    Initialize and register all tools
    
    Args:
        rag_engine: Optional RAG engine instance for the RAG tool
        
    Returns:
        ToolRegistry: The tool registry with all tools registered
    """
    logger.info("Initializing tools")
    
    # Register RAG tool if RAG engine is provided
    if rag_engine:
        logger.info("Registering RAG tool")
        rag_tool = RAGTool(rag_engine)
        tool_registry.register_tool(rag_tool)
    
    # Register calculator tool
    logger.info("Registering calculator tool")
    calculator_tool = CalculatorTool()
    tool_registry.register_tool(calculator_tool)
    
    # Register database tool
    logger.info("Registering database tool")
    database_tool = DatabaseTool()
    tool_registry.register_tool(database_tool)
    
    # Register PostgreSQL tool
    logger.info("Registering PostgreSQL tool")
    postgresql_tool = PostgreSQLTool()
    tool_registry.register_tool(postgresql_tool)
    
    logger.info(f"Registered {tool_registry.get_tool_count()} tools")
    
    return tool_registry

def get_tool_registry() -> ToolRegistry:
    """
    Get the tool registry
    
    Returns:
        ToolRegistry: The tool registry
    """
    return tool_registry

================
File: app/rag/vector_store.py
================
import logging
import os
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from uuid import UUID
import chromadb
from chromadb.config import Settings

from app.core.config import CHROMA_DB_DIR, DEFAULT_EMBEDDING_MODEL
from app.models.document import Document, Chunk
from app.rag.ollama_client import OllamaClient
from app.cache.vector_search_cache import VectorSearchCache

logger = logging.getLogger("app.rag.vector_store")

class VectorStore:
    """
    Vector store for document embeddings using ChromaDB with caching for performance
    and security filtering based on user permissions
    """
    def __init__(
        self,
        persist_directory: str = CHROMA_DB_DIR,
        embedding_model: str = DEFAULT_EMBEDDING_MODEL,
        enable_cache: bool = True,
        cache_ttl: int = 3600,  # 1 hour in seconds
        cache_max_size: int = 1000,
        cache_persist: bool = True,
        cache_persist_dir: str = "data/cache",
        user_id: Optional[UUID] = None
    ):
        self.persist_directory = persist_directory
        self.embedding_model = embedding_model
        self.ollama_client = None
        self.user_id = user_id  # Store the user ID for permission filtering
        
        # Cache settings
        self.enable_cache = enable_cache
        self.cache_ttl = cache_ttl
        self.cache_max_size = cache_max_size
        
        # Initialize the vector search cache
        if self.enable_cache:
            self.vector_cache = VectorSearchCache(
                ttl=cache_ttl,
                max_size=cache_max_size,
                persist=cache_persist,
                persist_dir=cache_persist_dir
            )
        
        # Initialize ChromaDB
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        
        # Create or get the collection
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
        
        logger.info(f"Vector store initialized with collection 'documents', caching {'enabled' if enable_cache else 'disabled'}")
    
    async def add_document(self, document: Document) -> None:
        """
        Add a document to the vector store with batch embedding
        """
        try:
            logger.info(f"Adding document {document.id} to vector store")
            
            # Make sure we have an Ollama client
            if self.ollama_client is None:
                self.ollama_client = OllamaClient()
            
            # Prepare chunks for batch processing
            chunks_to_embed = [chunk for chunk in document.chunks if not chunk.embedding]
            chunk_contents = [chunk.content for chunk in chunks_to_embed]
            
            # Create embeddings in batch if possible
            if chunk_contents:
                try:
                    # Batch embedding
                    embeddings = await self._batch_create_embeddings(chunk_contents)
                    
                    # Assign embeddings to chunks
                    for i, chunk in enumerate(chunks_to_embed):
                        chunk.embedding = embeddings[i]
                except Exception as batch_error:
                    logger.warning(f"Batch embedding failed: {str(batch_error)}. Falling back to sequential embedding.")
                    # Fall back to sequential embedding
                    for chunk in chunks_to_embed:
                        chunk.embedding = await self.ollama_client.create_embedding(
                            text=chunk.content,
                            model=self.embedding_model
                        )
            
            # Add chunks to the collection
            for chunk in document.chunks:
                if not chunk.embedding:
                    logger.warning(f"Chunk {chunk.id} has no embedding, skipping")
                    continue
                    
                # Prepare metadata - convert any lists to strings to satisfy ChromaDB requirements
                metadata = {
                    "document_id": document.id,
                    "chunk_index": chunk.metadata.get("index", 0),
                    "filename": document.filename,
                    "tags": ",".join(document.tags) if document.tags else "",
                    "folder": document.folder
                }
                
                # Add user context and permission information
                if hasattr(document, 'user_id') and document.user_id:
                    metadata["user_id"] = str(document.user_id)
                elif self.user_id:
                    metadata["user_id"] = str(self.user_id)
                
                # Add is_public flag for permission filtering
                if hasattr(document, 'is_public'):
                    metadata["is_public"] = document.is_public
                
                # Add document permissions information from chunk metadata
                if "shared_with" in chunk.metadata:
                    metadata["shared_with"] = chunk.metadata["shared_with"]
                
                if "shared_user_ids" in chunk.metadata:
                    metadata["shared_user_ids"] = chunk.metadata["shared_user_ids"]
                
                # Add any additional metadata from the chunk
                for key, value in chunk.metadata.items():
                    # Convert lists to strings if present
                    if isinstance(value, list):
                        metadata[key] = ",".join(str(item) for item in value)
                    else:
                        metadata[key] = value
                
                self.collection.add(
                    ids=[chunk.id],
                    embeddings=[chunk.embedding],
                    documents=[chunk.content],
                    metadatas=[metadata]
                )
            
            # Clear the cache to ensure we're using the latest embeddings
            self.clear_cache()
            
            logger.info(f"Added {len(document.chunks)} chunks to vector store for document {document.id}")
        except Exception as e:
            logger.error(f"Error adding document {document.id} to vector store: {str(e)}")
            raise
    
    async def _batch_create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Create embeddings for a list of texts (batched)"""
        try:
            # Attempt to use Langchain's embed_documents for batch embedding
            from langchain_community.embeddings import OllamaEmbeddings
            embeddings_model = OllamaEmbeddings(model=self.embedding_model)
            return embeddings_model.embed_documents(texts)
        except (ImportError, NotImplementedError) as e:
            # Handle the case where the provider doesn't support batch embedding
            logger.warning(f"Batch embedding not supported: {str(e)}. Falling back to sequential embedding.")
            embeddings = []
            for text in texts:
                embedding = await self.ollama_client.create_embedding(text=text, model=self.embedding_model)
                embeddings.append(embedding)
            return embeddings
    
    async def update_document_metadata(self, document_id: str, metadata_update: Dict[str, Any]) -> None:
        """
        Update metadata for all chunks of a document
        """
        try:
            logger.info(f"Updating metadata for document {document_id}")
            
            # Get all chunks for the document
            results = self.collection.get(
                where={"document_id": document_id}
            )
            
            if not results["ids"]:
                logger.warning(f"No chunks found for document {document_id}")
                return
            
            # Update each chunk's metadata
            for i, chunk_id in enumerate(results["ids"]):
                # Get current metadata
                current_metadata = results["metadatas"][i]
                
                # Update with new metadata
                updated_metadata = {**current_metadata, **metadata_update}
                
                # Update in collection
                self.collection.update(
                    ids=[chunk_id],
                    metadatas=[updated_metadata]
                )
            
            logger.info(f"Updated metadata for {len(results['ids'])} chunks of document {document_id}")
        except Exception as e:
            logger.error(f"Error updating metadata for document {document_id}: {str(e)}")
            raise
    
    async def search(
        self,
        query: str,
        top_k: int = 5,
        filter_criteria: Optional[Dict[str, Any]] = None,
        user_id: Optional[UUID] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for documents similar to the query with caching for performance
        and security filtering based on user permissions
        """
        try:
            # Use provided user_id or fall back to the instance's user_id
            effective_user_id = user_id or self.user_id
            
            # Apply security filtering
            secure_filter = self._apply_security_filter(filter_criteria, effective_user_id)
            
            # Check cache if enabled
            if self.enable_cache:
                # Include user_id in cache key to ensure proper isolation
                cache_key_parts = [query, top_k, secure_filter]
                if effective_user_id:
                    cache_key_parts.append(str(effective_user_id))
                
                cached_result = self.vector_cache.get_results(query, top_k, secure_filter)
                
                if cached_result:
                    logger.info(f"Cache hit for query: {query[:50]}...")
                    return cached_result
                
                logger.info(f"Cache miss for query: {query[:50]}...")
            
            logger.info(f"Searching for documents similar to query: {query[:50]}...")
            
            # Make sure we have an Ollama client
            if self.ollama_client is None:
                self.ollama_client = OllamaClient()
            
            # Create query embedding
            query_embedding = await self.ollama_client.create_embedding(
                text=query,
                model=self.embedding_model
            )
            
            # Log the query embedding for debugging
            logger.debug(f"Query embedding (first 5 values): {query_embedding[:5]}")
            
            # Log filter criteria if present
            if secure_filter:
                logger.info(f"Applying security filter criteria: {secure_filter}")
            
            # Search for similar documents
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=secure_filter
            )
            
            # Format results
            formatted_results = []
            if results["ids"] and len(results["ids"][0]) > 0:
                logger.info(f"Raw search results: {len(results['ids'][0])} chunks found")
                
                for i in range(len(results["ids"][0])):
                    chunk_id = results["ids"][0][i]
                    content = results["documents"][0][i]
                    metadata = results["metadatas"][0][i]
                    distance = results["distances"][0][i] if "distances" in results else None
                    
                    # Log each result for debugging
                    logger.debug(f"Result {i+1}:")
                    logger.debug(f"  Chunk ID: {chunk_id}")
                    logger.debug(f"  Distance: {distance}")
                    logger.debug(f"  Metadata: {metadata}")
                    logger.debug(f"  Content preview: {content[:100] if content is not None else 'None'}...")
                    
                    # Skip adding None content to results or provide a default value
                    if content is not None:
                        formatted_results.append({
                            "chunk_id": chunk_id,
                            "content": content,
                            "metadata": metadata,
                            "distance": distance
                        })
                    else:
                        logger.warning(f"Skipping result with chunk_id {chunk_id} due to None content")
                
                # Apply post-retrieval permission check
                if effective_user_id:
                    formatted_results = self._post_retrieval_permission_check(formatted_results, effective_user_id)
            else:
                logger.warning(f"No results found for query: {query[:50]}...")
            
            logger.info(f"Found {len(formatted_results)} similar documents")
            
            # Log document IDs for easier tracking
            if formatted_results:
                doc_ids = set(result["metadata"]["document_id"] for result in formatted_results)
                logger.info(f"Documents retrieved: {doc_ids}")
            
            # Cache results if enabled
            if self.enable_cache:
                self.vector_cache.set_results(query, top_k, formatted_results, secure_filter)
            
            return formatted_results
        except Exception as e:
            logger.error(f"Error searching for documents: {str(e)}")
            raise
    
    def _apply_security_filter(self, filter_criteria: Optional[Dict[str, Any]], user_id: Optional[UUID]) -> Dict[str, Any]:
        """
        Apply security filtering based on user permissions
        
        Args:
            filter_criteria: Original filter criteria
            user_id: User ID for permission filtering
            
        Returns:
            Updated filter criteria with security constraints
        """
        # Start with the original filter criteria or an empty dict
        secure_filter = filter_criteria.copy() if filter_criteria else {}
        
        # If no user_id, only allow public documents
        if not user_id:
            secure_filter["is_public"] = True
            return secure_filter
        
        # For authenticated users, allow:
        # 1. Documents owned by the user
        # 2. Public documents
        # 3. Documents explicitly shared with the user
        
        # Convert user_id to string for comparison
        user_id_str = str(user_id)
        
        # Create a filter that matches any of:
        # - Documents owned by the user
        # - Public documents
        # - Documents shared with the user
        permission_filter = {
            "$or": [
                {"user_id": user_id_str},                    # Documents owned by the user
                {"is_public": True},                         # Public documents
                {"shared_user_ids": {"$contains": user_id_str}}  # Documents shared with the user
            ]
        }
        
        # Log the permission filter for debugging
        logger.debug(f"Applied permission filter for user {user_id_str}: {permission_filter}")
        
        # If there are existing filters, combine them with the permission filter
        if secure_filter:
            # Combine existing filters with permission filter using $and
            return {"$and": [secure_filter, permission_filter]}
        else:
            # Just use the permission filter
            return permission_filter
    
    async def search_by_tags(
        self,
        query: str,
        tags: List[str],
        top_k: int = 5,
        additional_filters: Optional[Dict[str, Any]] = None,
        user_id: Optional[UUID] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for documents with specific tags
        """
        try:
            logger.info(f"Searching for documents with tags {tags} similar to query: {query[:50]}...")
            
            # Prepare filter criteria
            filter_criteria = additional_filters or {}
            
            # Add tag filter
            if tags:
                # Since tags are now stored as comma-separated strings, we need to use $contains
                # to check if any of the requested tags are in the document's tags string
                tag_conditions = []
                for tag in tags:
                    # For each tag, create a condition that checks if it's in the tags string
                    # We add commas to ensure we match whole tags, not substrings
                    tag_conditions.append({"$contains": tag})
                
                # Use $or to match any of the tag conditions
                if "tags" not in filter_criteria:
                    filter_criteria["tags"] = {"$or": tag_conditions}
                else:
                    # If there's already a tags filter, combine with it
                    existing_filter = filter_criteria["tags"]
                    filter_criteria["tags"] = {"$and": [existing_filter, {"$or": tag_conditions}]}
            
            # Perform search with filters
            return await self.search(query, top_k, filter_criteria, user_id)
        except Exception as e:
            logger.error(f"Error searching for documents by tags: {str(e)}")
            raise
    
    async def search_by_folder(
        self,
        query: str,
        folder: str,
        top_k: int = 5,
        additional_filters: Optional[Dict[str, Any]] = None,
        user_id: Optional[UUID] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for documents in a specific folder
        """
        try:
            logger.info(f"Searching for documents in folder {folder} similar to query: {query[:50]}...")
            
            # Prepare filter criteria
            filter_criteria = additional_filters or {}
            
            # Add folder filter
            if folder:
                filter_criteria["folder"] = folder
            
            # Perform search with filters
            return await self.search(query, top_k, filter_criteria, user_id)
        except Exception as e:
            logger.error(f"Error searching for documents by folder: {str(e)}")
            raise
    
    def delete_document(self, document_id: str) -> None:
        """
        Delete a document from the vector store
        """
        try:
            logger.info(f"Deleting document {document_id} from vector store")
            
            # Delete chunks with the given document_id
            self.collection.delete(
                where={"document_id": document_id}
            )
            
            # Invalidate cache entries for this document
            if self.enable_cache:
                self.vector_cache.invalidate_by_document_id(document_id)
                logger.info(f"Invalidated cache entries for document {document_id}")
            
            logger.info(f"Deleted document {document_id} from vector store")
        except Exception as e:
            logger.error(f"Error deleting document {document_id} from vector store: {str(e)}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the vector store
        """
        try:
            count = self.collection.count()
            stats = {
                "count": count,
                "embeddings_model": self.embedding_model
            }
            
            # Add cache stats if enabled
            if self.enable_cache:
                cache_stats = self.get_cache_stats()
                stats.update(cache_stats)
            
            return stats
        except Exception as e:
            logger.error(f"Error getting vector store stats: {str(e)}")
            raise
    
    def clear_cache(self) -> None:
        """
        Clear the cache to ensure fresh results
        """
        if self.enable_cache:
            self.vector_cache.clear()
            logger.info("Vector store cache cleared")
    
    def _post_retrieval_permission_check(self, results: List[Dict[str, Any]], user_id: UUID) -> List[Dict[str, Any]]:
        """
        Perform a secondary permission check on search results
        
        This provides an additional security layer beyond the initial query filtering.
        It verifies that each result is accessible to the user based on:
        1. Document ownership
        2. Public access
        3. Explicit sharing permissions
        
        Args:
            results: List of search results
            user_id: User ID for permission checking
            
        Returns:
            Filtered list of results that the user has permission to access
        """
        if not results:
            return results
            
        user_id_str = str(user_id)
        filtered_results = []
        unauthorized_access_attempts = 0
        
        for result in results:
            metadata = result.get("metadata", {})
            document_id = metadata.get("document_id")
            
            # Check permissions
            has_permission = False
            
            # Case 1: User owns the document
            if metadata.get("user_id") == user_id_str:
                has_permission = True
                
            # Case 2: Document is public
            elif metadata.get("is_public") is True:
                has_permission = True
                
            # Case 3: Document is shared with the user
            elif "shared_with" in metadata:
                try:
                    # Parse the shared_with JSON string
                    shared_with = json.loads(metadata["shared_with"])
                    if user_id_str in shared_with:
                        has_permission = True
                except (json.JSONDecodeError, TypeError):
                    logger.warning(f"Invalid shared_with format in document {document_id}")
            
            # Alternative check using shared_user_ids
            elif "shared_user_ids" in metadata:
                shared_user_ids = metadata["shared_user_ids"].split(",")
                if user_id_str in shared_user_ids:
                    has_permission = True
            
            if has_permission:
                filtered_results.append(result)
            else:
                unauthorized_access_attempts += 1
                logger.warning(f"Blocked unauthorized access attempt to document {document_id} by user {user_id_str}")
        
        # Log summary of permission filtering
        if unauthorized_access_attempts > 0:
            logger.warning(f"Filtered out {unauthorized_access_attempts} results due to permission restrictions")
            
        logger.info(f"Post-retrieval permission check: {len(filtered_results)}/{len(results)} results passed")
        
        return filtered_results
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the cache
        """
        if not self.enable_cache:
            return {"cache_enabled": False}
        
        # Get stats from the vector search cache
        cache_stats = self.vector_cache.get_stats()
        
        return {
            "cache_enabled": True,
            "cache_size": cache_stats["size"],
            "cache_max_size": cache_stats["max_size"],
            "cache_hits": cache_stats["hits"],
            "cache_misses": cache_stats["misses"],
            "cache_hit_ratio": cache_stats["hit_ratio"],
            "cache_ttl_seconds": cache_stats["ttl_seconds"],
            "cache_persist": cache_stats["persist"]
        }

================
File: app/static/css/document-manager.css
================
/* Document Management Sidebar Styles */
.document-section {
    margin-top: 20px;
    border-top: 1px solid var(--border-color);
    padding-top: 15px;
    overflow: hidden;
    max-height: 42px; /* Height of the header */
    transition: max-height 0.3s ease-out;
}

.document-section.expanded {
    max-height: 500px; /* Adjust based on expected content */
    overflow-y: auto;
}

.document-section-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 15px;
    cursor: pointer;
}

.document-section-title {
    font-weight: 600;
    font-size: 1rem;
    display: flex;
    align-items: center;
    gap: 8px;
    color: var(--accent-color);
}

.document-count {
    background-color: var(--accent-color);
    color: white;
    border-radius: 10px;
    padding: 2px 6px;
    font-size: 0.7rem;
    margin-left: 5px;
    transition: all 0.3s;
}

.document-count.has-documents {
    animation: pulse 1.5s infinite;
}

@keyframes pulse {
    0% { transform: scale(1); }
    50% { transform: scale(1.1); }
    100% { transform: scale(1); }
}

.document-list {
    margin-top: 15px;
    max-height: 300px;
    overflow-y: auto;
}

.sidebar-document-item {
    background-color: var(--input-bg);
    border-radius: 4px;
    padding: 8px 10px;
    margin-bottom: 8px;
    font-size: 0.9rem;
    border: 1px solid var(--border-color);
}

.doc-header {
    display: flex;
    align-items: center;
    margin-bottom: 5px;
}

.doc-select-label {
    display: flex;
    align-items: center;
    gap: 5px;
    cursor: pointer;
    width: 100%;
    margin: 0;
}

.doc-title {
    font-weight: 500;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

.doc-meta {
    display: flex;
    justify-content: space-between;
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-bottom: 5px;
}

/* Document Tags */
.doc-tags {
    display: flex;
    flex-wrap: wrap;
    gap: 4px;
    margin-bottom: 5px;
}

.doc-tag {
    background-color: var(--primary-color);
    color: white;
    padding: 2px 6px;
    border-radius: 10px;
    font-size: 0.7rem;
    white-space: nowrap;
}

.light-mode .doc-tag {
    color: white;
}

/* Document Folder */
.doc-folder {
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-bottom: 5px;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

.doc-folder i {
    margin-right: 4px;
}

.doc-actions {
    display: flex;
    justify-content: flex-end;
    gap: 5px;
}

.doc-action {
    background-color: transparent;
    color: var(--muted-color);
    border: none;
    padding: 2px 5px;
    border-radius: 3px;
    cursor: pointer;
    font-size: 0.8rem;
}

.doc-action:hover {
    background-color: var(--border-color);
    color: var(--text-color);
}

.process-btn:hover {
    color: var(--accent-color);
}

.delete-btn:hover {
    color: var(--error-color);
}

.edit-btn:hover {
    color: var(--secondary-color);
}

.document-upload {
    margin-top: 15px;
    margin-bottom: 15px;
}

.upload-form {
    display: flex;
    flex-direction: column;
    gap: 10px;
}

.upload-input {
    display: flex;
    align-items: center;
}

.upload-input input[type="file"] {
    flex: 1;
    font-size: 0.8rem;
}

.upload-button {
    padding: 6px 12px;
    font-size: 0.8rem;
}

.progress-bar {
    height: 6px;
    background-color: var(--border-color);
    border-radius: 3px;
    margin-top: 5px;
    display: none;
}

.progress-bar-fill {
    height: 100%;
    background-color: var(--accent-color);
    border-radius: 3px;
    width: 0;
    transition: width 0.3s;
}

.batch-actions {
    display: flex;
    gap: 8px;
    margin-top: 10px;
}

.batch-actions button {
    flex: 1;
    padding: 6px 10px;
    font-size: 0.8rem;
}

.document-loading,
.document-empty,
.document-error {
    text-align: center;
    padding: 10px;
    color: var(--muted-color);
    font-size: 0.9rem;
}

.document-error {
    color: var(--error-color);
}

/* Tag Management */
.tag-input-container {
    position: relative;
    margin-bottom: 10px;
}

.tag-input {
    width: 100%;
    padding-right: 30px;
}

.tag-suggestions {
    position: absolute;
    top: 100%;
    left: 0;
    right: 0;
    background-color: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 4px;
    max-height: 150px;
    overflow-y: auto;
    z-index: 10;
    display: none;
}

.tag-suggestion-item {
    padding: 5px 10px;
    cursor: pointer;
}

.tag-suggestion-item:hover {
    background-color: var(--border-color);
}

.tag-list {
    display: flex;
    flex-wrap: wrap;
    gap: 5px;
    margin-top: 10px;
}

.tag-item {
    background-color: var(--primary-color);
    color: white;
    padding: 3px 8px;
    border-radius: 12px;
    font-size: 0.8rem;
    display: flex;
    align-items: center;
    gap: 5px;
}

.light-mode .tag-item {
    color: white;
}

.tag-remove {
    cursor: pointer;
    font-weight: bold;
}

/* Folder Selection */
.folder-select {
    width: 100%;
    margin-bottom: 10px;
}

.folder-path {
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-top: 5px;
}

/* Filter Panel */
.filter-panel {
    margin-bottom: 15px;
    padding: 10px;
    border: 1px solid var(--border-color);
    border-radius: 4px;
    background-color: var(--input-bg);
}

.filter-title {
    font-weight: bold;
    margin-bottom: 8px;
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.filter-toggle {
    background: none;
    border: none;
    color: var(--text-color);
    cursor: pointer;
    padding: 0;
}

.filter-content {
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.filter-content.show {
    max-height: 300px;
    overflow-y: auto;
}

.filter-section {
    margin-bottom: 10px;
}

.filter-section-title {
    font-weight: 500;
    margin-bottom: 5px;
    font-size: 0.9rem;
}

.filter-tags, .filter-folders {
    display: flex;
    flex-wrap: wrap;
    gap: 5px;
    margin-bottom: 10px;
}

.filter-tag, .filter-folder {
    background-color: var(--border-color);
    color: var(--text-color);
    padding: 3px 8px;
    border-radius: 12px;
    font-size: 0.8rem;
    cursor: pointer;
}

.filter-tag.active, .filter-folder.active {
    background-color: var(--primary-color);
    color: white;
}

.light-mode .filter-tag.active,
.light-mode .filter-folder.active {
    color: white;
}

.filter-actions {
    display: flex;
    justify-content: space-between;
    margin-top: 10px;
}

.filter-actions button {
    padding: 4px 8px;
    font-size: 0.8rem;
}

/* Document Edit Modal */
.modal {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-color: rgba(0, 0, 0, 0.5);
    z-index: 1000;
    align-items: center;
    justify-content: center;
}

.modal-content {
    background-color: var(--card-bg);
    border-radius: 8px;
    padding: 20px;
    width: 90%;
    max-width: 500px;
    max-height: 90vh;
    overflow-y: auto;
}

.modal-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 15px;
}

.modal-title {
    font-weight: bold;
    font-size: 1.2rem;
}

.modal-close {
    background: none;
    border: none;
    font-size: 1.5rem;
    cursor: pointer;
    color: var(--text-color);
}

.modal-body {
    margin-bottom: 20px;
}

.modal-footer {
    display: flex;
    justify-content: flex-end;
    gap: 10px;
}

/* Responsive adjustments */
@media (max-width: 768px) {
    .document-section.expanded {
        max-height: 300px;
    }
    
    .modal-content {
        width: 95%;
    }
    
    /* Improve touch targets for mobile */
    .doc-action {
        padding: 8px 10px;
        margin: 2px;
    }
    
    .filter-tag, .filter-folder {
        padding: 8px 12px;
        margin-bottom: 8px;
    }
    
    /* Adjust document items for better mobile experience */
    .sidebar-document-item {
        padding: 12px;
    }
    
    /* Make checkboxes easier to tap */
    input[type="checkbox"] {
        width: 20px;
        height: 20px;
    }
    
    /* Improve form elements */
    input[type="text"],
    input[type="file"],
    select,
    button {
        min-height: 44px; /* Apple's recommended minimum touch target size */
        font-size: 16px; /* Prevents iOS zoom on focus */
    }
    
    /* Add pull-to-refresh indicator */
    .pull-to-refresh {
        height: 50px;
        display: flex;
        align-items: center;
        justify-content: center;
        color: var(--muted-color);
        font-size: 0.9rem;
    }
    
    .pull-to-refresh .spinner {
        margin-right: 10px;
    }
    
    /* Add swipe actions for document items */
    .sidebar-document-item {
        position: relative;
        transition: transform 0.3s;
    }
    
    .sidebar-document-item.show-actions {
        transform: translateX(-80px);
    }
    
    .swipe-actions {
        position: absolute;
        right: -80px;
        top: 0;
        bottom: 0;
        width: 80px;
        display: flex;
        align-items: center;
        justify-content: center;
    }
    
    .swipe-action {
        width: 40px;
        height: 40px;
        border-radius: 50%;
        background-color: var(--error-color);
        color: white;
        display: flex;
        align-items: center;
        justify-content: center;
    }
}

================
File: app/static/css/document-upload-enhanced.css
================
/* Enhanced Document Upload Styles */
:root {
    --primary-color: #1a5d1a;
    --secondary-color: #2e8b57;
    --accent-color: #50c878;
    --text-color: #f0f0f0;
    --bg-color: #121212;
    --card-bg: #1e1e1e;
    --border-color: #333;
    --input-bg: #2a2a2a;
    --hover-color: #3a7a5d;
    --muted-color: #888;
    --success-color: #2ecc71;
    --error-color: #e74c3c;
    --warning-color: #f39c12;
    --info-color: #3498db;
    --uploading-color: #f39c12;
    --complete-color: #2ecc71;
    --queued-color: #888;
    --processing-color: #3498db;
}

.light-mode {
    --primary-color: #1a5d1a;
    --secondary-color: #2e8b57;
    --accent-color: #50c878;
    --text-color: #333;
    --bg-color: #f5f5f5;
    --card-bg: #ffffff;
    --border-color: #ddd;
    --input-bg: #f9f9f9;
    --hover-color: #3a7a5d;
    --muted-color: #777;
}

/* Collapsible Section Styles */
.collapsible-section {
    margin-bottom: 20px;
    border: 1px solid var(--border-color);
    border-radius: 8px;
    overflow: hidden;
    background-color: var(--card-bg);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    transition: all 0.3s ease;
}

.section-header {
    padding: 15px;
    display: flex;
    justify-content: space-between;
    align-items: center;
    cursor: pointer;
    background-color: var(--card-bg);
    border-bottom: 1px solid var(--border-color);
}

.section-header h3 {
    margin: 0;
    font-size: 1.1rem;
    color: var(--accent-color);
    display: flex;
    align-items: center;
    gap: 8px;
}

.section-header h3 i {
    font-size: 1.2rem;
}

.section-actions {
    display: flex;
    gap: 10px;
    align-items: center;
}

.toggle-btn {
    background: none;
    border: none;
    color: var(--muted-color);
    cursor: pointer;
    font-size: 1rem;
    transition: transform 0.3s;
}

.section-content {
    padding: 20px;
    background-color: var(--card-bg);
}

.collapsible-section.collapsed .section-content {
    display: none;
}

.collapsible-section.collapsed .toggle-btn i {
    transform: rotate(-90deg);
}

/* Enhanced Drop Zone */
.drop-zone {
    border: 2px dashed var(--border-color);
    border-radius: 8px;
    padding: 30px;
    text-align: center;
    transition: all 0.3s;
    background-color: var(--input-bg);
    margin-bottom: 20px;
}

.drop-zone.drag-over {
    border-color: var(--accent-color);
    background-color: rgba(80, 200, 120, 0.1);
    transform: scale(1.02);
}

.drop-zone-icon {
    font-size: 48px;
    color: var(--muted-color);
    margin-bottom: 15px;
}

.supported-formats {
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-top: 10px;
}

.select-files-btn {
    background-color: var(--primary-color);
    color: white;
    border: none;
    padding: 10px 20px;
    border-radius: 4px;
    cursor: pointer;
    margin-top: 15px;
    font-size: 0.9rem;
    transition: all 0.2s;
}

.select-files-btn:hover {
    background-color: var(--hover-color);
    transform: translateY(-2px);
}

/* Hide the actual file input */
#document-file {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    border: 0;
}

/* File Preview Styles */
.view-toggle-container {
    display: flex;
    gap: 5px;
    margin-bottom: 15px;
}

.view-toggle-btn {
    background-color: var(--input-bg);
    border: 1px solid var(--border-color);
    border-radius: 4px;
    padding: 5px 10px;
    cursor: pointer;
    color: var(--text-color);
}

.view-toggle-btn.active {
    background-color: var(--primary-color);
    color: white;
    border-color: var(--primary-color);
}

.file-list {
    margin-top: 15px;
    display: grid;
    gap: 15px;
    transition: all 0.3s;
}

.file-list.grid-view {
    grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
}

.file-list.list-view {
    grid-template-columns: 1fr;
}

.file-preview {
    border: 1px solid var(--border-color);
    border-radius: 8px;
    overflow: hidden;
    position: relative;
    background-color: var(--card-bg);
    transition: transform 0.2s, box-shadow 0.2s;
}

.file-preview:hover {
    transform: translateY(-3px);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
}

.file-thumbnail {
    height: 120px;
    background-color: var(--input-bg);
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 40px;
    color: var(--muted-color);
}

.list-view .file-thumbnail {
    height: 60px;
    width: 60px;
    float: left;
    margin-right: 15px;
}

.file-info {
    padding: 10px;
}

.list-view .file-info {
    display: flex;
    align-items: center;
    justify-content: space-between;
    flex-grow: 1;
}

.file-name {
    font-weight: 500;
    margin-bottom: 5px;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

.file-meta {
    display: flex;
    justify-content: space-between;
    font-size: 0.8rem;
    color: var(--muted-color);
}

.file-date {
    font-size: 0.8rem;
    color: var(--muted-color);
}

.file-remove {
    position: absolute;
    top: 5px;
    right: 5px;
    background-color: var(--error-color);
    color: white;
    border: none;
    width: 24px;
    height: 24px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    cursor: pointer;
    font-size: 1rem;
    opacity: 0.8;
    transition: opacity 0.2s;
}

.file-remove:hover {
    opacity: 1;
}

.file-actions {
    display: flex;
    justify-content: space-between;
    margin-top: 15px;
}

/* Enhanced Progress Indicators */
.progress-container {
    margin-top: 20px;
}

.overall-progress {
    margin-bottom: 20px;
}

.progress-header {
    display: flex;
    justify-content: space-between;
    margin-bottom: 5px;
}

.progress-title {
    font-weight: 500;
    color: var(--text-color);
}

.progress-stats {
    font-size: 0.9rem;
    color: var(--muted-color);
}

.progress-bar {
    height: 10px;
    background-color: var(--input-bg);
    border-radius: 5px;
    overflow: hidden;
    margin-bottom: 5px;
}

.progress-bar-fill {
    height: 100%;
    background-color: var(--accent-color);
    border-radius: 5px;
    transition: width 0.3s;
}

.progress-details {
    display: flex;
    justify-content: space-between;
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-top: 5px;
}

.file-progress-list {
    display: flex;
    flex-direction: column;
    gap: 10px;
}

.file-progress-item {
    display: flex;
    align-items: center;
    gap: 10px;
    padding: 10px;
    border: 1px solid var(--border-color);
    border-radius: 4px;
    background-color: var(--card-bg);
}

.file-progress-icon {
    font-size: 1.5rem;
    color: var(--muted-color);
    width: 30px;
    text-align: center;
}

.file-progress-content {
    flex-grow: 1;
}

.file-progress-header {
    display: flex;
    justify-content: space-between;
    margin-bottom: 5px;
}

.file-progress-name {
    font-weight: 500;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
    max-width: 200px;
}

.file-progress-status {
    font-size: 0.8rem;
    padding: 2px 6px;
    border-radius: 10px;
    background-color: var(--queued-color);
    color: white;
}

.file-progress-status[data-status="uploading"] {
    background-color: var(--uploading-color);
}

.file-progress-status[data-status="processing"] {
    background-color: var(--processing-color);
}

.file-progress-status[data-status="complete"] {
    background-color: var(--complete-color);
}

.file-progress-status[data-status="error"] {
    background-color: var(--error-color);
}

.file-progress-bar {
    height: 6px;
    background-color: var(--input-bg);
    border-radius: 3px;
    overflow: hidden;
    margin-bottom: 5px;
}

.file-progress-fill {
    height: 100%;
    background-color: var(--accent-color);
    border-radius: 3px;
    transition: width 0.3s;
}

.file-progress-details {
    display: flex;
    justify-content: space-between;
    font-size: 0.8rem;
    color: var(--muted-color);
}

/* Batch Actions Toolbar */
.batch-actions-toolbar {
    position: fixed;
    bottom: -60px;
    left: 0;
    right: 0;
    background-color: var(--card-bg);
    border-top: 1px solid var(--border-color);
    padding: 15px 20px;
    display: flex;
    justify-content: space-between;
    align-items: center;
    transition: bottom 0.3s;
    z-index: 100;
    box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
}

.batch-actions-toolbar.visible {
    bottom: 0;
}

.selection-count {
    font-weight: 500;
    color: var(--text-color);
}

.batch-actions-buttons {
    display: flex;
    gap: 10px;
}

.batch-btn {
    padding: 8px 15px;
    border-radius: 4px;
    border: 1px solid var(--border-color);
    background-color: var(--card-bg);
    cursor: pointer;
    display: flex;
    align-items: center;
    gap: 5px;
    font-size: 0.9rem;
    color: var(--text-color);
    transition: all 0.2s;
}

.batch-btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
}

.batch-btn.primary {
    background-color: var(--primary-color);
    color: white;
    border-color: var(--primary-color);
}

.batch-btn.danger {
    background-color: var(--error-color);
    color: white;
    border-color: var(--error-color);
}

/* Modal Styles for Batch Operations */
.modal-overlay {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-color: rgba(0, 0, 0, 0.5);
    display: flex;
    align-items: center;
    justify-content: center;
    z-index: 1000;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s;
}

.modal-overlay.visible {
    opacity: 1;
    visibility: visible;
}

.modal {
    background-color: var(--card-bg);
    border-radius: 8px;
    width: 90%;
    max-width: 500px;
    max-height: 90vh;
    overflow-y: auto;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
    transform: translateY(20px);
    transition: transform 0.3s;
}

.modal-overlay.visible .modal {
    transform: translateY(0);
}

.modal-header {
    padding: 15px 20px;
    border-bottom: 1px solid var(--border-color);
    display: flex;
    justify-content: space-between;
    align-items: center;
}

.modal-title {
    margin: 0;
    font-size: 1.2rem;
    color: var(--accent-color);
}

.modal-close {
    background: none;
    border: none;
    font-size: 1.5rem;
    cursor: pointer;
    color: var(--muted-color);
}

.modal-body {
    padding: 20px;
}

.modal-footer {
    padding: 15px 20px;
    border-top: 1px solid var(--border-color);
    display: flex;
    justify-content: flex-end;
    gap: 10px;
}

/* Tag Input Styles */
.tag-input-container {
    position: relative;
    margin-bottom: 15px;
}

.tag-input {
    width: 100%;
    padding: 10px;
    border: 1px solid var(--border-color);
    border-radius: 4px;
    background-color: var(--input-bg);
    color: var(--text-color);
    font-size: 0.9rem;
}

.tag-list {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
    margin-bottom: 15px;
}

.tag-item {
    background-color: var(--primary-color);
    color: white;
    padding: 5px 10px;
    border-radius: 15px;
    font-size: 0.9rem;
    display: flex;
    align-items: center;
    gap: 5px;
}

.tag-remove {
    cursor: pointer;
    font-weight: bold;
}

.batch-options {
    margin-top: 15px;
}

.checkbox-label {
    display: flex;
    align-items: center;
    gap: 8px;
    cursor: pointer;
    color: var(--text-color);
}

/* Upload Summary Styles */
.summary-stats {
    display: flex;
    justify-content: space-around;
    margin-bottom: 20px;
}

.summary-stat {
    text-align: center;
}

.stat-value {
    font-size: 2rem;
    font-weight: bold;
    display: block;
    color: var(--accent-color);
}

.stat-label {
    font-size: 0.9rem;
    color: var(--muted-color);
}

.summary-list {
    margin-bottom: 15px;
}

.summary-item {
    display: flex;
    align-items: center;
    gap: 10px;
    padding: 8px;
    border-bottom: 1px solid var(--border-color);
    color: var(--text-color);
}

.summary-item.success i {
    color: var(--success-color);
}

.summary-item.error i {
    color: var(--error-color);
}

.summary-item-error {
    font-size: 0.8rem;
    color: var(--error-color);
    margin-left: auto;
}

/* Button Styles */
.btn {
    padding: 8px 15px;
    border-radius: 4px;
    border: 1px solid var(--border-color);
    background-color: var(--card-bg);
    cursor: pointer;
    font-size: 0.9rem;
    color: var(--text-color);
    transition: all 0.2s;
}

.btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
}

.btn.primary {
    background-color: var(--primary-color);
    color: white;
    border-color: var(--primary-color);
}

.btn.danger {
    background-color: var(--error-color);
    color: white;
    border-color: var(--error-color);
}

/* Responsive Adjustments */
@media (max-width: 768px) {
    .file-list.grid-view {
        grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));
    }

    .batch-actions-toolbar {
        flex-direction: column;
        gap: 10px;
    }

    .batch-actions-buttons {
        width: 100%;
        justify-content: space-between;
    }

    .file-progress-name {
        max-width: 150px;
    }
}

================
File: app/static/css/fonts.css
================
@font-face {
    font-family: 'Azonix';
    src: url('../fonts/Azonix.otf') format('opentype');
    font-weight: normal;
    font-style: normal;
    font-display: swap;
}

/* Create empty font files to prevent 404 errors */
@font-face {
    font-family: 'Azonix-woff2';
    src: url('../fonts/Azonix.otf') format('opentype');
    font-weight: normal;
    font-style: normal;
    font-display: swap;
}

@font-face {
    font-family: 'Azonix-woff';
    src: url('../fonts/Azonix.otf') format('opentype');
    font-weight: normal;
    font-style: normal;
    font-display: swap;
}

@font-face {
    font-family: 'Azonix-ttf';
    src: url('../fonts/Azonix.otf') format('opentype');
    font-weight: normal;
    font-style: normal;
    font-display: swap;
}

/* Fallback font definition */
.azonix-font {
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
    letter-spacing: 1px;
}

================
File: app/static/css/login.css
================
.login-container {
    max-width: 400px;
    margin: 100px auto;
    padding: 20px;
    background-color: #fff;
    border-radius: 8px;
    box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
}
.form-group {
    margin-bottom: 15px;
}
.form-group label {
    display: block;
    margin-bottom: 5px;
    font-weight: bold;
}
.form-group input {
    width: 100%;
    padding: 8px;
    border: 1px solid #ddd;
    border-radius: 4px;
}
.btn {
    display: inline-block;
    padding: 10px 15px;
    background-color: #007bff;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}
.btn:hover {
    background-color: #0069d9;
}
.error-message {
    color: red;
    margin-bottom: 15px;
}
.register-link {
    margin-top: 15px;
    text-align: center;
}

================
File: app/static/css/register.css
================
.register-container {
    max-width: 400px;
    margin: 100px auto;
    padding: 20px;
    background-color: #fff;
    border-radius: 8px;
    box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
}
.form-group {
    margin-bottom: 15px;
}
.form-group label {
    display: block;
    margin-bottom: 5px;
    font-weight: bold;
}
.form-group input {
    width: 100%;
    padding: 8px;
    border: 1px solid #ddd;
    border-radius: 4px;
}
.btn {
    display: inline-block;
    padding: 10px 15px;
    background-color: #007bff;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}
.btn:hover {
    background-color: #0069d9;
}
.error-message {
    color: red;
    margin-bottom: 15px;
}
.login-link {
    margin-top: 15px;
    text-align: center;
}

================
File: app/static/css/schema.css
================
/* Schema Viewer Styles */

.schema-controls {
    display: flex;
    flex-wrap: wrap;
    gap: 20px;
    margin-bottom: 20px;
    padding: 15px;
    background-color: #f5f7fa;
    border-radius: 5px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
}

.connection-selector,
.schema-selector,
.table-selector {
    display: flex;
    flex-direction: column;
    gap: 5px;
}

.connection-selector label,
.schema-selector label,
.table-selector label {
    font-weight: 600;
    color: #333;
}

.connection-selector select,
.schema-selector select,
.table-selector select {
    padding: 8px 12px;
    border: 1px solid #ccc;
    border-radius: 4px;
    min-width: 200px;
}

#refresh-connections {
    margin-top: 5px;
    padding: 6px 12px;
    background-color: #f0f0f0;
    border: 1px solid #ccc;
    border-radius: 4px;
    cursor: pointer;
}

#refresh-connections:hover {
    background-color: #e0e0e0;
}

.tabs {
    display: flex;
    border-bottom: 1px solid #ccc;
    margin-bottom: 20px;
}

.tab-button {
    padding: 10px 20px;
    background-color: #f5f5f5;
    border: 1px solid #ccc;
    border-bottom: none;
    border-radius: 4px 4px 0 0;
    margin-right: 5px;
    cursor: pointer;
    font-weight: 500;
}

.tab-button:hover {
    background-color: #e9e9e9;
}

.tab-button.active {
    background-color: #fff;
    border-bottom: 1px solid #fff;
    margin-bottom: -1px;
    color: #007bff;
}

.tab-pane {
    display: none;
    padding: 20px;
    background-color: #fff;
    border: 1px solid #ccc;
    border-top: none;
    border-radius: 0 0 4px 4px;
}

.tab-pane.active {
    display: block;
}

.table-info {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    gap: 15px;
    margin-bottom: 20px;
    padding: 15px;
    background-color: #f9f9f9;
    border-radius: 5px;
}

.info-item {
    display: flex;
    flex-direction: column;
}

.info-item .label {
    font-weight: 600;
    color: #555;
    margin-bottom: 5px;
}

.info-item .value {
    color: #333;
}

.content-area {
    max-height: 500px;
    overflow: auto;
    padding: 15px;
    background-color: #f9f9f9;
    border-radius: 5px;
}

table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 20px;
}

table th {
    background-color: #f0f0f0;
    padding: 10px;
    text-align: left;
    font-weight: 600;
    border: 1px solid #ddd;
}

table td {
    padding: 10px;
    border: 1px solid #ddd;
}

table tr:nth-child(even) {
    background-color: #f9f9f9;
}

table tr:hover {
    background-color: #f0f0f0;
}

.explain-controls {
    display: flex;
    flex-direction: column;
    gap: 10px;
    margin-bottom: 20px;
}

#query-input {
    width: 100%;
    height: 100px;
    padding: 10px;
    border: 1px solid #ccc;
    border-radius: 4px;
    font-family: monospace;
    resize: vertical;
}

.explain-options {
    display: flex;
    flex-wrap: wrap;
    gap: 15px;
    margin-bottom: 10px;
}

#explain-button {
    padding: 8px 16px;
    background-color: #007bff;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    align-self: flex-start;
}

#explain-button:hover {
    background-color: #0069d9;
}

#explain-content {
    font-family: monospace;
    white-space: pre-wrap;
    padding: 15px;
    background-color: #f5f5f5;
    border-radius: 5px;
    border: 1px solid #ddd;
}

.loading-overlay {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: rgba(0, 0, 0, 0.5);
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    z-index: 1000;
}

.spinner {
    border: 4px solid rgba(255, 255, 255, 0.3);
    border-radius: 50%;
    border-top: 4px solid #fff;
    width: 40px;
    height: 40px;
    animation: spin 1s linear infinite;
    margin-bottom: 10px;
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

.loading-overlay p {
    color: white;
    font-size: 18px;
}

.hidden {
    display: none;
}

.modal {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: rgba(0, 0, 0, 0.5);
    display: flex;
    justify-content: center;
    align-items: center;
    z-index: 1000;
}

.modal-content {
    background-color: white;
    padding: 20px;
    border-radius: 5px;
    max-width: 500px;
    width: 100%;
    position: relative;
}

.close {
    position: absolute;
    top: 10px;
    right: 10px;
    font-size: 24px;
    cursor: pointer;
}

.close:hover {
    color: #999;
}

================
File: app/static/css/styles.css
================
/* Base styles */
:root {
    --primary-color: #1a5d1a;
    --secondary-color: #2e8b57;
    --accent-color: #50c878;
    --text-color: #f0f0f0;
    --bg-color: #121212;
    --card-bg: #1e1e1e;
    --border-color: #333;
    --input-bg: #2a2a2a;
    --hover-color: #3a7a5d;
    --muted-color: #888;
    --sidebar-width: 280px;
    --success-color: #2ecc71;
    --error-color: #e74c3c;
    --warning-color: #f39c12;
    --info-color: #3498db;
    --chat-user-bg: #1e1e1e;
    --chat-bot-bg: var(--secondary-color);
    --chat-container-bg: var(--card-bg);
    --source-bg: rgba(80, 200, 120, 0.2);
    --shadow-sm: 0 2px 4px rgba(0, 0, 0, 0.1);
    --shadow-md: 0 4px 8px rgba(0, 0, 0, 0.2);
    --shadow-lg: 0 8px 16px rgba(0, 0, 0, 0.3);
    --border-radius-sm: 4px;
    --border-radius-md: 8px;
    --border-radius-lg: 12px;
    --ginkgo-green: #50c878;
    --ginkgo-dark: #121212;
}

.light-mode {
    --primary-color: #1a5d1a;
    --secondary-color: #2e8b57;
    --accent-color: #50c878;
    --text-color: #333;
    --bg-color: #f5f5f5;
    --card-bg: #ffffff;
    --border-color: #ddd;
    --input-bg: #f9f9f9;
    --hover-color: #3a7a5d;
    --muted-color: #777;
    --chat-user-bg: #f0f0f0;
    --chat-bot-bg: rgba(46, 139, 87, 0.1);
    --chat-container-bg: var(--card-bg);
    --source-bg: rgba(80, 200, 120, 0.1);
    --shadow-sm: 0 2px 4px rgba(0, 0, 0, 0.05);
    --shadow-md: 0 4px 8px rgba(0, 0, 0, 0.1);
    --shadow-lg: 0 8px 16px rgba(0, 0, 0, 0.15);
    --ginkgo-green: #2e8b57;
    --ginkgo-dark: #333;
}

* {
    box-sizing: border-box;
    transition: background-color 0.3s, color 0.3s, box-shadow 0.3s, transform 0.2s;
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    margin: 0;
    padding: 0;
    background-color: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
    height: 100vh;
    overflow: hidden;
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
}

.app-container {
    display: flex;
    height: 100vh;
    overflow: hidden;
    background-image: linear-gradient(to bottom right, rgba(26, 93, 26, 0.05), rgba(80, 200, 120, 0.05));
}

/* Sidebar Styles */
.sidebar {
    width: var(--sidebar-width);
    background-color: var(--card-bg);
    border-right: 1px solid var(--border-color);
    padding: 20px;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    box-shadow: var(--shadow-md);
    z-index: 10;
}

.sidebar-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 20px;
    padding-bottom: 12px;
    border-bottom: 1px solid var(--border-color);
}

.sidebar-content {
    flex: 1;
    overflow-y: auto;
    padding-right: 5px;
}

/* Custom scrollbar for sidebar */
.sidebar-content::-webkit-scrollbar {
    width: 6px;
}

.sidebar-content::-webkit-scrollbar-track {
    background: transparent;
}

.sidebar-content::-webkit-scrollbar-thumb {
    background-color: var(--border-color);
    border-radius: 10px;
}

/* Main Content Styles */
.main-content {
    flex: 1;
    display: flex;
    flex-direction: column;
    padding: 24px;
    overflow: hidden;
}

.chat-area {
    flex: 1;
    display: flex;
    flex-direction: column;
    overflow: hidden;
}

.chat-container {
    flex: 1;
    background-color: var(--chat-container-bg);
    border-radius: var(--border-radius-md);
    padding: 24px;
    margin-bottom: 24px;
    box-shadow: var(--shadow-md);
    overflow-y: auto;
    scroll-behavior: smooth;
}

/* Custom scrollbar for chat container */
.chat-container::-webkit-scrollbar {
    width: 6px;
}

.chat-container::-webkit-scrollbar-track {
    background: transparent;
}

.chat-container::-webkit-scrollbar-thumb {
    background-color: var(--border-color);
    border-radius: 10px;
}

.input-area {
    background-color: var(--card-bg);
    border-radius: var(--border-radius-md);
    padding: 24px;
    box-shadow: var(--shadow-md);
}

h1 {
    color: var(--ginkgo-green);
    margin: 0;
    font-size: 1.8rem;
    flex: 1;
    margin-right: 10px;
    font-weight: normal;
    letter-spacing: 1px;
    display: flex;
    align-items: center;
    gap: 8px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
}

.theme-toggle {
    background: var(--ginkgo-dark);
    color: var(--ginkgo-green);
    border: 1px solid var(--ginkgo-green);
    padding: 6px 10px;
    border-radius: 20px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 4px;
    font-size: 0.8rem;
    transition: all 0.2s ease;
    box-shadow: var(--shadow-sm);
}

.theme-toggle:hover {
    background: var(--ginkgo-green);
    color: var(--ginkgo-dark);
    transform: translateY(-1px);
    box-shadow: var(--shadow-md);
}

.theme-toggle:active {
    transform: translateY(0);
}

/* Message Styles */
.message {
    margin-bottom: 20px;
    padding: 14px 18px;
    border-radius: var(--border-radius-md);
    position: relative;
    animation: messageAppear 0.4s forwards;
    box-shadow: var(--shadow-sm);
    line-height: 1.5;
    max-width: 85%;
}

@keyframes messageAppear {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

.user-message {
    background-color: var(--chat-user-bg);
    color: var(--ginkgo-green);
    margin-left: auto;
    margin-right: 10px;
    border-bottom-right-radius: 4px;
    align-self: flex-end;
    border: 1px solid rgba(0, 255, 0, 0.3);
}

.bot-message {
    background-color: rgba(0, 255, 0, 0.05);
    color: white;
    margin-left: 10px;
    margin-right: auto;
    border-bottom-left-radius: 4px;
    white-space: pre-wrap;
    align-self: flex-start;
    border-left: 3px solid var(--ginkgo-green);
}

.light-mode .user-message,
.light-mode .bot-message {
    color: var(--text-color);
}

.message-header {
    font-weight: normal;
    margin-bottom: 8px;
    font-size: 0.9rem;
    opacity: 0.9;
    color: var(--ginkgo-green);
    text-transform: uppercase;
    letter-spacing: 1px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.copy-button {
    position: absolute;
    top: 10px;
    right: 10px;
    background-color: rgba(255, 255, 255, 0.2);
    color: white;
    border: none;
    border-radius: var(--border-radius-sm);
    padding: 4px 10px;
    font-size: 0.8rem;
    cursor: pointer;
    opacity: 0;
    transition: opacity 0.2s ease;
}

.message:hover .copy-button {
    opacity: 1;
}

.copy-button:hover {
    background-color: rgba(255, 255, 255, 0.3);
    transform: translateY(-1px);
}

.copy-button:active {
    transform: translateY(0);
}

/* Form Elements */
.form-group {
    margin-bottom: 18px;
}

label {
    display: block;
    margin-bottom: 6px;
    font-weight: normal;
    font-size: 0.9rem;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    letter-spacing: 0.5px;
    text-transform: uppercase;
    color: var(--ginkgo-green);
}

.param-description {
    font-size: 0.85rem;
    color: var(--muted-color);
    margin-top: 3px;
    margin-bottom: 6px;
    line-height: 1.4;
}

.tooltip {
    display: inline-block;
    margin-left: 5px;
    color: var(--ginkgo-green);
    cursor: help;
    position: relative;
}

.tooltip .tooltip-text {
    visibility: hidden;
    width: 280px;
    background-color: var(--card-bg);
    color: var(--text-color);
    text-align: left;
    border-radius: var(--border-radius-md);
    padding: 12px;
    position: absolute;
    z-index: 100;
    bottom: 125%;
    left: 50%;
    margin-left: -140px;
    opacity: 0;
    transition: opacity 0.3s, transform 0.3s;
    box-shadow: var(--shadow-md);
    border: 1px solid var(--border-color);
    font-weight: normal;
    font-size: 0.9rem;
    transform: translateY(5px);
    line-height: 1.5;
}

.tooltip:hover .tooltip-text {
    visibility: visible;
    opacity: 1;
    transform: translateY(0);
}

input[type="text"],
input[type="number"],
textarea,
select {
    width: 100%;
    padding: 12px;
    border: 1px solid var(--border-color);
    border-radius: var(--border-radius-sm);
    background-color: var(--input-bg);
    color: var(--text-color);
    font-family: inherit;
    font-size: 0.95rem;
    transition: border-color 0.3s, box-shadow 0.3s;
}

input[type="text"]:focus,
input[type="number"]:focus,
textarea:focus,
select:focus {
    outline: none;
    border-color: var(--accent-color);
    box-shadow: 0 0 0 2px rgba(80, 200, 120, 0.2);
}

textarea {
    resize: vertical;
    min-height: 100px;
    line-height: 1.5;
}

.checkbox-container {
    display: flex;
    align-items: center;
    gap: 10px;
    padding: 5px 0;
}

input[type="checkbox"] {
    width: 18px;
    height: 18px;
    cursor: pointer;
}

button {
    background-color: var(--ginkgo-green);
    color: var(--ginkgo-dark);
    padding: 10px 20px;
    border: none;
    border-radius: var(--border-radius-sm);
    cursor: pointer;
    font-size: 0.9rem;
    font-weight: normal;
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 8px;
    transition: all 0.2s ease;
    box-shadow: var(--shadow-sm);
    text-transform: uppercase;
    letter-spacing: 1px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

button:hover {
    background-color: var(--hover-color);
    transform: translateY(-1px);
    box-shadow: var(--shadow-md);
}

button:active {
    transform: translateY(0);
}

button.secondary {
    background-color: transparent;
    color: var(--ginkgo-green);
    border: 1px solid var(--ginkgo-green);
}

button.secondary:hover {
    background-color: rgba(0, 255, 0, 0.1);
}

button.danger {
    background-color: transparent;
    color: #ff3333;
    border: 1px solid #ff3333;
}

button.danger:hover {
    background-color: rgba(255, 51, 51, 0.1);
}

/* Retry button for streaming errors */
.retry-button {
    display: inline-block;
    margin-top: 10px;
    padding: 8px 16px;
    background-color: var(--accent-color);
    color: white;
    border: none;
    border-radius: var(--border-radius-sm);
    cursor: pointer;
    font-size: 14px;
    transition: background-color 0.2s;
    box-shadow: var(--shadow-sm);
}

.retry-button:hover {
    background-color: var(--hover-color);
    transform: translateY(-1px);
    box-shadow: var(--shadow-md);
}

/* Advanced Options */
.advanced-options {
    margin-top: 20px;
    border-top: 1px solid var(--border-color);
    padding-top: 15px;
}

.advanced-toggle {
    background: none;
    border: none;
    color: var(--ginkgo-green);
    cursor: pointer;
    padding: 0;
    font-size: 0.9rem;
    display: flex;
    align-items: center;
    gap: 5px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    letter-spacing: 0.5px;
    text-transform: uppercase;
}

.advanced-content {
    display: none;
    margin-top: 15px;
}

.advanced-content.show {
    display: block;
}

.parameter-grid {
    display: grid;
    grid-template-columns: 1fr;
    gap: 15px;
}

/* Action Buttons */
.action-buttons, .batch-actions, .filter-actions {
    display: flex;
    gap: 10px;
    margin-top: 20px;
}

.action-buttons button, .batch-actions button, .filter-actions button {
    flex: 1;
}

/* Token Usage Indicator */
.token-usage {
    position: fixed;
    bottom: 15px;
    right: 15px;
    background-color: var(--card-bg);
    padding: 10px 15px;
    border-radius: var(--border-radius-md);
    font-size: 0.85rem;
    box-shadow: var(--shadow-md);
    display: none;
    border: 1px solid var(--border-color);
    z-index: 100;
    transition: opacity 0.3s, transform 0.3s;
    transform: translateY(0);
}

.token-usage:hover {
    transform: translateY(-2px);
}

.token-usage-title {
    font-weight: 600;
    margin-bottom: 6px;
    display: flex;
    align-items: center;
    gap: 6px;
}

.token-usage-bar {
    width: 100%;
    height: 8px;
    background-color: var(--border-color);
    border-radius: 4px;
    margin-bottom: 6px;
    overflow: hidden;
}

#token-usage-fill {
    height: 100%;
    width: 0%;
    background-color: var(--ginkgo-green);
    border-radius: 4px;
    transition: width 0.5s ease-out;
}

/* Source Citation Styles */
.sources-section {
    margin-top: 0.75rem;
    font-size: 0.85rem;
    color: var(--text-color);
    opacity: 0.9;
    padding-top: 0.5rem;
    border-top: 1px solid rgba(255, 255, 255, 0.1);
}

.source-item {
    display: inline-block;
    background-color: var(--source-bg);
    margin-right: 0.5rem;
    margin-bottom: 0.5rem;
    padding: 0.3rem 0.6rem;
    border-radius: var(--border-radius-sm);
    font-size: 0.8rem;
    transition: transform 0.2s;
}

.source-item:hover {
    transform: translateY(-1px);
}

/* Loading Indicator */
.loading {
    display: none;
    margin-top: 25px;
    text-align: center;
    color: var(--ginkgo-green);
    font-weight: 500;
}

.loading.show {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 10px;
}

.spinner {
    display: inline-block;
    width: 24px;
    height: 24px;
    border: 3px solid rgba(0, 255, 0, 0.1);
    border-radius: 50%;
    border-top-color: var(--ginkgo-green);
    animation: spin 1s ease-in-out infinite;
    vertical-align: middle;
}

@keyframes spin {
    to { transform: rotate(360deg); }
}

/* Utility Classes */
.text-center {
    text-align: center;
}

.mb-1 {
    margin-bottom: 0.5rem;
}

.mb-2 {
    margin-bottom: 1rem;
}

.mb-3 {
    margin-bottom: 1.5rem;
}

.success {
    color: var(--success-color);
}

.error {
    color: var(--error-color);
}

.warning {
    color: var(--warning-color);
}

.info {
    color: var(--info-color);
}

/* Responsive Styles */
@media (max-width: 768px) {
    .app-container {
        flex-direction: column;
    }
    
    .sidebar {
        width: 100%;
        height: auto;
        max-height: 40vh;
        border-right: none;
        border-bottom: 1px solid var(--border-color);
        padding: 15px;
    }
    
    .main-content {
        height: 60vh;
        padding: 15px;
    }
    
    .parameter-grid {
        grid-template-columns: 1fr;
    }
    
    .message {
        max-width: 90%;
        padding: 12px 15px;
    }
    
    h1 {
        font-size: 1.5rem;
    }
    
    .chat-container {
        padding: 15px;
        margin-bottom: 15px;
    }
    
    .input-area {
        padding: 15px;
    }
    
    .token-usage {
        bottom: 10px;
        right: 10px;
        padding: 8px 12px;
        font-size: 0.8rem;
    }
}

@media (max-width: 480px) {
    .message {
        max-width: 95%;
    }
    
    .copy-button {
        padding: 3px 6px;
        font-size: 0.7rem;
    }
    
    .sidebar-header {
        flex-direction: column;
        align-items: flex-start;
        gap: 10px;
    }
    
    .theme-toggle {
        align-self: flex-end;
    }
}

================
File: app/static/css/tasks.css
================
/* Tasks Page Styles */

/* Sidebar Navigation */
.sidebar-nav {
    display: flex;
    flex-direction: column;
    gap: 20px;
}

.sidebar-section {
    margin-bottom: 20px;
    padding-bottom: 20px;
    border-bottom: 1px solid var(--border-color);
}

.sidebar-section:last-child {
    border-bottom: none;
}

.sidebar-section h3 {
    font-size: 1rem;
    margin-bottom: 15px;
    color: var(--ginkgo-green);
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
    letter-spacing: 1px;
}

.nav-list {
    list-style: none;
    padding: 0;
    margin: 0;
}

.nav-list li {
    margin-bottom: 10px;
}

.nav-list a {
    display: flex;
    align-items: center;
    gap: 10px;
    padding: 10px;
    border-radius: var(--border-radius-sm);
    color: var(--text-color);
    text-decoration: none;
    transition: all 0.2s ease;
}

.nav-list a:hover {
    background-color: rgba(80, 200, 120, 0.1);
    color: var(--ginkgo-green);
    transform: translateX(3px);
}

.nav-list a.active {
    background-color: rgba(80, 200, 120, 0.2);
    color: var(--ginkgo-green);
    font-weight: 500;
}

.filter-group {
    margin-bottom: 15px;
}

.filter-group label {
    display: block;
    margin-bottom: 5px;
    font-size: 0.85rem;
    color: var(--text-color);
}

.refresh-interval {
    margin-top: 10px;
}

.refresh-interval label {
    display: block;
    margin-bottom: 5px;
    font-size: 0.85rem;
    color: var(--text-color);
}

/* Task Cards and Lists */
.stats-section {
    margin-bottom: 30px;
}

.stats-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
    gap: 20px;
    margin-bottom: 30px;
}

.stat-card {
    background-color: var(--card-bg);
    border-radius: var(--border-radius-md);
    padding: 20px;
    box-shadow: var(--shadow-md);
    text-align: center;
    border: 1px solid var(--border-color);
}

.stat-card h3 {
    margin-top: 0;
    margin-bottom: 10px;
    font-size: 0.9rem;
    color: var(--text-color);
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
    letter-spacing: 1px;
}

.stat-value {
    font-size: 2rem;
    font-weight: bold;
    color: var(--ginkgo-green);
}

.card {
    background-color: var(--card-bg);
    border-radius: var(--border-radius-md);
    box-shadow: var(--shadow-md);
    margin-bottom: 30px;
    border: 1px solid var(--border-color);
}

.card-header {
    padding: 15px 20px;
    border-bottom: 1px solid var(--border-color);
    display: flex;
    justify-content: space-between;
    align-items: center;
}

.card-title {
    margin: 0;
    font-size: 1.1rem;
    color: var(--ginkgo-green);
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
    letter-spacing: 1px;
}

.card-body {
    padding: 20px;
}

/* Task Table */
.table {
    width: 100%;
    border-collapse: collapse;
}

.table th,
.table td {
    padding: 12px 15px;
    text-align: left;
    border-bottom: 1px solid var(--border-color);
}

.table th {
    font-weight: 500;
    color: var(--ginkgo-green);
    text-transform: uppercase;
    font-size: 0.85rem;
    letter-spacing: 0.5px;
}

.table tbody tr:hover {
    background-color: rgba(80, 200, 120, 0.05);
}

.table .progress {
    height: 8px;
    background-color: var(--border-color);
    border-radius: 4px;
    overflow: hidden;
}

.table .progress-bar {
    height: 100%;
    background-color: var(--ginkgo-green);
    border-radius: 4px;
}

/* Status Badges */
.badge {
    display: inline-block;
    padding: 4px 8px;
    border-radius: 4px;
    font-size: 0.75rem;
    font-weight: 500;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.bg-light {
    background-color: var(--border-color);
}

.bg-info {
    background-color: var(--info-color);
}

.bg-primary {
    background-color: var(--primary-color);
}

.bg-success {
    background-color: var(--success-color);
}

.bg-danger {
    background-color: var(--error-color);
}

.bg-warning {
    background-color: var(--warning-color);
}

.bg-secondary {
    background-color: var(--muted-color);
}

.text-white {
    color: white;
}

.text-dark {
    color: var(--text-color);
}

/* Pagination */
.pagination {
    display: flex;
    justify-content: center;
    list-style: none;
    padding: 0;
    margin: 20px 0 0 0;
}

.page-item {
    margin: 0 5px;
}

.page-link {
    display: block;
    padding: 8px 12px;
    border-radius: var(--border-radius-sm);
    background-color: var(--card-bg);
    color: var(--text-color);
    text-decoration: none;
    border: 1px solid var(--border-color);
    transition: all 0.2s ease;
}

.page-item.active .page-link {
    background-color: var(--ginkgo-green);
    color: white;
    border-color: var(--ginkgo-green);
}

.page-item.disabled .page-link {
    opacity: 0.5;
    cursor: not-allowed;
}

.page-link:hover:not(.page-item.disabled .page-link) {
    background-color: rgba(80, 200, 120, 0.1);
    color: var(--ginkgo-green);
    transform: translateY(-2px);
}

/* Modals */
.modal-content {
    background-color: var(--card-bg);
    color: var(--text-color);
    border: 1px solid var(--border-color);
}

.modal-header {
    border-bottom: 1px solid var(--border-color);
}

.modal-footer {
    border-top: 1px solid var(--border-color);
}

.modal-title {
    color: var(--ginkgo-green);
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
    letter-spacing: 1px;
}

/* System Load Bar */
#system-load-bar.bg-success {
    background-color: var(--success-color);
}

#system-load-bar.bg-warning {
    background-color: var(--warning-color);
}

#system-load-bar.bg-danger {
    background-color: var(--error-color);
}

/* Task Details */
pre {
    background-color: var(--input-bg);
    padding: 15px;
    border-radius: var(--border-radius-sm);
    overflow: auto;
    font-family: 'Consolas', 'Monaco', monospace;
    font-size: 0.9rem;
    line-height: 1.5;
    max-height: 200px;
}

/* Responsive Styles */
@media (max-width: 768px) {
    .stats-grid {
        grid-template-columns: repeat(2, 1fr);
    }
    
    .table-responsive {
        overflow-x: auto;
    }
    
    .card-header {
        flex-direction: column;
        align-items: flex-start;
        gap: 10px;
    }
    
    .btn-group {
        align-self: flex-end;
    }
}

@media (max-width: 480px) {
    .stats-grid {
        grid-template-columns: 1fr;
    }
    
    .pagination {
        flex-wrap: wrap;
    }
}

================
File: app/static/js/chat.js
================
// Chat functionality
document.addEventListener('DOMContentLoaded', function() {
    // Elements
    const chatContainer = document.getElementById('chat-container');
    const userInput = document.getElementById('user-input');
    const sendButton = document.getElementById('send-button');
    const clearButton = document.getElementById('clear-chat');
    const saveButton = document.getElementById('save-chat');
    const modelSelect = document.getElementById('model');
    const ragToggle = document.getElementById('rag-toggle');
    const streamToggle = document.getElementById('stream-toggle');
    const loadingIndicator = document.getElementById('loading');
    const maxResults = document.getElementById('max-results');
    const temperature = document.getElementById('temperature');
    const metadataFilters = document.getElementById('metadata-filters');
    
    // Store conversation ID for maintaining context between messages
    let currentConversationId = null;
    
    // Toggle RAG parameters visibility
    if (ragToggle) {
        ragToggle.addEventListener('change', function() {
            const ragParams = document.querySelectorAll('.rag-param');
            ragParams.forEach(param => {
                param.style.display = this.checked ? 'block' : 'none';
            });
        });
    }
    
    // Load available models
    if (modelSelect) {
        console.log('Loading models...');
        authenticatedFetch('/api/system/models')
            .then(response => {
                console.log('Response status:', response.status);
                return response.json();
            })
            .then(models => {
                console.log('Models fetched:', models);
                console.log('Number of models:', models.length);
                
                // Clear the dropdown
                modelSelect.innerHTML = '';
                
                if (models && models.length > 0) {
                    // Add models to dropdown
                    models.forEach(model => {
                        const option = document.createElement('option');
                        option.value = model.name;
                        option.textContent = model.name;
                        modelSelect.appendChild(option);
                        console.log('Added model to dropdown:', model.name);
                    });
                } else {
                    // Add a default option if no models are available
                    const option = document.createElement('option');
                    option.value = 'gemma3:4b';
                    option.textContent = 'gemma3:4b (default)';
                    modelSelect.appendChild(option);
                    console.log('No models available, added default model');
                }
            })
            .catch(error => {
                console.error('Error loading models:', error);
            });
    }
    
    // Send message
    function sendMessage() {
        const message = userInput.value.trim();
        if (!message) return;
        
        // Check if user is authenticated
        if (!isAuthenticated()) {
            // Redirect to login page
            window.location.href = `/login?redirect=${encodeURIComponent(window.location.pathname)}`;
            return;
        }
        
        // Log the selected model
        console.log('Selected model for this message:', modelSelect.value);
        
        // Add user message to conversation
        addMessage('user', message);
        
        // Clear input
        userInput.value = '';
        
        // Show loading indicator
        loadingIndicator.style.display = 'block';
        
        // Prepare query
        const query = {
            message: message,
            model: modelSelect.value || 'gemma3:4b', // Use default model if none selected
            use_rag: ragToggle.checked,
            conversation_id: currentConversationId, // Include conversation ID if available
            model_parameters: {
                temperature: parseFloat(temperature.value),
                max_results: ragToggle.checked ? parseInt(maxResults.value) : 0
            }
        };
        
        // Log the query being sent
        console.log('Sending query with model:', query.model);
        
        // Parse metadata filters if provided
        if (ragToggle.checked && metadataFilters.value.trim()) {
            try {
                query.metadata_filters = JSON.parse(metadataFilters.value);
            } catch (e) {
                console.error('Invalid metadata filter JSON:', e);
                addMessage('assistant', 'Error: Invalid metadata filter format. Please use valid JSON.');
                loadingIndicator.style.display = 'none';
                return;
            }
        }
        // Use streaming based on the toggle
        query.stream = streamToggle.checked;
        
        // Create message element for assistant response
        const messageDiv = document.createElement('div');
        messageDiv.className = 'message assistant-message';
        
        // Add header
        const headerDiv = document.createElement('div');
        headerDiv.className = 'message-header';
        headerDiv.textContent = 'Metis:';
        messageDiv.appendChild(headerDiv);
        
        // Add content div for streaming response
        const contentDiv = document.createElement('div');
        contentDiv.id = 'streaming-response';
        messageDiv.appendChild(contentDiv);
        
        // Add to chat container
        chatContainer.appendChild(messageDiv);
        
        // Scroll to bottom
        chatContainer.scrollTop = chatContainer.scrollHeight;
        
        // Send to API with a timeout
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 60000); // 60 second timeout
        
        authenticatedFetch('/api/chat/query', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(query),
            signal: controller.signal
        })
        .then(response => {
            if (!response.ok) {
                throw new Error('Network response was not ok');
            }
            
            if (query.stream) {
                // Handle streaming response
                // Create a new reader for the response
                const reader = response.body.getReader();
                let decoder = new TextDecoder();
                let fullResponse = '';
                
                // Variable to track if the previous line was a conversation_id event
                let previousLineWasConversationIdEvent = false;
                
                // Function to process the stream with improved error handling
                function processStream() {
                    let streamTimeout;
                    let lastActivityTime = Date.now();
                    
                    // Set a timeout for the stream reading
                    const setStreamTimeout = () => {
                        clearTimeout(streamTimeout);
                        streamTimeout = setTimeout(() => {
                            // Check if we've had activity in the last 10 seconds
                            const inactiveTime = Date.now() - lastActivityTime;
                            if (inactiveTime > 10000) {
                                console.warn(`Stream reading timeout after ${inactiveTime}ms of inactivity - aborting`);
                                reader.cancel('Timeout');
                                
                                // Try again without streaming
                                if (streamToggle.checked) {
                                    // Update UI to show we're retrying
                                    contentDiv.textContent = 'The streaming response timed out. Retrying without streaming...';
                                    
                                    // Disable streaming and retry
                                    streamToggle.checked = false;
                                    sendButton.click();
                                }
                            } else {
                                // Reset the timeout
                                setStreamTimeout();
                            }
                        }, 5000); // Check every 5 seconds
                    };
                    
                    // Start the timeout
                    setStreamTimeout();
                    
                    return reader.read().then(({ done, value }) => {
                        // Update the last activity time
                        lastActivityTime = Date.now();
                        
                        if (done) {
                            // Clear the timeout when done
                            clearTimeout(streamTimeout);
                            
                            // Hide loading indicator when done
                            loadingIndicator.style.display = 'none';
                            return;
                        }
                        
                        // Decode the chunk and append to the response
                        const chunk = decoder.decode(value, { stream: true });
                        
                        // Process the chunk (which may contain multiple SSE events)
                        const lines = chunk.split('\n');
                        for (const line of lines) {
                            // Check for event type
                            if (line.startsWith('event:')) {
                                const eventType = line.substring(6).trim();
                                // Handle conversation_id event
                                if (eventType === 'conversation_id') {
                                    // The next line should be the data
                                    previousLineWasConversationIdEvent = true;
                                    continue;
                                }
                            }
                            else if (line.startsWith('data:')) {
                                const data = line.substring(5).trim();
                                if (data) {
                                    try {
                                        // Try to parse as JSON (for newer format)
                                        try {
                                            const jsonData = JSON.parse(data);
                                            
                                            // Check if this is conversation ID data
                                            if (previousLineWasConversationIdEvent) {
                                                try {
                                                    // The data should be the conversation ID
                                                    // Remove any quotes if present (in case it's a JSON string)
                                                    currentConversationId = data.replace(/^"|"$/g, '');
                                                    console.log('Conversation ID received in stream:', currentConversationId);
                                                } catch (e) {
                                                    console.error('Error parsing conversation ID:', e);
                                                }
                                                previousLineWasConversationIdEvent = false;
                                                continue; // Skip adding this to the response
                                            }
                                            
                                            if (jsonData.chunk) {
                                                fullResponse += jsonData.chunk;
                                            } else {
                                                fullResponse += data;
                                            }
                                        } catch (e) {
                                            // Skip if this is the conversation ID data that wasn't properly caught earlier
                                            if (previousLineWasConversationIdEvent) {
                                                // The data should be the conversation ID
                                                currentConversationId = data.replace(/^"|"$/g, '');
                                                console.log('Conversation ID received in stream (fallback):', currentConversationId);
                                                previousLineWasConversationIdEvent = false;
                                            } else {
                                                // If not JSON and not conversation ID, append the data (older format)
                                                // With streaming tokens from Ollama, we should not add spaces
                                                // as the model already handles proper spacing
                                                fullResponse += data;
                                            }
                                        }
                                    } catch (e) {
                                        console.error('Error processing chunk:', e);
                                    }
                                }
                            }
                        }
                        
                        // Update the content div with the current response
                        // Check if the response starts with a UUID pattern (conversation ID)
                        const uuidPattern = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\s/i;
                        if (fullResponse.match(uuidPattern)) {
                            // Remove the UUID from the beginning of the response
                            contentDiv.textContent = fullResponse.replace(uuidPattern, '');
                        } else {
                            contentDiv.textContent = fullResponse;
                        }
                        
                        // Scroll to bottom
                        chatContainer.scrollTop = chatContainer.scrollHeight;
                        
                        // Continue reading
                        return processStream();
                    }).catch(error => {
                        // Clear the timeout
                        clearTimeout(streamTimeout);
                        
                        console.error('Error reading stream:', error);
                        
                        // If we already have some response, show it
                        if (fullResponse.length > 0) {
                            contentDiv.textContent = fullResponse + "\n\n[Response was cut off due to a connection issue]";
                            
                            // Add a note about the error
                            const noteDiv = document.createElement('div');
                            noteDiv.className = 'streaming-note';
                            noteDiv.textContent = '(Note: The response was cut off due to a connection issue)';
                            noteDiv.style.fontSize = '0.8em';
                            noteDiv.style.fontStyle = 'italic';
                            noteDiv.style.marginTop = '10px';
                            noteDiv.style.color = 'var(--muted-color)';
                            messageDiv.appendChild(noteDiv);
                        } else {
                            // Update the content div with an error message
                            contentDiv.textContent = 'There was an error processing your request. ' +
                                'This might be due to a connection issue with the language model. ' +
                                'Try disabling streaming mode or check if the Ollama server is running properly.';
                        }
                        
                        // Hide loading indicator
                        loadingIndicator.style.display = 'none';
                        
                        // Add a retry button
                        const retryButton = document.createElement('button');
                        retryButton.textContent = 'Retry without streaming';
                        retryButton.className = 'retry-button';
                        retryButton.onclick = function() {
                            // Disable streaming and retry
                            if (streamToggle && streamToggle.checked) {
                                streamToggle.checked = false;
                                sendButton.click();
                            }
                        };
                        messageDiv.appendChild(retryButton);
                    });
                }
                
                // Start processing the stream
                return processStream();
            } else {
                // Handle non-streaming response
                return response.json().then(data => {
                    // Hide loading indicator
                    loadingIndicator.style.display = 'none';
                    
                    // Store conversation ID for future messages
                    if (data.conversation_id) {
                        currentConversationId = data.conversation_id;
                        console.log('Conversation ID updated:', currentConversationId);
                    }
                    
                    // Display the response
                    // Check if the response starts with a UUID pattern (conversation ID)
                    const uuidPattern = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\s/i;
                    if (data.message && data.message.match(uuidPattern)) {
                        // Remove the UUID from the beginning of the response
                        contentDiv.textContent = data.message.replace(uuidPattern, '');
                    } else {
                        contentDiv.textContent = data.message;
                    }
                    
                    // Scroll to bottom
                    chatContainer.scrollTop = chatContainer.scrollHeight;
                    
                    // Add citations if available
                    if (data.citations && data.citations.length > 0) {
                        const citationsDiv = document.createElement('div');
                        citationsDiv.className = 'sources-section';
                        citationsDiv.innerHTML = '<strong>Sources:</strong> ';
                        
                        data.citations.forEach(citation => {
                            const sourceSpan = document.createElement('span');
                            sourceSpan.className = 'source-item';
                            sourceSpan.textContent = citation.document_id;
                            sourceSpan.title = citation.excerpt;
                            citationsDiv.appendChild(sourceSpan);
                        });
                        
                        messageDiv.appendChild(citationsDiv);
                    }
                });
            }
        })
        .catch(error => {
            console.error('Error:', error);
            
            // Check if it's an abort error (timeout)
            if (error.name === 'AbortError') {
                console.log('Request timed out, trying again without streaming');
                
                // If streaming was enabled, try again without streaming
                if (streamToggle.checked) {
                    // Update UI to show we're retrying
                    contentDiv.textContent = 'The streaming request timed out. Retrying without streaming...';
                    
                    // Disable streaming and retry
                    query.stream = false;
                    
                    // Clear the previous timeout
                    clearTimeout(timeoutId);
                    
                    // Send the request again without streaming
                    authenticatedFetch('/api/chat/query', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(query)
                    })
                    .then(response => {
                        if (!response.ok) {
                            throw new Error('Network response was not ok');
                        }
                        return response.json();
                    })
                    .then(data => {
                        // Hide loading indicator
                        loadingIndicator.style.display = 'none';
                        
                        // Store conversation ID for future messages
                        if (data.conversation_id) {
                            currentConversationId = data.conversation_id;
                            console.log('Conversation ID updated:', currentConversationId);
                        }
                        
                        // Display the response
                        contentDiv.textContent = data.message;
                        
                        // Add a note that streaming was disabled
                        const noteDiv = document.createElement('div');
                        noteDiv.className = 'streaming-note';
                        noteDiv.textContent = '(Note: Streaming was disabled due to timeout)';
                        noteDiv.style.fontSize = '0.8em';
                        noteDiv.style.fontStyle = 'italic';
                        noteDiv.style.marginTop = '10px';
                        noteDiv.style.color = 'var(--muted-color)';
                        messageDiv.appendChild(noteDiv);
                        
                        // Uncheck streaming toggle for future requests
                        streamToggle.checked = false;
                    })
                    .catch(fallbackError => {
                        console.error('Error in fallback request:', fallbackError);
                        handleErrorMessage(fallbackError, contentDiv, message, ragToggle, streamToggle);
                        loadingIndicator.style.display = 'none';
                    });
                } else {
                    // If streaming was already disabled, show a regular error
                    handleErrorMessage(error, contentDiv, message, ragToggle, streamToggle);
                    loadingIndicator.style.display = 'none';
                }
            } else {
                // For other types of errors
                handleErrorMessage(error, contentDiv, message, ragToggle, streamToggle);
                loadingIndicator.style.display = 'none';
            }
        });
        
        // Helper function to handle error messages
        function handleErrorMessage(error, contentDiv, message, ragToggle, streamToggle) {
            // Add error message with more details
            contentDiv.textContent = 'Sorry, there was an error processing your request. ';
            
            // Check if the query is about future events
            const currentYear = new Date().getFullYear();
            const queryLower = message.toLowerCase();
            const yearMatch = queryLower.match(/\b(20\d\d|19\d\d)\b/);
            
            if (yearMatch && parseInt(yearMatch[1]) > currentYear) {
                contentDiv.textContent = `I cannot provide information about events in ${yearMatch[1]} as it's in the future. ` +
                    `The current year is ${currentYear}. I can only provide information about past or current events.`;
            }
            // Check if the query is about speculative future events
            else if (/what will happen|what is going to happen|predict the future|future events|in the future/.test(queryLower)) {
                contentDiv.textContent = "I cannot predict future events or provide information about what will happen in the future. " +
                    "I can only provide information about past or current events based on available data.";
            }
            // Add suggestion based on RAG status
            else if (ragToggle.checked) {
                contentDiv.textContent += 'This might be because there are no documents available for RAG. ' +
                    'Try uploading some documents or disabling the RAG feature.';
            }
            // Add suggestion based on streaming status
            else if (streamToggle.checked) {
                contentDiv.textContent += 'You might try disabling streaming mode for better error handling. ';
                
                // Add a retry button
                const retryButton = document.createElement('button');
                retryButton.textContent = 'Retry without streaming';
                retryButton.className = 'retry-button';
                retryButton.onclick = function() {
                    // Disable streaming and retry
                    streamToggle.checked = false;
                    sendButton.click();
                };
                messageDiv.appendChild(retryButton);
            }
            else {
                contentDiv.textContent += 'Please try again later or with different parameters.';
            }
        }
    }
    
    // Clear chat
    if (clearButton) {
        clearButton.addEventListener('click', function() {
            if (confirm('Are you sure you want to clear the chat history?')) {
                clearConversation();
                chatContainer.innerHTML = '';
                
                // Add welcome message
                const welcomeMessage = document.createElement('div');
                welcomeMessage.className = 'message bot-message';
                welcomeMessage.innerHTML = `
                    <div class="message-header">Metis:</div>
                    Hello! I'm your Metis RAG assistant. Ask me anything about your uploaded documents or chat with me directly.
                `;
                chatContainer.appendChild(welcomeMessage);
            }
        });
    }
    
    // Add message to chat
    function addMessage(role, content, citations = null) {
        // Create message element
        const messageDiv = document.createElement('div');
        messageDiv.className = `message ${role}-message`;
        
        // Add header
        const headerDiv = document.createElement('div');
        headerDiv.className = 'message-header';
        headerDiv.textContent = role === 'user' ? 'You:' : 'Metis:';
        messageDiv.appendChild(headerDiv);
        
        // Add content
        const contentDiv = document.createElement('div');
        contentDiv.textContent = content;
        messageDiv.appendChild(contentDiv);
        
        // Add citations if available
        if (citations && citations.length > 0) {
            const citationsDiv = document.createElement('div');
            citationsDiv.className = 'sources-section';
            citationsDiv.innerHTML = '<strong>Sources:</strong> ';
            
            citations.forEach(citation => {
                const sourceSpan = document.createElement('span');
                sourceSpan.className = 'source-item';
                sourceSpan.textContent = citation.document_id;
                sourceSpan.title = citation.excerpt;
                citationsDiv.appendChild(sourceSpan);
            });
            
            messageDiv.appendChild(citationsDiv);
        }
        
        // Add to chat container
        chatContainer.appendChild(messageDiv);
        
        // Scroll to bottom
        chatContainer.scrollTop = chatContainer.scrollHeight;
    }
    
    // Clear conversation
    function clearConversation() {
        // Reset conversation ID
        currentConversationId = null;
        console.log('Conversation ID reset');
        
        // Clear conversation from local storage or API
        authenticatedFetch('/api/chat/clear', {
            method: 'DELETE'
        })
        .then(response => response.json())
        .then(data => {
            console.log('Conversation cleared:', data);
            // Use the globally exposed clearConversation function from main.js
            // This ensures proper clearing of localStorage and updating the UI
            if (window.clearConversation) {
                window.clearConversation();
            } else {
                // Fallback if the global function isn't available
                localStorage.removeItem('metis_conversation');
                console.warn('window.clearConversation not found, using fallback clear method');
            }
        })
        .catch(error => {
            console.error('Error clearing conversation:', error);
        });
    }
    
    // Event listeners
    if (sendButton) {
        sendButton.addEventListener('click', sendMessage);
    }
    
    if (userInput) {
        userInput.addEventListener('keydown', function(e) {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendMessage();
            }
        });
    }
});

================
File: app/static/js/document-manager.js
================
// Document Management Functionality
class DocumentManager {
    constructor() {
        this.documents = [];
        this.selectedDocuments = [];
        this.isExpanded = false;
        this.isUploading = false;
        this.allTags = [];
        this.allFolders = ['/'];
        this.activeFilters = {
            tags: [],
            folder: null
        };
        
        // Initialize elements after DOM is loaded
        document.addEventListener('DOMContentLoaded', () => this.initialize());
    }
    
    initialize() {
        // Get elements
        this.docSection = document.getElementById('document-section');
        this.docList = document.getElementById('document-list');
        this.uploadForm = document.getElementById('upload-form');
        this.documentFile = document.getElementById('document-file');
        this.dropZone = document.getElementById('drop-zone');
        this.fileList = document.getElementById('file-list');
        this.fileProgressList = document.getElementById('file-progress-list');
        this.overallProgress = document.getElementById('overall-progress');
        this.overallProgressFill = document.getElementById('overall-progress-fill');
        this.toggleBtn = document.getElementById('toggle-documents');
        this.processSelectedBtn = document.getElementById('process-selected-btn');
        this.deleteSelectedBtn = document.getElementById('delete-selected-btn');
        this.documentCount = document.getElementById('document-count');
        this.tagInput = document.getElementById('doc-tags');
        this.folderSelect = document.getElementById('doc-folder');
        this.filterPanel = document.getElementById('filter-panel');
        this.editModal = document.getElementById('document-edit-modal');
        
        // File queue for uploads
        this.fileQueue = [];
        
        // Mobile detection
        this.isMobile = window.innerWidth <= 768;
        
        if (!this.docSection) return;
        
        // Set up event listeners
        this.setupEventListeners();
        
        // Load tags and folders
        this.loadTagsAndFolders();
        
        // Load documents
        this.loadDocuments();
        
        // Set up mobile-specific features
        if (this.isMobile) {
            this.setupMobileSupport();
        }
        
        // Listen for window resize to adjust mobile features
        window.addEventListener('resize', () => {
            const wasMobile = this.isMobile;
            this.isMobile = window.innerWidth <= 768;
            
            // If switching between mobile and desktop
            if (wasMobile !== this.isMobile) {
                if (this.isMobile) {
                    this.setupMobileSupport();
                } else {
                    this.removeMobileSupport();
                }
            }
        });
    }
    
    setupEventListeners() {
        // Toggle document section
        if (this.toggleBtn) {
            this.toggleBtn.addEventListener('click', () => this.toggleDocumentSection());
        }
        
        // Upload form
        if (this.uploadForm) {
            this.uploadForm.addEventListener('submit', (e) => this.handleUpload(e));
        }
        
        // File input change
        if (this.documentFile) {
            this.documentFile.addEventListener('change', (e) => this.handleFileSelection(e));
        }
        
        // Drag and drop functionality
        if (this.dropZone) {
            this.dropZone.addEventListener('dragover', (e) => {
                e.preventDefault();
                e.stopPropagation();
                this.dropZone.classList.add('active');
            });
            
            this.dropZone.addEventListener('dragleave', (e) => {
                e.preventDefault();
                e.stopPropagation();
                this.dropZone.classList.remove('active');
            });
            
            this.dropZone.addEventListener('drop', (e) => {
                e.preventDefault();
                e.stopPropagation();
                this.dropZone.classList.remove('active');
                
                if (e.dataTransfer.files.length > 0) {
                    this.handleFileSelection({ target: { files: e.dataTransfer.files } });
                }
            });
        }
        
        // Process selected documents
        if (this.processSelectedBtn) {
            this.processSelectedBtn.addEventListener('click', () => this.processSelected());
        }
        
        // Delete selected documents
        if (this.deleteSelectedBtn) {
            this.deleteSelectedBtn.addEventListener('click', () => this.deleteSelected());
        }
        
        // Filter toggle
        const filterToggle = document.getElementById('filter-toggle');
        if (filterToggle) {
            filterToggle.addEventListener('click', () => {
                const filterContent = document.getElementById('filter-content');
                filterContent.classList.toggle('show');
                const icon = filterToggle.querySelector('i');
                if (icon) {
                    icon.className = filterContent.classList.contains('show') ?
                        'fas fa-chevron-up' : 'fas fa-chevron-down';
                }
            });
        }
        
        // Apply filters button
        const applyFiltersBtn = document.getElementById('apply-filters');
        if (applyFiltersBtn) {
            applyFiltersBtn.addEventListener('click', () => this.applyFilters());
        }
        
        // Clear filters button
        const clearFiltersBtn = document.getElementById('clear-filters');
        if (clearFiltersBtn) {
            clearFiltersBtn.addEventListener('click', () => this.clearFilters());
        }
        
        // Modal close button
        const modalClose = document.querySelector('.modal-close');
        if (modalClose) {
            modalClose.addEventListener('click', () => {
                if (this.editModal) {
                    this.editModal.style.display = 'none';
                }
            });
        }
        
        // Save changes button in modal
        const saveChangesBtn = document.getElementById('save-changes');
        if (saveChangesBtn) {
            saveChangesBtn.addEventListener('click', () => this.saveDocumentChanges());
        }
        
        // Tag input for suggestions
        if (this.tagInput) {
            this.tagInput.addEventListener('input', () => this.showTagSuggestions());
            this.tagInput.addEventListener('keydown', (e) => {
                if (e.key === 'Enter' && this.tagInput.value.trim()) {
                    e.preventDefault();
                    this.addTag(this.tagInput.value.trim());
                    this.tagInput.value = '';
                }
            });
        }
    }
    
    loadTagsAndFolders() {
        // Load all tags
        authenticatedFetch('/api/documents/tags')
            .then(response => response.json())
            .then(data => {
                this.allTags = data.tags || [];
                this.renderTagFilters();
            })
            .catch(error => {
                console.error('Error loading tags:', error);
            });
        
        // Load all folders
        authenticatedFetch('/api/documents/folders')
            .then(response => response.json())
            .then(data => {
                this.allFolders = data.folders || ['/'];
                this.renderFolderFilters();
                this.populateFolderSelect();
            })
            .catch(error => {
                console.error('Error loading folders:', error);
            });
    }
    
    renderTagFilters() {
        const filterTagsContainer = document.getElementById('filter-tags');
        if (!filterTagsContainer || !this.allTags.length) return;
        
        filterTagsContainer.innerHTML = '';
        
        this.allTags.forEach(tag => {
            const tagEl = document.createElement('div');
            tagEl.className = 'filter-tag';
            tagEl.textContent = tag;
            tagEl.dataset.tag = tag;
            
            if (this.activeFilters.tags.includes(tag)) {
                tagEl.classList.add('active');
            }
            
            tagEl.addEventListener('click', () => {
                tagEl.classList.toggle('active');
            });
            
            filterTagsContainer.appendChild(tagEl);
        });
    }
    
    renderFolderFilters() {
        const filterFoldersContainer = document.getElementById('filter-folders');
        if (!filterFoldersContainer || !this.allFolders.length) return;
        
        filterFoldersContainer.innerHTML = '';
        
        this.allFolders.forEach(folder => {
            const folderEl = document.createElement('div');
            folderEl.className = 'filter-folder';
            folderEl.textContent = folder === '/' ? 'Root' : folder.replace('/', '');
            folderEl.dataset.folder = folder;
            
            if (this.activeFilters.folder === folder) {
                folderEl.classList.add('active');
            }
            
            folderEl.addEventListener('click', () => {
                // Deactivate all folders
                document.querySelectorAll('.filter-folder').forEach(el => {
                    el.classList.remove('active');
                });
                
                // Activate this folder
                folderEl.classList.add('active');
            });
            
            filterFoldersContainer.appendChild(folderEl);
        });
    }
    
    populateFolderSelect() {
        if (!this.folderSelect || !this.allFolders.length) return;
        
        this.folderSelect.innerHTML = '';
        
        this.allFolders.forEach(folder => {
            const option = document.createElement('option');
            option.value = folder;
            option.textContent = folder === '/' ? 'Root' : folder.replace('/', '');
            this.folderSelect.appendChild(option);
        });
    }
    
    applyFilters() {
        // Get selected tags
        const selectedTags = Array.from(document.querySelectorAll('.filter-tag.active'))
            .map(el => el.dataset.tag);
        
        // Get selected folder
        const selectedFolder = document.querySelector('.filter-folder.active')?.dataset.folder;
        
        this.activeFilters = {
            tags: selectedTags,
            folder: selectedFolder
        };
        
        // Load filtered documents
        this.loadFilteredDocuments();
        
        // Close filter panel
        const filterContent = document.getElementById('filter-content');
        if (filterContent) {
            filterContent.classList.remove('show');
        }
        
        const filterToggle = document.getElementById('filter-toggle');
        if (filterToggle) {
            const icon = filterToggle.querySelector('i');
            if (icon) {
                icon.className = 'fas fa-chevron-down';
            }
        }
    }
    
    clearFilters() {
        // Clear active filters
        document.querySelectorAll('.filter-tag.active, .filter-folder.active').forEach(el => {
            el.classList.remove('active');
        });
        
        this.activeFilters = {
            tags: [],
            folder: null
        };
        
        // Load all documents
        this.loadDocuments();
    }
    
    loadFilteredDocuments() {
        if (!this.docList) return;
        
        // Show loading indicator
        this.docList.innerHTML = '<div class="document-loading">Loading documents...</div>';
        
        // Prepare filter request
        const filterRequest = {
            tags: this.activeFilters.tags.length > 0 ? this.activeFilters.tags : null,
            folder: this.activeFilters.folder
        };
        
        authenticatedFetch('/api/documents/filter', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(filterRequest)
        })
            .then(response => response.json())
            .then(documents => {
                this.documents = documents;
                this.renderDocuments();
            })
            .catch(error => {
                console.error('Error loading filtered documents:', error);
                this.docList.innerHTML = '<div class="document-error">Error loading documents</div>';
            });
    }
    
    toggleDocumentSection() {
        if (!this.docSection) return;
        
        this.isExpanded = !this.isExpanded;
        this.docSection.classList.toggle('expanded', this.isExpanded);
        
        // Update toggle button icon and text
        if (this.toggleBtn) {
            const icon = this.toggleBtn.querySelector('i');
            if (icon) {
                icon.className = this.isExpanded ? 'fas fa-chevron-up' : 'fas fa-chevron-down';
            }
        }
        
        // Load documents if expanding and not already loaded
        if (this.isExpanded && this.documents.length === 0) {
            this.loadDocuments();
        }
    }
    
    loadDocuments() {
        if (!this.docList) return Promise.resolve();
        
        // Show loading indicator
        this.docList.innerHTML = '<div class="document-loading">Loading documents...</div>';
        
        // Return the promise for chaining
        return authenticatedFetch('/api/documents/list')
            .then(response => response.json())
            .then(documents => {
                this.documents = documents;
                this.renderDocuments();
                return documents;
            })
            .catch(error => {
                console.error('Error loading documents:', error);
                this.docList.innerHTML = '<div class="document-error">Error loading documents</div>';
                return [];
            });
    }
    
    renderDocuments() {
        if (!this.docList) return;
        
        this.docList.innerHTML = '';
        
        if (this.documents.length === 0) {
            this.docList.innerHTML = '<div class="document-empty">No documents found</div>';
            this.updateDocumentCount(0);
            return;
        }
        
        this.documents.forEach(doc => {
            const docEl = this.createDocumentElement(doc);
            this.docList.appendChild(docEl);
        });
        
        this.updateDocumentCount(this.documents.length);
        this.updateBatchButtons();
    }
    
    createDocumentElement(doc) {
        const docEl = document.createElement('div');
        docEl.className = 'sidebar-document-item';
        docEl.dataset.id = doc.id;
        
        const date = new Date(doc.uploaded);
        const formattedDate = date.toLocaleDateString();
        
        // Create tags HTML
        const tagsHtml = doc.tags && doc.tags.length > 0
            ? `<div class="doc-tags">${doc.tags.map(tag => `<span class="doc-tag">${tag}</span>`).join('')}</div>`
            : '';
        
        // Create folder HTML
        const folderHtml = doc.folder
            ? `<div class="doc-folder"><i class="fas fa-folder"></i> ${doc.folder === '/' ? 'Root' : doc.folder.replace('/', '')}</div>`
            : '';
        
        docEl.innerHTML = `
            <div class="doc-header">
                <label class="doc-select-label">
                    <input type="checkbox" class="doc-select" data-id="${doc.id}">
                    <span class="doc-title" title="${doc.filename}">${this.truncateFilename(doc.filename)}</span>
                </label>
            </div>
            ${tagsHtml}
            ${folderHtml}
            <div class="doc-meta">
                <span class="doc-chunks">${doc.chunk_count} chunks</span>
                <span class="doc-date">${formattedDate}</span>
            </div>
            <div class="doc-actions">
                <button class="doc-action edit-btn" data-id="${doc.id}" title="Edit Document">
                    <i class="fas fa-edit"></i>
                </button>
                <button class="doc-action process-btn" data-id="${doc.id}" title="Process Document">
                    <i class="fas fa-sync-alt"></i>
                </button>
                <button class="doc-action delete-btn" data-id="${doc.id}" title="Delete Document">
                    <i class="fas fa-trash"></i>
                </button>
            </div>
            ${this.isMobile ? `
            <div class="swipe-actions">
                <button class="swipe-action delete-action" title="Delete Document">
                    <i class="fas fa-trash"></i>
                </button>
            </div>` : ''}
        `;
        
        // Add event listeners
        const checkbox = docEl.querySelector('.doc-select');
        checkbox.addEventListener('change', () => {
            if (checkbox.checked) {
                this.selectedDocuments.push(doc.id);
            } else {
                const index = this.selectedDocuments.indexOf(doc.id);
                if (index !== -1) {
                    this.selectedDocuments.splice(index, 1);
                }
            }
            this.updateBatchButtons();
        });
        
        const editBtn = docEl.querySelector('.edit-btn');
        editBtn.addEventListener('click', () => {
            this.openEditModal(doc);
        });
        
        const processBtn = docEl.querySelector('.process-btn');
        processBtn.addEventListener('click', () => {
            this.processDocuments([doc.id]);
        });
        
        const deleteBtn = docEl.querySelector('.delete-btn');
        deleteBtn.addEventListener('click', () => {
            this.deleteDocument(doc.id);
        });
        
        return docEl;
    }
    
    openEditModal(doc) {
        if (!this.editModal) return;
        
        // Set document ID
        this.editModal.dataset.documentId = doc.id;
        
        // Set document title
        const modalTitle = this.editModal.querySelector('.modal-title');
        if (modalTitle) {
            modalTitle.textContent = `Edit: ${this.truncateFilename(doc.filename)}`;
        }
        
        // Clear existing tags
        const tagList = document.getElementById('edit-tag-list');
        if (tagList) {
            tagList.innerHTML = '';
            
            // Add current tags
            if (doc.tags && doc.tags.length > 0) {
                doc.tags.forEach(tag => {
                    const tagEl = document.createElement('div');
                    tagEl.className = 'tag-item';
                    tagEl.innerHTML = `
                        ${tag}
                        <span class="tag-remove" data-tag="${tag}">&times;</span>
                    `;
                    
                    const removeBtn = tagEl.querySelector('.tag-remove');
                    removeBtn.addEventListener('click', () => {
                        tagEl.remove();
                    });
                    
                    tagList.appendChild(tagEl);
                });
            }
        }
        
        // Set folder
        const folderSelect = document.getElementById('edit-folder');
        if (folderSelect) {
            // Populate folder options
            folderSelect.innerHTML = '';
            this.allFolders.forEach(folder => {
                const option = document.createElement('option');
                option.value = folder;
                option.textContent = folder === '/' ? 'Root' : folder.replace('/', '');
                option.selected = folder === doc.folder;
                folderSelect.appendChild(option);
            });
        }
        
        // Show modal
        this.editModal.style.display = 'flex';
        
        // Set up tag input
        const tagInput = document.getElementById('edit-tag-input');
        if (tagInput) {
            tagInput.value = '';
            tagInput.focus();
            
            // Remove existing event listeners
            const newTagInput = tagInput.cloneNode(true);
            tagInput.parentNode.replaceChild(newTagInput, tagInput);
            
            // Add event listeners
            newTagInput.addEventListener('keydown', (e) => {
                if (e.key === 'Enter' && newTagInput.value.trim()) {
                    e.preventDefault();
                    this.addTagToModal(newTagInput.value.trim());
                    newTagInput.value = '';
                }
            });
            
            // Tag suggestions
            newTagInput.addEventListener('input', () => {
                this.showTagSuggestionsInModal(newTagInput.value);
            });
        }
    }
    
    addTagToModal(tag) {
        const tagList = document.getElementById('edit-tag-list');
        if (!tagList) return;
        
        // Check if tag already exists
        const existingTags = Array.from(tagList.querySelectorAll('.tag-item'))
            .map(el => el.textContent.trim().replace('×', ''));
        
        if (existingTags.includes(tag)) return;
        
        // Add tag
        const tagEl = document.createElement('div');
        tagEl.className = 'tag-item';
        tagEl.innerHTML = `
            ${tag}
            <span class="tag-remove" data-tag="${tag}">&times;</span>
        `;
        
        const removeBtn = tagEl.querySelector('.tag-remove');
        removeBtn.addEventListener('click', () => {
            tagEl.remove();
        });
        
        tagList.appendChild(tagEl);
    }
    
    showTagSuggestionsInModal(input) {
        const suggestionsContainer = document.getElementById('tag-suggestions');
        if (!suggestionsContainer || !input) {
            if (suggestionsContainer) {
                suggestionsContainer.style.display = 'none';
            }
            return;
        }
        
        // Filter tags
        const matchingTags = this.allTags.filter(tag =>
            tag.toLowerCase().includes(input.toLowerCase()) &&
            !Array.from(document.getElementById('edit-tag-list').querySelectorAll('.tag-item'))
                .map(el => el.textContent.trim().replace('×', ''))
                .includes(tag)
        );
        
        if (matchingTags.length === 0) {
            suggestionsContainer.style.display = 'none';
            return;
        }
        
        // Render suggestions
        suggestionsContainer.innerHTML = '';
        matchingTags.forEach(tag => {
            const suggestionEl = document.createElement('div');
            suggestionEl.className = 'tag-suggestion-item';
            suggestionEl.textContent = tag;
            suggestionEl.addEventListener('click', () => {
                this.addTagToModal(tag);
                document.getElementById('edit-tag-input').value = '';
                suggestionsContainer.style.display = 'none';
            });
            
            suggestionsContainer.appendChild(suggestionEl);
        });
        
        suggestionsContainer.style.display = 'block';
    }
    
    saveDocumentChanges() {
        const documentId = this.editModal.dataset.documentId;
        if (!documentId) return;
        
        // Get tags
        const tags = Array.from(document.getElementById('edit-tag-list').querySelectorAll('.tag-item'))
            .map(el => el.textContent.trim().replace('×', ''));
        
        // Get folder
        const folder = document.getElementById('edit-folder').value;
        
        // Update tags
        this.updateDocumentTags(documentId, tags);
        
        // Update folder
        this.updateDocumentFolder(documentId, folder);
        
        // Close modal
        this.editModal.style.display = 'none';
    }
    
    updateDocumentTags(documentId, tags) {
        authenticatedFetch(`/api/documents/${documentId}/tags`, {
            method: 'PUT',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ tags })
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification('Tags updated successfully');
                this.loadDocuments();
                this.loadTagsAndFolders();
            } else {
                showNotification('Error updating tags: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error updating tags:', error);
            showNotification('Error updating tags', 'warning');
        });
    }
    
    updateDocumentFolder(documentId, folder) {
        authenticatedFetch(`/api/documents/${documentId}/folder`, {
            method: 'PUT',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ folder })
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification('Folder updated successfully');
                this.loadDocuments();
                this.loadTagsAndFolders();
            } else {
                showNotification('Error updating folder: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error updating folder:', error);
            showNotification('Error updating folder', 'warning');
        });
    }
    
    truncateFilename(filename) {
        if (filename.length > 20) {
            return filename.substring(0, 17) + '...';
        }
        return filename;
    }
    
    handleFileSelection(e) {
        const files = Array.from(e.target.files || []);
        
        if (files.length === 0) return;
        
        // Add files to queue
        files.forEach(file => {
            // Check if file is already in queue
            const existingFile = this.fileQueue.find(f => f.name === file.name && f.size === file.size);
            if (!existingFile) {
                this.fileQueue.push(file);
            }
        });
        
        // Update file list display
        this.updateFileList();
    }
    
    updateFileList() {
        if (!this.fileList) return;
        
        this.fileList.innerHTML = '';
        
        if (this.fileQueue.length === 0) {
            this.fileList.style.display = 'none';
            return;
        }
        
        this.fileList.style.display = 'block';
        
        this.fileQueue.forEach((file, index) => {
            const fileItem = document.createElement('div');
            fileItem.className = 'file-item';
            
            const fileSize = this.formatFileSize(file.size);
            
            fileItem.innerHTML = `
                <div class="file-item-name">${file.name}</div>
                <div class="file-item-size">${fileSize}</div>
                <div class="file-item-remove" data-index="${index}">×</div>
            `;
            
            const removeBtn = fileItem.querySelector('.file-item-remove');
            removeBtn.addEventListener('click', () => {
                this.fileQueue.splice(index, 1);
                this.updateFileList();
            });
            
            this.fileList.appendChild(fileItem);
        });
    }
    
    formatFileSize(bytes) {
        if (bytes === 0) return '0 Bytes';
        
        const k = 1024;
        const sizes = ['Bytes', 'KB', 'MB', 'GB'];
        const i = Math.floor(Math.log(bytes) / Math.log(k));
        
        return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
    }
    
    createProgressElement(file) {
        const progressItem = document.createElement('div');
        progressItem.className = 'file-progress-item';
        progressItem.dataset.filename = file.name;
        
        progressItem.innerHTML = `
            <div class="file-progress-name">${this.truncateFilename(file.name)}</div>
            <div class="file-progress-bar">
                <div class="file-progress-fill" style="width: 0%"></div>
            </div>
        `;
        
        return progressItem;
    }
    
    updateFileProgress(filename, percent) {
        const progressItem = this.fileProgressList.querySelector(`.file-progress-item[data-filename="${filename}"]`);
        if (progressItem) {
            const progressFill = progressItem.querySelector('.file-progress-fill');
            if (progressFill) {
                progressFill.style.width = `${percent}%`;
            }
        }
    }
    
    updateOverallProgress(percent) {
        if (this.overallProgressFill) {
            this.overallProgressFill.style.width = `${percent}%`;
        }
    }
    
    handleUpload(e) {
        e.preventDefault();
        
        if (this.isUploading) return;
        
        if (this.fileQueue.length === 0) {
            showNotification('Please select files to upload', 'warning');
            return;
        }
        
        // Clear previous progress elements
        if (this.fileProgressList) {
            this.fileProgressList.innerHTML = '';
        }
        
        // Reset overall progress
        this.updateOverallProgress(0);
        
        // Create progress elements for each file
        this.fileQueue.forEach(file => {
            const progressElement = this.createProgressElement(file);
            this.fileProgressList.appendChild(progressElement);
        });
        
        this.isUploading = true;
        
        // Check if we should use the batch upload or individual uploads
        if (this.fileQueue.length > 1 && this.supportsMultipleFileUpload()) {
            this.uploadMultipleFiles();
        } else {
            this.uploadedCount = 0;
            this.successfulUploads = [];
            // Start uploading files one by one
            this.uploadNextFile();
        }
    }
    
    supportsMultipleFileUpload() {
        // Feature detection for multiple file upload support
        // This can be expanded to check for browser compatibility or server capabilities
        return true;
    }
    
    uploadMultipleFiles() {
        const formData = new FormData();
        
        // Add all files to FormData
        this.fileQueue.forEach(file => {
            formData.append('files', file);
        });
        
        // Add tags if provided
        if (this.tagInput && this.tagInput.value.trim()) {
            formData.append('tags', this.tagInput.value.trim());
        }
        
        // Add folder if provided
        if (this.folderSelect && this.folderSelect.value) {
            formData.append('folder', this.folderSelect.value);
        }
        
        // Upload files
        const xhr = new XMLHttpRequest();
        xhr.open('POST', '/api/documents/upload-multiple', true);
        
        // Add authorization header if authenticated
        if (isAuthenticated()) {
            xhr.setRequestHeader('Authorization', `Bearer ${getToken()}`);
        }
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
                const percentComplete = (e.loaded / e.total) * 100;
                this.updateOverallProgress(percentComplete);
                
                // Update individual file progress based on overall progress
                // This is an approximation since we can't track individual files in a batch upload
                this.fileQueue.forEach(file => {
                    this.updateFileProgress(file.name, percentComplete);
                });
            }
        };
        
        xhr.onload = () => {
            this.isUploading = false;
            
            if (xhr.status === 200) {
                const response = JSON.parse(xhr.responseText);
                
                if (response.success) {
                    // Update progress for successful uploads
                    if (response.documents && response.documents.length > 0) {
                        response.documents.forEach(doc => {
                            const file = this.fileQueue.find(f => f.name === doc.filename);
                            if (file) {
                                this.updateFileProgress(file.name, 100);
                            }
                        });
                        
                        // Process the documents
                        const documentIds = response.documents.map(doc => doc.document_id);
                        this.processDocuments(documentIds);
                    }
                    
                    // Show completion notification
                    const message = response.documents.length === 1
                        ? 'Document uploaded successfully!'
                        : `${response.documents.length} documents uploaded successfully!`;
                    showNotification(message);
                    
                    // Clear file queue and input
                    this.fileQueue = [];
                    if (this.documentFile) {
                        this.documentFile.value = '';
                    }
                    if (this.tagInput) {
                        this.tagInput.value = '';
                    }
                    this.updateFileList();
                    
                    // Reload documents and tags/folders
                    this.loadDocuments();
                    this.loadTagsAndFolders();
                } else {
                    showNotification('Error uploading documents: ' + response.message, 'warning');
                    
                    // Update progress for failed uploads
                    if (response.errors && response.errors.length > 0) {
                        response.errors.forEach(error => {
                            const file = this.fileQueue.find(f => f.name === error.filename);
                            if (file) {
                                this.updateFileProgress(file.name, 0);
                            }
                        });
                    }
                }
            } else {
                showNotification('Error uploading documents', 'warning');
                
                // Mark all as failed
                this.fileQueue.forEach(file => {
                    this.updateFileProgress(file.name, 0);
                });
            }
            
            // Hide progress after delay
            setTimeout(() => {
                if (this.fileProgressList) {
                    this.fileProgressList.innerHTML = '';
                }
                this.updateOverallProgress(0);
            }, 3000);
        };
        
        xhr.onerror = () => {
            this.isUploading = false;
            showNotification('Error uploading documents', 'warning');
            
            // Mark all as failed
            this.fileQueue.forEach(file => {
                this.updateFileProgress(file.name, 0);
            });
            
            // Hide progress after delay
            setTimeout(() => {
                if (this.fileProgressList) {
                    this.fileProgressList.innerHTML = '';
                }
                this.updateOverallProgress(0);
            }, 3000);
        };
        
        xhr.send(formData);
    }
    
    uploadNextFile() {
        if (this.uploadedCount >= this.fileQueue.length) {
            // All files uploaded
            this.handleUploadCompletion();
            return;
        }
        
        const file = this.fileQueue[this.uploadedCount];
        const formData = new FormData();
        formData.append('file', file);
        
        // Add tags if provided
        if (this.tagInput && this.tagInput.value.trim()) {
            formData.append('tags', this.tagInput.value.trim());
        }
        
        // Add folder if provided
        if (this.folderSelect && this.folderSelect.value) {
            formData.append('folder', this.folderSelect.value);
        }
        
        // Upload file
        const xhr = new XMLHttpRequest();
        xhr.open('POST', '/api/documents/upload', true);
        
        // Add authorization header if authenticated
        if (isAuthenticated()) {
            xhr.setRequestHeader('Authorization', `Bearer ${getToken()}`);
        }
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
                const percentComplete = (e.loaded / e.total) * 100;
                this.updateFileProgress(file.name, percentComplete);
                
                // Update overall progress
                const overallPercent = ((this.uploadedCount + (e.loaded / e.total)) / this.fileQueue.length) * 100;
                this.updateOverallProgress(overallPercent);
            }
        };
        
        xhr.onload = () => {
            this.uploadedCount++;
            
            if (xhr.status === 200) {
                const response = JSON.parse(xhr.responseText);
                if (response.success) {
                    this.updateFileProgress(file.name, 100);
                    this.successfulUploads.push(response.document_id);
                } else {
                    this.updateFileProgress(file.name, 0);
                    showNotification(`Error uploading ${file.name}: ${response.message}`, 'warning');
                }
            } else {
                this.updateFileProgress(file.name, 0);
                showNotification(`Error uploading ${file.name}`, 'warning');
            }
            
            // Upload next file
            this.uploadNextFile();
        };
        
        xhr.onerror = () => {
            this.uploadedCount++;
            this.updateFileProgress(file.name, 0);
            showNotification(`Error uploading ${file.name}`, 'warning');
            
            // Upload next file
            this.uploadNextFile();
        };
        
        xhr.send(formData);
    }
    
    handleUploadCompletion() {
        this.isUploading = false;
        
        // Show completion notification
        if (this.successfulUploads.length > 0) {
            const message = this.successfulUploads.length === 1
                ? 'Document uploaded successfully!'
                : `${this.successfulUploads.length} documents uploaded successfully!`;
            showNotification(message);
            
            // Process the documents
            this.processDocuments(this.successfulUploads);
            
            // Clear file queue and input
            this.fileQueue = [];
            if (this.documentFile) {
                this.documentFile.value = '';
            }
            if (this.tagInput) {
                this.tagInput.value = '';
            }
            this.updateFileList();
            
            // Reload documents and tags/folders
            this.loadDocuments();
            this.loadTagsAndFolders();
        } else {
            showNotification('No documents were uploaded successfully', 'warning');
        }
        
        // Update overall progress to complete
        this.updateOverallProgress(100);
        
        // Hide progress after delay
        setTimeout(() => {
            if (this.fileProgressList) {
                this.fileProgressList.innerHTML = '';
            }
            this.updateOverallProgress(0);
        }, 3000);
    }
    
    processDocuments(documentIds) {
        // Get chunking strategy options if available
        const chunkingStrategy = document.getElementById('chunking-strategy')?.value || 'recursive';
        const chunkSize = document.getElementById('chunk-size')?.value || null;
        const chunkOverlap = document.getElementById('chunk-overlap')?.value || null;
        
        const request = {
            document_ids: documentIds,
            force_reprocess: false,
            chunking_strategy: chunkingStrategy,
            chunk_size: chunkSize ? parseInt(chunkSize) : null,
            chunk_overlap: chunkOverlap ? parseInt(chunkOverlap) : null
        };
        
        // Log the processing request
        console.log('Processing documents with options:', request);
        
        authenticatedFetch('/api/documents/process', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(request)
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification(`Processing started for ${documentIds.length} document(s) with ${chunkingStrategy} chunking strategy`);
            } else {
                showNotification('Error processing documents: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error processing documents:', error);
            showNotification('Error processing documents', 'warning');
        });
    }
    
    deleteDocument(documentId) {
        if (!confirm('Are you sure you want to delete this document?')) {
            return;
        }
        
        authenticatedFetch(`/api/documents/${documentId}`, {
            method: 'DELETE'
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification('Document deleted successfully');
                
                // Remove from selected documents
                const index = this.selectedDocuments.indexOf(documentId);
                if (index !== -1) {
                    this.selectedDocuments.splice(index, 1);
                }
                
                this.loadDocuments();
                this.loadTagsAndFolders();
            } else {
                showNotification('Error deleting document: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error deleting document:', error);
            showNotification('Error deleting document', 'warning');
        });
    }
    
    processSelected() {
        if (this.selectedDocuments.length === 0) return;
        this.processDocuments([...this.selectedDocuments]);
    }
    
    deleteSelected() {
        if (this.selectedDocuments.length === 0) return;
        
        if (!confirm('Are you sure you want to delete ' + this.selectedDocuments.length + ' document(s)?')) {
            return;
        }
        
        const promises = this.selectedDocuments.map(id => {
            return authenticatedFetch(`/api/documents/${id}`, {
                method: 'DELETE'
            }).then(response => response.json());
        });
        
        Promise.all(promises)
            .then(() => {
                showNotification('Documents deleted successfully');
                this.selectedDocuments = [];
                this.loadDocuments();
                this.loadTagsAndFolders();
            })
            .catch(error => {
                console.error('Error deleting documents:', error);
                showNotification('Error deleting documents', 'warning');
            });
    }
    
    updateBatchButtons() {
        const hasSelected = this.selectedDocuments.length > 0;
        
        if (this.processSelectedBtn) {
            this.processSelectedBtn.disabled = !hasSelected;
        }
        
        if (this.deleteSelectedBtn) {
            this.deleteSelectedBtn.disabled = !hasSelected;
        }
    }
    
    updateDocumentCount(count) {
        if (this.documentCount) {
            this.documentCount.textContent = count;
            
            // If there are documents and the section is not expanded, add a visual indicator
            if (count > 0 && !this.isExpanded) {
                this.documentCount.classList.add('has-documents');
            } else {
                this.documentCount.classList.remove('has-documents');
            }
        }
    }
    
    setupMobileSupport() {
        // Add pull-to-refresh functionality
        this.setupPullToRefresh();
        
        // Add swipe gestures for document items
        this.setupSwipeGestures();
    }
    
    removeMobileSupport() {
        // Remove mobile-specific event listeners
        if (this.docList) {
            this.docList.removeEventListener('touchstart', this.touchStartHandler);
            this.docList.removeEventListener('touchmove', this.touchMoveHandler);
            this.docList.removeEventListener('touchend', this.touchEndHandler);
        }
        
        // Remove pull-to-refresh indicator if it exists
        const refreshIndicator = document.querySelector('.pull-to-refresh');
        if (refreshIndicator) {
            refreshIndicator.parentNode.removeChild(refreshIndicator);
        }
    }
    
    setupPullToRefresh() {
        if (!this.docList) return;
        
        // Create pull-to-refresh indicator
        const refreshIndicator = document.createElement('div');
        refreshIndicator.className = 'pull-to-refresh';
        refreshIndicator.style.display = 'none';
        refreshIndicator.innerHTML = '<span class="spinner"></span> Pull to refresh';
        
        // Insert before document list
        this.docList.parentNode.insertBefore(refreshIndicator, this.docList);
        
        // Touch event variables
        let touchStartY = 0;
        let touchEndY = 0;
        
        // Touch event handlers
        this.touchStartHandler = (e) => {
            touchStartY = e.touches[0].clientY;
        };
        
        this.touchMoveHandler = (e) => {
            touchEndY = e.touches[0].clientY;
            
            // If scrolled to top and pulling down
            if (this.docList.scrollTop === 0 && touchEndY > touchStartY) {
                refreshIndicator.style.display = 'flex';
                e.preventDefault(); // Prevent default scroll
            }
        };
        
        this.touchEndHandler = () => {
            if (refreshIndicator.style.display === 'flex') {
                refreshIndicator.innerHTML = '<span class="spinner"></span> Refreshing...';
                
                // Reload documents
                this.loadDocuments().then(() => {
                    refreshIndicator.style.display = 'none';
                });
            }
        };
        
        // Add touch event listeners
        this.docList.addEventListener('touchstart', this.touchStartHandler);
        this.docList.addEventListener('touchmove', this.touchMoveHandler);
        this.docList.addEventListener('touchend', this.touchEndHandler);
    }
    
    setupSwipeGestures() {
        // Set up swipe gestures for document items
        document.querySelectorAll('.sidebar-document-item').forEach(item => {
            // Add delete action event listener to existing swipe actions
            const deleteAction = item.querySelector('.swipe-action.delete-action');
            if (deleteAction) {
                deleteAction.addEventListener('click', () => {
                    const docId = item.dataset.id;
                    if (docId) {
                        this.deleteDocument(docId);
                    }
                });
            }
            
            // Touch variables
            let touchStartX = 0;
            let touchEndX = 0;
            
            // Touch event handlers
            item.addEventListener('touchstart', (e) => {
                touchStartX = e.touches[0].clientX;
            });
            
            item.addEventListener('touchend', (e) => {
                touchEndX = e.changedTouches[0].clientX;
                
                // Swipe left to show delete button
                if (touchStartX - touchEndX > 50) {
                    item.classList.add('show-actions');
                }
                
                // Swipe right to hide actions
                if (touchEndX - touchStartX > 50) {
                    item.classList.remove('show-actions');
                }
            });
        });
    }
}

// Initialize document manager
const documentManager = new DocumentManager();

================
File: app/static/js/document-upload-enhanced.js
================
/**
 * Enhanced Document Upload Functionality
 * Implements the Phase 2 improvements for the document upload interface
 */
class EnhancedDocumentUpload {
    constructor() {
        // Initialize properties
        this.fileQueue = [];
        this.selectedFiles = [];
        this.uploadStartTime = null;
        this.fileStartTimes = {};
        this.isUploading = false;
        this.viewMode = 'grid'; // 'grid' or 'list'
        
        // Initialize after DOM is loaded
        document.addEventListener('DOMContentLoaded', () => this.initialize());
    }
    
    /**
     * Initialize the enhanced document upload functionality
     */
    initialize() {
        console.log('Initializing Enhanced Document Upload');
        
        // Add the CSS file
        this.addStylesheet();
        
        // Convert existing sections to collapsible sections
        this.setupCollapsibleSections();
        
        // Setup the enhanced drop zone
        this.setupDropZone();
        
        // Setup batch actions
        this.setupBatchActions();
        
        // Setup modals
        this.setupModals();
    }
    
    /**
     * Add the enhanced CSS stylesheet
     */
    addStylesheet() {
        if (!document.querySelector('link[href*="document-upload-enhanced.css"]')) {
            const link = document.createElement('link');
            link.rel = 'stylesheet';
            link.href = '/static/css/document-upload-enhanced.css';
            document.head.appendChild(link);
        }
    }
    
    /**
     * Convert existing sections to collapsible sections
     */
    setupCollapsibleSections() {
        // Get all sections that should be collapsible
        const uploadForm = document.querySelector('.upload-form');
        const filterSection = document.querySelector('.filter-section');
        const processingOptions = document.querySelector('.processing-options')?.closest('.document-section');
        
        if (uploadForm) {
            this.convertToCollapsibleSection(uploadForm, 'Upload Documents', 'fa-cloud-upload-alt');
        }
        
        if (filterSection) {
            this.convertToCollapsibleSection(filterSection, 'Filter Documents', 'fa-filter');
        }
        
        if (processingOptions) {
            this.convertToCollapsibleSection(processingOptions, 'Document Processing Options', 'fa-cogs');
        }
        
        // Create a new section for selected files
        this.createSelectedFilesSection();
        
        // Create a new section for upload progress
        this.createProgressSection();
    }
    
    /**
     * Convert an element to a collapsible section
     */
    convertToCollapsibleSection(element, title, iconClass) {
        // Create the collapsible section
        const section = document.createElement('div');
        section.className = 'collapsible-section';
        
        // Create the header
        const header = document.createElement('div');
        header.className = 'section-header';
        header.innerHTML = `
            <h3><i class="fas ${iconClass}"></i> ${title}</h3>
            <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
        `;
        
        // Create the content container
        const content = document.createElement('div');
        content.className = 'section-content';
        
        // Move the original element's content to the content container
        while (element.firstChild) {
            content.appendChild(element.firstChild);
        }
        
        // Add the header and content to the section
        section.appendChild(header);
        section.appendChild(content);
        
        // Replace the original element with the section
        element.parentNode.replaceChild(section, element);
        
        // Add event listener for toggling
        header.addEventListener('click', () => {
            section.classList.toggle('collapsed');
        });
        
        return section;
    }
    
    /**
     * Create a new section for selected files
     */
    createSelectedFilesSection() {
        // Check if file list already exists
        const existingFileList = document.getElementById('file-list');
        if (!existingFileList) return;
        
        // Create the section
        const section = document.createElement('div');
        section.className = 'collapsible-section';
        section.id = 'selected-files-section';
        
        // Create the header
        const header = document.createElement('div');
        header.className = 'section-header';
        header.innerHTML = `
            <h3><i class="fas fa-file-alt"></i> Selected Files (<span id="file-count">0</span>)</h3>
            <div class="section-actions">
                <button id="view-toggle-grid" class="view-toggle-btn active" title="Grid View">
                    <i class="fas fa-th-large"></i>
                </button>
                <button id="view-toggle-list" class="view-toggle-btn" title="List View">
                    <i class="fas fa-list"></i>
                </button>
                <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
            </div>
        `;
        
        // Create the content container
        const content = document.createElement('div');
        content.className = 'section-content';
        
        // Create the file list container
        const fileList = document.createElement('div');
        fileList.id = 'file-list';
        fileList.className = 'file-list grid-view';
        
        // Create the file actions
        const fileActions = document.createElement('div');
        fileActions.className = 'file-actions';
        fileActions.innerHTML = `
            <button id="clear-files-btn" class="btn">Clear All</button>
            <button id="upload-files-btn" class="btn primary">Upload Files</button>
        `;
        
        // Add the file list and actions to the content
        content.appendChild(fileList);
        content.appendChild(fileActions);
        
        // Add the header and content to the section
        section.appendChild(header);
        section.appendChild(content);
        
        // Insert the section after the upload form
        const uploadForm = document.querySelector('.collapsible-section');
        if (uploadForm) {
            uploadForm.parentNode.insertBefore(section, uploadForm.nextSibling);
        }
        
        // Add event listeners
        header.addEventListener('click', (e) => {
            if (!e.target.closest('.view-toggle-btn')) {
                section.classList.toggle('collapsed');
            }
        });
        
        document.getElementById('view-toggle-grid')?.addEventListener('click', () => this.setViewMode('grid'));
        document.getElementById('view-toggle-list')?.addEventListener('click', () => this.setViewMode('list'));
        document.getElementById('clear-files-btn')?.addEventListener('click', () => this.clearFileQueue());
        document.getElementById('upload-files-btn')?.addEventListener('click', () => this.uploadFiles());
    }
    
    /**
     * Create a new section for upload progress
     */
    createProgressSection() {
        // Check if progress container already exists
        const existingProgress = document.querySelector('.progress-container');
        if (!existingProgress) return;
        
        // Create the section
        const section = document.createElement('div');
        section.className = 'collapsible-section';
        section.id = 'progress-section';
        
        // Create the header
        const header = document.createElement('div');
        header.className = 'section-header';
        header.innerHTML = `
            <h3><i class="fas fa-tasks"></i> Upload Progress</h3>
            <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
        `;
        
        // Create the content container
        const content = document.createElement('div');
        content.className = 'section-content';
        
        // Create the progress container
        const progressContainer = document.createElement('div');
        progressContainer.className = 'progress-container';
        
        // Create the overall progress
        const overallProgress = document.createElement('div');
        overallProgress.className = 'overall-progress';
        overallProgress.innerHTML = `
            <div class="progress-header">
                <span class="progress-title">Overall Progress</span>
                <span class="progress-stats">
                    <span id="completed-count">0</span>/<span id="total-count">0</span> files
                </span>
            </div>
            <div class="progress-bar" id="overall-progress">
                <div class="progress-bar-fill" id="overall-progress-fill"></div>
            </div>
            <div class="progress-details">
                <span id="overall-percent">0%</span>
                <span id="time-remaining">Estimating time...</span>
            </div>
        `;
        
        // Create the file progress list
        const fileProgressList = document.createElement('div');
        fileProgressList.id = 'file-progress-list';
        fileProgressList.className = 'file-progress-list';
        
        // Add the overall progress and file progress list to the progress container
        progressContainer.appendChild(overallProgress);
        progressContainer.appendChild(fileProgressList);
        
        // Add the progress container to the content
        content.appendChild(progressContainer);
        
        // Add the header and content to the section
        section.appendChild(header);
        section.appendChild(content);
        
        // Insert the section after the selected files section
        const selectedFilesSection = document.getElementById('selected-files-section');
        if (selectedFilesSection) {
            selectedFilesSection.parentNode.insertBefore(section, selectedFilesSection.nextSibling);
        }
        
        // Add event listener for toggling
        header.addEventListener('click', () => {
            section.classList.toggle('collapsed');
        });
        
        // Initially collapse the progress section
        section.classList.add('collapsed');
    }
    
    /**
     * Setup the enhanced drop zone
     */
    setupDropZone() {
        const dropZone = document.getElementById('drop-zone');
        const fileInput = document.getElementById('document-file');
        
        if (!dropZone || !fileInput) return;
        
        // Add the drop zone icon
        const dropZoneContent = dropZone.innerHTML;
        dropZone.innerHTML = `
            <div class="drop-zone-icon">
                <i class="fas fa-cloud-upload-alt"></i>
            </div>
            ${dropZoneContent}
        `;
        
        // Replace the file input with a button
        const fileInputParent = fileInput.parentNode;
        const selectButton = document.createElement('button');
        selectButton.type = 'button';
        selectButton.className = 'select-files-btn';
        selectButton.textContent = 'Select Files';
        selectButton.addEventListener('click', () => fileInput.click());
        
        // Keep the file input but hide it
        fileInputParent.appendChild(selectButton);
        
        // Add drag and drop event listeners
        dropZone.addEventListener('dragover', (e) => {
            e.preventDefault();
            dropZone.classList.add('drag-over');
        });
        
        dropZone.addEventListener('dragleave', () => {
            dropZone.classList.remove('drag-over');
        });
        
        dropZone.addEventListener('drop', (e) => {
            e.preventDefault();
            dropZone.classList.remove('drag-over');
            
            if (e.dataTransfer.files.length > 0) {
                this.handleFileSelection(e.dataTransfer.files);
            }
        });
        
        // Add file input change event listener
        fileInput.addEventListener('change', (e) => {
            this.handleFileSelection(e.target.files);
        });
    }
    
    /**
     * Handle file selection
     */
    handleFileSelection(files) {
        if (!files || files.length === 0) return;
        
        // Add files to queue
        Array.from(files).forEach(file => {
            // Check if file is already in queue
            const existingFile = this.fileQueue.find(f => f.name === file.name && f.size === file.size);
            if (!existingFile) {
                // Add unique ID and metadata
                const fileWithId = file;
                fileWithId.id = 'file_' + Math.random().toString(36).substr(2, 9);
                fileWithId.selected = false;
                this.fileQueue.push(fileWithId);
            }
        });
        
        // Update file list display
        this.updateFileList();
        
        // Update file count
        this.updateFileCount();
        
        // Expand the selected files section if collapsed
        const selectedFilesSection = document.getElementById('selected-files-section');
        if (selectedFilesSection && selectedFilesSection.classList.contains('collapsed')) {
            selectedFilesSection.classList.remove('collapsed');
        }
    }
    
    /**
     * Update the file list display
     */
    updateFileList() {
        const fileList = document.getElementById('file-list');
        if (!fileList) return;
        
        // Clear the file list
        fileList.innerHTML = '';
        
        // Add files to the list
        this.fileQueue.forEach(file => {
            const filePreview = this.createFilePreview(file);
            fileList.appendChild(filePreview);
        });
        
        // Show/hide the file list
        if (this.fileQueue.length === 0) {
            fileList.style.display = 'none';
        } else {
            fileList.style.display = 'grid';
        }
    }
    
    /**
     * Create a file preview element
     */
    createFilePreview(file) {
        const preview = document.createElement('div');
        preview.className = 'file-preview';
        preview.dataset.id = file.id;
        preview.dataset.filename = file.name;
        
        // Create thumbnail based on file type
        const thumbnail = document.createElement('div');
        thumbnail.className = 'file-thumbnail';
        
        // Determine the appropriate icon based on file type
        let iconClass = 'fa-file';
        const fileExt = file.name.split('.').pop().toLowerCase() || '';
        
        switch (fileExt) {
            case 'pdf': iconClass = 'fa-file-pdf'; break;
            case 'doc': case 'docx': iconClass = 'fa-file-word'; break;
            case 'xls': case 'xlsx': iconClass = 'fa-file-excel'; break;
            case 'txt': iconClass = 'fa-file-alt'; break;
            case 'csv': iconClass = 'fa-file-csv'; break;
            case 'jpg': case 'jpeg': case 'png': iconClass = 'fa-file-image'; break;
            case 'html': case 'htm': iconClass = 'fa-file-code'; break;
        }
        
        thumbnail.innerHTML = `<i class="fas ${iconClass}"></i>`;
        
        // Add file info
        const fileInfo = document.createElement('div');
        fileInfo.className = 'file-info';
        fileInfo.innerHTML = `
            <div class="file-name">${file.name}</div>
            <div class="file-meta">
                <span class="file-size">${this.formatFileSize(file.size)}</span>
                <span class="file-type">${fileExt.toUpperCase()}</span>
            </div>
            <div class="file-date">Modified: ${new Date(file.lastModified).toLocaleDateString()}</div>
        `;
        
        // Add checkbox for selection
        const checkbox = document.createElement('input');
        checkbox.type = 'checkbox';
        checkbox.className = 'file-select';
        checkbox.checked = file.selected;
        checkbox.addEventListener('change', () => {
            file.selected = checkbox.checked;
            this.updateSelectedFiles();
        });
        
        // Add remove button
        const removeBtn = document.createElement('button');
        removeBtn.className = 'file-remove';
        removeBtn.innerHTML = '&times;';
        removeBtn.addEventListener('click', () => this.removeFileFromQueue(file.id));
        
        // Add elements to preview
        preview.appendChild(checkbox);
        preview.appendChild(thumbnail);
        preview.appendChild(fileInfo);
        preview.appendChild(removeBtn);
        
        return preview;
    }
    
    /**
     * Format file size
     */
    formatFileSize(bytes) {
        if (bytes === 0) return '0 Bytes';
        
        const k = 1024;
        const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB'];
        const i = Math.floor(Math.log(bytes) / Math.log(k));
        
        return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
    }
    
    /**
     * Remove a file from the queue
     */
    removeFileFromQueue(fileId) {
        const index = this.fileQueue.findIndex(file => file.id === fileId);
        if (index !== -1) {
            this.fileQueue.splice(index, 1);
            this.updateFileList();
            this.updateFileCount();
            this.updateSelectedFiles();
        }
    }
    
    /**
     * Clear the file queue
     */
    clearFileQueue() {
        this.fileQueue = [];
        this.updateFileList();
        this.updateFileCount();
        this.updateSelectedFiles();
    }
    
    /**
     * Update the file count display
     */
    updateFileCount() {
        const fileCount = document.getElementById('file-count');
        if (fileCount) {
            fileCount.textContent = this.fileQueue.length;
        }
    }
    
    /**
     * Set the view mode (grid or list)
     */
    setViewMode(mode) {
        this.viewMode = mode;
        
        const fileList = document.getElementById('file-list');
        const gridBtn = document.getElementById('view-toggle-grid');
        const listBtn = document.getElementById('view-toggle-list');
        
        if (fileList) {
            fileList.className = `file-list ${mode}-view`;
        }
        
        if (gridBtn) {
            gridBtn.classList.toggle('active', mode === 'grid');
        }
        
        if (listBtn) {
            listBtn.classList.toggle('active', mode === 'list');
        }
    }
    
    /**
     * Setup batch actions
     */
    setupBatchActions() {
        // Create batch actions toolbar
        this.createBatchActionsToolbar();
    }
    
    /**
     * Create the batch actions toolbar
     */
    createBatchActionsToolbar() {
        // Check if toolbar already exists
        if (document.getElementById('batch-actions-toolbar')) return;
        
        // Create the toolbar
        const toolbar = document.createElement('div');
        toolbar.id = 'batch-actions-toolbar';
        toolbar.className = 'batch-actions-toolbar';
        toolbar.innerHTML = `
            <div class="selection-count">
                <span id="selected-count">0</span> files selected
            </div>
            <div class="batch-actions-buttons">
                <button id="batch-tag-btn" class="batch-btn">
                    <i class="fas fa-tags"></i> Tag
                </button>
                <button id="batch-folder-btn" class="batch-btn">
                    <i class="fas fa-folder"></i> Move
                </button>
                <button id="batch-delete-btn" class="batch-btn danger">
                    <i class="fas fa-trash"></i> Delete
                </button>
                <button id="batch-process-btn" class="batch-btn primary">
                    <i class="fas fa-cogs"></i> Process
                </button>
            </div>
        `;
        
        // Add the toolbar to the body
        document.body.appendChild(toolbar);
        
        // Add event listeners for batch actions
        // These would be implemented in a real application
    }
    
    /**
     * Update the selected files list
     */
    updateSelectedFiles() {
        this.selectedFiles = this.fileQueue.filter(file => file.selected);
        
        // Update the selected count
        const selectedCount = document.getElementById('selected-count');
        if (selectedCount) {
            selectedCount.textContent = this.selectedFiles.length;
        }
        
        // Show/hide the batch actions toolbar
        const toolbar = document.getElementById('batch-actions-toolbar');
        if (toolbar) {
            toolbar.classList.toggle('visible', this.selectedFiles.length > 0);
        }
    }
    
    /**
     * Setup modals for batch operations
     */
    setupModals() {
        // In a real implementation, this would create modals for:
        // - Batch tagging
        // - Batch folder assignment
        // - Batch deletion confirmation
        // - Upload summary
    }
    
    /**
     * Upload files
     */
    uploadFiles() {
        // This would implement the actual file upload functionality
        // with enhanced progress tracking
        console.log('Upload files functionality would be implemented here');
        showNotification('Upload functionality implemented in the full version', 'info');
    }
}

// Initialize the enhanced document upload
const enhancedUpload = new EnhancedDocumentUpload();

================
File: app/static/js/document-upload-fix.js
================
/**
 * Document Upload Fix - Ensures multiple file selection works properly
 */
document.addEventListener('DOMContentLoaded', function() {
    console.log("Document upload fix script loaded");
    
    // Get the file input element
    const fileInput = document.getElementById('document-file');
    
    if (fileInput) {
        console.log("File input found:", fileInput);
        console.log("Multiple attribute:", fileInput.multiple);
        
        // Ensure multiple attribute is set
        fileInput.setAttribute('multiple', 'multiple');
        console.log("Multiple attribute after fix:", fileInput.multiple);
        
        // Add a direct event listener to log file selection
        fileInput.addEventListener('change', function(e) {
            const files = e.target.files;
            console.log(`Files selected: ${files ? files.length : 0}`);
            
            if (files && files.length > 0) {
                console.log("Selected files:");
                for (let i = 0; i < files.length; i++) {
                    console.log(`- ${files[i].name} (${files[i].size} bytes)`);
                }
            }
        });
        
        // Check if the file input is inside a form
        const form = fileInput.closest('form');
        if (form) {
            console.log("Form found:", form);
            
            // Add submit event listener to log form submission
            form.addEventListener('submit', function(e) {
                console.log("Form submitted");
                const files = fileInput.files;
                console.log(`Files in form submission: ${files ? files.length : 0}`);
            });
        }
    } else {
        console.error("File input element not found");
    }
    
    // Check browser compatibility
    const isChrome = /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor);
    const isFirefox = /Firefox/.test(navigator.userAgent);
    const isSafari = /Safari/.test(navigator.userAgent) && !/Chrome/.test(navigator.userAgent);
    const isEdge = /Edg/.test(navigator.userAgent);
    
    console.log("Browser detection:", {
        isChrome,
        isFirefox,
        isSafari,
        isEdge
    });
    
    // Check if the browser supports multiple file selection
    const input = document.createElement('input');
    input.type = 'file';
    input.multiple = true;
    console.log("Browser supports multiple attribute:", input.multiple);
});

// Fix for the DocumentManager class - wait for it to be defined
document.addEventListener('DOMContentLoaded', function() {
    // Wait a short time to ensure DocumentManager is loaded
    setTimeout(function() {
        if (typeof DocumentManager !== 'undefined') {
            try {
                const originalInitialize = DocumentManager.prototype.initialize;
                
                if (originalInitialize) {
                    DocumentManager.prototype.initialize = function() {
                        // Call the original initialize method
                        originalInitialize.apply(this, arguments);
                        
                        // Add our fixes after initialization
                        if (this.documentFile) {
                            console.log("Fixing document file input in DocumentManager");
                            
                            // Ensure multiple attribute is set
                            this.documentFile.setAttribute('multiple', 'multiple');
                            
                            // Replace the event listener to ensure it works
                            this.documentFile.removeEventListener('change', this.handleFileSelection);
                            this.documentFile.addEventListener('change', (e) => {
                                console.log("File selection event triggered");
                                this.handleFileSelection(e);
                            });
                        }
                    };
                    console.log("DocumentManager.initialize successfully patched");
                } else {
                    console.error("DocumentManager.prototype.initialize not found");
                }
            } catch (error) {
                console.error("Error patching DocumentManager:", error);
            }
        } else {
            console.error("DocumentManager not defined");
        }
    }, 500); // Wait 500ms to ensure DocumentManager is loaded
});

================
File: app/static/js/error-feedback-enhancement.js
================
/**
 * Error Feedback Enhancement - Improves error handling and user feedback
 */
document.addEventListener('DOMContentLoaded', function() {
    console.log("Error feedback enhancement script loaded");
    
    // Enhanced notification function with support for HTML content
    window.showDetailedNotification = function(message, details = null, type = 'info') {
        // Use existing notification function if available
        if (typeof showNotification === 'function') {
            if (details) {
                // Create a more detailed notification
                const notificationContent = document.createElement('div');
                
                // Add main message
                const messageEl = document.createElement('div');
                messageEl.className = 'notification-message';
                messageEl.textContent = message;
                notificationContent.appendChild(messageEl);
                
                // Add details in collapsible section
                const detailsEl = document.createElement('div');
                detailsEl.className = 'notification-details';
                
                if (Array.isArray(details)) {
                    // Handle array of error details (multiple files)
                    const list = document.createElement('ul');
                    list.className = 'error-list';
                    
                    details.forEach(item => {
                        const listItem = document.createElement('li');
                        listItem.textContent = `${item.filename}: ${item.error}`;
                        list.appendChild(listItem);
                    });
                    
                    detailsEl.appendChild(list);
                } else {
                    // Handle string details
                    detailsEl.textContent = details;
                }
                
                notificationContent.appendChild(detailsEl);
                
                // Convert to string for the notification function
                const notificationHTML = notificationContent.outerHTML;
                
                // Create a temporary div to hold the HTML
                const tempDiv = document.createElement('div');
                tempDiv.innerHTML = notificationHTML;
                
                // Call the original notification function with the HTML content
                const notificationElement = showNotification(message, type);
                
                // Replace the text content with our HTML
                if (notificationElement) {
                    notificationElement.innerHTML = '';
                    notificationElement.appendChild(tempDiv.firstChild);
                    
                    // Add close button
                    const closeBtn = document.createElement('span');
                    closeBtn.innerHTML = '&times;';
                    closeBtn.style.marginLeft = '10px';
                    closeBtn.style.cursor = 'pointer';
                    closeBtn.style.fontWeight = 'bold';
                    closeBtn.onclick = function() {
                        document.body.removeChild(notificationElement);
                    };
                    notificationElement.appendChild(closeBtn);
                }
                
                return notificationElement;
            } else {
                // Just use the standard notification for simple messages
                return showNotification(message, type);
            }
        } else {
            console.error("Base showNotification function not available");
            alert(`${message} ${details ? '\n\n' + JSON.stringify(details) : ''}`);
        }
    };
    
    // Enhance the DocumentManager to use better error handling
    // Wait for DocumentManager to be fully loaded
    setTimeout(function() {
        if (typeof DocumentManager !== 'undefined') {
            try {
                // Store original methods
                const originalUploadMultipleFiles = DocumentManager.prototype.uploadMultipleFiles;
                const originalUploadNextFile = DocumentManager.prototype.uploadNextFile;
                
                // Enhance uploadMultipleFiles method
                if (originalUploadMultipleFiles) {
                    DocumentManager.prototype.uploadMultipleFiles = function() {
                        // Call original method
                        originalUploadMultipleFiles.apply(this, arguments);
                        
                        // Enhance XHR error handling
                        const xhr = this.xhr;
                        if (xhr) {
                            const originalOnload = xhr.onload;
                            xhr.onload = function() {
                                if (xhr.status === 200) {
                                    const response = JSON.parse(xhr.responseText);
                                    
                                    if (!response.success && response.errors && response.errors.length > 0) {
                                        // Show detailed error notification
                                        showDetailedNotification(
                                            `Error uploading ${response.errors.length} document(s)`,
                                            response.errors,
                                            'warning'
                                        );
                                    }
                                }
                                
                                // Call original handler
                                if (originalOnload) {
                                    originalOnload.apply(this, arguments);
                                }
                            };
                        }
                    };
                    console.log("Enhanced uploadMultipleFiles method");
                }
                
                // Enhance uploadNextFile method
                if (originalUploadNextFile) {
                    DocumentManager.prototype.uploadNextFile = function() {
                        // Call original method
                        originalUploadNextFile.apply(this, arguments);
                        
                        // Enhance XHR error handling
                        const xhr = this.xhr;
                        if (xhr) {
                            const originalOnload = xhr.onload;
                            xhr.onload = function() {
                                if (xhr.status === 200) {
                                    const response = JSON.parse(xhr.responseText);
                                    
                                    if (!response.success && response.message) {
                                        // Show detailed error notification
                                        showDetailedNotification(
                                            `Error uploading document`,
                                            response.message,
                                            'warning'
                                        );
                                    }
                                }
                                
                                // Call original handler
                                if (originalOnload) {
                                    originalOnload.apply(this, arguments);
                                }
                            };
                        }
                    };
                    console.log("Enhanced uploadNextFile method");
                }
            } catch (error) {
                console.error("Error enhancing DocumentManager:", error);
            }
        } else {
            console.error("DocumentManager not defined for error enhancement");
        }
    }, 1000); // Wait 1 second to ensure DocumentManager is fully loaded
    
    // Add CSS for enhanced notifications
    const style = document.createElement('style');
    style.textContent = `
        .notification-message {
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .notification-details {
            font-size: 0.9em;
            max-height: 100px;
            overflow-y: auto;
            border-top: 1px solid rgba(255, 255, 255, 0.2);
            padding-top: 5px;
            margin-top: 5px;
        }
        
        .error-list {
            margin: 0;
            padding-left: 20px;
        }
        
        .error-list li {
            margin-bottom: 3px;
        }
    `;
    document.head.appendChild(style);
});

================
File: app/static/js/login_handler.js
================
// Enable debug mode for troubleshooting
const debugMode = true;

function debugLog(message) {
    if (debugMode) {
        const debugInfo = document.getElementById('debug-info');
        if (debugInfo) {
            debugInfo.style.display = 'block';
            debugInfo.innerHTML += message + '<br>';
        }
        console.log(message);
    }
}

// Check for credentials in URL and clean them up
function checkAndCleanCredentialsInUrl() {
    const urlParams = new URLSearchParams(window.location.search);
    const hasUsername = urlParams.has('username');
    const hasPassword = urlParams.has('password');
    
    if (hasUsername || hasPassword) {
        // Show warning
        const errorMessage = document.getElementById('error-message');
        if (errorMessage) {
            errorMessage.textContent = 'WARNING: Credentials should never be included in URLs as this is a security risk. The URL has been cleaned.';
            errorMessage.style.display = 'block';
        }
        
        // Save redirect parameter if present
        const redirect = urlParams.get('redirect');
        
        // Clean the URL (keep only redirect parameter if it exists)
        const cleanUrl = redirect
            ? `/login?redirect=${encodeURIComponent(redirect)}`
            : '/login';
        
        // Replace current URL without reloading
        window.history.replaceState({}, document.title, cleanUrl);
        
        debugLog('Credentials detected in URL and cleaned');
    }
}

document.addEventListener('DOMContentLoaded', function() {
    debugLog('Page loaded');
    
    // Check for credentials in URL
    checkAndCleanCredentialsInUrl();
    
    const urlParams = new URLSearchParams(window.location.search);
    
    // Check if there's a registered parameter
    const registered = urlParams.get('registered');
    if (registered === 'true') {
        const successMessage = document.getElementById('success-message');
        if (successMessage) {
            successMessage.textContent = 'Registration successful! Please log in.';
            successMessage.style.display = 'block';
        }
        
        // Clean up the URL
        const cleanUrl = window.location.pathname;
        window.history.replaceState({}, document.title, cleanUrl);
    }
    
    // Check if there's a security warning
    const securityWarning = urlParams.get('security_warning');
    if (securityWarning === 'credentials_in_url') {
        const errorMessage = document.getElementById('error-message');
        if (errorMessage) {
            errorMessage.textContent = 'WARNING: Credentials should never be included in URLs as this is a security risk.';
            errorMessage.style.display = 'block';
        }
        
        // Clean up the URL
        const redirect = urlParams.get('redirect');
        const cleanUrl = redirect
            ? `/login?redirect=${encodeURIComponent(redirect)}`
            : '/login';
        window.history.replaceState({}, document.title, cleanUrl);
    }
    
    // Set up login form submission handler
    const loginForm = document.getElementById('login-form');
    if (loginForm) {
        loginForm.addEventListener('submit', async function(e) {
            e.preventDefault(); // Prevent the default form submission
            debugLog('Form submitted');
            
            const username = document.getElementById('username').value;
            const password = document.getElementById('password').value;
            
            try {
                debugLog('Sending login request');
                // Always use the API endpoint directly
                const response = await fetch('/api/auth/token', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/x-www-form-urlencoded',
                    },
                    body: `username=${encodeURIComponent(username)}&password=${encodeURIComponent(password)}`
                });
                
                debugLog(`Response status: ${response.status}`);
                const data = await response.json();
                debugLog(`Response data: ${JSON.stringify(data)}`);
                
                if (response.ok) {
                    // Store token and username in localStorage
                    localStorage.setItem('access_token', data.access_token);
                    localStorage.setItem('token_type', data.token_type);
                    localStorage.setItem('username', username);
                    
                    // Show success message
                    const successMessage = document.getElementById('success-message');
                    if (successMessage) {
                        successMessage.textContent = 'Login successful! Redirecting...';
                        successMessage.style.display = 'block';
                    }
                    
                    const errorMessage = document.getElementById('error-message');
                    if (errorMessage) {
                        errorMessage.textContent = '';
                    }
                    
                    // Check for redirect parameter
                    const urlParams = new URLSearchParams(window.location.search);
                    const redirect = urlParams.get('redirect');
                    
                    // Log the redirect
                    debugLog(`Redirecting to: ${redirect || '/'}`);
                    
                    // Redirect to the specified page or home page after a short delay
                    setTimeout(() => {
                        window.location.href = redirect || '/';
                    }, 1000);
                } else {
                    // Display error message
                    const errorMessage = document.getElementById('error-message');
                    if (errorMessage) {
                        errorMessage.textContent = data.detail || 'Login failed';
                        errorMessage.style.display = 'block';
                    }
                    
                    const successMessage = document.getElementById('success-message');
                    if (successMessage) {
                        successMessage.style.display = 'none';
                    }
                }
            } catch (error) {
                console.error('Error:', error);
                debugLog(`Error: ${error.message}`);
                
                const errorMessage = document.getElementById('error-message');
                if (errorMessage) {
                    errorMessage.textContent = 'An error occurred during login';
                    errorMessage.style.display = 'block';
                }
                
                const successMessage = document.getElementById('success-message');
                if (successMessage) {
                    successMessage.style.display = 'none';
                }
            }
        });
    }
});

================
File: app/static/js/login.js
================
document.addEventListener('DOMContentLoaded', function() {
    document.getElementById('login-form').addEventListener('submit', async function(e) {
        e.preventDefault();
        
        const username = document.getElementById('username').value;
        const password = document.getElementById('password').value;
        
        try {
            const response = await fetch('/api/auth/token', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/x-www-form-urlencoded',
                },
                body: `username=${encodeURIComponent(username)}&password=${encodeURIComponent(password)}`
            });
            
            const data = await response.json();
            
            if (response.ok) {
                // Store token and username in localStorage
                localStorage.setItem('access_token', data.access_token);
                localStorage.setItem('token_type', data.token_type);
                localStorage.setItem('username', username);
                
                // Set auth token as a cookie for server-side authentication
                const expirationDate = new Date();
                expirationDate.setDate(expirationDate.getDate() + 7); // 7 days expiration
                document.cookie = `auth_token=${data.access_token}; path=/; expires=${expirationDate.toUTCString()}; SameSite=Strict`;
                
                // Check for redirect parameter
                const urlParams = new URLSearchParams(window.location.search);
                const redirect = urlParams.get('redirect');
                
                // Redirect to the specified page or home page
                window.location.href = redirect || '/';
            } else {
                // Display error message
                document.getElementById('error-message').textContent = data.detail || 'Login failed';
            }
        } catch (error) {
            console.error('Error:', error);
            document.getElementById('error-message').textContent = 'An error occurred during login';
        }
    });
});

================
File: app/static/js/main.js
================
// Theme switching functionality
document.addEventListener('DOMContentLoaded', function() {
    // Check for saved theme preference or default to 'dark'
    const savedTheme = localStorage.getItem('theme') || 'dark';
    document.documentElement.setAttribute('data-theme', savedTheme);
    
    // Update theme toggle button
    updateThemeToggle(savedTheme);
    
    // Set up theme toggle button
    const themeToggle = document.getElementById('theme-toggle');
    if (themeToggle) {
        themeToggle.addEventListener('click', toggleTheme);
    }
});

// Toggle theme function
function toggleTheme() {
    const currentTheme = document.documentElement.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    
    document.documentElement.setAttribute('data-theme', newTheme);
    localStorage.setItem('theme', newTheme);
    updateThemeToggle(newTheme);
}

// Update theme toggle button
function updateThemeToggle(theme) {
    const toggle = document.getElementById('theme-toggle');
    if (toggle) {
        toggle.innerHTML = theme === 'dark' ? '<i class="fas fa-sun"></i>' : '<i class="fas fa-moon"></i>';
        toggle.title = theme === 'dark' ? 'Switch to light mode' : 'Switch to dark mode';
    }
}

// Show notification
function showNotification(message, type = 'info') {
    // Create notification element
    const notification = document.createElement('div');
    notification.className = `notification ${type}`;
    notification.style.position = 'fixed';
    notification.style.top = '20px';
    notification.style.right = '20px';
    notification.style.backgroundColor = type === 'warning' ? '#ff9800' : 'var(--secondary-color)';
    notification.style.color = 'white';
    notification.style.padding = '10px 15px';
    notification.style.borderRadius = '4px';
    notification.style.boxShadow = '0 2px 10px rgba(0, 0, 0, 0.2)';
    notification.style.zIndex = '1000';
    notification.style.maxWidth = '300px';
    notification.textContent = message;
    
    // Add close button
    const closeBtn = document.createElement('span');
    closeBtn.innerHTML = '&times;';
    closeBtn.style.marginLeft = '10px';
    closeBtn.style.cursor = 'pointer';
    closeBtn.style.fontWeight = 'bold';
    closeBtn.onclick = function() {
        document.body.removeChild(notification);
    };
    notification.appendChild(closeBtn);
    
    // Add to body
    document.body.appendChild(notification);
    
    // Auto remove after 5 seconds
    setTimeout(() => {
        if (document.body.contains(notification)) {
            document.body.removeChild(notification);
        }
    }, 5000);
}

// Copy to clipboard function
function copyToClipboard(text) {
    navigator.clipboard.writeText(text).then(() => {
        showNotification('Copied to clipboard!');
    }).catch(err => {
        console.error('Could not copy text: ', err);
        showNotification('Failed to copy to clipboard', 'warning');
    });
}

// Conversation Management
// Make conversation globally accessible
window.conversation = {
    messages: [],
    metadata: {
        estimatedTokens: 0,
        maxTokens: 4096,
        lastUpdated: new Date().toISOString()
    }
};
// Reference for local use
let conversation = window.conversation;

// Token estimation (rough approximation - 1 token ≈ 4 characters)
function estimateTokens(text) {
    return Math.ceil(text.length / 4);
}

// Add message to conversation
function addMessage(role, content, sources = null) {
    conversation.messages.push({
        role: role,
        content: content,
        sources: sources,
        timestamp: new Date().toISOString()
    });
    
    const tokens = estimateTokens(content);
    conversation.metadata.estimatedTokens += tokens;
    conversation.metadata.lastUpdated = new Date().toISOString();
    
    saveToLocalStorage();
    updateTokenDisplay();
}

// Get formatted conversation history for prompt
function getConversationHistory() {
    let history = '';
    conversation.messages.forEach(msg => {
        const role = msg.role === 'user' ? 'User' : 'Metis';
        history += `${role}: ${msg.content}\n\n`;
    });
    return history;
}

// Get formatted conversation for Ollama
function getFormattedPrompt(newPrompt) {
    // First check if we need to trim the conversation
    trimConversationToFit(conversation.metadata.maxTokens);
    
    // Then format the conversation with the new prompt
    let formattedPrompt = '';
    
    // Add conversation history
    conversation.messages.forEach(msg => {
        const role = msg.role === 'user' ? 'User' : 'Metis';
        formattedPrompt += `${role}: ${msg.content}\n\n`;
    });
    
    // Add the new prompt
    formattedPrompt += `User: ${newPrompt}\n\nMetis:`;
    
    return formattedPrompt;
}

// Clear conversation
function clearConversation() {
    window.conversation = {
        messages: [],
        metadata: {
            estimatedTokens: 0,
            maxTokens: parseInt(document.getElementById('num_ctx')?.value || 4096),
            lastUpdated: new Date().toISOString()
        }
    };
    // Update local reference
    conversation = window.conversation;
    saveToLocalStorage();
    updateTokenDisplay();
}

// Make clearConversation globally accessible
window.clearConversation = clearConversation;

// Save conversation to localStorage
function saveToLocalStorage() {
    localStorage.setItem('metis_conversation', JSON.stringify(conversation));
}

// Load conversation from localStorage
function loadFromLocalStorage() {
    const saved = localStorage.getItem('metis_conversation');
    if (saved) {
        try {
            conversation = JSON.parse(saved);
            // Update max tokens from current form value
            const numCtxElement = document.getElementById('num_ctx');
            if (numCtxElement) {
                conversation.metadata.maxTokens = parseInt(numCtxElement.value) || 4096;
            }
            updateTokenDisplay();
            return true;
        } catch (e) {
            console.error('Error loading conversation:', e);
            return false;
        }
    }
    return false;
}

// Get total token count for conversation
function getConversationTokenCount() {
    return conversation.metadata.estimatedTokens;
}

// Trim conversation to fit within token limit
function trimConversationToFit(maxTokens) {
    // Reserve tokens for the new prompt and response (rough estimate)
    const reservedTokens = 1000;
    const availableTokens = maxTokens - reservedTokens;
    
    // If we're already under the limit, no need to trim
    if (getConversationTokenCount() <= availableTokens) {
        return;
    }
    
    // Remove oldest messages until we're under the limit
    while (getConversationTokenCount() > availableTokens && conversation.messages.length > 0) {
        const removedMsg = conversation.messages.shift();
        conversation.metadata.estimatedTokens -= estimateTokens(removedMsg.content);
    }
    
    // Update localStorage and UI
    saveToLocalStorage();
    updateTokenDisplay();
    
    // Show a notification that some messages were removed
    showNotification('Some older messages were removed to stay within the token limit.');
}

// Update token display
function updateTokenDisplay() {
    const tokenUsage = document.getElementById('token-usage');
    const tokenUsageFill = document.getElementById('token-usage-fill');
    const tokenUsageText = document.getElementById('token-usage-text');
    
    if (!tokenUsage || !tokenUsageFill || !tokenUsageText) return;
    
    const currentTokens = getConversationTokenCount();
    const maxTokens = conversation.metadata.maxTokens;
    const percentage = Math.min((currentTokens / maxTokens) * 100, 100);
    
    tokenUsageFill.style.width = `${percentage}%`;
    tokenUsageText.textContent = `${currentTokens} / ${maxTokens} tokens`;
    
    // Set color based on usage
    if (percentage > 90) {
        tokenUsageFill.style.backgroundColor = '#d32f2f'; // Red for high usage
    } else if (percentage > 70) {
        tokenUsageFill.style.backgroundColor = '#ff9800'; // Orange for medium usage
    } else {
        tokenUsageFill.style.backgroundColor = 'var(--accent-color)'; // Default color
    }
    
    // Show the token usage indicator if we have messages
    if (conversation.messages.length > 0) {
        tokenUsage.style.display = 'block';
    } else {
        tokenUsage.style.display = 'none';
    }
}

// Initialize conversation
function initConversation() {
    // Try to load from localStorage
    if (!loadFromLocalStorage()) {
        // If no saved conversation, initialize a new one
        clearConversation();
    }
    
    // Render the conversation if there's a chat container
    const chatContainer = document.getElementById('chat-container');
    if (chatContainer) {
        renderConversation();
    }
    
    // Update token display
    updateTokenDisplay();
}

// Render conversation in UI
function renderConversation() {
    const chatContainer = document.getElementById('chat-container');
    if (!chatContainer) return;
    
    chatContainer.innerHTML = '';
    
    conversation.messages.forEach(msg => {
        const messageDiv = document.createElement('div');
        messageDiv.className = `message ${msg.role === 'user' ? 'user-message' : 'bot-message'}`;
        
        const headerDiv = document.createElement('div');
        headerDiv.className = 'message-header';
        headerDiv.textContent = msg.role === 'user' ? 'You:' : 'Metis:';
        messageDiv.appendChild(headerDiv);
        
        const contentText = document.createTextNode(msg.content);
        messageDiv.appendChild(contentText);
        
        if (msg.role === 'assistant') {
            const copyButton = document.createElement('button');
            copyButton.className = 'copy-button';
            copyButton.innerHTML = '<i class="fas fa-copy"></i> Copy';
            copyButton.onclick = function() {
                copyToClipboard(msg.content);
            };
            messageDiv.appendChild(copyButton);
            
            // Add sources if available
            if (msg.sources && msg.sources.length > 0) {
                const sourcesDiv = document.createElement('div');
                sourcesDiv.className = 'sources-section';
                sourcesDiv.innerHTML = '<strong>Sources:</strong> ';
                
                msg.sources.forEach(source => {
                    const sourceSpan = document.createElement('span');
                    sourceSpan.className = 'source-item';
                    sourceSpan.textContent = source.filename || source;
                    sourcesDiv.appendChild(sourceSpan);
                });
                
                messageDiv.appendChild(sourcesDiv);
            }
        }
        
        chatContainer.appendChild(messageDiv);
    });
    
    // Scroll to bottom
    chatContainer.scrollTop = chatContainer.scrollHeight;
}

// Authentication functions
function isAuthenticated() {
    return localStorage.getItem('access_token') !== null;
}

function getToken() {
    return localStorage.getItem('access_token');
}

// Authenticated fetch function
function authenticatedFetch(url, options = {}) {
    // Clone the options to avoid modifying the original
    const fetchOptions = { ...options };
    
    // Add headers if not present
    if (!fetchOptions.headers) {
        fetchOptions.headers = {};
    }
    
    // Add authorization header if authenticated
    if (isAuthenticated()) {
        fetchOptions.headers['Authorization'] = `Bearer ${getToken()}`;
    }
    
    // Return the fetch promise
    return fetch(url, fetchOptions);
}

// Make authentication functions globally available
window.isAuthenticated = isAuthenticated;
window.getToken = getToken;
window.authenticatedFetch = authenticatedFetch;
console.log('Auth functions made global');

function logout() {
    localStorage.removeItem('access_token');
    localStorage.removeItem('token_type');
    localStorage.removeItem('username');
    updateAuthUI();
    
    // Redirect to login page if on a protected page
    const protectedPages = ['/documents', '/chat', '/analytics', '/system'];
    const currentPath = window.location.pathname;
    if (protectedPages.includes(currentPath)) {
        window.location.href = '/login';
    } else {
        showNotification('Logged out successfully');
    }
}

function getCurrentUser() {
    if (!isAuthenticated()) {
        return null;
    }
    
    return fetch('/api/auth/me', {
        headers: {
            'Authorization': `Bearer ${getToken()}`
        }
    })
    .then(response => {
        if (response.ok) {
            return response.json();
        }
        throw new Error('Failed to get user info');
    })
    .catch(error => {
        console.error('Error getting current user:', error);
        // If we can't get the user info, the token might be invalid
        logout();
        return null;
    });
}

function updateAuthUI() {
    const loginButton = document.getElementById('login-button');
    const logoutButton = document.getElementById('logout-button');
    const usernameDisplay = document.getElementById('username-display');
    
    if (!loginButton || !logoutButton || !usernameDisplay) {
        return;
    }
    
    if (isAuthenticated()) {
        loginButton.style.display = 'none';
        logoutButton.style.display = 'inline-block';
        
        // Display username if available
        const username = localStorage.getItem('username');
        if (username) {
            usernameDisplay.textContent = username;
        } else {
            // Try to get username from API
            getCurrentUser().then(user => {
                if (user) {
                    localStorage.setItem('username', user.username);
                    usernameDisplay.textContent = user.username;
                }
            });
        }
    } else {
        loginButton.style.display = 'inline-block';
        logoutButton.style.display = 'none';
        usernameDisplay.textContent = '';
    }
}

function setupAuthListeners() {
    const loginButton = document.getElementById('login-button');
    const logoutButton = document.getElementById('logout-button');
    
    if (loginButton) {
        loginButton.addEventListener('click', () => {
            window.location.href = '/login';
        });
    }
    
    if (logoutButton) {
        logoutButton.addEventListener('click', logout);
    }
}

// Initialize when page loads
document.addEventListener('DOMContentLoaded', function() {
    // Initialize conversation
    initConversation();
    
    // Set up authentication UI
    updateAuthUI();
    setupAuthListeners();
    
    // Check if we need to redirect to login
    const protectedPages = ['/documents', '/chat', '/analytics', '/system'];
    const currentPath = window.location.pathname;
    if (protectedPages.includes(currentPath) && !isAuthenticated()) {
        window.location.href = '/login?redirect=' + encodeURIComponent(currentPath);
    }
    
    // Set up advanced options toggle if it exists
    const advancedToggle = document.getElementById('advanced-toggle');
    const advancedContent = document.getElementById('advanced-content');
    const advancedIcon = document.getElementById('advanced-icon');
    
    if (advancedToggle && advancedContent && advancedIcon) {
        // Show advanced options if they were previously shown
        if (localStorage.getItem('advancedOptions') === 'shown') {
            advancedContent.classList.add('show');
            advancedIcon.classList.replace('fa-chevron-down', 'fa-chevron-up');
        }
        
        advancedToggle.addEventListener('click', function() {
            advancedContent.classList.toggle('show');
            if (advancedContent.classList.contains('show')) {
                advancedIcon.classList.replace('fa-chevron-down', 'fa-chevron-up');
                localStorage.setItem('advancedOptions', 'shown');
            } else {
                advancedIcon.classList.replace('fa-chevron-up', 'fa-chevron-down');
                localStorage.setItem('advancedOptions', 'hidden');
            }
        });
    }
    
    // Set up context window size change listener if it exists
    const numCtxElement = document.getElementById('num_ctx');
    if (numCtxElement) {
        numCtxElement.addEventListener('change', function() {
            conversation.metadata.maxTokens = parseInt(this.value) || 4096;
            updateTokenDisplay();
            saveToLocalStorage();
        });
    }
});

================
File: app/static/js/register.js
================
document.addEventListener('DOMContentLoaded', function() {
    document.getElementById('register-form').addEventListener('submit', async function(e) {
        e.preventDefault();
        
        const username = document.getElementById('username').value;
        const email = document.getElementById('email').value;
        const fullName = document.getElementById('full_name').value;
        const password = document.getElementById('password').value;
        const confirmPassword = document.getElementById('confirm_password').value;
        
        // Validate passwords match
        if (password !== confirmPassword) {
            document.getElementById('error-message').textContent = 'Passwords do not match';
            return;
        }
        
        try {
            const response = await fetch('/api/auth/register', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    username: username,
                    email: email,
                    full_name: fullName,
                    password: password,
                    is_active: true,
                    is_admin: false
                })
            });
            
            const data = await response.json();
            
            if (response.ok) {
                // Redirect to login page
                window.location.href = '/login?registered=true';
            } else {
                // Display error message
                document.getElementById('error-message').textContent = data.detail || 'Registration failed';
            }
        } catch (error) {
            console.error('Error:', error);
            document.getElementById('error-message').textContent = 'An error occurred during registration';
        }
    });
});

================
File: app/static/js/schema.js
================
// Schema Viewer JavaScript

document.addEventListener('DOMContentLoaded', function() {
    // Elements
    const connectionSelect = document.getElementById('connection-select');
    const schemaSelect = document.getElementById('schema-select');
    const tableSelect = document.getElementById('table-select');
    const refreshConnectionsBtn = document.getElementById('refresh-connections');
    const tabButtons = document.querySelectorAll('.tab-button');
    const tabPanes = document.querySelectorAll('.tab-pane');
    const explainButton = document.getElementById('explain-button');
    const queryInput = document.getElementById('query-input');
    const loadingOverlay = document.getElementById('loading-overlay');
    const errorModal = document.getElementById('error-modal');
    const errorMessage = document.getElementById('error-message');
    const closeModalBtn = document.querySelector('.close');

    // Initialize
    loadConnections();

    // Event listeners
    refreshConnectionsBtn.addEventListener('click', loadConnections);
    connectionSelect.addEventListener('change', handleConnectionChange);
    schemaSelect.addEventListener('change', handleSchemaChange);
    tableSelect.addEventListener('change', handleTableChange);
    explainButton.addEventListener('click', explainQuery);
    closeModalBtn.addEventListener('click', () => errorModal.classList.add('hidden'));

    // Tab switching
    tabButtons.forEach(button => {
        button.addEventListener('click', () => {
            // Remove active class from all buttons and panes
            tabButtons.forEach(btn => btn.classList.remove('active'));
            tabPanes.forEach(pane => pane.classList.remove('active'));
            
            // Add active class to clicked button and corresponding pane
            button.classList.add('active');
            const tabId = button.getAttribute('data-tab');
            document.getElementById(tabId).classList.add('active');
        });
    });

    // Functions
    async function loadConnections() {
        showLoading();
        try {
            const response = await fetch('/api/schema/connections');
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            const data = await response.json();
            
            // Clear existing options
            connectionSelect.innerHTML = '<option value="">Select a connection...</option>';
            
            // Add new options
            data.connections.forEach(conn => {
                const option = document.createElement('option');
                option.value = conn.id;
                option.textContent = conn.connection_string;
                connectionSelect.appendChild(option);
            });
            
            // Reset dependent selects
            schemaSelect.innerHTML = '<option value="">Select a schema...</option>';
            schemaSelect.disabled = true;
            tableSelect.innerHTML = '<option value="">Select a table...</option>';
            tableSelect.disabled = true;
            
            // Clear content areas
            clearContentAreas();
            
        } catch (error) {
            showError(`Failed to load connections: ${error.message}`);
        } finally {
            hideLoading();
        }
    }

    async function handleConnectionChange() {
        const connectionId = connectionSelect.value;
        
        if (!connectionId) {
            schemaSelect.innerHTML = '<option value="">Select a schema...</option>';
            schemaSelect.disabled = true;
            tableSelect.innerHTML = '<option value="">Select a table...</option>';
            tableSelect.disabled = true;
            clearContentAreas();
            return;
        }
        
        showLoading();
        try {
            const response = await fetch(`/api/schema/schemas?connection_id=${connectionId}`);
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            const data = await response.json();
            
            // Clear existing options
            schemaSelect.innerHTML = '<option value="">Select a schema...</option>';
            
            // Add new options
            data.schemas.forEach(schema => {
                const option = document.createElement('option');
                option.value = schema.schema_name;
                option.textContent = schema.schema_name;
                schemaSelect.appendChild(option);
            });
            
            // Enable schema select
            schemaSelect.disabled = false;
            
            // Reset table select
            tableSelect.innerHTML = '<option value="">Select a table...</option>';
            tableSelect.disabled = true;
            
            // Clear content areas
            clearContentAreas();
            
        } catch (error) {
            showError(`Failed to load schemas: ${error.message}`);
        } finally {
            hideLoading();
        }
    }

    async function handleSchemaChange() {
        const connectionId = connectionSelect.value;
        const schema = schemaSelect.value;
        
        if (!connectionId || !schema) {
            tableSelect.innerHTML = '<option value="">Select a table...</option>';
            tableSelect.disabled = true;
            clearContentAreas();
            return;
        }
        
        showLoading();
        try {
            const response = await fetch(`/api/schema/tables?connection_id=${connectionId}&schema=${schema}`);
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            const data = await response.json();
            
            // Clear existing options
            tableSelect.innerHTML = '<option value="">Select a table...</option>';
            
            // Add new options
            data.tables.forEach(table => {
                const option = document.createElement('option');
                option.value = table.table_name;
                option.textContent = `${table.table_name} (${table.type})`;
                tableSelect.appendChild(option);
            });
            
            // Enable table select
            tableSelect.disabled = false;
            
            // Clear content areas
            clearContentAreas();
            
        } catch (error) {
            showError(`Failed to load tables: ${error.message}`);
        } finally {
            hideLoading();
        }
    }

    async function handleTableChange() {
        const connectionId = connectionSelect.value;
        const schema = schemaSelect.value;
        const table = tableSelect.value;
        
        if (!connectionId || !schema || !table) {
            clearContentAreas();
            return;
        }
        
        showLoading();
        try {
            // Load table structure
            const response = await fetch(`/api/schema/table-structure?connection_id=${connectionId}&table_name=${table}&schema=${schema}`);
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            const data = await response.json();
            const tableStructure = data.table_structure;
            
            // Update table info
            document.getElementById('table-name').textContent = tableStructure.table_name;
            document.getElementById('table-description').textContent = tableStructure.description || 'No description available';
            document.getElementById('table-owner').textContent = tableStructure.owner || 'Unknown';
            document.getElementById('table-row-count').textContent = tableStructure.exact_row_count !== null ? 
                tableStructure.exact_row_count : `~${tableStructure.row_estimate} (estimate)`;
            document.getElementById('table-size').textContent = tableStructure.total_size || 'Unknown';
            
            // Update structure content
            const structureContent = document.getElementById('structure-content');
            structureContent.innerHTML = `
                <h3>Table Overview</h3>
                <p>This table has ${tableStructure.columns.length} columns, 
                   ${tableStructure.indexes.length} indexes, and 
                   ${tableStructure.constraints.length} constraints.</p>
            `;
            
            // Update columns content
            const columnsContent = document.getElementById('columns-content');
            columnsContent.innerHTML = `
                <table>
                    <thead>
                        <tr>
                            <th>Column Name</th>
                            <th>Data Type</th>
                            <th>Nullable</th>
                            <th>Default</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        ${tableStructure.columns.map(column => `
                            <tr>
                                <td>${column.column_name}</td>
                                <td>${column.data_type}</td>
                                <td>${column.is_nullable}</td>
                                <td>${column.default_value || ''}</td>
                                <td>${column.description || ''}</td>
                            </tr>
                        `).join('')}
                    </tbody>
                </table>
            `;
            
            // Update indexes content
            const indexesContent = document.getElementById('indexes-content');
            if (tableStructure.indexes.length > 0) {
                indexesContent.innerHTML = `
                    <table>
                        <thead>
                            <tr>
                                <th>Index Name</th>
                                <th>Type</th>
                                <th>Definition</th>
                                <th>Unique</th>
                                <th>Primary</th>
                                <th>Size</th>
                            </tr>
                        </thead>
                        <tbody>
                            ${tableStructure.indexes.map(index => `
                                <tr>
                                    <td>${index.index_name}</td>
                                    <td>${index.index_type}</td>
                                    <td>${index.index_definition}</td>
                                    <td>${index.is_unique}</td>
                                    <td>${index.is_primary}</td>
                                    <td>${index.index_size || ''}</td>
                                </tr>
                            `).join('')}
                        </tbody>
                    </table>
                `;
            } else {
                indexesContent.innerHTML = '<p>No indexes found for this table.</p>';
            }
            
            // Update constraints content
            const constraintsContent = document.getElementById('constraints-content');
            if (tableStructure.constraints.length > 0) {
                constraintsContent.innerHTML = `
                    <table>
                        <thead>
                            <tr>
                                <th>Constraint Name</th>
                                <th>Type</th>
                                <th>Definition</th>
                                <th>Deferrable</th>
                                <th>Deferred</th>
                                <th>Validated</th>
                            </tr>
                        </thead>
                        <tbody>
                            ${tableStructure.constraints.map(constraint => `
                                <tr>
                                    <td>${constraint.constraint_name}</td>
                                    <td>${constraint.constraint_type}</td>
                                    <td>${constraint.definition}</td>
                                    <td>${constraint.is_deferrable ? 'YES' : 'NO'}</td>
                                    <td>${constraint.is_deferred ? 'YES' : 'NO'}</td>
                                    <td>${constraint.is_validated ? 'YES' : 'NO'}</td>
                                </tr>
                            `).join('')}
                        </tbody>
                    </table>
                `;
            } else {
                constraintsContent.innerHTML = '<p>No constraints found for this table.</p>';
            }
            
            // Update foreign keys content
            const foreignKeysContent = document.getElementById('foreign-keys-content');
            if (tableStructure.foreign_keys.length > 0) {
                foreignKeysContent.innerHTML = `
                    <table>
                        <thead>
                            <tr>
                                <th>Constraint Name</th>
                                <th>Referenced Schema</th>
                                <th>Referenced Table</th>
                                <th>Columns</th>
                                <th>Referenced Columns</th>
                                <th>Update Rule</th>
                                <th>Delete Rule</th>
                            </tr>
                        </thead>
                        <tbody>
                            ${tableStructure.foreign_keys.map(fk => `
                                <tr>
                                    <td>${fk.constraint_name}</td>
                                    <td>${fk.referenced_schema}</td>
                                    <td>${fk.referenced_table}</td>
                                    <td>${Array.isArray(fk.column_names) ? fk.column_names.join(', ') : fk.column_names}</td>
                                    <td>${Array.isArray(fk.referenced_columns) ? fk.referenced_columns.join(', ') : fk.referenced_columns}</td>
                                    <td>${fk.update_rule}</td>
                                    <td>${fk.delete_rule}</td>
                                </tr>
                            `).join('')}
                        </tbody>
                    </table>
                `;
            } else {
                foreignKeysContent.innerHTML = '<p>No foreign keys found for this table.</p>';
            }
            
        } catch (error) {
            showError(`Failed to load table structure: ${error.message}`);
        } finally {
            hideLoading();
        }
    }

    async function explainQuery() {
        const connectionId = connectionSelect.value;
        const query = queryInput.value.trim();
        const explainType = document.querySelector('input[name="explain-type"]:checked').value;
        
        if (!connectionId) {
            showError('Please select a database connection first.');
            return;
        }
        
        if (!query) {
            showError('Please enter a SQL query to explain.');
            return;
        }
        
        showLoading();
        try {
            const response = await fetch('/api/schema/explain-query', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    connection_id: connectionId,
                    query: query,
                    explain_type: explainType
                })
            });
            
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const data = await response.json();
            const explainContent = document.getElementById('explain-content');
            
            if (explainType === 'json' || explainType === 'analyze_json') {
                // Format JSON for display
                explainContent.innerHTML = `<pre>${JSON.stringify(data.plan, null, 2)}</pre>`;
            } else {
                // Display text plan
                explainContent.innerHTML = `<pre>${data.plan_text}</pre>`;
            }
            
        } catch (error) {
            showError(`Failed to explain query: ${error.message}`);
        } finally {
            hideLoading();
        }
    }

    function clearContentAreas() {
        document.getElementById('table-name').textContent = '';
        document.getElementById('table-description').textContent = '';
        document.getElementById('table-owner').textContent = '';
        document.getElementById('table-row-count').textContent = '';
        document.getElementById('table-size').textContent = '';
        
        document.getElementById('structure-content').innerHTML = '';
        document.getElementById('columns-content').innerHTML = '';
        document.getElementById('indexes-content').innerHTML = '';
        document.getElementById('constraints-content').innerHTML = '';
        document.getElementById('foreign-keys-content').innerHTML = '';
        document.getElementById('explain-content').innerHTML = '';
    }

    function showLoading() {
        loadingOverlay.classList.remove('hidden');
    }

    function hideLoading() {
        loadingOverlay.classList.add('hidden');
    }

    function showError(message) {
        errorMessage.textContent = message;
        errorModal.classList.remove('hidden');
    }
});

================
File: app/static/js/tasks.js
================
/**
 * Background Tasks Management JavaScript
 */
document.addEventListener('DOMContentLoaded', function() {
    // DOM Elements
    const statusFilter = document.getElementById('status-filter');
    const typeFilter = document.getElementById('type-filter');
    const applyFiltersBtn = document.getElementById('apply-filters');
    const clearFiltersBtn = document.getElementById('clear-filters');
    const createTaskBtn = document.getElementById('create-task-btn');
    const submitTaskBtn = document.getElementById('submit-task');
    const autoRefreshToggle = document.getElementById('auto-refresh');
    const refreshIntervalSelect = document.getElementById('refresh-interval');
    
    // State
    let currentStatus = '';
    let currentType = '';
    let currentPage = 1;
    let pageSize = 10;
    let totalTasks = 0;
    let refreshInterval = 5000;
    let refreshTimer = null;
    
    // Initialize
    loadTasks();
    loadStats();
    
    // Set up auto-refresh
    autoRefreshToggle.addEventListener('change', function() {
        if (this.checked) {
            startAutoRefresh();
        } else {
            stopAutoRefresh();
        }
    });
    
    refreshIntervalSelect.addEventListener('change', function() {
        refreshInterval = parseInt(this.value);
        if (autoRefreshToggle.checked) {
            stopAutoRefresh();
            startAutoRefresh();
        }
    });
    
    // Start auto-refresh by default
    startAutoRefresh();
    
    // Set up filters
    applyFiltersBtn.addEventListener('click', function() {
        currentStatus = statusFilter.value;
        currentType = typeFilter.value;
        currentPage = 1;
        loadTasks();
    });
    
    clearFiltersBtn.addEventListener('click', function() {
        statusFilter.value = '';
        typeFilter.value = '';
        currentStatus = '';
        currentType = '';
        currentPage = 1;
        loadTasks();
    });
    
    // Set up task creation
    submitTaskBtn.addEventListener('click', function() {
        createTask();
    });
    
    /**
     * Start auto-refresh timer
     */
    function startAutoRefresh() {
        stopAutoRefresh();
        refreshTimer = setInterval(function() {
            loadTasks(false);
            loadStats();
        }, refreshInterval);
    }
    
    /**
     * Stop auto-refresh timer
     */
    function stopAutoRefresh() {
        if (refreshTimer) {
            clearInterval(refreshTimer);
            refreshTimer = null;
        }
    }
    
    /**
     * Load tasks from API
     * @param {boolean} resetPage - Whether to reset to page 1
     */
    function loadTasks(resetPage = true) {
        if (resetPage) {
            currentPage = 1;
        }
        
        // Build query parameters
        const params = new URLSearchParams();
        if (currentStatus) {
            params.append('status', currentStatus);
        }
        if (currentType) {
            params.append('task_type', currentType);
        }
        params.append('limit', pageSize);
        params.append('offset', (currentPage - 1) * pageSize);
        
        // Show loading state
        document.getElementById('task-list').innerHTML = '<tr><td colspan="8" class="text-center">Loading tasks...</td></tr>';
        
        // Fetch tasks
        fetch(`/api/v1/tasks?${params.toString()}`)
            .then(response => {
                if (!response.ok) {
                    throw new Error('Network response was not ok');
                }
                return response.json();
            })
            .then(data => {
                renderTasks(data);
            })
            .catch(error => {
                console.error('Error loading tasks:', error);
                document.getElementById('task-list').innerHTML = 
                    '<tr><td colspan="8" class="text-center text-danger">Error loading tasks. Please try again.</td></tr>';
            });
    }
    
    /**
     * Render tasks in the table
     * @param {Object} data - Task data from API
     */
    function renderTasks(data) {
        const taskList = document.getElementById('task-list');
        
        // Update total tasks count
        totalTasks = data.total;
        
        // Check if there are tasks
        if (!data.tasks || data.tasks.length === 0) {
            taskList.innerHTML = '<tr><td colspan="8" class="text-center">No tasks found</td></tr>';
            document.getElementById('pagination').innerHTML = '';
            return;
        }
        
        // Render tasks
        let html = '';
        data.tasks.forEach(task => {
            // Format dates
            const createdDate = new Date(task.created_at);
            
            // Determine status class
            let statusClass = '';
            switch (task.status) {
                case 'pending':
                    statusClass = 'bg-light text-dark';
                    break;
                case 'scheduled':
                    statusClass = 'bg-info text-white';
                    break;
                case 'running':
                    statusClass = 'bg-primary text-white';
                    break;
                case 'completed':
                    statusClass = 'bg-success text-white';
                    break;
                case 'failed':
                    statusClass = 'bg-danger text-white';
                    break;
                case 'cancelled':
                    statusClass = 'bg-secondary text-white';
                    break;
                case 'waiting':
                    statusClass = 'bg-warning text-dark';
                    break;
            }
            
            // Determine priority class
            let priorityClass = '';
            switch (task.priority) {
                case 'low':
                    priorityClass = 'text-muted';
                    break;
                case 'normal':
                    priorityClass = '';
                    break;
                case 'high':
                    priorityClass = 'text-primary fw-bold';
                    break;
                case 'critical':
                    priorityClass = 'text-danger fw-bold';
                    break;
            }
            
            html += `
                <tr>
                    <td><small>${task.id.substring(0, 8)}...</small></td>
                    <td>${task.name}</td>
                    <td>${task.task_type}</td>
                    <td><span class="badge ${statusClass}">${task.status}</span></td>
                    <td><span class="${priorityClass}">${task.priority}</span></td>
                    <td>
                        <div class="progress">
                            <div class="progress-bar" role="progressbar" style="width: ${task.progress}%;" 
                                aria-valuenow="${task.progress}" aria-valuemin="0" aria-valuemax="100">
                                ${Math.round(task.progress)}%
                            </div>
                        </div>
                    </td>
                    <td><small>${createdDate.toLocaleString()}</small></td>
                    <td>
                        <button type="button" class="btn btn-sm btn-outline-primary view-task" data-task-id="${task.id}">
                            <i class="fas fa-eye"></i>
                        </button>
                        ${task.status === 'pending' || task.status === 'scheduled' || task.status === 'running' ? 
                            `<button type="button" class="btn btn-sm btn-outline-danger cancel-task" data-task-id="${task.id}">
                                <i class="fas fa-times"></i>
                            </button>` : ''}
                    </td>
                </tr>
            `;
        });
        
        taskList.innerHTML = html;
        
        // Update pagination
        renderPagination();
        
        // Set up task detail view
        document.querySelectorAll('.view-task').forEach(button => {
            button.addEventListener('click', function() {
                const taskId = this.getAttribute('data-task-id');
                showTaskDetails(taskId);
            });
        });
        
        // Set up task cancellation
        document.querySelectorAll('.cancel-task').forEach(button => {
            button.addEventListener('click', function() {
                const taskId = this.getAttribute('data-task-id');
                cancelTask(taskId);
            });
        });
    }
    
    /**
     * Render pagination controls
     */
    function renderPagination() {
        const pagination = document.getElementById('pagination');
        const totalPages = Math.ceil(totalTasks / pageSize);
        
        if (totalPages <= 1) {
            pagination.innerHTML = '';
            return;
        }
        
        let html = '';
        
        // Previous button
        html += `
            <li class="page-item ${currentPage === 1 ? 'disabled' : ''}">
                <a class="page-link" href="#" data-page="${currentPage - 1}">Previous</a>
            </li>
        `;
        
        // Page numbers
        for (let i = 1; i <= totalPages; i++) {
            html += `
                <li class="page-item ${currentPage === i ? 'active' : ''}">
                    <a class="page-link" href="#" data-page="${i}">${i}</a>
                </li>
            `;
        }
        
        // Next button
        html += `
            <li class="page-item ${currentPage === totalPages ? 'disabled' : ''}">
                <a class="page-link" href="#" data-page="${currentPage + 1}">Next</a>
            </li>
        `;
        
        pagination.innerHTML = html;
        
        // Set up pagination click handlers
        document.querySelectorAll('.page-link').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const page = parseInt(this.getAttribute('data-page'));
                if (page >= 1 && page <= totalPages) {
                    currentPage = page;
                    loadTasks(false);
                }
            });
        });
    }
    
    /**
     * Load system statistics
     */
    function loadStats() {
        fetch('/api/v1/tasks/stats')
            .then(response => {
                if (!response.ok) {
                    throw new Error('Network response was not ok');
                }
                return response.json();
            })
            .then(data => {
                // Update task counts
                document.getElementById('pending-count').textContent = data.pending_tasks;
                document.getElementById('running-count').textContent = data.running_tasks;
                document.getElementById('completed-count').textContent = data.completed_tasks;
                document.getElementById('failed-count').textContent = data.failed_tasks;
                
                // Update system load
                const loadPercent = Math.round(data.system_load * 100);
                const systemLoadBar = document.getElementById('system-load-bar');
                systemLoadBar.style.width = loadPercent + '%';
                systemLoadBar.setAttribute('aria-valuenow', loadPercent);
                systemLoadBar.textContent = loadPercent + '%';
                
                // Set color based on load
                systemLoadBar.className = 'progress-bar';
                if (loadPercent < 50) {
                    systemLoadBar.classList.add('bg-success');
                } else if (loadPercent < 80) {
                    systemLoadBar.classList.add('bg-warning');
                } else {
                    systemLoadBar.classList.add('bg-danger');
                }
                
                // Update resource alerts
                const alertsList = document.getElementById('alerts-list');
                if (data.resource_alerts && data.resource_alerts.length > 0) {
                    let alertsHtml = '';
                    data.resource_alerts.forEach(alert => {
                        const date = new Date(alert.timestamp * 1000);
                        alertsHtml += `
                            <tr>
                                <td>${date.toLocaleString()}</td>
                                <td>${alert.resource_type}</td>
                                <td>${alert.current_value.toFixed(1)}%</td>
                                <td>${alert.threshold.toFixed(1)}%</td>
                                <td>${alert.message}</td>
                            </tr>
                        `;
                    });
                    alertsList.innerHTML = alertsHtml;
                } else {
                    alertsList.innerHTML = '<tr><td colspan="5" class="text-center">No alerts</td></tr>';
                }
            })
            .catch(error => {
                console.error('Error loading stats:', error);
            });
    }
    
    /**
     * Show task details
     * @param {string} taskId - Task ID
     */
    function showTaskDetails(taskId) {
        fetch(`/api/v1/tasks/${taskId}`)
            .then(response => {
                if (!response.ok) {
                    throw new Error('Network response was not ok');
                }
                return response.json();
            })
            .then(task => {
                // Basic details
                document.getElementById('detail-id').textContent = task.id;
                document.getElementById('detail-name').textContent = task.name;
                document.getElementById('detail-type').textContent = task.task_type;
                
                // Status with badge
                let statusClass = '';
                switch (task.status) {
                    case 'pending':
                        statusClass = 'bg-light text-dark';
                        break;
                    case 'scheduled':
                        statusClass = 'bg-info text-white';
                        break;
                    case 'running':
                        statusClass = 'bg-primary text-white';
                        break;
                    case 'completed':
                        statusClass = 'bg-success text-white';
                        break;
                    case 'failed':
                        statusClass = 'bg-danger text-white';
                        break;
                    case 'cancelled':
                        statusClass = 'bg-secondary text-white';
                        break;
                    case 'waiting':
                        statusClass = 'bg-warning text-dark';
                        break;
                }
                document.getElementById('detail-status').innerHTML = `<span class="badge ${statusClass}">${task.status}</span>`;
                
                // Priority
                document.getElementById('detail-priority').textContent = task.priority;
                
                // Dates
                document.getElementById('detail-created').textContent = task.created_at ? new Date(task.created_at).toLocaleString() : 'N/A';
                document.getElementById('detail-started').textContent = task.started_at ? new Date(task.started_at).toLocaleString() : 'N/A';
                document.getElementById('detail-completed').textContent = task.completed_at ? new Date(task.completed_at).toLocaleString() : 'N/A';
                
                // Execution time
                document.getElementById('detail-execution-time').textContent = task.execution_time_ms ? `${(task.execution_time_ms / 1000).toFixed(2)}s` : 'N/A';
                
                // Retries
                document.getElementById('detail-retries').textContent = `${task.retry_count} / ${task.max_retries}`;
                
                // Progress bar
                const progressBar = document.getElementById('detail-progress-bar');
                progressBar.style.width = task.progress + '%';
                progressBar.setAttribute('aria-valuenow', task.progress);
                progressBar.textContent = Math.round(task.progress) + '%';
                
                // Parameters
                document.getElementById('detail-params').textContent = JSON.stringify(task.params, null, 2);
                
                // Result
                if (task.result) {
                    document.getElementById('detail-result').textContent = JSON.stringify(task.result, null, 2);
                } else {
                    document.getElementById('detail-result').textContent = 'No result yet';
                }
                
                // Error
                const errorContainer = document.getElementById('detail-error-container');
                if (task.error) {
                    document.getElementById('detail-error').textContent = task.error;
                    errorContainer.style.display = 'block';
                } else {
                    errorContainer.style.display = 'none';
                }
                
                // Cancel button
                const cancelBtn = document.getElementById('cancel-task');
                if (task.status === 'pending' || task.status === 'scheduled' || task.status === 'running') {
                    cancelBtn.style.display = 'block';
                    cancelBtn.setAttribute('data-task-id', task.id);
                    
                    // Set up cancel button event handler
                    cancelBtn.onclick = function() {
                        const taskId = this.getAttribute('data-task-id');
                        cancelTask(taskId);
                    };
                } else {
                    cancelBtn.style.display = 'none';
                }
                
                // Show modal
                const modal = new bootstrap.Modal(document.getElementById('taskDetailsModal'));
                modal.show();
            })
            .catch(error => {
                console.error('Error loading task details:', error);
                alert('Error loading task details. Please try again.');
            });
    }
    
    /**
     * Create a new task
     */
    function createTask() {
        // Get form values
        const name = document.getElementById('task-name').value;
        const taskType = document.getElementById('task-type').value;
        const priority = document.getElementById('task-priority').value;
        let params = {};
        
        // Validate form
        if (!name || !taskType) {
            alert('Please fill in all required fields');
            return;
        }
        
        // Parse parameters
        try {
            const paramsText = document.getElementById('task-params').value;
            if (paramsText) {
                params = JSON.parse(paramsText);
            }
        } catch (e) {
            alert('Invalid JSON in parameters field');
            return;
        }
        
        // Create task data
        const taskData = {
            name: name,
            task_type: taskType,
            priority: priority,
            params: params
        };
        
        // Submit task
        fetch('/api/v1/tasks', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(taskData)
        })
        .then(response => {
            if (!response.ok) {
                return response.json().then(data => {
                    throw new Error(data.detail || 'Error creating task');
                });
            }
            return response.json();
        })
        .then(data => {
            // Close modal
            const modal = bootstrap.Modal.getInstance(document.getElementById('createTaskModal'));
            modal.hide();
            
            // Reset form
            document.getElementById('create-task-form').reset();
            
            // Refresh tasks
            loadTasks();
            loadStats();
            
            // Show success message
            alert('Task created successfully');
        })
        .catch(error => {
            console.error('Error creating task:', error);
            alert('Error creating task: ' + error.message);
        });
    }
    
    /**
     * Cancel a task
     * @param {string} taskId - Task ID
     */
    function cancelTask(taskId) {
        if (confirm('Are you sure you want to cancel this task?')) {
            fetch(`/api/v1/tasks/${taskId}/cancel`, {
                method: 'POST'
            })
            .then(response => {
                if (!response.ok) {
                    return response.json().then(data => {
                        throw new Error(data.detail || 'Error cancelling task');
                    });
                }
                return response.json();
            })
            .then(data => {
                // Close details modal if open
                const detailsModal = bootstrap.Modal.getInstance(document.getElementById('taskDetailsModal'));
                if (detailsModal) {
                    detailsModal.hide();
                }
                
                // Refresh tasks
                loadTasks();
                loadStats();
                
                // Show success message
                alert('Task cancelled successfully');
            })
            .catch(error => {
                console.error('Error cancelling task:', error);
                alert('Error cancelling task: ' + error.message);
            });
        }
    }
});

================
File: app/static/js/test_models.js
================
// This script will be loaded by the browser and will log any errors
console.log('Test script loaded');

document.addEventListener('DOMContentLoaded', function() {
    console.log('DOM loaded, initializing test script');
    
    // Get elements
    const modelSelect = document.getElementById('test-model-select');
    const modelList = document.getElementById('model-list');
    
    if (modelSelect) {
        console.log('Found model select element');
    } else {
        console.error('Model select element not found');
    }
    
    if (modelList) {
        console.log('Found model list element');
    } else {
        console.error('Model list element not found');
    }
    
    // Try to fetch models
    authenticatedFetch('/api/system/models')
        .then(response => {
            console.log('Response status:', response.status);
            return response.json();
        })
        .then(models => {
            console.log('Models fetched:', models);
            console.log('Number of models:', models.length);
            
            // Log each model
            models.forEach(model => {
                console.log('Model:', model.name);
            });
            
            // Update UI if elements exist
            if (modelSelect) {
                // Clear the dropdown
                modelSelect.innerHTML = '';
                
                // Add models to dropdown
                models.forEach(model => {
                    const option = document.createElement('option');
                    option.value = model.name;
                    option.textContent = model.name;
                    modelSelect.appendChild(option);
                    console.log('Added model to dropdown:', model.name);
                });
            }
            
            if (modelList) {
                // Clear the list
                modelList.innerHTML = '';
                
                // Add models to list
                models.forEach(model => {
                    const modelItem = document.createElement('div');
                    modelItem.className = 'model-item';
                    modelItem.textContent = model.name;
                    modelList.appendChild(modelItem);
                    console.log('Added model to list:', model.name);
                });
            }
        })
        .catch(error => {
            console.error('Error fetching models:', error);
            
            if (modelSelect) {
                modelSelect.innerHTML = '<option value="">Error loading models</option>';
            }
            
            if (modelList) {
                modelList.innerHTML = '<div>Error loading models: ' + error.message + '</div>';
            }
        });
});

================
File: app/tasks/__init__.py
================
"""
Background Task System for Metis_RAG

This package provides a comprehensive background task system for executing
resource-intensive operations asynchronously, improving responsiveness and scalability.

Key components:
- TaskManager: Central manager for background tasks
- ResourceMonitor: Monitors system resources and provides adaptive throttling
- Scheduler: Handles task scheduling, prioritization, and dependencies
- Task: Model for representing background tasks
"""

from app.tasks.task_manager import TaskManager
from app.tasks.resource_monitor import ResourceMonitor
from app.tasks.scheduler import Scheduler
from app.tasks.task_models import Task, TaskStatus, TaskPriority, TaskDependency

__all__ = [
    "TaskManager",
    "ResourceMonitor",
    "Scheduler",
    "Task",
    "TaskStatus",
    "TaskPriority",
    "TaskDependency"
]

================
File: app/tasks/example_tasks.py
================
"""
Example task handlers for the Background Task System
"""
import time
import asyncio
import logging
import random
from typing import Dict, Any

from app.tasks.task_models import Task, TaskStatus
from app.tasks.task_manager import TaskManager

# Initialize logger
logger = logging.getLogger("app.tasks.example_tasks")

async def document_processing_handler(task: Task) -> Dict[str, Any]:
    """
    Example handler for document processing tasks
    
    Args:
        task: Task to execute
        
    Returns:
        Task result
    """
    logger.info(f"Processing document task {task.id}")
    
    # Get document ID from task parameters
    document_id = task.params.get("document_id")
    if not document_id:
        raise ValueError("Missing document_id parameter")
    
    # Simulate document processing
    total_steps = 5
    for step in range(1, total_steps + 1):
        # Update progress
        progress = (step / total_steps) * 100
        task.update_progress(progress)
        
        # Simulate processing step
        logger.info(f"Document {document_id} processing step {step}/{total_steps}")
        await asyncio.sleep(1)  # Simulate work
    
    # Return result
    return {
        "document_id": document_id,
        "status": "processed",
        "chunks": random.randint(5, 20),
        "processing_time_ms": random.randint(1000, 5000)
    }

async def vector_store_update_handler(task: Task) -> Dict[str, Any]:
    """
    Example handler for vector store update tasks
    
    Args:
        task: Task to execute
        
    Returns:
        Task result
    """
    logger.info(f"Updating vector store task {task.id}")
    
    # Get document IDs from task parameters
    document_ids = task.params.get("document_ids", [])
    if not document_ids:
        raise ValueError("Missing document_ids parameter")
    
    # Simulate vector store update
    total_documents = len(document_ids)
    for i, doc_id in enumerate(document_ids):
        # Update progress
        progress = ((i + 1) / total_documents) * 100
        task.update_progress(progress)
        
        # Simulate update step
        logger.info(f"Updating vector store for document {doc_id} ({i+1}/{total_documents})")
        await asyncio.sleep(0.5)  # Simulate work
    
    # Return result
    return {
        "document_count": total_documents,
        "vectors_added": random.randint(total_documents * 5, total_documents * 20),
        "update_time_ms": random.randint(500, 2000) * total_documents
    }

async def report_generation_handler(task: Task) -> Dict[str, Any]:
    """
    Example handler for report generation tasks
    
    Args:
        task: Task to execute
        
    Returns:
        Task result
    """
    logger.info(f"Generating report task {task.id}")
    
    # Get report parameters
    report_type = task.params.get("report_type", "summary")
    document_ids = task.params.get("document_ids", [])
    
    if not document_ids:
        raise ValueError("Missing document_ids parameter")
    
    # Simulate report generation
    logger.info(f"Generating {report_type} report for {len(document_ids)} documents")
    
    # Simulate work with random duration based on report type and document count
    duration = 0
    if report_type == "summary":
        duration = 2 + (0.2 * len(document_ids))
    elif report_type == "detailed":
        duration = 5 + (0.5 * len(document_ids))
    elif report_type == "comprehensive":
        duration = 10 + (1.0 * len(document_ids))
    
    # Update progress periodically
    total_steps = int(duration)
    for step in range(1, total_steps + 1):
        # Update progress
        progress = (step / total_steps) * 100
        task.update_progress(progress)
        
        # Simulate processing step
        logger.info(f"Report generation step {step}/{total_steps}")
        await asyncio.sleep(1)  # Simulate work
    
    # Return result
    return {
        "report_type": report_type,
        "document_count": len(document_ids),
        "page_count": random.randint(1, 5 + len(document_ids)),
        "generation_time_ms": int(duration * 1000)
    }

async def system_maintenance_handler(task: Task) -> Dict[str, Any]:
    """
    Example handler for system maintenance tasks
    
    Args:
        task: Task to execute
        
    Returns:
        Task result
    """
    logger.info(f"Performing system maintenance task {task.id}")
    
    # Get maintenance parameters
    maintenance_type = task.params.get("maintenance_type", "cleanup")
    
    # Simulate maintenance
    logger.info(f"Performing {maintenance_type} maintenance")
    
    # Different maintenance types
    if maintenance_type == "cleanup":
        # Simulate cleanup
        logger.info("Cleaning up old data")
        await asyncio.sleep(2)
        result = {
            "files_removed": random.randint(10, 100),
            "space_freed_mb": random.randint(50, 500)
        }
    elif maintenance_type == "optimization":
        # Simulate optimization
        logger.info("Optimizing database")
        await asyncio.sleep(5)
        result = {
            "tables_optimized": random.randint(5, 20),
            "indexes_rebuilt": random.randint(10, 30)
        }
    elif maintenance_type == "backup":
        # Simulate backup
        logger.info("Creating backup")
        await asyncio.sleep(10)
        result = {
            "backup_size_mb": random.randint(100, 1000),
            "backup_location": f"/backups/metis_rag_{int(time.time())}.bak"
        }
    else:
        raise ValueError(f"Unknown maintenance type: {maintenance_type}")
    
    # Return result with common fields
    result.update({
        "maintenance_type": maintenance_type,
        "execution_time_ms": random.randint(1000, 10000)
    })
    
    return result

def register_example_handlers(task_manager: TaskManager) -> None:
    """
    Register example task handlers with the task manager
    
    Args:
        task_manager: Task manager instance
    """
    task_manager.register_task_handler("document_processing", document_processing_handler)
    task_manager.register_task_handler("vector_store_update", vector_store_update_handler)
    task_manager.register_task_handler("report_generation", report_generation_handler)
    task_manager.register_task_handler("system_maintenance", system_maintenance_handler)
    
    logger.info("Registered example task handlers")

================
File: app/tasks/README.md
================
# Background Task System

The Background Task System enables Metis_RAG to perform resource-intensive operations asynchronously, improving responsiveness and scalability. This system manages task scheduling, prioritization, dependencies, and resource allocation.

## Key Components

### TaskManager

The TaskManager is the central component of the Background Task System. It manages the lifecycle of background tasks, including submission, execution, cancellation, and status tracking. It also provides concurrency control with configurable limits.

```python
from app.tasks.task_manager import TaskManager
from app.tasks.task_models import TaskPriority

# Create a task manager
task_manager = TaskManager(max_concurrent_tasks=10)

# Submit a task
task_id = await task_manager.submit(
    name="Process Document",
    task_type="document_processing",
    params={"document_id": "123"},
    priority=TaskPriority.HIGH
)

# Get task status
task = task_manager.get_task(task_id)
print(f"Task status: {task.status}")

# Cancel a task
cancelled = await task_manager.cancel(task_id)
```

### ResourceMonitor

The ResourceMonitor tracks system resources (CPU, memory, disk, I/O) and provides adaptive throttling based on system load. It also generates alerts when resource usage exceeds configured thresholds.

```python
from app.tasks.resource_monitor import ResourceMonitor, ResourceThreshold

# Create a resource monitor with custom thresholds
resource_monitor = ResourceMonitor(
    thresholds=ResourceThreshold(
        cpu_percent=80.0,
        memory_percent=80.0,
        disk_percent=90.0,
        io_wait_percent=30.0
    )
)

# Get system load
load = resource_monitor.get_system_load()
print(f"System load: {load:.2f}")

# Get recommended concurrency
concurrency = resource_monitor.get_recommended_concurrency(max_concurrency=10)
print(f"Recommended concurrency: {concurrency}")
```

### Scheduler

The Scheduler handles task scheduling, prioritization, and dependencies. It ensures that tasks are executed in the correct order and at the appropriate time.

```python
from app.tasks.scheduler import Scheduler
from app.tasks.task_models import Task, TaskDependency

# Create a scheduler
scheduler = Scheduler(resource_monitor=resource_monitor)

# Create a task with dependencies
task = Task(
    name="Generate Report",
    task_type="report_generation",
    dependencies=[
        TaskDependency(task_id="123"),
        TaskDependency(task_id="456")
    ]
)

# Schedule the task
await scheduler.schedule_task(task)
```

## Task Models

The Background Task System uses the following models to represent tasks and their properties:

- **Task**: Represents a background task with properties like name, type, parameters, priority, dependencies, etc.
- **TaskStatus**: Enum representing the status of a task (PENDING, SCHEDULED, RUNNING, COMPLETED, FAILED, CANCELLED, WAITING)
- **TaskPriority**: Enum representing the priority of a task (LOW, NORMAL, HIGH, CRITICAL)
- **TaskDependency**: Represents a dependency between tasks

## API Endpoints

The Background Task System provides the following API endpoints:

- `POST /api/v1/tasks`: Create a new background task
- `GET /api/v1/tasks`: List background tasks with optional filtering
- `GET /api/v1/tasks/{task_id}`: Get a task by ID
- `POST /api/v1/tasks/{task_id}/cancel`: Cancel a task
- `GET /api/v1/tasks/stats`: Get task statistics

## Task Types

The Background Task System supports the following task types:

- `document_processing`: Process a document (chunking, embedding, etc.)
- `vector_store_update`: Update the vector store with new documents
- `report_generation`: Generate a report based on document analysis
- `data_export`: Export data to a file or external system
- `system_maintenance`: Perform system maintenance tasks (cleanup, optimization, etc.)

## Task Dependencies

Tasks can depend on other tasks, forming a directed acyclic graph (DAG) of dependencies. A task will only be executed when all its dependencies are satisfied.

```python
# Create a task with dependencies
task = Task(
    name="Generate Report",
    task_type="report_generation",
    dependencies=[
        TaskDependency(task_id="123", required_status=TaskStatus.COMPLETED),
        TaskDependency(task_id="456", required_status=TaskStatus.COMPLETED)
    ]
)
```

## Adaptive Scheduling

The Background Task System uses adaptive scheduling to optimize resource usage. It adjusts the number of concurrent tasks based on system load, ensuring that the system remains responsive even under heavy load.

## Resource Monitoring

The ResourceMonitor tracks the following system resources:

- CPU usage
- Memory usage
- Disk usage
- I/O wait

When resource usage exceeds configured thresholds, the ResourceMonitor generates alerts and may throttle task execution to prevent system overload.

## Performance Considerations

- The Background Task System is designed to handle a large number of tasks efficiently.
- Tasks are prioritized based on their priority level and wait time.
- The system automatically adjusts concurrency based on system load.
- Long-running tasks should be broken down into smaller, more manageable tasks when possible.
- Tasks with dependencies should be designed carefully to avoid deadlocks.

## Error Handling

The Background Task System provides robust error handling:

- Tasks can be configured with a maximum number of retries.
- Failed tasks are automatically retried with exponential backoff.
- Task errors are logged and can be viewed through the API.
- System-level errors (e.g., resource constraints) are handled gracefully.

## Monitoring and Dashboards

The Background Task System provides comprehensive monitoring and dashboards:

- Task statistics (pending, running, completed, failed, etc.)
- System resource usage (CPU, memory, disk, I/O)
- Resource alerts
- Task execution history

## Integration with Other Components

The Background Task System integrates with other components of Metis_RAG:

- Document processing
- Vector store updates
- Response quality evaluation
- System maintenance

================
File: app/tasks/resource_monitor.py
================
"""
Resource Monitor - Monitors system resources and provides adaptive throttling
"""
import os
import time
import asyncio
import logging
import platform
import psutil
from typing import Dict, Any, List, Optional, Callable, Tuple

class ResourceThreshold:
    """Resource threshold configuration"""
    def __init__(
        self,
        cpu_percent: float = 80.0,
        memory_percent: float = 80.0,
        disk_percent: float = 90.0,
        io_wait_percent: float = 30.0
    ):
        self.cpu_percent = cpu_percent
        self.memory_percent = memory_percent
        self.disk_percent = disk_percent
        self.io_wait_percent = io_wait_percent

class ResourceAlert:
    """Resource alert model"""
    def __init__(
        self,
        resource_type: str,
        current_value: float,
        threshold: float,
        message: str,
        timestamp: float = None
    ):
        self.resource_type = resource_type
        self.current_value = current_value
        self.threshold = threshold
        self.message = message
        self.timestamp = timestamp or time.time()

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            "resource_type": self.resource_type,
            "current_value": self.current_value,
            "threshold": self.threshold,
            "message": self.message,
            "timestamp": self.timestamp
        }

class ResourceMonitor:
    """
    Monitors system resources and provides adaptive throttling
    """
    def __init__(
        self,
        thresholds: ResourceThreshold = None,
        check_interval_seconds: float = 5.0,
        history_size: int = 60,  # Keep 5 minutes of history with 5-second intervals
        alert_callbacks: List[Callable[[ResourceAlert], None]] = None
    ):
        self.thresholds = thresholds or ResourceThreshold()
        self.check_interval_seconds = check_interval_seconds
        self.history_size = history_size
        self.alert_callbacks = alert_callbacks or []
        
        # Resource history
        self.cpu_history: List[float] = []
        self.memory_history: List[float] = []
        self.disk_history: List[float] = []
        self.io_history: List[float] = []
        
        # Alert history
        self.alerts: List[ResourceAlert] = []
        
        # Monitoring state
        self.running = False
        self.monitor_task = None
        
        # Logger
        self.logger = logging.getLogger("app.tasks.resource_monitor")
    
    async def start(self) -> None:
        """
        Start resource monitoring
        """
        if self.running:
            return
        
        self.running = True
        self.monitor_task = asyncio.create_task(self._monitor_resources())
        self.logger.info("Resource monitor started")
    
    async def stop(self) -> None:
        """
        Stop resource monitoring
        """
        if not self.running:
            return
        
        self.running = False
        if self.monitor_task:
            self.monitor_task.cancel()
            try:
                await self.monitor_task
            except asyncio.CancelledError:
                pass
            self.monitor_task = None
        
        self.logger.info("Resource monitor stopped")
    
    async def _monitor_resources(self) -> None:
        """
        Monitor system resources periodically
        """
        while self.running:
            try:
                # Get current resource usage
                usage = self.get_resource_usage()
                
                # Update history
                self._update_history(usage)
                
                # Check thresholds and generate alerts
                self._check_thresholds(usage)
                
                # Wait for next check
                await asyncio.sleep(self.check_interval_seconds)
            except Exception as e:
                self.logger.error(f"Error monitoring resources: {str(e)}")
                await asyncio.sleep(self.check_interval_seconds)
    
    def get_resource_usage(self) -> Dict[str, float]:
        """
        Get current resource usage
        
        Returns:
            Dictionary with resource usage metrics
        """
        # Get CPU usage
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # Get memory usage
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        # Get disk usage for the main disk
        disk = psutil.disk_usage('/')
        disk_percent = disk.percent
        
        # Get I/O wait (platform-specific)
        io_wait_percent = 0.0
        if platform.system() != 'Windows':  # Not available on Windows
            try:
                # Get CPU times including I/O wait
                cpu_times = psutil.cpu_times_percent(interval=0.1)
                io_wait_percent = getattr(cpu_times, 'iowait', 0.0)
            except Exception:
                io_wait_percent = 0.0
        
        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "disk_percent": disk_percent,
            "io_wait_percent": io_wait_percent
        }
    
    def _update_history(self, usage: Dict[str, float]) -> None:
        """
        Update resource usage history
        
        Args:
            usage: Current resource usage
        """
        # Update CPU history
        self.cpu_history.append(usage["cpu_percent"])
        if len(self.cpu_history) > self.history_size:
            self.cpu_history.pop(0)
        
        # Update memory history
        self.memory_history.append(usage["memory_percent"])
        if len(self.memory_history) > self.history_size:
            self.memory_history.pop(0)
        
        # Update disk history
        self.disk_history.append(usage["disk_percent"])
        if len(self.disk_history) > self.history_size:
            self.disk_history.pop(0)
        
        # Update I/O history
        self.io_history.append(usage["io_wait_percent"])
        if len(self.io_history) > self.history_size:
            self.io_history.pop(0)
    
    def _check_thresholds(self, usage: Dict[str, float]) -> None:
        """
        Check resource thresholds and generate alerts
        
        Args:
            usage: Current resource usage
        """
        # Check CPU threshold
        if usage["cpu_percent"] > self.thresholds.cpu_percent:
            alert = ResourceAlert(
                resource_type="cpu",
                current_value=usage["cpu_percent"],
                threshold=self.thresholds.cpu_percent,
                message=f"CPU usage is high: {usage['cpu_percent']:.1f}% (threshold: {self.thresholds.cpu_percent:.1f}%)"
            )
            self._add_alert(alert)
        
        # Check memory threshold
        if usage["memory_percent"] > self.thresholds.memory_percent:
            alert = ResourceAlert(
                resource_type="memory",
                current_value=usage["memory_percent"],
                threshold=self.thresholds.memory_percent,
                message=f"Memory usage is high: {usage['memory_percent']:.1f}% (threshold: {self.thresholds.memory_percent:.1f}%)"
            )
            self._add_alert(alert)
        
        # Check disk threshold
        if usage["disk_percent"] > self.thresholds.disk_percent:
            alert = ResourceAlert(
                resource_type="disk",
                current_value=usage["disk_percent"],
                threshold=self.thresholds.disk_percent,
                message=f"Disk usage is high: {usage['disk_percent']:.1f}% (threshold: {self.thresholds.disk_percent:.1f}%)"
            )
            self._add_alert(alert)
        
        # Check I/O wait threshold
        if usage["io_wait_percent"] > self.thresholds.io_wait_percent:
            alert = ResourceAlert(
                resource_type="io_wait",
                current_value=usage["io_wait_percent"],
                threshold=self.thresholds.io_wait_percent,
                message=f"I/O wait is high: {usage['io_wait_percent']:.1f}% (threshold: {self.thresholds.io_wait_percent:.1f}%)"
            )
            self._add_alert(alert)
    
    def _add_alert(self, alert: ResourceAlert) -> None:
        """
        Add an alert and notify callbacks
        
        Args:
            alert: Resource alert
        """
        self.alerts.append(alert)
        if len(self.alerts) > self.history_size:
            self.alerts.pop(0)
        
        # Log alert
        self.logger.warning(alert.message)
        
        # Notify callbacks
        for callback in self.alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                self.logger.error(f"Error in alert callback: {str(e)}")
    
    def get_resource_history(self) -> Dict[str, List[float]]:
        """
        Get resource usage history
        
        Returns:
            Dictionary with resource history
        """
        return {
            "cpu_history": self.cpu_history.copy(),
            "memory_history": self.memory_history.copy(),
            "disk_history": self.disk_history.copy(),
            "io_history": self.io_history.copy()
        }
    
    def get_alerts(self, limit: int = None) -> List[Dict[str, Any]]:
        """
        Get recent alerts
        
        Args:
            limit: Maximum number of alerts to return
            
        Returns:
            List of alerts
        """
        alerts = self.alerts.copy()
        if limit:
            alerts = alerts[-limit:]
        return [alert.to_dict() for alert in alerts]
    
    def get_system_load(self) -> float:
        """
        Get overall system load factor (0.0 to 1.0)
        
        Returns:
            System load factor
        """
        if not self.cpu_history or not self.memory_history:
            return 0.0
        
        # Calculate average CPU and memory usage over recent history
        recent_cpu = sum(self.cpu_history[-5:]) / min(5, len(self.cpu_history))
        recent_memory = sum(self.memory_history[-5:]) / min(5, len(self.memory_history))
        
        # Calculate load factor (weighted average of CPU and memory)
        load_factor = (recent_cpu * 0.7 + recent_memory * 0.3) / 100.0
        return min(1.0, max(0.0, load_factor))
    
    def get_recommended_concurrency(self, max_concurrency: int) -> int:
        """
        Get recommended concurrency based on system load
        
        Args:
            max_concurrency: Maximum concurrency
            
        Returns:
            Recommended concurrency
        """
        load_factor = self.get_system_load()
        
        # Calculate recommended concurrency
        if load_factor < 0.5:
            # Low load, use full concurrency
            return max_concurrency
        elif load_factor < 0.7:
            # Medium load, reduce concurrency by 25%
            return max(1, int(max_concurrency * 0.75))
        elif load_factor < 0.9:
            # High load, reduce concurrency by 50%
            return max(1, int(max_concurrency * 0.5))
        else:
            # Very high load, reduce concurrency by 75%
            return max(1, int(max_concurrency * 0.25))
    
    def should_throttle(self) -> Tuple[bool, str]:
        """
        Check if task execution should be throttled
        
        Returns:
            Tuple of (should_throttle, reason)
        """
        load_factor = self.get_system_load()
        
        if load_factor > 0.95:
            return True, "System load is very high"
        
        # Check for critical resource alerts
        for alert in self.alerts[-5:]:  # Check recent alerts
            if alert.current_value > alert.threshold * 1.2:  # 20% over threshold
                return True, f"Critical resource alert: {alert.message}"
        
        return False, ""

================
File: app/tasks/scheduler.py
================
"""
Scheduler - Handles task scheduling, prioritization, and dependencies
"""
import time
import asyncio
import logging
import heapq
from datetime import datetime, timedelta
from typing import Dict, Any, List, Set, Optional, Tuple, Callable, Awaitable

from app.tasks.task_models import Task, TaskStatus, TaskPriority, TaskDependency
from app.tasks.resource_monitor import ResourceMonitor

class ScheduleEntry:
    """
    Entry in the scheduler queue
    """
    def __init__(self, task: Task, scheduled_time: float, score: float):
        self.task = task
        self.scheduled_time = scheduled_time
        self.score = score
        
    def __lt__(self, other):
        # First compare by score (higher score = higher priority)
        if self.score != other.score:
            return self.score > other.score
        # Then compare by scheduled time (earlier time = higher priority)
        return self.scheduled_time < other.scheduled_time

class Scheduler:
    """
    Handles task scheduling, prioritization, and dependencies
    """
    def __init__(
        self,
        resource_monitor: ResourceMonitor,
        check_interval_seconds: float = 1.0,
        max_lookahead_seconds: float = 60.0
    ):
        self.resource_monitor = resource_monitor
        self.check_interval_seconds = check_interval_seconds
        self.max_lookahead_seconds = max_lookahead_seconds
        
        # Task queues
        self.pending_tasks: Dict[str, Task] = {}
        self.scheduled_queue: List[ScheduleEntry] = []  # Priority queue
        self.running_tasks: Dict[str, Task] = {}
        self.completed_tasks: Dict[str, Task] = {}
        
        # Scheduling state
        self.running = False
        self.scheduler_task = None
        
        # Logger
        self.logger = logging.getLogger("app.tasks.scheduler")
    
    async def start(self) -> None:
        """
        Start the scheduler
        """
        if self.running:
            return
        
        self.running = True
        self.scheduler_task = asyncio.create_task(self._scheduler_loop())
        self.logger.info("Scheduler started")
    
    async def stop(self) -> None:
        """
        Stop the scheduler
        """
        if not self.running:
            return
        
        self.running = False
        if self.scheduler_task:
            self.scheduler_task.cancel()
            try:
                await self.scheduler_task
            except asyncio.CancelledError:
                pass
            self.scheduler_task = None
        
        self.logger.info("Scheduler stopped")
    
    async def schedule_task(self, task: Task) -> str:
        """
        Schedule a task for execution
        
        Args:
            task: Task to schedule
            
        Returns:
            Task ID
        """
        # Add task to pending queue
        self.pending_tasks[task.id] = task
        self.logger.info(f"Task {task.id} ({task.name}) added to pending queue")
        
        # If task has a specific schedule time, update its status
        if task.schedule_time:
            task.update_status(TaskStatus.SCHEDULED)
            self.logger.info(f"Task {task.id} scheduled for {task.schedule_time.isoformat()}")
        
        return task.id
    
    async def cancel_task(self, task_id: str) -> bool:
        """
        Cancel a task
        
        Args:
            task_id: Task ID
            
        Returns:
            True if task was cancelled, False otherwise
        """
        # Check pending tasks
        if task_id in self.pending_tasks:
            task = self.pending_tasks.pop(task_id)
            task.update_status(TaskStatus.CANCELLED)
            self.completed_tasks[task_id] = task
            self.logger.info(f"Cancelled pending task {task_id}")
            return True
        
        # Check scheduled queue
        for i, entry in enumerate(self.scheduled_queue):
            if entry.task.id == task_id:
                # Remove from scheduled queue
                self.scheduled_queue.pop(i)
                heapq.heapify(self.scheduled_queue)
                
                # Update task status
                entry.task.update_status(TaskStatus.CANCELLED)
                self.completed_tasks[task_id] = entry.task
                self.logger.info(f"Cancelled scheduled task {task_id}")
                return True
        
        # Cannot cancel running tasks directly
        self.logger.warning(f"Cannot cancel task {task_id}: not found or already running")
        return False
    
    def get_task(self, task_id: str) -> Optional[Task]:
        """
        Get a task by ID
        
        Args:
            task_id: Task ID
            
        Returns:
            Task if found, None otherwise
        """
        # Check all task collections
        if task_id in self.pending_tasks:
            return self.pending_tasks[task_id]
        if task_id in self.running_tasks:
            return self.running_tasks[task_id]
        if task_id in self.completed_tasks:
            return self.completed_tasks[task_id]
        
        # Check scheduled queue
        for entry in self.scheduled_queue:
            if entry.task.id == task_id:
                return entry.task
        
        return None
    
    def get_tasks_by_status(self, status: Optional[TaskStatus] = None) -> List[Task]:
        """
        Get tasks by status
        
        Args:
            status: Task status filter (optional)
            
        Returns:
            List of tasks
        """
        tasks = []
        
        # Collect from all collections
        all_tasks = list(self.pending_tasks.values())
        all_tasks.extend(self.running_tasks.values())
        all_tasks.extend(self.completed_tasks.values())
        all_tasks.extend(entry.task for entry in self.scheduled_queue)
        
        # Filter by status if specified
        if status:
            return [task for task in all_tasks if task.status == status]
        
        return all_tasks
    
    async def _scheduler_loop(self) -> None:
        """
        Main scheduler loop
        """
        while self.running:
            try:
                # Process pending tasks
                await self._process_pending_tasks()
                
                # Check for tasks ready to run
                ready_tasks = self._get_ready_tasks()
                
                # Update task statuses
                for task in ready_tasks:
                    task.update_status(TaskStatus.RUNNING)
                    self.running_tasks[task.id] = task
                    self.logger.info(f"Task {task.id} ({task.name}) is now running")
                
                # Wait for next check
                await asyncio.sleep(self.check_interval_seconds)
            except Exception as e:
                self.logger.error(f"Error in scheduler loop: {str(e)}")
                await asyncio.sleep(self.check_interval_seconds)
    
    async def _process_pending_tasks(self) -> None:
        """
        Process pending tasks and update the scheduled queue
        """
        # Get current time
        now = time.time()
        
        # Process each pending task
        pending_task_ids = list(self.pending_tasks.keys())
        for task_id in pending_task_ids:
            task = self.pending_tasks[task_id]
            
            # Skip tasks that are waiting for dependencies
            if task.dependencies and not self._check_dependencies(task):
                task.update_status(TaskStatus.WAITING)
                continue
            
            # Calculate scheduled time
            scheduled_time = now
            if task.schedule_time:
                scheduled_time = task.schedule_time.timestamp()
            
            # Calculate priority score
            score = self._calculate_priority_score(task)
            
            # Add to scheduled queue
            entry = ScheduleEntry(task, scheduled_time, score)
            heapq.heappush(self.scheduled_queue, entry)
            
            # Remove from pending queue
            del self.pending_tasks[task_id]
            
            # Update task status
            task.update_status(TaskStatus.SCHEDULED)
            self.logger.info(f"Task {task_id} ({task.name}) moved to scheduled queue with score {score:.2f}")
    
    def _get_ready_tasks(self) -> List[Task]:
        """
        Get tasks that are ready to run
        
        Returns:
            List of ready tasks
        """
        # Get current time
        now = time.time()
        
        # Get recommended concurrency
        max_concurrency = 10  # Default
        recommended_concurrency = self.resource_monitor.get_recommended_concurrency(max_concurrency)
        available_slots = max(0, recommended_concurrency - len(self.running_tasks))
        
        # Check if we should throttle
        should_throttle, reason = self.resource_monitor.should_throttle()
        if should_throttle:
            self.logger.warning(f"Throttling task execution: {reason}")
            return []
        
        # Get tasks ready to run
        ready_tasks = []
        lookahead_time = now + self.max_lookahead_seconds
        
        while self.scheduled_queue and len(ready_tasks) < available_slots:
            # Peek at the highest priority task
            entry = self.scheduled_queue[0]
            
            # Check if it's time to run the task
            if entry.scheduled_time > lookahead_time:
                # Not yet time to run this task
                break
            
            # Remove from scheduled queue
            heapq.heappop(self.scheduled_queue)
            
            # Add to ready tasks
            ready_tasks.append(entry.task)
        
        return ready_tasks
    
    def _check_dependencies(self, task: Task) -> bool:
        """
        Check if all task dependencies are satisfied
        
        Args:
            task: Task to check
            
        Returns:
            True if all dependencies are satisfied, False otherwise
        """
        if not task.dependencies:
            return True
        
        for dependency in task.dependencies:
            # Get dependent task
            dependent_task = self.get_task(dependency.task_id)
            if not dependent_task:
                # Dependency not found
                self.logger.warning(f"Task {task.id} depends on missing task {dependency.task_id}")
                return False
            
            # Check status
            if dependent_task.status != dependency.required_status:
                # Dependency not satisfied
                return False
        
        # All dependencies satisfied
        return True
    
    def _calculate_priority_score(self, task: Task) -> float:
        """
        Calculate priority score for a task
        
        Args:
            task: Task to score
            
        Returns:
            Priority score (higher = higher priority)
        """
        # Base score from priority enum
        base_score = task.priority.value
        
        # Adjust for wait time
        wait_time = 0
        if task.created_at:
            wait_time = (datetime.now() - task.created_at).total_seconds()
        
        # Adjust for dependencies
        dependency_factor = 1.0
        if task.dependencies:
            # Tasks with dependencies get a slight priority boost
            dependency_factor = 1.1
        
        # Calculate final score
        # Formula: base_score * dependency_factor + (wait_time / 60)
        # This gives a small boost to tasks that have been waiting longer
        score = base_score * dependency_factor + (wait_time / 60.0)
        
        return score
    
    def task_completed(self, task_id: str, result: Any = None) -> None:
        """
        Mark a task as completed
        
        Args:
            task_id: Task ID
            result: Task result
        """
        if task_id in self.running_tasks:
            task = self.running_tasks.pop(task_id)
            task.set_result(result)
            self.completed_tasks[task_id] = task
            self.logger.info(f"Task {task_id} ({task.name}) completed")
    
    def task_failed(self, task_id: str, error: str) -> None:
        """
        Mark a task as failed
        
        Args:
            task_id: Task ID
            error: Error message
        """
        if task_id in self.running_tasks:
            task = self.running_tasks.pop(task_id)
            
            # Check if we should retry
            if task.retry_count < task.max_retries:
                # Increment retry count
                task.retry_count += 1
                
                # Calculate backoff delay
                backoff_seconds = 2 ** task.retry_count  # Exponential backoff
                
                # Schedule for retry
                task.schedule_time = datetime.now() + timedelta(seconds=backoff_seconds)
                task.status = TaskStatus.PENDING
                self.pending_tasks[task_id] = task
                
                self.logger.info(f"Task {task_id} ({task.name}) failed, retrying in {backoff_seconds}s (attempt {task.retry_count}/{task.max_retries})")
            else:
                # Mark as failed
                task.set_error(error)
                self.completed_tasks[task_id] = task
                self.logger.error(f"Task {task_id} ({task.name}) failed: {error}")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get scheduler statistics
        
        Returns:
            Dictionary with scheduler statistics
        """
        return {
            "pending_tasks": len(self.pending_tasks),
            "scheduled_tasks": len(self.scheduled_queue),
            "running_tasks": len(self.running_tasks),
            "completed_tasks": len(self.completed_tasks),
            "total_tasks": len(self.pending_tasks) + len(self.scheduled_queue) + len(self.running_tasks) + len(self.completed_tasks),
            "resource_load": self.resource_monitor.get_system_load()
        }

================
File: app/tasks/task_manager.py
================
"""
Task Manager - Central manager for background tasks
"""
import os
import time
import uuid
import asyncio
import logging
import traceback
from datetime import datetime, timedelta
from typing import Dict, Any, List, Set, Optional, Tuple, Callable, Awaitable, Union, Type

from app.tasks.task_models import Task, TaskStatus, TaskPriority, TaskDependency, TaskHandler
from app.tasks.resource_monitor import ResourceMonitor
from app.tasks.scheduler import Scheduler

class TaskManager:
    """
    Central manager for background tasks
    """
    def __init__(
        self,
        max_concurrent_tasks: int = 10,
        resource_check_interval: float = 5.0,
        scheduler_check_interval: float = 1.0,
        task_handlers: Dict[str, TaskHandler] = None
    ):
        # Initialize components
        self.resource_monitor = ResourceMonitor(check_interval_seconds=resource_check_interval)
        self.scheduler = Scheduler(
            resource_monitor=self.resource_monitor,
            check_interval_seconds=scheduler_check_interval
        )
        
        # Task handlers
        self.task_handlers: Dict[str, TaskHandler] = task_handlers or {}
        
        # Task execution
        self.max_concurrent_tasks = max_concurrent_tasks
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.running_tasks: Dict[str, asyncio.Task] = {}
        
        # Task registry
        self.task_registry: Dict[str, Task] = {}
        
        # Execution state
        self.running = False
        self.executor_task = None
        
        # Logger
        self.logger = logging.getLogger("app.tasks.task_manager")
    
    async def start(self) -> None:
        """
        Start the task manager
        """
        if self.running:
            return
        
        # Start components
        await self.resource_monitor.start()
        await self.scheduler.start()
        
        # Start executor
        self.running = True
        self.executor_task = asyncio.create_task(self._executor_loop())
        
        self.logger.info(f"Task manager started with max concurrency {self.max_concurrent_tasks}")
    
    async def stop(self) -> None:
        """
        Stop the task manager
        """
        if not self.running:
            return
        
        # Stop executor
        self.running = False
        if self.executor_task:
            self.executor_task.cancel()
            try:
                await self.executor_task
            except asyncio.CancelledError:
                pass
            self.executor_task = None
        
        # Cancel all running tasks
        for task_id, task in list(self.running_tasks.items()):
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
        self.running_tasks.clear()
        
        # Stop components
        await self.scheduler.stop()
        await self.resource_monitor.stop()
        
        self.logger.info("Task manager stopped")
    
    async def submit(
        self,
        name: str,
        task_type: str,
        params: Dict[str, Any] = None,
        priority: TaskPriority = TaskPriority.NORMAL,
        dependencies: List[Union[str, TaskDependency]] = None,
        schedule_time: Optional[datetime] = None,
        timeout_seconds: Optional[int] = None,
        max_retries: int = 0,
        metadata: Dict[str, Any] = None
    ) -> str:
        """
        Submit a task for execution
        
        Args:
            name: Task name
            task_type: Task type (must be registered with a handler)
            params: Task parameters
            priority: Task priority
            dependencies: List of task dependencies (task IDs or TaskDependency objects)
            schedule_time: Time to schedule the task
            timeout_seconds: Task timeout in seconds
            max_retries: Maximum number of retries
            metadata: Additional metadata
            
        Returns:
            Task ID
        """
        # Validate task type
        if task_type not in self.task_handlers:
            raise ValueError(f"Unknown task type: {task_type}")
        
        # Process dependencies
        processed_dependencies = []
        if dependencies:
            for dep in dependencies:
                if isinstance(dep, str):
                    # Convert task ID to TaskDependency
                    processed_dependencies.append(TaskDependency(task_id=dep))
                else:
                    processed_dependencies.append(dep)
        
        # Create task
        task = Task(
            name=name,
            task_type=task_type,
            params=params or {},
            priority=priority,
            dependencies=processed_dependencies,
            schedule_time=schedule_time,
            timeout_seconds=timeout_seconds,
            max_retries=max_retries,
            metadata=metadata or {}
        )
        
        # Register task
        self.task_registry[task.id] = task
        
        # Schedule task
        await self.scheduler.schedule_task(task)
        
        self.logger.info(f"Submitted task {task.id} ({name}) of type {task_type}")
        
        return task.id
    
    async def cancel(self, task_id: str) -> bool:
        """
        Cancel a task
        
        Args:
            task_id: Task ID
            
        Returns:
            True if task was cancelled, False otherwise
        """
        # Try to cancel in scheduler
        cancelled = await self.scheduler.cancel_task(task_id)
        if cancelled:
            return True
        
        # Try to cancel running task
        if task_id in self.running_tasks:
            self.running_tasks[task_id].cancel()
            self.logger.info(f"Cancelled running task {task_id}")
            return True
        
        return False
    
    def get_task(self, task_id: str) -> Optional[Task]:
        """
        Get a task by ID
        
        Args:
            task_id: Task ID
            
        Returns:
            Task if found, None otherwise
        """
        # Check task registry
        if task_id in self.task_registry:
            return self.task_registry[task_id]
        
        # Check scheduler
        return self.scheduler.get_task(task_id)
    
    def get_tasks(
        self,
        status: Optional[TaskStatus] = None,
        task_type: Optional[str] = None,
        limit: Optional[int] = None,
        offset: int = 0
    ) -> List[Task]:
        """
        Get tasks with optional filtering
        
        Args:
            status: Filter by status
            task_type: Filter by task type
            limit: Maximum number of tasks to return
            offset: Offset for pagination
            
        Returns:
            List of tasks
        """
        # Get all tasks
        tasks = list(self.task_registry.values())
        
        # Filter by status
        if status:
            tasks = [task for task in tasks if task.status == status]
        
        # Filter by task type
        if task_type:
            tasks = [task for task in tasks if task.task_type == task_type]
        
        # Sort by created_at (newest first)
        tasks.sort(key=lambda t: t.created_at, reverse=True)
        
        # Apply pagination
        if offset:
            tasks = tasks[offset:]
        if limit:
            tasks = tasks[:limit]
        
        return tasks
    
    def register_task_handler(self, task_type: str, handler: TaskHandler) -> None:
        """
        Register a task handler
        
        Args:
            task_type: Task type
            handler: Task handler function
        """
        self.task_handlers[task_type] = handler
        self.logger.info(f"Registered handler for task type: {task_type}")
    
    async def _executor_loop(self) -> None:
        """
        Main executor loop
        """
        while self.running:
            try:
                # Get tasks from scheduler
                ready_tasks = self.scheduler.get_tasks_by_status(TaskStatus.RUNNING)
                
                # Start execution for each task
                for task in ready_tasks:
                    # Skip tasks that are already being executed
                    if task.id in self.running_tasks:
                        continue
                    
                    # Start task execution
                    execution_task = asyncio.create_task(self._execute_task(task))
                    self.running_tasks[task.id] = execution_task
                    
                    # Set up completion callback
                    execution_task.add_done_callback(
                        lambda t, task_id=task.id: self._task_completed(task_id, t)
                    )
                
                # Wait for next check
                await asyncio.sleep(0.1)
            except Exception as e:
                self.logger.error(f"Error in executor loop: {str(e)}")
                await asyncio.sleep(1.0)
    
    async def _execute_task(self, task: Task) -> Any:
        """
        Execute a task
        
        Args:
            task: Task to execute
            
        Returns:
            Task result
        """
        start_time = time.time()
        self.logger.info(f"Executing task {task.id} ({task.name}) of type {task.task_type}")
        
        try:
            # Get task handler
            handler = self.task_handlers.get(task.task_type)
            if not handler:
                raise ValueError(f"No handler registered for task type: {task.task_type}")
            
            # Execute task with semaphore
            async with self.semaphore:
                # Update task status
                task.update_status(TaskStatus.RUNNING)
                
                # Set up timeout if specified
                if task.timeout_seconds:
                    result = await asyncio.wait_for(
                        handler(task),
                        timeout=task.timeout_seconds
                    )
                else:
                    result = await handler(task)
                
                # Update task with result
                elapsed_time = time.time() - start_time
                self.logger.info(f"Task {task.id} completed successfully in {elapsed_time:.2f}s")
                
                return result
        except asyncio.TimeoutError:
            elapsed_time = time.time() - start_time
            error_msg = f"Task {task.id} timed out after {elapsed_time:.2f}s"
            self.logger.error(error_msg)
            raise TimeoutError(error_msg)
        except Exception as e:
            elapsed_time = time.time() - start_time
            error_msg = f"Task {task.id} failed after {elapsed_time:.2f}s: {str(e)}"
            self.logger.error(error_msg)
            self.logger.error(traceback.format_exc())
            raise
    
    def _task_completed(self, task_id: str, task: asyncio.Task) -> None:
        """
        Handle task completion
        
        Args:
            task_id: Task ID
            task: Asyncio task
        """
        # Remove from running tasks
        if task_id in self.running_tasks:
            del self.running_tasks[task_id]
        
        try:
            # Get result or exception
            if task.cancelled():
                # Task was cancelled
                self.scheduler.task_failed(task_id, "Task was cancelled")
            elif task.exception():
                # Task raised an exception
                exception = task.exception()
                self.scheduler.task_failed(task_id, str(exception))
            else:
                # Task completed successfully
                result = task.result()
                self.scheduler.task_completed(task_id, result)
        except asyncio.CancelledError:
            # Task was cancelled
            self.scheduler.task_failed(task_id, "Task was cancelled")
        except Exception as e:
            # Error getting task result
            self.logger.error(f"Error handling task completion: {str(e)}")
            self.scheduler.task_failed(task_id, str(e))
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get task manager statistics
        
        Returns:
            Dictionary with task manager statistics
        """
        scheduler_stats = self.scheduler.get_stats()
        resource_stats = {
            "system_load": self.resource_monitor.get_system_load(),
            "recommended_concurrency": self.resource_monitor.get_recommended_concurrency(self.max_concurrent_tasks)
        }
        
        return {
            "scheduler": scheduler_stats,
            "resources": resource_stats,
            "task_types": list(self.task_handlers.keys()),
            "running_tasks": len(self.running_tasks),
            "registered_tasks": len(self.task_registry)
        }
    
    def get_resource_history(self) -> Dict[str, List[float]]:
        """
        Get resource usage history
        
        Returns:
            Dictionary with resource history
        """
        return self.resource_monitor.get_resource_history()
    
    def get_resource_alerts(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get recent resource alerts
        
        Args:
            limit: Maximum number of alerts to return
            
        Returns:
            List of alerts
        """
        return self.resource_monitor.get_alerts(limit=limit)

================
File: app/tasks/task_models.py
================
"""
Task Models - Data models for the Background Task System
"""
import uuid
import enum
from datetime import datetime
from typing import Dict, Any, List, Optional, Set, Union, Callable, Awaitable

class TaskStatus(enum.Enum):
    """Task status enum"""
    PENDING = "pending"
    SCHEDULED = "scheduled"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    WAITING = "waiting"  # Waiting for dependencies

class TaskPriority(enum.Enum):
    """Task priority enum"""
    LOW = 0
    NORMAL = 50
    HIGH = 100
    CRITICAL = 200

class TaskDependency:
    """
    Represents a dependency between tasks
    """
    def __init__(self, task_id: str, required_status: TaskStatus = TaskStatus.COMPLETED):
        self.task_id = task_id
        self.required_status = required_status

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            "task_id": self.task_id,
            "required_status": self.required_status.value
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TaskDependency":
        """Create from dictionary"""
        return cls(
            task_id=data["task_id"],
            required_status=TaskStatus(data["required_status"])
        )

class Task:
    """
    Model for background tasks
    """
    def __init__(
        self,
        name: str,
        task_type: str,
        params: Dict[str, Any] = None,
        priority: TaskPriority = TaskPriority.NORMAL,
        dependencies: List[TaskDependency] = None,
        schedule_time: Optional[datetime] = None,
        timeout_seconds: Optional[int] = None,
        max_retries: int = 0,
        task_id: Optional[str] = None,
        metadata: Dict[str, Any] = None
    ):
        self.id = task_id or str(uuid.uuid4())
        self.name = name
        self.task_type = task_type
        self.params = params or {}
        self.priority = priority
        self.dependencies = dependencies or []
        self.schedule_time = schedule_time
        self.timeout_seconds = timeout_seconds
        self.max_retries = max_retries
        self.metadata = metadata or {}
        
        # Runtime attributes
        self.status = TaskStatus.PENDING
        self.created_at = datetime.now()
        self.scheduled_at = None
        self.started_at = None
        self.completed_at = None
        self.retry_count = 0
        self.result = None
        self.error = None
        self.progress = 0.0
        self.resource_usage = {}
        self.execution_time_ms = None
        
    def update_status(self, status: TaskStatus) -> None:
        """
        Update task status and related timestamps
        
        Args:
            status: New status
        """
        self.status = status
        
        # Update timestamps based on status
        now = datetime.now()
        if status == TaskStatus.SCHEDULED and not self.scheduled_at:
            self.scheduled_at = now
        elif status == TaskStatus.RUNNING and not self.started_at:
            self.started_at = now
        elif status in (TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED):
            self.completed_at = now
            if self.started_at:
                self.execution_time_ms = (now - self.started_at).total_seconds() * 1000
    
    def update_progress(self, progress: float) -> None:
        """
        Update task progress
        
        Args:
            progress: Progress value (0.0 to 100.0)
        """
        self.progress = max(0.0, min(100.0, progress))
    
    def update_resource_usage(self, resource_usage: Dict[str, Any]) -> None:
        """
        Update resource usage metrics
        
        Args:
            resource_usage: Resource usage metrics
        """
        self.resource_usage.update(resource_usage)
    
    def set_result(self, result: Any) -> None:
        """
        Set task result
        
        Args:
            result: Task result
        """
        self.result = result
        self.update_status(TaskStatus.COMPLETED)
    
    def set_error(self, error: str) -> None:
        """
        Set task error
        
        Args:
            error: Error message
        """
        self.error = error
        self.update_status(TaskStatus.FAILED)
    
    def can_execute(self, completed_task_ids: Set[str]) -> bool:
        """
        Check if task can be executed based on dependencies
        
        Args:
            completed_task_ids: Set of completed task IDs
            
        Returns:
            True if all dependencies are satisfied, False otherwise
        """
        return all(dep.task_id in completed_task_ids for dep in self.dependencies)
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert task to dictionary
        
        Returns:
            Dictionary representation of the task
        """
        return {
            "id": self.id,
            "name": self.name,
            "task_type": self.task_type,
            "params": self.params,
            "priority": self.priority.value,
            "dependencies": [dep.to_dict() for dep in self.dependencies],
            "schedule_time": self.schedule_time.isoformat() if self.schedule_time else None,
            "timeout_seconds": self.timeout_seconds,
            "max_retries": self.max_retries,
            "metadata": self.metadata,
            "status": self.status.value,
            "created_at": self.created_at.isoformat(),
            "scheduled_at": self.scheduled_at.isoformat() if self.scheduled_at else None,
            "started_at": self.started_at.isoformat() if self.started_at else None,
            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
            "retry_count": self.retry_count,
            "result": self.result,
            "error": self.error,
            "progress": self.progress,
            "resource_usage": self.resource_usage,
            "execution_time_ms": self.execution_time_ms
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Task":
        """
        Create task from dictionary
        
        Args:
            data: Dictionary representation of the task
            
        Returns:
            Task instance
        """
        task = cls(
            name=data["name"],
            task_type=data["task_type"],
            params=data.get("params", {}),
            priority=TaskPriority(data.get("priority", TaskPriority.NORMAL.value)),
            dependencies=[TaskDependency.from_dict(dep) for dep in data.get("dependencies", [])],
            schedule_time=datetime.fromisoformat(data["schedule_time"]) if data.get("schedule_time") else None,
            timeout_seconds=data.get("timeout_seconds"),
            max_retries=data.get("max_retries", 0),
            task_id=data.get("id"),
            metadata=data.get("metadata", {})
        )
        
        # Set runtime attributes
        task.status = TaskStatus(data.get("status", TaskStatus.PENDING.value))
        task.created_at = datetime.fromisoformat(data["created_at"]) if data.get("created_at") else datetime.now()
        task.scheduled_at = datetime.fromisoformat(data["scheduled_at"]) if data.get("scheduled_at") else None
        task.started_at = datetime.fromisoformat(data["started_at"]) if data.get("started_at") else None
        task.completed_at = datetime.fromisoformat(data["completed_at"]) if data.get("completed_at") else None
        task.retry_count = data.get("retry_count", 0)
        task.result = data.get("result")
        task.error = data.get("error")
        task.progress = data.get("progress", 0.0)
        task.resource_usage = data.get("resource_usage", {})
        task.execution_time_ms = data.get("execution_time_ms")
        
        return task

# Type alias for task handler functions
TaskHandler = Callable[[Task], Awaitable[Any]]

================
File: app/tasks/task_repository.py
================
"""
Task Repository - Database operations for the Background Task System
"""
import uuid
import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple

from sqlalchemy import select, update, delete, desc, func, and_, or_
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session

from app.db.repositories.base import BaseRepository
from app.db.models import BackgroundTask
from app.tasks.task_models import Task, TaskStatus, TaskPriority, TaskDependency

class TaskRepository(BaseRepository):
    """
    Repository for background tasks
    """
    def __init__(self, session: Union[Session, AsyncSession]):
        super().__init__(session)
    
    async def create(self, task: Task) -> Task:
        """
        Create a new task in the database
        
        Args:
            task: Task to create
            
        Returns:
            Created task
        """
        # Convert task to database model
        db_task = BackgroundTask(
            id=task.id,
            name=task.name,
            task_type=task.task_type,
            params=task.params,
            priority=task.priority.value,
            dependencies=json.dumps([dep.to_dict() for dep in task.dependencies]),
            schedule_time=task.schedule_time,
            timeout_seconds=task.timeout_seconds,
            max_retries=task.max_retries,
            metadata=task.metadata,
            status=task.status.value,
            created_at=task.created_at,
            scheduled_at=task.scheduled_at,
            started_at=task.started_at,
            completed_at=task.completed_at,
            retry_count=task.retry_count,
            result=json.dumps(task.result) if task.result is not None else None,
            error=task.error,
            progress=task.progress,
            resource_usage=task.resource_usage,
            execution_time_ms=task.execution_time_ms
        )
        
        # Add to database
        self.session.add(db_task)
        await self.session.commit()
        
        return task
    
    async def update(self, task: Task) -> Task:
        """
        Update a task in the database
        
        Args:
            task: Task to update
            
        Returns:
            Updated task
        """
        # Update database model
        stmt = update(BackgroundTask).where(BackgroundTask.id == task.id).values(
            name=task.name,
            task_type=task.task_type,
            params=task.params,
            priority=task.priority.value,
            dependencies=json.dumps([dep.to_dict() for dep in task.dependencies]),
            schedule_time=task.schedule_time,
            timeout_seconds=task.timeout_seconds,
            max_retries=task.max_retries,
            metadata=task.metadata,
            status=task.status.value,
            scheduled_at=task.scheduled_at,
            started_at=task.started_at,
            completed_at=task.completed_at,
            retry_count=task.retry_count,
            result=json.dumps(task.result) if task.result is not None else None,
            error=task.error,
            progress=task.progress,
            resource_usage=task.resource_usage,
            execution_time_ms=task.execution_time_ms
        )
        
        # Execute update
        await self.session.execute(stmt)
        await self.session.commit()
        
        return task
    
    async def update_status(
        self,
        task_id: str,
        status: TaskStatus,
        result: Any = None,
        error: str = None,
        progress: float = None
    ) -> bool:
        """
        Update task status
        
        Args:
            task_id: Task ID
            status: New status
            result: Task result (for completed tasks)
            error: Error message (for failed tasks)
            progress: Task progress
            
        Returns:
            True if task was updated, False otherwise
        """
        # Build update values
        values = {"status": status.value}
        
        # Set timestamps based on status
        now = datetime.now()
        if status == TaskStatus.SCHEDULED:
            values["scheduled_at"] = now
        elif status == TaskStatus.RUNNING:
            values["started_at"] = now
        elif status in (TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED):
            values["completed_at"] = now
        
        # Set result or error
        if result is not None:
            values["result"] = json.dumps(result)
        if error is not None:
            values["error"] = error
        if progress is not None:
            values["progress"] = progress
        
        # Update database model
        stmt = update(BackgroundTask).where(BackgroundTask.id == task_id).values(**values)
        
        # Execute update
        result = await self.session.execute(stmt)
        await self.session.commit()
        
        return result.rowcount > 0
    
    async def get_by_id(self, task_id: str) -> Optional[Task]:
        """
        Get a task by ID
        
        Args:
            task_id: Task ID
            
        Returns:
            Task if found, None otherwise
        """
        # Query database
        stmt = select(BackgroundTask).where(BackgroundTask.id == task_id)
        result = await self.session.execute(stmt)
        db_task = result.scalars().first()
        
        # Convert to task model
        if db_task:
            return self._db_to_task(db_task)
        
        return None
    
    async def get_by_status(
        self,
        status: Union[TaskStatus, List[TaskStatus]],
        limit: Optional[int] = None,
        offset: int = 0
    ) -> List[Task]:
        """
        Get tasks by status
        
        Args:
            status: Task status or list of statuses
            limit: Maximum number of tasks to return
            offset: Offset for pagination
            
        Returns:
            List of tasks
        """
        # Convert status to list
        if isinstance(status, TaskStatus):
            status_values = [status.value]
        else:
            status_values = [s.value for s in status]
        
        # Query database
        stmt = select(BackgroundTask).where(BackgroundTask.status.in_(status_values))
        
        # Apply pagination
        if limit:
            stmt = stmt.limit(limit)
        if offset:
            stmt = stmt.offset(offset)
        
        # Order by created_at
        stmt = stmt.order_by(desc(BackgroundTask.created_at))
        
        # Execute query
        result = await self.session.execute(stmt)
        db_tasks = result.scalars().all()
        
        # Convert to task models
        return [self._db_to_task(db_task) for db_task in db_tasks]
    
    async def get_by_type(
        self,
        task_type: str,
        status: Optional[Union[TaskStatus, List[TaskStatus]]] = None,
        limit: Optional[int] = None,
        offset: int = 0
    ) -> List[Task]:
        """
        Get tasks by type
        
        Args:
            task_type: Task type
            status: Optional status filter
            limit: Maximum number of tasks to return
            offset: Offset for pagination
            
        Returns:
            List of tasks
        """
        # Build query
        stmt = select(BackgroundTask).where(BackgroundTask.task_type == task_type)
        
        # Add status filter if specified
        if status:
            if isinstance(status, TaskStatus):
                stmt = stmt.where(BackgroundTask.status == status.value)
            else:
                status_values = [s.value for s in status]
                stmt = stmt.where(BackgroundTask.status.in_(status_values))
        
        # Apply pagination
        if limit:
            stmt = stmt.limit(limit)
        if offset:
            stmt = stmt.offset(offset)
        
        # Order by created_at
        stmt = stmt.order_by(desc(BackgroundTask.created_at))
        
        # Execute query
        result = await self.session.execute(stmt)
        db_tasks = result.scalars().all()
        
        # Convert to task models
        return [self._db_to_task(db_task) for db_task in db_tasks]
    
    async def search(
        self,
        query: str,
        status: Optional[Union[TaskStatus, List[TaskStatus]]] = None,
        task_type: Optional[str] = None,
        limit: Optional[int] = None,
        offset: int = 0
    ) -> List[Task]:
        """
        Search tasks
        
        Args:
            query: Search query
            status: Optional status filter
            task_type: Optional task type filter
            limit: Maximum number of tasks to return
            offset: Offset for pagination
            
        Returns:
            List of tasks
        """
        # Build query
        conditions = []
        
        # Add search condition
        search_term = f"%{query}%"
        conditions.append(or_(
            BackgroundTask.name.ilike(search_term),
            BackgroundTask.task_type.ilike(search_term),
            BackgroundTask.id.ilike(search_term)
        ))
        
        # Add status filter if specified
        if status:
            if isinstance(status, TaskStatus):
                conditions.append(BackgroundTask.status == status.value)
            else:
                status_values = [s.value for s in status]
                conditions.append(BackgroundTask.status.in_(status_values))
        
        # Add task type filter if specified
        if task_type:
            conditions.append(BackgroundTask.task_type == task_type)
        
        # Build final query
        stmt = select(BackgroundTask).where(and_(*conditions))
        
        # Apply pagination
        if limit:
            stmt = stmt.limit(limit)
        if offset:
            stmt = stmt.offset(offset)
        
        # Order by created_at
        stmt = stmt.order_by(desc(BackgroundTask.created_at))
        
        # Execute query
        result = await self.session.execute(stmt)
        db_tasks = result.scalars().all()
        
        # Convert to task models
        return [self._db_to_task(db_task) for db_task in db_tasks]
    
    async def count_by_status(self) -> Dict[str, int]:
        """
        Count tasks by status
        
        Returns:
            Dictionary with counts by status
        """
        # Query database
        stmt = select(
            BackgroundTask.status,
            func.count(BackgroundTask.id)
        ).group_by(BackgroundTask.status)
        
        # Execute query
        result = await self.session.execute(stmt)
        counts = {status: count for status, count in result.all()}
        
        # Ensure all statuses are included
        for status in TaskStatus:
            if status.value not in counts:
                counts[status.value] = 0
        
        return counts
    
    async def delete(self, task_id: str) -> bool:
        """
        Delete a task
        
        Args:
            task_id: Task ID
            
        Returns:
            True if task was deleted, False otherwise
        """
        # Delete from database
        stmt = delete(BackgroundTask).where(BackgroundTask.id == task_id)
        result = await self.session.execute(stmt)
        await self.session.commit()
        
        return result.rowcount > 0
    
    async def clean_old_tasks(self, days: int = 30) -> int:
        """
        Clean up old completed/failed/cancelled tasks
        
        Args:
            days: Age in days
            
        Returns:
            Number of tasks deleted
        """
        # Calculate cutoff date
        cutoff_date = datetime.now() - datetime.timedelta(days=days)
        
        # Delete old tasks
        stmt = delete(BackgroundTask).where(
            and_(
                BackgroundTask.completed_at < cutoff_date,
                BackgroundTask.status.in_([
                    TaskStatus.COMPLETED.value,
                    TaskStatus.FAILED.value,
                    TaskStatus.CANCELLED.value
                ])
            )
        )
        
        # Execute delete
        result = await self.session.execute(stmt)
        await self.session.commit()
        
        return result.rowcount
    
    def _db_to_task(self, db_task: BackgroundTask) -> Task:
        """
        Convert database model to task model
        
        Args:
            db_task: Database task model
            
        Returns:
            Task model
        """
        # Parse dependencies
        dependencies = []
        if db_task.dependencies:
            try:
                deps_data = json.loads(db_task.dependencies)
                dependencies = [TaskDependency.from_dict(dep) for dep in deps_data]
            except Exception:
                # Invalid dependencies JSON
                pass
        
        # Parse result
        result = None
        if db_task.result:
            try:
                result = json.loads(db_task.result)
            except Exception:
                # Invalid result JSON
                result = db_task.result
        
        # Create task
        task = Task(
            name=db_task.name,
            task_type=db_task.task_type,
            params=db_task.params,
            priority=TaskPriority(db_task.priority),
            dependencies=dependencies,
            schedule_time=db_task.schedule_time,
            timeout_seconds=db_task.timeout_seconds,
            max_retries=db_task.max_retries,
            task_id=db_task.id,
            metadata=db_task.metadata
        )
        
        # Set runtime attributes
        task.status = TaskStatus(db_task.status)
        task.created_at = db_task.created_at
        task.scheduled_at = db_task.scheduled_at
        task.started_at = db_task.started_at
        task.completed_at = db_task.completed_at
        task.retry_count = db_task.retry_count
        task.result = result
        task.error = db_task.error
        task.progress = db_task.progress
        task.resource_usage = db_task.resource_usage or {}
        task.execution_time_ms = db_task.execution_time_ms
        
        return task

================
File: app/templates/admin.html
================
{% extends "base.html" %}

{% block title %}Admin - Metis RAG{% endblock %}

{% block head %}
<style>
    .admin-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }

    .admin-tabs {
        display: flex;
        border-bottom: 1px solid #ddd;
        margin-bottom: 20px;
    }

    .admin-tab {
        padding: 10px 15px;
        cursor: pointer;
        border: 1px solid transparent;
        border-bottom: none;
        margin-right: 5px;
    }

    .admin-tab.active {
        border-color: #ddd;
        border-radius: 5px 5px 0 0;
        background-color: white;
        margin-bottom: -1px;
        border-bottom: 1px solid white;
    }

    .admin-tab:hover:not(.active) {
        background-color: #f5f5f5;
    }

    .admin-content {
        background-color: white;
        border: 1px solid #ddd;
        border-top: none;
        padding: 20px;
        border-radius: 0 0 5px 5px;
    }

    .admin-panel {
        display: none;
    }

    .admin-panel.active {
        display: block;
    }

    .user-list {
        width: 100%;
        border-collapse: collapse;
    }

    .user-list th, .user-list td {
        padding: 10px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }

    .user-list th {
        background-color: #f5f5f5;
    }

    .user-list tr:hover {
        background-color: #f9f9f9;
    }

    .action-buttons {
        display: flex;
        gap: 5px;
    }

    .btn-edit, .btn-delete {
        padding: 5px 10px;
        border: none;
        border-radius: 3px;
        cursor: pointer;
    }

    .btn-edit {
        background-color: #007bff;
        color: white;
    }

    .btn-delete {
        background-color: #dc3545;
        color: white;
    }

    .search-bar {
        display: flex;
        margin-bottom: 20px;
    }

    .search-bar input {
        flex-grow: 1;
        padding: 8px;
        border: 1px solid #ddd;
        border-radius: 4px 0 0 4px;
    }

    .search-bar button {
        padding: 8px 15px;
        background-color: #007bff;
        color: white;
        border: none;
        border-radius: 0 4px 4px 0;
        cursor: pointer;
    }

    .pagination {
        display: flex;
        justify-content: center;
        margin-top: 20px;
    }

    .pagination button {
        padding: 5px 10px;
        margin: 0 5px;
        border: 1px solid #ddd;
        background-color: white;
        cursor: pointer;
    }

    .pagination button.active {
        background-color: #007bff;
        color: white;
        border-color: #007bff;
    }

    .pagination button:hover:not(.active) {
        background-color: #f5f5f5;
    }

    .modal {
        display: none;
        position: fixed;
        z-index: 1000;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        background-color: rgba(0, 0, 0, 0.5);
    }

    .modal-content {
        background-color: white;
        margin: 10% auto;
        padding: 20px;
        border-radius: 5px;
        width: 50%;
        max-width: 500px;
    }

    .modal-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 15px;
    }

    .modal-header h3 {
        margin: 0;
    }

    .close-modal {
        font-size: 24px;
        cursor: pointer;
    }

    .form-group {
        margin-bottom: 15px;
    }

    .form-group label {
        display: block;
        margin-bottom: 5px;
        font-weight: bold;
    }

    .form-group input, .form-group select {
        width: 100%;
        padding: 8px;
        border: 1px solid #ddd;
        border-radius: 4px;
    }

    .form-actions {
        display: flex;
        justify-content: flex-end;
        gap: 10px;
        margin-top: 20px;
    }

    .btn {
        padding: 8px 15px;
        border: none;
        border-radius: 4px;
        cursor: pointer;
    }

    .btn-primary {
        background-color: #007bff;
        color: white;
    }

    .btn-secondary {
        background-color: #6c757d;
        color: white;
    }

    .btn-success {
        background-color: #28a745;
        color: white;
    }

    .btn-danger {
        background-color: #dc3545;
        color: white;
    }

    .error-message {
        color: #dc3545;
        margin-top: 5px;
    }

    .success-message {
        color: #28a745;
        margin-top: 5px;
    }

    .add-user-btn {
        margin-bottom: 20px;
    }
</style>
{% endblock %}

{% block content %}
<div class="admin-container">
    <h1>Admin Dashboard</h1>
    
    <div class="admin-tabs">
        <div class="admin-tab active" data-tab="users">User Management</div>
        <div class="admin-tab" data-tab="settings">System Settings</div>
    </div>
    
    <div class="admin-content">
        <!-- User Management Panel -->
        <div class="admin-panel active" id="users-panel">
            <button class="btn btn-success add-user-btn" id="add-user-btn">Add New User</button>
            
            <div class="search-bar">
                <input type="text" id="user-search" placeholder="Search users...">
                <button id="search-btn">Search</button>
            </div>
            
            <table class="user-list">
                <thead>
                    <tr>
                        <th>Username</th>
                        <th>Email</th>
                        <th>Full Name</th>
                        <th>Status</th>
                        <th>Admin</th>
                        <th>Created</th>
                        <th>Last Login</th>
                        <th>Actions</th>
                    </tr>
                </thead>
                <tbody id="user-list-body">
                    <!-- User rows will be populated here -->
                </tbody>
            </table>
            
            <div class="pagination" id="user-pagination">
                <!-- Pagination buttons will be populated here -->
            </div>
        </div>
        
        <!-- System Settings Panel -->
        <div class="admin-panel" id="settings-panel">
            <h2>System Settings</h2>
            <p>System settings will be implemented in a future update.</p>
        </div>
    </div>
</div>

<!-- User Modal -->
<div class="modal" id="user-modal">
    <div class="modal-content">
        <div class="modal-header">
            <h3 id="modal-title">Add User</h3>
            <span class="close-modal">&times;</span>
        </div>
        <form id="user-form">
            <input type="hidden" id="user-id">
            <div class="form-group">
                <label for="username">Username</label>
                <input type="text" id="username" name="username" required>
            </div>
            <div class="form-group">
                <label for="email">Email</label>
                <input type="email" id="email" name="email" required>
            </div>
            <div class="form-group">
                <label for="full_name">Full Name</label>
                <input type="text" id="full_name" name="full_name">
            </div>
            <div class="form-group">
                <label for="password">Password</label>
                <input type="password" id="password" name="password">
                <div id="password-note" class="note">Leave blank to keep current password (when editing)</div>
            </div>
            <div class="form-group">
                <label for="is_active">Status</label>
                <select id="is_active" name="is_active">
                    <option value="true">Active</option>
                    <option value="false">Inactive</option>
                </select>
            </div>
            <div class="form-group">
                <label for="is_admin">Admin</label>
                <select id="is_admin" name="is_admin">
                    <option value="false">No</option>
                    <option value="true">Yes</option>
                </select>
            </div>
            <div id="error-message" class="error-message"></div>
            <div class="form-actions">
                <button type="button" class="btn btn-secondary" id="cancel-btn">Cancel</button>
                <button type="submit" class="btn btn-primary" id="save-btn">Save</button>
            </div>
        </form>
    </div>
</div>

<!-- Delete Confirmation Modal -->
<div class="modal" id="delete-modal">
    <div class="modal-content">
        <div class="modal-header">
            <h3>Confirm Delete</h3>
            <span class="close-modal">&times;</span>
        </div>
        <p>Are you sure you want to delete this user? This action cannot be undone.</p>
        <input type="hidden" id="delete-user-id">
        <div class="form-actions">
            <button type="button" class="btn btn-secondary" id="delete-cancel-btn">Cancel</button>
            <button type="button" class="btn btn-danger" id="confirm-delete-btn">Delete</button>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Variables
        let currentPage = 1;
        const pageSize = 10;
        let totalUsers = 0;
        let searchTerm = '';
        
        // DOM Elements
        const userListBody = document.getElementById('user-list-body');
        const userPagination = document.getElementById('user-pagination');
        const userModal = document.getElementById('user-modal');
        const deleteModal = document.getElementById('delete-modal');
        const userForm = document.getElementById('user-form');
        const modalTitle = document.getElementById('modal-title');
        const userIdInput = document.getElementById('user-id');
        const usernameInput = document.getElementById('username');
        const emailInput = document.getElementById('email');
        const fullNameInput = document.getElementById('full_name');
        const passwordInput = document.getElementById('password');
        const isActiveInput = document.getElementById('is_active');
        const isAdminInput = document.getElementById('is_admin');
        const passwordNote = document.getElementById('password-note');
        const errorMessage = document.getElementById('error-message');
        const deleteUserIdInput = document.getElementById('delete-user-id');
        const userSearch = document.getElementById('user-search');
        const searchBtn = document.getElementById('search-btn');
        
        // Tab Navigation
        const tabs = document.querySelectorAll('.admin-tab');
        const panels = document.querySelectorAll('.admin-panel');
        
        tabs.forEach(tab => {
            tab.addEventListener('click', () => {
                const tabId = tab.getAttribute('data-tab');
                
                // Update active tab
                tabs.forEach(t => t.classList.remove('active'));
                tab.classList.add('active');
                
                // Update active panel
                panels.forEach(p => p.classList.remove('active'));
                document.getElementById(`${tabId}-panel`).classList.add('active');
            });
        });
        
        // Load Users
        loadUsers();
        
        // Event Listeners
        document.getElementById('add-user-btn').addEventListener('click', showAddUserModal);
        document.querySelectorAll('.close-modal').forEach(btn => {
            btn.addEventListener('click', closeModals);
        });
        document.getElementById('cancel-btn').addEventListener('click', closeModals);
        document.getElementById('delete-cancel-btn').addEventListener('click', closeModals);
        document.getElementById('confirm-delete-btn').addEventListener('click', deleteUser);
        userForm.addEventListener('submit', saveUser);
        searchBtn.addEventListener('click', () => {
            searchTerm = userSearch.value;
            currentPage = 1;
            loadUsers();
        });
        userSearch.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                searchTerm = userSearch.value;
                currentPage = 1;
                loadUsers();
            }
        });
        
        // Functions
        async function loadUsers() {
            try {
                // Get auth token
                const token = localStorage.getItem('access_token');
                if (!token) {
                    window.location.href = '/login?redirect=/admin';
                    return;
                }
                
                // Build URL
                let url = `/api/admin/users?skip=${(currentPage - 1) * pageSize}&limit=${pageSize}`;
                if (searchTerm) {
                    url += `&search=${encodeURIComponent(searchTerm)}`;
                }
                
                // Fetch users
                const response = await fetch(url, {
                    headers: {
                        'Authorization': `Bearer ${token}`
                    }
                });
                
                if (response.status === 401) {
                    // Unauthorized, redirect to login
                    window.location.href = '/login?redirect=/admin';
                    return;
                }
                
                if (!response.ok) {
                    throw new Error('Failed to load users');
                }
                
                const users = await response.json();
                
                // Estimate total users for pagination
                totalUsers = users.length < pageSize ? (currentPage - 1) * pageSize + users.length : currentPage * pageSize + 1;
                
                // Render users
                renderUsers(users);
                renderPagination();
            } catch (error) {
                console.error('Error loading users:', error);
            }
        }
        
        function renderUsers(users) {
            userListBody.innerHTML = '';
            
            if (users.length === 0) {
                const row = document.createElement('tr');
                row.innerHTML = `<td colspan="8" style="text-align: center;">No users found</td>`;
                userListBody.appendChild(row);
                return;
            }
            
            users.forEach(user => {
                const row = document.createElement('tr');
                row.innerHTML = `
                    <td>${user.username}</td>
                    <td>${user.email}</td>
                    <td>${user.full_name || '-'}</td>
                    <td>${user.is_active ? 'Active' : 'Inactive'}</td>
                    <td>${user.is_admin ? 'Yes' : 'No'}</td>
                    <td>${new Date(user.created_at).toLocaleDateString()}</td>
                    <td>${user.last_login ? new Date(user.last_login).toLocaleDateString() : 'Never'}</td>
                    <td>
                        <div class="action-buttons">
                            <button class="btn-edit" data-id="${user.id}">Edit</button>
                            <button class="btn-delete" data-id="${user.id}">Delete</button>
                        </div>
                    </td>
                `;
                userListBody.appendChild(row);
            });
            
            // Add event listeners to edit and delete buttons
            document.querySelectorAll('.btn-edit').forEach(btn => {
                btn.addEventListener('click', () => showEditUserModal(btn.getAttribute('data-id')));
            });
            
            document.querySelectorAll('.btn-delete').forEach(btn => {
                btn.addEventListener('click', () => showDeleteModal(btn.getAttribute('data-id')));
            });
        }
        
        function renderPagination() {
            userPagination.innerHTML = '';
            
            const totalPages = Math.ceil(totalUsers / pageSize);
            
            if (totalPages <= 1) {
                return;
            }
            
            // Previous button
            const prevBtn = document.createElement('button');
            prevBtn.textContent = 'Previous';
            prevBtn.disabled = currentPage === 1;
            prevBtn.addEventListener('click', () => {
                if (currentPage > 1) {
                    currentPage--;
                    loadUsers();
                }
            });
            userPagination.appendChild(prevBtn);
            
            // Page buttons
            for (let i = 1; i <= totalPages; i++) {
                const pageBtn = document.createElement('button');
                pageBtn.textContent = i;
                pageBtn.classList.toggle('active', i === currentPage);
                pageBtn.addEventListener('click', () => {
                    currentPage = i;
                    loadUsers();
                });
                userPagination.appendChild(pageBtn);
            }
            
            // Next button
            const nextBtn = document.createElement('button');
            nextBtn.textContent = 'Next';
            nextBtn.disabled = currentPage === totalPages;
            nextBtn.addEventListener('click', () => {
                if (currentPage < totalPages) {
                    currentPage++;
                    loadUsers();
                }
            });
            userPagination.appendChild(nextBtn);
        }
        
        function showAddUserModal() {
            modalTitle.textContent = 'Add User';
            userIdInput.value = '';
            userForm.reset();
            passwordNote.style.display = 'none';
            passwordInput.required = true;
            errorMessage.textContent = '';
            userModal.style.display = 'block';
        }
        
        async function showEditUserModal(userId) {
            try {
                modalTitle.textContent = 'Edit User';
                userIdInput.value = userId;
                passwordNote.style.display = 'block';
                passwordInput.required = false;
                errorMessage.textContent = '';
                
                // Get auth token
                const token = localStorage.getItem('access_token');
                if (!token) {
                    window.location.href = '/login?redirect=/admin';
                    return;
                }
                
                // Fetch user
                const response = await fetch(`/api/admin/users/${userId}`, {
                    headers: {
                        'Authorization': `Bearer ${token}`
                    }
                });
                
                if (!response.ok) {
                    throw new Error('Failed to load user');
                }
                
                const user = await response.json();
                
                // Populate form
                usernameInput.value = user.username;
                emailInput.value = user.email;
                fullNameInput.value = user.full_name || '';
                isActiveInput.value = user.is_active.toString();
                isAdminInput.value = user.is_admin.toString();
                passwordInput.value = '';
                
                userModal.style.display = 'block';
            } catch (error) {
                console.error('Error loading user:', error);
            }
        }
        
        function showDeleteModal(userId) {
            deleteUserIdInput.value = userId;
            deleteModal.style.display = 'block';
        }
        
        function closeModals() {
            userModal.style.display = 'none';
            deleteModal.style.display = 'none';
        }
        
        async function saveUser(e) {
            e.preventDefault();
            
            try {
                // Get auth token
                const token = localStorage.getItem('access_token');
                if (!token) {
                    window.location.href = '/login?redirect=/admin';
                    return;
                }
                
                const userId = userIdInput.value;
                const isEdit = !!userId;
                
                // Prepare data
                const userData = {
                    username: usernameInput.value,
                    email: emailInput.value,
                    full_name: fullNameInput.value || null,
                    is_active: isActiveInput.value === 'true',
                    is_admin: isAdminInput.value === 'true'
                };
                
                if (passwordInput.value) {
                    userData.password = passwordInput.value;
                }
                
                // API endpoint and method
                const url = isEdit ? `/api/admin/users/${userId}` : '/api/admin/users';
                const method = isEdit ? 'PUT' : 'POST';
                
                // Send request
                const response = await fetch(url, {
                    method: method,
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${token}`
                    },
                    body: JSON.stringify(userData)
                });
                
                if (!response.ok) {
                    const data = await response.json();
                    throw new Error(data.detail || 'Failed to save user');
                }
                
                // Close modal and reload users
                closeModals();
                loadUsers();
            } catch (error) {
                errorMessage.textContent = error.message;
                console.error('Error saving user:', error);
            }
        }
        
        async function deleteUser() {
            try {
                // Get auth token
                const token = localStorage.getItem('access_token');
                if (!token) {
                    window.location.href = '/login?redirect=/admin';
                    return;
                }
                
                const userId = deleteUserIdInput.value;
                
                // Send request
                const response = await fetch(`/api/admin/users/${userId}`, {
                    method: 'DELETE',
                    headers: {
                        'Authorization': `Bearer ${token}`
                    }
                });
                
                if (!response.ok) {
                    const data = await response.json();
                    throw new Error(data.detail || 'Failed to delete user');
                }
                
                // Close modal and reload users
                closeModals();
                loadUsers();
            } catch (error) {
                console.error('Error deleting user:', error);
                alert('Error: ' + error.message);
            }
        }
    });
</script>
{% endblock %}

================
File: app/templates/analytics.html
================
{% extends "base.html" %}

{% block title %}Analytics - Metis RAG{% endblock %}

{% block head %}
<style>
    .analytics-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }
    
    .analytics-section {
        margin-bottom: 30px;
        background-color: var(--card-bg);
        border-radius: 8px;
        padding: 20px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    
    .analytics-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 20px;
    }
    
    .analytics-title {
        font-size: 1.5rem;
        font-weight: 600;
        margin: 0;
    }
    
    .analytics-controls {
        display: flex;
        gap: 10px;
    }
    
    .analytics-card {
        background-color: var(--input-bg);
        border-radius: 8px;
        padding: 15px;
        margin-bottom: 15px;
    }
    
    .analytics-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 15px;
        margin-bottom: 20px;
    }
    
    .stat-card {
        background-color: var(--input-bg);
        border-radius: 8px;
        padding: 15px;
        text-align: center;
    }
    
    .stat-value {
        font-size: 2rem;
        font-weight: bold;
        margin: 10px 0;
        color: var(--accent-color);
    }
    
    .stat-label {
        font-size: 0.9rem;
        color: var(--muted-color);
    }
    
    .chart-container {
        height: 300px;
        margin-bottom: 20px;
    }
    
    .table-container {
        overflow-x: auto;
    }
    
    table {
        width: 100%;
        border-collapse: collapse;
    }
    
    th, td {
        padding: 10px;
        text-align: left;
        border-bottom: 1px solid var(--border-color);
    }
    
    th {
        background-color: var(--card-bg);
        font-weight: 600;
    }
    
    tr:hover {
        background-color: var(--border-color);
    }
    
    .loading {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 200px;
        font-size: 1.2rem;
        color: var(--muted-color);
    }
    
    .spinner {
        border: 4px solid var(--border-color);
        border-top: 4px solid var(--accent-color);
        border-radius: 50%;
        width: 30px;
        height: 30px;
        animation: spin 1s linear infinite;
        margin-right: 10px;
    }
    
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
</style>
{% endblock %}

{% block content %}
<div class="analytics-container">
    <h1>Analytics Dashboard</h1>
    
    <div class="analytics-section">
        <div class="analytics-header">
            <h2 class="analytics-title">System Overview</h2>
            <div class="analytics-controls">
                <button id="refresh-stats" class="secondary">
                    <i class="fas fa-sync-alt"></i> Refresh
                </button>
            </div>
        </div>
        
        <div id="system-stats" class="analytics-grid">
            <div class="loading">
                <div class="spinner"></div> Loading system stats...
            </div>
        </div>
    </div>
    
    <div class="analytics-section">
        <div class="analytics-header">
            <h2 class="analytics-title">Query Analytics</h2>
            <div class="analytics-controls">
                <select id="query-time-period">
                    <option value="all">All Time</option>
                    <option value="day">Last 24 Hours</option>
                    <option value="week">Last 7 Days</option>
                    <option value="month">Last 30 Days</option>
                </select>
            </div>
        </div>
        
        <div id="query-stats" class="analytics-grid">
            <div class="loading">
                <div class="spinner"></div> Loading query stats...
            </div>
        </div>
        
        <h3>Most Common Queries</h3>
        <div id="common-queries" class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Query</th>
                        <th>Count</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Will be populated by JavaScript -->
                </tbody>
            </table>
        </div>
        
        <h3>Recent Queries</h3>
        <div id="recent-queries" class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Query</th>
                        <th>Model</th>
                        <th>RAG</th>
                        <th>Response Time</th>
                        <th>Timestamp</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Will be populated by JavaScript -->
                </tbody>
            </table>
        </div>
    </div>
    
    <div class="analytics-section">
        <div class="analytics-header">
            <h2 class="analytics-title">Document Usage</h2>
            <div class="analytics-controls">
                <select id="document-time-period">
                    <option value="all">All Time</option>
                    <option value="day">Last 24 Hours</option>
                    <option value="week">Last 7 Days</option>
                    <option value="month">Last 30 Days</option>
                </select>
            </div>
        </div>
        
        <div id="document-stats" class="analytics-grid">
            <div class="loading">
                <div class="spinner"></div> Loading document stats...
            </div>
        </div>
        
        <h3>Most Used Documents</h3>
        <div id="most-used-documents" class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Document ID</th>
                        <th>Usage Count</th>
                        <th>Last Used</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Will be populated by JavaScript -->
                </tbody>
            </table>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Load initial data
        loadSystemStats();
        loadQueryStats();
        loadDocumentStats();
        
        // Set up event listeners
        document.getElementById('refresh-stats').addEventListener('click', function() {
            loadSystemStats();
            loadQueryStats();
            loadDocumentStats();
        });
        
        document.getElementById('query-time-period').addEventListener('change', function() {
            loadQueryStats();
        });
        
        document.getElementById('document-time-period').addEventListener('change', function() {
            loadDocumentStats();
        });
    });
    
    function loadSystemStats() {
        const statsContainer = document.getElementById('system-stats');
        statsContainer.innerHTML = '<div class="loading"><div class="spinner"></div> Loading system stats...</div>';
        
        fetch('/api/analytics/system_stats')
            .then(response => response.json())
            .then(data => {
                statsContainer.innerHTML = '';
                
                // Vector store stats
                const vectorStats = data.vector_store || {};
                
                // Add document count stat
                addStatCard(statsContainer, data.document_count || 0, 'Documents');
                
                // Add vector store document count
                addStatCard(statsContainer, vectorStats.count || 0, 'Vector Chunks');
                
                // Add query count stat
                addStatCard(statsContainer, data.query_count || 0, 'Total Queries');
                
                // Add embedding model
                addStatCard(statsContainer, vectorStats.embeddings_model || 'N/A', 'Embedding Model');
                
                // Add cache stats if available
                if (vectorStats.cache_enabled) {
                    addStatCard(statsContainer, vectorStats.cache_hit_ratio ? (vectorStats.cache_hit_ratio * 100).toFixed(1) + '%' : '0%', 'Cache Hit Ratio');
                    addStatCard(statsContainer, vectorStats.cache_size || 0, 'Cache Size');
                }
            })
            .catch(error => {
                console.error('Error loading system stats:', error);
                statsContainer.innerHTML = '<div class="analytics-card">Error loading system stats</div>';
            });
    }
    
    function loadQueryStats() {
        const statsContainer = document.getElementById('query-stats');
        const commonQueriesTable = document.getElementById('common-queries').querySelector('tbody');
        const recentQueriesTable = document.getElementById('recent-queries').querySelector('tbody');
        const timePeriod = document.getElementById('query-time-period').value;
        
        statsContainer.innerHTML = '<div class="loading"><div class="spinner"></div> Loading query stats...</div>';
        commonQueriesTable.innerHTML = '<tr><td colspan="2">Loading...</td></tr>';
        recentQueriesTable.innerHTML = '<tr><td colspan="5">Loading...</td></tr>';
        
        fetch(`/api/analytics/query_stats?time_period=${timePeriod}`)
            .then(response => response.json())
            .then(data => {
                statsContainer.innerHTML = '';
                
                // Add query count stat
                addStatCard(statsContainer, data.query_count || 0, 'Queries');
                
                // Add average response time
                addStatCard(statsContainer, data.avg_response_time_ms ? data.avg_response_time_ms.toFixed(0) + ' ms' : 'N/A', 'Avg Response Time');
                
                // Add average token count
                addStatCard(statsContainer, data.avg_token_count ? data.avg_token_count.toFixed(0) : 'N/A', 'Avg Token Count');
                
                // Add RAG usage percentage
                addStatCard(statsContainer, data.rag_usage_percent ? data.rag_usage_percent.toFixed(1) + '%' : '0%', 'RAG Usage');
                
                // Populate most common queries table
                commonQueriesTable.innerHTML = '';
                if (data.most_common_queries && data.most_common_queries.length > 0) {
                    data.most_common_queries.forEach(query => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${query.query}</td>
                            <td>${query.count}</td>
                        `;
                        commonQueriesTable.appendChild(row);
                    });
                } else {
                    commonQueriesTable.innerHTML = '<tr><td colspan="2">No queries found</td></tr>';
                }
                
                // Populate recent queries table
                recentQueriesTable.innerHTML = '';
                if (data.recent_queries && data.recent_queries.length > 0) {
                    data.recent_queries.forEach(query => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${query.query}</td>
                            <td>${query.model}</td>
                            <td>${query.use_rag ? 'Yes' : 'No'}</td>
                            <td>${query.response_time_ms ? query.response_time_ms.toFixed(0) + ' ms' : 'N/A'}</td>
                            <td>${formatTimestamp(query.timestamp)}</td>
                        `;
                        recentQueriesTable.appendChild(row);
                    });
                } else {
                    recentQueriesTable.innerHTML = '<tr><td colspan="5">No queries found</td></tr>';
                }
            })
            .catch(error => {
                console.error('Error loading query stats:', error);
                statsContainer.innerHTML = '<div class="analytics-card">Error loading query stats</div>';
                commonQueriesTable.innerHTML = '<tr><td colspan="2">Error loading data</td></tr>';
                recentQueriesTable.innerHTML = '<tr><td colspan="5">Error loading data</td></tr>';
            });
    }
    
    function loadDocumentStats() {
        const statsContainer = document.getElementById('document-stats');
        const documentsTable = document.getElementById('most-used-documents').querySelector('tbody');
        const timePeriod = document.getElementById('document-time-period').value;
        
        statsContainer.innerHTML = '<div class="loading"><div class="spinner"></div> Loading document stats...</div>';
        documentsTable.innerHTML = '<tr><td colspan="3">Loading...</td></tr>';
        
        fetch(`/api/analytics/document_usage?time_period=${timePeriod}`)
            .then(response => response.json())
            .then(data => {
                statsContainer.innerHTML = '';
                
                // Add document count stat
                addStatCard(statsContainer, data.document_count || 0, 'Documents Used');
                
                // Populate most used documents table
                documentsTable.innerHTML = '';
                if (data.most_used && data.most_used.length > 0) {
                    data.most_used.forEach(doc => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${doc.id}</td>
                            <td>${doc.usage_count}</td>
                            <td>${formatTimestamp(doc.last_used)}</td>
                        `;
                        documentsTable.appendChild(row);
                    });
                } else {
                    documentsTable.innerHTML = '<tr><td colspan="3">No document usage data found</td></tr>';
                }
            })
            .catch(error => {
                console.error('Error loading document stats:', error);
                statsContainer.innerHTML = '<div class="analytics-card">Error loading document stats</div>';
                documentsTable.innerHTML = '<tr><td colspan="3">Error loading data</td></tr>';
            });
    }
    
    function addStatCard(container, value, label) {
        const card = document.createElement('div');
        card.className = 'stat-card';
        card.innerHTML = `
            <div class="stat-value">${value}</div>
            <div class="stat-label">${label}</div>
        `;
        container.appendChild(card);
    }
    
    function formatTimestamp(timestamp) {
        if (!timestamp) return 'N/A';
        
        try {
            const date = new Date(timestamp);
            return date.toLocaleString();
        } catch (e) {
            return timestamp;
        }
    }
</script>
{% endblock %}

================
File: app/templates/base.html
================
<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Security-Policy" content="default-src 'self'; style-src 'self' 'unsafe-inline' https://cdnjs.cloudflare.com https://fonts.googleapis.com; script-src 'self' 'unsafe-inline'; font-src 'self' https://cdnjs.cloudflare.com https://fonts.gstatic.com; connect-src 'self';">
    <title>{% block title %}Metis RAG{% endblock %}</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="{{ url_for('static', path='css/fonts.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', path='css/styles.css') }}">
    <meta name="description" content="Metis RAG - Retrieval Augmented Generation with Ollama">
    <style>
        .header-controls {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .user-controls {
            display: flex;
            align-items: center;
            gap: 8px;
            margin-left: 10px;
        }
        #username-display {
            font-size: 0.8em;
            color: var(--text-color-secondary);
            max-width: 100px;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
        }
        .auth-button {
            background: none;
            border: none;
            color: var(--text-color);
            cursor: pointer;
            font-size: 1em;
            padding: 5px;
            border-radius: 4px;
        }
        .auth-button:hover {
            background-color: var(--hover-color);
        }
    </style>
    {% block head %}{% endblock %}
</head>
<body>
    <div class="app-container">
        <!-- Left Sidebar -->
        <div class="sidebar">
            <div class="sidebar-header">
                <h1><i class="fas fa-brain" style="font-size: 1.1em; color: var(--ginkgo-green);"></i> Metis RAG</h1>
                <div class="header-controls">
                    <button id="theme-toggle" class="theme-toggle" title="Toggle light/dark mode">
                        <i class="fas fa-sun"></i>
                    </button>
                    <div id="user-controls" class="user-controls">
                        <span id="username-display"></span>
                        <button id="login-button" class="auth-button" title="Login">
                            <i class="fas fa-sign-in-alt"></i>
                        </button>
                        <button id="logout-button" class="auth-button" style="display: none;" title="Logout">
                            <i class="fas fa-sign-out-alt"></i>
                        </button>
                    </div>
                </div>
            </div>
            
            <div class="sidebar-content">
                {% block sidebar %}{% endblock %}
            </div>
        </div>
        
        <!-- Right Content Area -->
        <div class="main-content">
            {% block content %}{% endblock %}
        </div>
    </div>
    
    <!-- Token usage indicator -->
    <div class="token-usage" id="token-usage">
        <div class="token-usage-title">
            <i class="fas fa-microchip"></i> Token Usage
        </div>
        <div class="token-usage-bar">
            <div id="token-usage-fill"></div>
        </div>
        <div id="token-usage-text">0 / 4096 tokens</div>
    </div>
    
    <script src="{{ url_for('static', path='js/main.js') }}"></script>
    {% block scripts %}{% endblock %}
</body>
</html>

================
File: app/templates/chat.html
================
{% extends "base.html" %}

{% block title %}Chat - Metis RAG{% endblock %}

{% block head %}
<!-- Document manager styles -->
<link rel="stylesheet" href="{{ url_for('static', path='css/document-manager.css') }}">
{% endblock %}

{% block sidebar %}
<form id="chat-form">
    <!-- Model Selection -->
    <div class="form-group">
        <label for="model">
            Select Model:
            <span class="tooltip">
                <i class="fas fa-info-circle"></i>
                <span class="tooltip-text">Choose which AI model to use. Different models have different capabilities and performance characteristics.</span>
            </span>
        </label>
        <div class="param-description">Choose the AI model to use for generating responses.</div>
        <select id="model" name="model">
            <option value="llama3" selected>Llama 3</option>
            <!-- Other models will be loaded dynamically -->
        </select>
    </div>
    
    <!-- RAG Toggle -->
    <div class="form-group checkbox-container">
        <input type="checkbox" id="rag-toggle" name="use_rag" value="true">
        <label for="rag-toggle">
            Use RAG
            <span class="tooltip">
                <i class="fas fa-info-circle"></i>
                <span class="tooltip-text">When enabled, the model will use your documents to provide more informed responses.</span>
            </span>
            <div class="param-description">Enable Retrieval Augmented Generation with your documents</div>
        </label>
    </div>
    
    <!-- Streaming Toggle -->
    <div class="form-group checkbox-container">
        <input type="checkbox" id="stream-toggle" name="use_stream" value="true">
        <label for="stream-toggle">
            Use Streaming
            <span class="tooltip">
                <i class="fas fa-info-circle"></i>
                <span class="tooltip-text">When enabled, responses will be streamed in real-time. May cause errors with some models.</span>
            </span>
            <div class="param-description">Enable streaming responses (may be unstable)</div>
        </label>
    </div>
    <div class="streaming-warning" style="color: #ff9800; font-size: 0.8em; margin-top: 5px; padding: 5px; border-left: 3px solid #ff9800;">
        <i class="fas fa-exclamation-triangle"></i> Streaming mode may cause errors with some models. For best results, keep it disabled.
    </div>
    
    <!-- Advanced Parameters -->
    <div class="advanced-options">
        <button type="button" id="advanced-toggle" class="advanced-toggle">
            <i class="fas fa-cog"></i> Advanced Parameters
            <i id="advanced-icon" class="fas fa-chevron-down"></i>
        </button>
        
        <div id="advanced-content" class="advanced-content">
            <div class="parameter-grid">
                <div class="form-group">
                    <label for="temperature">
                        Temperature:
                        <span class="tooltip">
                            <i class="fas fa-info-circle"></i>
                            <span class="tooltip-text">Controls randomness. Lower values make responses more focused and deterministic. Higher values make output more random and creative.</span>
                        </span>
                    </label>
                    <div class="param-description">Controls randomness: lower = more focused, higher = more creative (0-1)</div>
                    <input type="number" id="temperature" name="temperature" step="0.1" min="0.0" max="1.0" value="0.7">
                </div>
                
                <!-- RAG-specific parameters -->
                <div class="form-group rag-param">
                    <label for="max-results">
                        Max Results:
                        <span class="tooltip">
                            <i class="fas fa-info-circle"></i>
                            <span class="tooltip-text">The maximum number of document chunks to retrieve for context.</span>
                        </span>
                    </label>
                    <div class="param-description">Number of document chunks to retrieve (1-10)</div>
                    <input type="number" id="max-results" name="max_results" min="1" max="10" value="4">
                </div>
                
                <!-- Metadata Filtering -->
                <div class="form-group rag-param">
                    <label for="metadata-filters">
                        Metadata Filters:
                        <span class="tooltip">
                            <i class="fas fa-info-circle"></i>
                            <span class="tooltip-text">Filter documents by metadata (e.g., {"filename": "report.pdf"}).</span>
                        </span>
                    </label>
                    <div class="param-description">JSON object for filtering documents by metadata</div>
                    <textarea id="metadata-filters" name="metadata_filters" rows="2" placeholder='{"filename": "example.pdf"}'></textarea>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Action Buttons for Sidebar -->
    <div class="action-buttons" style="margin-top: 20px;">
        <button type="button" id="clear-chat" class="secondary">
            <i class="fas fa-trash"></i> Clear Chat
        </button>
        <button type="button" id="save-chat" class="secondary">
            <i class="fas fa-save"></i> Save
        </button>
    </div>
</form>

<!-- Document Management Section -->
<div id="document-section" class="document-section">
    <div id="toggle-documents" class="document-section-header">
        <div class="document-section-title">
            <i class="fas fa-file-alt"></i> Documents
            <span id="document-count" class="document-count">0</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    
    <!-- Filter Panel -->
    <div id="filter-panel" class="filter-panel">
        <div class="filter-title">
            <span><i class="fas fa-filter"></i> Filter Documents</span>
            <button id="filter-toggle" class="filter-toggle">
                <i class="fas fa-chevron-down"></i>
            </button>
        </div>
        
        <div id="filter-content" class="filter-content">
            <div class="filter-section">
                <div class="filter-section-title">Tags</div>
                <div id="filter-tags" class="filter-tags">
                    <!-- Tags will be loaded dynamically -->
                </div>
            </div>
            
            <div class="filter-section">
                <div class="filter-section-title">Folders</div>
                <div id="filter-folders" class="filter-folders">
                    <!-- Folders will be loaded dynamically -->
                </div>
            </div>
            
            <div class="filter-actions">
                <button id="apply-filters">Apply</button>
                <button id="clear-filters">Clear</button>
            </div>
        </div>
    </div>
    
    <div class="document-upload">
        <form id="doc-upload-form" class="upload-form">
            <div class="upload-input">
                <input type="file" id="document-file" accept=".pdf,.txt,.csv,.md" required>
            </div>
            
            <div class="form-group">
                <input type="text" id="doc-tags" placeholder="Tags (comma separated)">
            </div>
            
            <div class="form-group">
                <select id="doc-folder">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
            </div>
            
            <button type="submit" class="upload-button">
                <i class="fas fa-upload"></i> Upload
            </button>
            <div id="upload-progress" class="progress-bar">
                <div id="upload-progress-fill" class="progress-bar-fill"></div>
            </div>
        </form>
    </div>
    
    <div id="document-list" class="document-list">
        <!-- Documents will be loaded dynamically -->
        <div class="document-loading">Loading documents...</div>
    </div>
    
    <div class="batch-actions">
        <button id="process-selected-btn" class="secondary" disabled>
            <i class="fas fa-sync-alt"></i> Process
        </button>
        <button id="delete-selected-btn" class="danger" disabled>
            <i class="fas fa-trash"></i> Delete
        </button>
    </div>
</div>

<!-- Document Edit Modal -->
<div id="document-edit-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Edit Document</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="form-group">
                <label for="edit-tag-input">Tags</label>
                <div class="tag-input-container">
                    <input type="text" id="edit-tag-input" class="tag-input" placeholder="Add a tag and press Enter">
                    <div id="tag-suggestions" class="tag-suggestions"></div>
                </div>
                <div id="edit-tag-list" class="tag-list">
                    <!-- Tags will be added here -->
                </div>
            </div>
            
            <div class="form-group">
                <label for="edit-folder">Folder</label>
                <select id="edit-folder" class="folder-select">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
                <div class="folder-path">Current path: <span id="current-folder-path">/</span></div>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="save-changes">Save Changes</button>
        </div>
    </div>
</div>
{% endblock %}

{% block content %}
<div class="chat-area">
    <!-- Chat Container -->
    <div class="chat-container" id="chat-container">
        <div class="message bot-message">
            <div class="message-header">Metis:</div>
            Hello! I'm your Metis RAG assistant. Ask me anything about your uploaded documents or chat with me directly.
        </div>
    </div>
    
    <!-- Input Area -->
    <div class="input-area">
        <div class="form-group">
            <label for="user-input">
                Your Message:
                <span class="tooltip">
                    <i class="fas fa-info-circle"></i>
                    <span class="tooltip-text">Enter your question or instruction for the AI model. Be specific for better results.</span>
                </span>
            </label>
            <div class="param-description">Type your message or question for the AI to respond to.</div>
            <textarea id="user-input" rows="4" placeholder="Type your message here..." required></textarea>
            <div class="keyboard-shortcuts">
                Press <strong>Enter</strong> to send, <strong>Shift+Enter</strong> for new line
            </div>
        </div>
        
        <div class="submit-container">
            <button type="button" id="send-button">
                <i class="fas fa-paper-plane"></i> Send Message
            </button>
        </div>
    </div>
</div>

<div id="loading" class="loading">
    <span class="spinner"></span> Generating response...
</div>
{% endblock %}

{% block scripts %}
<script src="{{ url_for('static', path='js/document-manager.js') }}"></script>
<script src="{{ url_for('static', path='js/chat.js') }}"></script>
{% endblock %}

================
File: app/templates/documents_enhanced.html
================
{% extends "base.html" %}

{% block title %}Documents - Metis RAG{% endblock %}

{% block head %}
<link rel="stylesheet" href="{{ url_for('static', path='css/document-manager.css') }}">
<link rel="stylesheet" href="{{ url_for('static', path='css/document-upload-enhanced.css') }}">
<style>
    .documents-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }
    
    .section-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 20px;
    }
    
    .auth-controls {
        display: flex;
        align-items: center;
        gap: 10px;
    }
    
    #logout-button-docs {
        background-color: var(--secondary-color);
        color: white;
        border: none;
        padding: 8px 15px;
        border-radius: 4px;
        cursor: pointer;
        font-size: 0.9em;
        display: flex;
        align-items: center;
        gap: 5px;
    }
    
    #logout-button-docs:hover {
        background-color: var(--hover-color);
    }
    
    #username-display-docs {
        font-weight: bold;
        color: var(--text-color);
    }
</style>
{% endblock %}

{% block content %}
<div class="documents-container">
    <div class="document-section">
        <div class="section-header">
            <h2>Document Management</h2>
            <div class="auth-controls">
                <span id="username-display-docs"></span>
                <button id="logout-button-docs" class="btn btn-secondary" title="Logout">
                    <i class="fas fa-sign-out-alt"></i> Logout
                </button>
            </div>
        </div>
        
        <div class="upload-form">
            <h3>Upload Documents</h3>
            <div id="drop-zone" class="drop-zone">
                <p>Drag and drop files here or click to select</p>
                <p class="supported-formats">Supported formats: PDF, Word, Text, CSV, Markdown, HTML, JSON, XML</p>
                <form id="upload-form">
                    <div class="upload-input">
                        <input type="file" id="document-file" accept=".pdf,.txt,.csv,.md,.docx,.doc,.rtf,.html,.json,.xml" multiple required>
                    </div>
                    
                    <div class="form-row">
                        <div class="form-group">
                            <label for="doc-tags">Tags (comma separated)</label>
                            <input type="text" id="doc-tags" placeholder="e.g. important, reference, work">
                        </div>
                        
                        <div class="form-group">
                            <label for="doc-folder">Folder</label>
                            <select id="doc-folder">
                                <option value="/">Root</option>
                                <!-- Folders will be loaded dynamically -->
                            </select>
                        </div>
                    </div>
                    
                    <button type="submit">Upload Documents</button>
                </form>
            </div>
            
            <div id="file-list" class="file-list">
                <!-- Selected files will be displayed here -->
            </div>
            
            <div class="progress-container">
                <div class="overall-progress">
                    <label>Overall Progress:</label>
                    <div class="progress-bar" id="overall-progress">
                        <div class="progress-bar-fill" id="overall-progress-fill"></div>
                    </div>
                </div>
                <div id="file-progress-list">
                    <!-- Individual file progress bars will be added here -->
                </div>
            </div>
        </div>
        
        <div class="filter-section">
            <div class="filter-title">
                <h3>Filter Documents</h3>
                <button id="filter-toggle" class="filter-toggle">
                    <i class="fas fa-chevron-down"></i>
                </button>
            </div>
            
            <div id="filter-content" class="filter-content">
                <div class="filter-section">
                    <div class="filter-section-title">Tags</div>
                    <div id="filter-tags" class="filter-tags">
                        <!-- Tags will be loaded dynamically -->
                    </div>
                </div>
                
                <div class="filter-section">
                    <div class="filter-section-title">Folders</div>
                    <div id="filter-folders" class="filter-folders">
                        <!-- Folders will be loaded dynamically -->
                    </div>
                </div>
                
                <div class="filter-actions">
                    <button id="apply-filters">Apply Filters</button>
                    <button id="clear-filters">Clear Filters</button>
                </div>
            </div>
        </div>
        
        <div class="document-section">
            <h3>Document Processing Options</h3>
            <div class="processing-options">
                <div class="form-row">
                    <div class="form-group">
                        <label for="chunking-strategy">Chunking Strategy</label>
                        <select id="chunking-strategy">
                            <option value="recursive">Recursive (Default)</option>
                            <option value="token">Token-based</option>
                            <option value="markdown">Markdown Headers</option>
                        </select>
                        <div class="param-description">Choose how documents are split into chunks</div>
                    </div>
                    
                    <div class="form-group">
                        <label for="chunk-size">Chunk Size</label>
                        <input type="number" id="chunk-size" placeholder="Default: 1000" min="100" max="4000">
                        <div class="param-description">Size of each chunk in characters</div>
                    </div>
                    
                    <div class="form-group">
                        <label for="chunk-overlap">Chunk Overlap</label>
                        <input type="number" id="chunk-overlap" placeholder="Default: 200" min="0" max="1000">
                        <div class="param-description">Overlap between chunks in characters</div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="batch-actions">
            <button id="process-selected-btn" disabled>Process Selected</button>
            <button id="delete-selected-btn" disabled>Delete Selected</button>
        </div>
        
        <div id="document-list" class="document-grid">
            <!-- Documents will be loaded dynamically -->
            <div class="document-loading">Loading documents...</div>
        </div>
    </div>
</div>

<!-- Document Edit Modal -->
<div id="document-edit-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Edit Document</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="form-group">
                <label for="edit-tag-input">Tags</label>
                <div class="tag-input-container">
                    <input type="text" id="edit-tag-input" class="tag-input" placeholder="Add a tag and press Enter">
                    <div id="tag-suggestions" class="tag-suggestions"></div>
                </div>
                <div id="edit-tag-list" class="tag-list">
                    <!-- Tags will be added here -->
                </div>
            </div>
            
            <div class="form-group">
                <label for="edit-folder">Folder</label>
                <select id="edit-folder" class="folder-select">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
                <div class="folder-path">Current path: <span id="current-folder-path">/</span></div>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="save-changes">Save Changes</button>
        </div>
    </div>
</div>

<!-- Batch Tag Modal -->
<div id="batch-tag-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Add Tags to <span id="tag-file-count">0</span> Files</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="tag-input-container">
                <input type="text" id="batch-tag-input" class="tag-input" placeholder="Add a tag and press Enter">
                <div id="batch-tag-suggestions" class="tag-suggestions"></div>
            </div>
            <div id="batch-tag-list" class="tag-list">
                <!-- Tags will be added here -->
            </div>
            
            <div class="batch-options">
                <label class="checkbox-label">
                    <input type="checkbox" id="merge-tags" checked>
                    Merge with existing tags
                </label>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="cancel-batch-tag" class="btn">Cancel</button>
            <button id="apply-batch-tag" class="btn primary">Apply Tags</button>
        </div>
    </div>
</div>

<!-- Batch Folder Modal -->
<div id="batch-folder-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Move <span id="folder-file-count">0</span> Files</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="folder-select-container">
                <label for="batch-folder-select">Select Destination Folder:</label>
                <select id="batch-folder-select" class="folder-select">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
                
                <div class="new-folder-container">
                    <label for="new-folder-input">Or Create New Folder:</label>
                    <div class="new-folder-input-group">
                        <input type="text" id="new-folder-input" placeholder="New folder name">
                        <button id="create-folder-btn" class="btn">Create</button>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="cancel-batch-folder" class="btn">Cancel</button>
            <button id="apply-batch-folder" class="btn primary">Move Files</button>
        </div>
    </div>
</div>

<!-- Batch Delete Modal -->
<div id="batch-delete-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Delete <span id="delete-file-count">0</span> Files</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="warning-message">
                <i class="fas fa-exclamation-triangle"></i>
                <p>Are you sure you want to delete these files? This action cannot be undone.</p>
            </div>
            
            <div id="files-to-delete" class="files-to-delete">
                <!-- File list will be populated dynamically -->
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="cancel-batch-delete" class="btn">Cancel</button>
            <button id="confirm-batch-delete" class="btn danger">Delete Files</button>
        </div>
    </div>
</div>

<!-- Upload Summary Modal -->
<div id="upload-summary-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Upload Complete</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="summary-stats">
                <div class="summary-stat">
                    <span class="stat-value" id="summary-total">0</span>
                    <span class="stat-label">Total Files</span>
                </div>
                <div class="summary-stat">
                    <span class="stat-value" id="summary-success">0</span>
                    <span class="stat-label">Successful</span>
                </div>
                <div class="summary-stat">
                    <span class="stat-value" id="summary-failed">0</span>
                    <span class="stat-label">Failed</span>
                </div>
            </div>
            
            <div class="summary-details">
                <h4>Successful Uploads</h4>
                <div id="summary-success-list" class="summary-list">
                    <!-- Successful files will be listed here -->
                </div>
                
                <h4>Failed Uploads</h4>
                <div id="summary-failed-list" class="summary-list">
                    <!-- Failed files will be listed here -->
                </div>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="process-uploaded-btn" class="btn primary">Process Files</button>
            <button id="close-summary-btn" class="btn">Close</button>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<!-- Load the main document manager script first -->
<script src="{{ url_for('static', path='js/document-manager.js') }}"></script>

<!-- Load the enhanced document upload script -->
<script src="{{ url_for('static', path='js/document-upload-enhanced.js') }}"></script>

<!-- Load our enhancement scripts after a small delay to ensure DocumentManager is initialized -->
<script>
    // Wait for document-manager.js to initialize
    document.addEventListener('DOMContentLoaded', function() {
        // Set up the additional logout button
        const logoutButtonDocs = document.getElementById('logout-button-docs');
        const usernameDisplayDocs = document.getElementById('username-display-docs');
        
        if (logoutButtonDocs) {
            logoutButtonDocs.addEventListener('click', function() {
                // Call the logout function from main.js
                if (typeof logout === 'function') {
                    logout();
                } else {
                    // Fallback if logout function is not available
                    localStorage.removeItem('access_token');
                    localStorage.removeItem('token_type');
                    localStorage.removeItem('username');
                    window.location.href = '/login';
                }
            });
        }
        
        // Display username in the documents page
        if (usernameDisplayDocs) {
            const username = localStorage.getItem('username');
            if (username) {
                usernameDisplayDocs.textContent = username;
            } else if (typeof isAuthenticated === 'function' && isAuthenticated()) {
                // Try to get username from API if authenticated
                fetch('/api/auth/me', {
                    headers: {
                        'Authorization': `Bearer ${localStorage.getItem('access_token')}`
                    }
                })
                .then(response => response.json())
                .then(user => {
                    if (user && user.username) {
                        localStorage.setItem('username', user.username);
                        usernameDisplayDocs.textContent = user.username;
                    }
                })
                .catch(error => console.error('Error fetching user info:', error));
            }
        }
        
        // Load enhancement scripts with a small delay
        setTimeout(function() {
            // Create and append document-upload-fix.js
            var uploadFixScript = document.createElement('script');
            uploadFixScript.src = "{{ url_for('static', path='js/document-upload-fix.js') }}";
            document.body.appendChild(uploadFixScript);
            
            // Create and append error-feedback-enhancement.js
            var errorFeedbackScript = document.createElement('script');
            errorFeedbackScript.src = "{{ url_for('static', path='js/error-feedback-enhancement.js') }}";
            document.body.appendChild(errorFeedbackScript);
        }, 300);
    });
</script>

<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Initialize notification function if not already defined
        if (typeof showNotification !== 'function') {
            window.showNotification = function(message, type = 'info') {
                const notification = document.createElement('div');
                notification.className = `notification ${type}`;
                notification.style.position = 'fixed';
                notification.style.top = '20px';
                notification.style.right = '20px';
                notification.style.backgroundColor = type === 'warning' ? '#ff9800' : 'var(--secondary-color)';
                notification.style.color = 'white';
                notification.style.padding = '10px 15px';
                notification.style.borderRadius = '4px';
                notification.style.boxShadow = '0 2px 10px rgba(0, 0, 0, 0.2)';
                notification.style.zIndex = '1000';
                notification.style.maxWidth = '300px';
                notification.textContent = message;
                
                // Add close button
                const closeBtn = document.createElement('span');
                closeBtn.innerHTML = '&times;';
                closeBtn.style.marginLeft = '10px';
                closeBtn.style.cursor = 'pointer';
                closeBtn.style.fontWeight = 'bold';
                closeBtn.onclick = function() {
                    document.body.removeChild(notification);
                };
                notification.appendChild(closeBtn);
                
                // Add to body
                document.body.appendChild(notification);
                
                // Auto remove after 5 seconds
                setTimeout(() => {
                    if (document.body.contains(notification)) {
                        document.body.removeChild(notification);
                    }
                }, 5000);
            };
        }
    });
</script>
{% endblock %}

================
File: app/templates/documents.html
================
{% extends "base.html" %}

{% block title %}Documents - Metis RAG{% endblock %}

{% block head %}
<link rel="stylesheet" href="{{ url_for('static', path='css/document-manager.css') }}">
<style>
    .documents-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }

    .document-section {
        margin-bottom: 30px;
    }

    .document-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
        gap: 20px;
        margin-top: 20px;
    }

    .document-card {
        border: 1px solid var(--border-color);
        border-radius: 5px;
        padding: 15px;
        background-color: var(--card-bg);
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    .document-card h3 {
        margin-top: 0;
        margin-bottom: 10px;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .document-meta {
        font-size: 0.9em;
        color: var(--muted-color);
        margin-bottom: 10px;
    }

    .document-actions {
        display: flex;
        justify-content: space-between;
        margin-top: 15px;
    }

    .upload-form {
        margin-bottom: 20px;
        padding: 20px;
        border: 1px solid var(--border-color);
        border-radius: 5px;
        background-color: var(--card-bg);
    }

    .upload-input {
        margin-bottom: 15px;
    }

    .progress-bar {
        height: 10px;
        background-color: var(--border-color);
        border-radius: 5px;
        margin-top: 10px;
    }

    .progress-bar-fill {
        height: 100%;
        background-color: var(--primary-color);
        border-radius: 5px;
        width: 0;
        transition: width 0.3s;
    }

    .batch-actions {
        margin-bottom: 20px;
    }
    
    .drop-zone {
        border: 2px dashed var(--border-color);
        border-radius: 5px;
        padding: 20px;
        text-align: center;
        transition: all 0.3s ease;
        margin-bottom: 15px;
    }
    
    .supported-formats {
        font-size: 0.8em;
        color: var(--muted-color);
        margin-top: 5px;
    }

    .drop-zone.active {
        border-color: var(--primary-color);
        background-color: rgba(0, 123, 255, 0.1);
    }

    .file-list {
        margin-top: 15px;
        margin-bottom: 15px;
    }

    .file-item {
        display: flex;
        align-items: center;
        margin-bottom: 10px;
        padding: 8px;
        border: 1px solid var(--border-color);
        border-radius: 4px;
        background-color: var(--card-bg);
    }

    .file-item-name {
        flex-grow: 1;
        margin-right: 10px;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .file-item-size {
        margin-right: 10px;
        color: var(--muted-color);
        font-size: 0.9em;
    }

    .file-item-remove {
        cursor: pointer;
        color: var(--danger-color, #dc3545);
    }

    .progress-container {
        margin-top: 15px;
    }

    .overall-progress {
        margin-bottom: 10px;
    }

    .file-progress {
        margin-bottom: 8px;
    }

    .file-progress-item {
        display: flex;
        align-items: center;
        margin-bottom: 5px;
    }

    .file-progress-name {
        width: 200px;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
        margin-right: 10px;
    }

    .file-progress-bar {
        flex-grow: 1;
        height: 8px;
        background-color: var(--border-color);
        border-radius: 4px;
    }

    .file-progress-fill {
        height: 100%;
        background-color: var(--primary-color);
        border-radius: 4px;
        width: 0;
        transition: width 0.3s;
    }
    
    .filter-section {
        margin-bottom: 20px;
        padding: 15px;
        border: 1px solid var(--border-color);
        border-radius: 5px;
        background-color: var(--card-bg);
    }
    
    .card-tags {
        display: flex;
        flex-wrap: wrap;
        gap: 5px;
        margin-bottom: 10px;
    }
    
    .card-tag {
        background-color: var(--primary-color);
        color: white;
        padding: 3px 8px;
        border-radius: 12px;
        font-size: 0.8rem;
    }
    
    .card-folder {
        font-size: 0.9em;
        color: var(--muted-color);
        margin-bottom: 10px;
    }
    
    .card-folder i {
        margin-right: 5px;
    }
    
    .form-row {
        display: flex;
        gap: 15px;
        margin-bottom: 15px;
    }
    
    .form-group {
        flex: 1;
    }
    
    .form-group label {
        display: block;
        margin-bottom: 5px;
        font-weight: 500;
    }
    
    .form-group input,
    .form-group select {
        width: 100%;
        padding: 8px;
        border: 1px solid var(--border-color);
        border-radius: 4px;
        background-color: var(--input-bg);
        color: var(--text-color);
    }
</style>
{% endblock %}

{% block content %}
<div class="documents-container">
    <div class="document-section">
        <div class="section-header">
            <h2>Document Management</h2>
            <div class="auth-controls">
                <span id="username-display-docs"></span>
                <button id="logout-button-docs" class="btn btn-secondary" title="Logout">
                    <i class="fas fa-sign-out-alt"></i> Logout
                </button>
            </div>
        </div>
        <style>
            .section-header {
                display: flex;
                justify-content: space-between;
                align-items: center;
                margin-bottom: 20px;
            }
            .auth-controls {
                display: flex;
                align-items: center;
                gap: 10px;
            }
            #logout-button-docs {
                background-color: var(--secondary-color);
                color: white;
                border: none;
                padding: 8px 15px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 0.9em;
                display: flex;
                align-items: center;
                gap: 5px;
            }
            #logout-button-docs:hover {
                background-color: var(--secondary-color-dark, #0056b3);
            }
            #username-display-docs {
                font-weight: bold;
                color: var(--text-color);
            }
        </style>
        
        <div class="upload-form">
            <h3>Upload Documents</h3>
            <div id="drop-zone" class="drop-zone">
                <p>Drag and drop files here or click to select</p>
                <p class="supported-formats">Supported formats: PDF, Word, Text, CSV, Markdown, HTML, JSON, XML</p>
                <form id="upload-form">
                    <div class="upload-input">
                        <input type="file" id="document-file" accept=".pdf,.txt,.csv,.md,.docx,.doc,.rtf,.html,.json,.xml" multiple required>
                    </div>
                    
                    <div class="form-row">
                        <div class="form-group">
                            <label for="doc-tags">Tags (comma separated)</label>
                            <input type="text" id="doc-tags" placeholder="e.g. important, reference, work">
                        </div>
                        
                        <div class="form-group">
                            <label for="doc-folder">Folder</label>
                            <select id="doc-folder">
                                <option value="/">Root</option>
                                <!-- Folders will be loaded dynamically -->
                            </select>
                        </div>
                    </div>
                    
                    <button type="submit">Upload Documents</button>
                </form>
            </div>
            
            <div id="file-list" class="file-list">
                <!-- Selected files will be displayed here -->
            </div>
            
            <div class="progress-container">
                <div class="overall-progress">
                    <label>Overall Progress:</label>
                    <div class="progress-bar" id="overall-progress">
                        <div class="progress-bar-fill" id="overall-progress-fill"></div>
                    </div>
                </div>
                <div id="file-progress-list">
                    <!-- Individual file progress bars will be added here -->
                </div>
            </div>
        </div>
        
        <div class="filter-section">
            <div class="filter-title">
                <h3>Filter Documents</h3>
                <button id="filter-toggle" class="filter-toggle">
                    <i class="fas fa-chevron-down"></i>
                </button>
            </div>
            
            <div id="filter-content" class="filter-content">
                <div class="filter-section">
                    <div class="filter-section-title">Tags</div>
                    <div id="filter-tags" class="filter-tags">
                        <!-- Tags will be loaded dynamically -->
                    </div>
                </div>
                
                <div class="filter-section">
                    <div class="filter-section-title">Folders</div>
                    <div id="filter-folders" class="filter-folders">
                        <!-- Folders will be loaded dynamically -->
                    </div>
                </div>
                
                <div class="filter-actions">
                    <button id="apply-filters">Apply Filters</button>
                    <button id="clear-filters">Clear Filters</button>
                </div>
            </div>
        </div>
        
        <div class="document-section">
            <h3>Document Processing Options</h3>
            <div class="processing-options">
                <div class="form-row">
                    <div class="form-group">
                        <label for="chunking-strategy">Chunking Strategy</label>
                        <select id="chunking-strategy">
                            <option value="recursive">Recursive (Default)</option>
                            <option value="token">Token-based</option>
                            <option value="markdown">Markdown Headers</option>
                        </select>
                        <div class="param-description">Choose how documents are split into chunks</div>
                    </div>
                    
                    <div class="form-group">
                        <label for="chunk-size">Chunk Size</label>
                        <input type="number" id="chunk-size" placeholder="Default: 1000" min="100" max="4000">
                        <div class="param-description">Size of each chunk in characters</div>
                    </div>
                    
                    <div class="form-group">
                        <label for="chunk-overlap">Chunk Overlap</label>
                        <input type="number" id="chunk-overlap" placeholder="Default: 200" min="0" max="1000">
                        <div class="param-description">Overlap between chunks in characters</div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="batch-actions">
            <button id="process-selected-btn" disabled>Process Selected</button>
            <button id="delete-selected-btn" disabled>Delete Selected</button>
        </div>
        
        <div id="document-list" class="document-grid">
            <!-- Documents will be loaded dynamically -->
            <div class="document-loading">Loading documents...</div>
        </div>
    </div>
</div>

<!-- Document Edit Modal -->
<div id="document-edit-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Edit Document</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="form-group">
                <label for="edit-tag-input">Tags</label>
                <div class="tag-input-container">
                    <input type="text" id="edit-tag-input" class="tag-input" placeholder="Add a tag and press Enter">
                    <div id="tag-suggestions" class="tag-suggestions"></div>
                </div>
                <div id="edit-tag-list" class="tag-list">
                    <!-- Tags will be added here -->
                </div>
            </div>
            
            <div class="form-group">
                <label for="edit-folder">Folder</label>
                <select id="edit-folder" class="folder-select">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
                <div class="folder-path">Current path: <span id="current-folder-path">/</span></div>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="save-changes">Save Changes</button>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<!-- Load the main document manager script first -->
<script src="{{ url_for('static', path='js/document-manager.js') }}"></script>

<!-- Load our enhancement scripts after a small delay to ensure DocumentManager is initialized -->
<script>
    // Wait for document-manager.js to initialize
    document.addEventListener('DOMContentLoaded', function() {
        // Set up the additional logout button
        const logoutButtonDocs = document.getElementById('logout-button-docs');
        const usernameDisplayDocs = document.getElementById('username-display-docs');
        
        if (logoutButtonDocs) {
            logoutButtonDocs.addEventListener('click', function() {
                // Call the logout function from main.js
                if (typeof logout === 'function') {
                    logout();
                } else {
                    // Fallback if logout function is not available
                    localStorage.removeItem('access_token');
                    localStorage.removeItem('token_type');
                    localStorage.removeItem('username');
                    window.location.href = '/login';
                }
            });
            
            console.log("Logout button initialized");
        }
        
        // Display username in the documents page
        if (usernameDisplayDocs) {
            const username = localStorage.getItem('username');
            if (username) {
                usernameDisplayDocs.textContent = username;
            } else if (typeof isAuthenticated === 'function' && isAuthenticated()) {
                // Try to get username from API if authenticated
                fetch('/api/auth/me', {
                    headers: {
                        'Authorization': `Bearer ${localStorage.getItem('access_token')}`
                    }
                })
                .then(response => response.json())
                .then(user => {
                    if (user && user.username) {
                        localStorage.setItem('username', user.username);
                        usernameDisplayDocs.textContent = user.username;
                    }
                })
                .catch(error => console.error('Error fetching user info:', error));
            }
        }
        
        // Load enhancement scripts with a small delay
        setTimeout(function() {
            // Create and append document-upload-fix.js
            var uploadFixScript = document.createElement('script');
            uploadFixScript.src = "{{ url_for('static', path='js/document-upload-fix.js') }}";
            document.body.appendChild(uploadFixScript);
            
            // Create and append error-feedback-enhancement.js
            var errorFeedbackScript = document.createElement('script');
            errorFeedbackScript.src = "{{ url_for('static', path='js/error-feedback-enhancement.js') }}";
            document.body.appendChild(errorFeedbackScript);
            
            console.log("Enhancement scripts loaded");
        }, 300);
    });
</script>

<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Initialize notification function if not already defined
        if (typeof showNotification !== 'function') {
            window.showNotification = function(message, type = 'info') {
                const notification = document.createElement('div');
                notification.className = `notification ${type}`;
                notification.style.position = 'fixed';
                notification.style.top = '20px';
                notification.style.right = '20px';
                notification.style.backgroundColor = type === 'warning' ? '#ff9800' : 'var(--secondary-color)';
                notification.style.color = 'white';
                notification.style.padding = '10px 15px';
                notification.style.borderRadius = '4px';
                notification.style.boxShadow = '0 2px 10px rgba(0, 0, 0, 0.2)';
                notification.style.zIndex = '1000';
                notification.style.maxWidth = '300px';
                notification.textContent = message;
                
                // Add close button
                const closeBtn = document.createElement('span');
                closeBtn.innerHTML = '&times;';
                closeBtn.style.marginLeft = '10px';
                closeBtn.style.cursor = 'pointer';
                closeBtn.style.fontWeight = 'bold';
                closeBtn.onclick = function() {
                    document.body.removeChild(notification);
                };
                notification.appendChild(closeBtn);
                
                // Add to body
                document.body.appendChild(notification);
                
                // Auto remove after 5 seconds
                setTimeout(() => {
                    if (document.body.contains(notification)) {
                        document.body.removeChild(notification);
                    }
                }, 5000);
            };
        }
    });
</script>
{% endblock %}

================
File: app/templates/forgot_password.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG - Forgot Password</title>
    <link rel="stylesheet" href="/static/css/styles.css">
    <style>
        .forgot-password-container {
            max-width: 400px;
            margin: 100px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        .form-group {
            margin-bottom: 15px;
        }
        .form-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        .form-group input {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .btn {
            display: inline-block;
            padding: 10px 15px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            width: 100%;
        }
        .btn:hover {
            background-color: #0069d9;
        }
        .error-message {
            color: red;
            margin-bottom: 15px;
        }
        .success-message {
            color: green;
            margin-bottom: 15px;
        }
        .login-link {
            margin-top: 15px;
            text-align: center;
        }
        .login-link a {
            color: #007bff;
            text-decoration: none;
        }
        .login-link a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="forgot-password-container">
        <h2>Forgot Password</h2>
        <p>Enter your email address below and we'll send you a link to reset your password.</p>
        <div id="success-message" class="success-message" style="display: none;"></div>
        <div id="error-message" class="error-message"></div>
        <form id="forgot-password-form">
            <div class="form-group">
                <label for="email">Email</label>
                <input type="email" id="email" name="email" required>
            </div>
            <button type="submit" class="btn">Send Reset Link</button>
        </form>
        <div class="login-link">
            <p>Remember your password? <a href="/login">Login</a></p>
        </div>
    </div>

    <script>
        document.getElementById('forgot-password-form').addEventListener('submit', async function(e) {
            e.preventDefault();
            
            const email = document.getElementById('email').value;
            const errorMessage = document.getElementById('error-message');
            const successMessage = document.getElementById('success-message');
            
            errorMessage.textContent = '';
            successMessage.style.display = 'none';
            
            try {
                const response = await fetch('/api/password-reset/request-reset', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        email: email
                    })
                });
                
                const data = await response.json();
                
                if (response.ok) {
                    // Show success message
                    successMessage.textContent = data.message || 'Password reset link sent to your email.';
                    successMessage.style.display = 'block';
                    document.getElementById('forgot-password-form').style.display = 'none';
                } else {
                    // Display error message
                    errorMessage.textContent = data.detail || 'Failed to send reset link';
                }
            } catch (error) {
                console.error('Error:', error);
                errorMessage.textContent = 'An error occurred while processing your request';
            }
        });
    </script>
</body>
</html>

================
File: app/templates/login.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG - Login</title>
    <link rel="stylesheet" href="/static/css/styles.css">
    <style>
        .login-container {
            max-width: 400px;
            margin: 100px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        .form-group {
            margin-bottom: 15px;
        }
        .form-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        .form-group input {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .btn {
            display: inline-block;
            padding: 10px 15px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            width: 100%;
            margin-top: 10px;
        }
        .btn:hover {
            background-color: #0069d9;
        }
        .error-message {
            color: red;
            margin-bottom: 15px;
        }
        .success-message {
            color: green;
            margin-bottom: 15px;
        }
        .register-link, .forgot-password-link {
            margin-top: 15px;
            text-align: center;
        }
        .forgot-password-link a {
            color: #6c757d;
            text-decoration: none;
            font-size: 0.9em;
        }
        .forgot-password-link a:hover {
            text-decoration: underline;
        }
        #debug-info {
            margin-top: 20px;
            padding: 10px;
            background-color: #f8f9fa;
            border-radius: 4px;
            font-size: 0.8em;
            color: #6c757d;
            display: none;
        }
    </style>
</head>
<body>
    <div class="login-container">
        <h2>Login to Metis RAG</h2>
        <div id="success-message" class="success-message" style="display: none;"></div>
        <div id="error-message" class="error-message"></div>
        
        <!-- Security notice -->
        <div class="security-notice" style="margin-bottom: 15px; padding: 8px; background-color: #fff8e6; border-left: 4px solid #ffe066; font-size: 0.9em;">
            <strong>Security Notice:</strong> Never include your credentials in URLs or bookmarks. Always use this secure login form.
        </div>
        <!-- Specify the correct API endpoint for the form action -->
        <form id="login-form" method="post" action="/api/auth/token">
            <div class="form-group">
                <label for="username">Username</label>
                <input type="text" id="username" name="username" required>
            </div>
            <div class="form-group">
                <label for="password">Password</label>
                <input type="password" id="password" name="password" required>
            </div>
            <div class="forgot-password-link">
                <a href="/forgot-password">Forgot your password?</a>
            </div>
            <button type="submit" class="btn">Login</button>
        </form>
        <div class="register-link">
            <p>Don't have an account? <a href="/register">Register</a></p>
        </div>
        <div id="debug-info"></div>
    </div>
<!-- Include the external JavaScript file instead of inline script -->
<script src="/static/js/login_handler.js"></script>
</body>
</html>

================
File: app/templates/register.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG - Register</title>
    <link rel="stylesheet" href="/static/css/styles.css">
    <style>
        .register-container {
            max-width: 400px;
            margin: 100px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        .form-group {
            margin-bottom: 15px;
        }
        .form-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        .form-group input {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .btn {
            display: inline-block;
            padding: 10px 15px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        .btn:hover {
            background-color: #0069d9;
        }
        .error-message {
            color: red;
            margin-bottom: 15px;
        }
        .login-link {
            margin-top: 15px;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="register-container">
        <h2>Register for Metis RAG</h2>
        <div id="error-message" class="error-message"></div>
        <form id="register-form">
            <div class="form-group">
                <label for="username">Username</label>
                <input type="text" id="username" name="username" required>
            </div>
            <div class="form-group">
                <label for="email">Email</label>
                <input type="email" id="email" name="email" required>
            </div>
            <div class="form-group">
                <label for="full_name">Full Name</label>
                <input type="text" id="full_name" name="full_name">
            </div>
            <div class="form-group">
                <label for="password">Password</label>
                <input type="password" id="password" name="password" required>
            </div>
            <div class="form-group">
                <label for="confirm_password">Confirm Password</label>
                <input type="password" id="confirm_password" name="confirm_password" required>
            </div>
            <button type="submit" class="btn">Register</button>
        </form>
        <div class="login-link">
            <p>Already have an account? <a href="/login">Login</a></p>
        </div>
    </div>

    <script>
        document.getElementById('register-form').addEventListener('submit', async function(e) {
            e.preventDefault();
            
            const username = document.getElementById('username').value;
            const email = document.getElementById('email').value;
            const fullName = document.getElementById('full_name').value;
            const password = document.getElementById('password').value;
            const confirmPassword = document.getElementById('confirm_password').value;
            
            // Validate passwords match
            if (password !== confirmPassword) {
                document.getElementById('error-message').textContent = 'Passwords do not match';
                return;
            }
            
            try {
                const response = await fetch('/api/auth/register', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        username: username,
                        email: email,
                        full_name: fullName,
                        password: password,
                        is_active: true,
                        is_admin: false
                    })
                });
                
                const data = await response.json();
                
                if (response.ok) {
                    // Redirect to login page
                    window.location.href = '/login?registered=true';
                } else {
                    // Display error message
                    document.getElementById('error-message').textContent = data.detail || 'Registration failed';
                }
            } catch (error) {
                console.error('Error:', error);
                document.getElementById('error-message').textContent = 'An error occurred during registration';
            }
        });
    </script>
</body>
</html>

================
File: app/templates/reset_password.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG - Reset Password</title>
    <link rel="stylesheet" href="/static/css/styles.css">
    <style>
        .reset-password-container {
            max-width: 400px;
            margin: 100px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        .form-group {
            margin-bottom: 15px;
        }
        .form-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        .form-group input {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .btn {
            display: inline-block;
            padding: 10px 15px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        .btn:hover {
            background-color: #0069d9;
        }
        .error-message {
            color: red;
            margin-bottom: 15px;
        }
        .success-message {
            color: green;
            margin-bottom: 15px;
        }
        .login-link {
            margin-top: 15px;
            text-align: center;
        }
        .password-requirements {
            font-size: 0.8em;
            color: #666;
            margin-top: 5px;
        }
    </style>
</head>
<body>
    <div class="reset-password-container">
        <h2>Reset Password</h2>
        <div id="message" class="success-message" style="display: none;"></div>
        <div id="error-message" class="error-message"></div>
        <form id="reset-password-form">
            <input type="hidden" id="token" name="token" value="{{ token }}">
            <div class="form-group">
                <label for="password">New Password</label>
                <input type="password" id="password" name="password" required>
                <div class="password-requirements">
                    Password must be at least 8 characters long and include a mix of letters, numbers, and special characters.
                </div>
            </div>
            <div class="form-group">
                <label for="confirm_password">Confirm Password</label>
                <input type="password" id="confirm_password" name="confirm_password" required>
            </div>
            <button type="submit" class="btn">Reset Password</button>
        </form>
        <div class="login-link" id="login-link" style="display: none;">
            <p>Password reset successful! <a href="/login">Login</a></p>
        </div>
    </div>

    <script>
        document.getElementById('reset-password-form').addEventListener('submit', async function(e) {
            e.preventDefault();
            
            const token = document.getElementById('token').value;
            const password = document.getElementById('password').value;
            const confirmPassword = document.getElementById('confirm_password').value;
            const errorMessage = document.getElementById('error-message');
            const message = document.getElementById('message');
            const loginLink = document.getElementById('login-link');
            
            errorMessage.textContent = '';
            message.style.display = 'none';
            loginLink.style.display = 'none';
            
            // Validate password
            if (password.length < 8) {
                errorMessage.textContent = 'Password must be at least 8 characters long';
                return;
            }
            
            // Validate passwords match
            if (password !== confirmPassword) {
                errorMessage.textContent = 'Passwords do not match';
                return;
            }
            
            try {
                const response = await fetch('/api/password-reset/reset-password', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        token: token,
                        password: password,
                        confirm_password: confirmPassword
                    })
                });
                
                const data = await response.json();
                
                if (response.ok) {
                    // Show success message
                    message.textContent = data.message;
                    message.style.display = 'block';
                    document.getElementById('reset-password-form').style.display = 'none';
                    loginLink.style.display = 'block';
                } else {
                    // Display error message
                    errorMessage.textContent = data.detail || 'An error occurred';
                }
            } catch (error) {
                console.error('Error:', error);
                errorMessage.textContent = 'An error occurred while processing your request';
            }
        });
    </script>
</body>
</html>

================
File: app/templates/schema.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Database Schema Viewer - Metis RAG</title>
    <link rel="stylesheet" href="/static/css/main.css">
    <link rel="stylesheet" href="/static/css/schema.css">
    <script src="/static/js/auth.js" defer></script>
    <script src="/static/js/schema.js" defer></script>
</head>
<body>
    <header>
        <div class="logo">
            <img src="/static/img/logo.svg" alt="Metis RAG Logo">
            <h1>Metis RAG</h1>
        </div>
        <nav>
            <ul>
                <li><a href="/">Chat</a></li>
                <li><a href="/documents">Documents</a></li>
                <li><a href="/system">System</a></li>
                <li><a href="/analytics">Analytics</a></li>
                <li><a href="/schema" class="active">Schema</a></li>
                <li><a href="/admin">Admin</a></li>
                <li><a href="#" id="logout-link">Logout</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <div class="container">
            <h1>Database Schema Viewer</h1>
            <p>Explore and analyze your PostgreSQL database schema.</p>

            <div class="schema-controls">
                <div class="connection-selector">
                    <label for="connection-select">Database Connection:</label>
                    <select id="connection-select">
                        <option value="">Select a connection...</option>
                    </select>
                    <button id="refresh-connections">Refresh</button>
                </div>

                <div class="schema-selector">
                    <label for="schema-select">Schema:</label>
                    <select id="schema-select" disabled>
                        <option value="">Select a schema...</option>
                    </select>
                </div>

                <div class="table-selector">
                    <label for="table-select">Table:</label>
                    <select id="table-select" disabled>
                        <option value="">Select a table...</option>
                    </select>
                </div>
            </div>

            <div class="tabs">
                <button class="tab-button active" data-tab="structure">Structure</button>
                <button class="tab-button" data-tab="columns">Columns</button>
                <button class="tab-button" data-tab="indexes">Indexes</button>
                <button class="tab-button" data-tab="constraints">Constraints</button>
                <button class="tab-button" data-tab="foreign-keys">Foreign Keys</button>
                <button class="tab-button" data-tab="explain">Query Explain</button>
            </div>

            <div class="tab-content">
                <div id="structure" class="tab-pane active">
                    <h2>Table Structure</h2>
                    <div class="table-info">
                        <div class="info-item">
                            <span class="label">Table Name:</span>
                            <span class="value" id="table-name"></span>
                        </div>
                        <div class="info-item">
                            <span class="label">Description:</span>
                            <span class="value" id="table-description"></span>
                        </div>
                        <div class="info-item">
                            <span class="label">Owner:</span>
                            <span class="value" id="table-owner"></span>
                        </div>
                        <div class="info-item">
                            <span class="label">Row Count:</span>
                            <span class="value" id="table-row-count"></span>
                        </div>
                        <div class="info-item">
                            <span class="label">Size:</span>
                            <span class="value" id="table-size"></span>
                        </div>
                    </div>
                    <div id="structure-content" class="content-area"></div>
                </div>

                <div id="columns" class="tab-pane">
                    <h2>Columns</h2>
                    <div id="columns-content" class="content-area"></div>
                </div>

                <div id="indexes" class="tab-pane">
                    <h2>Indexes</h2>
                    <div id="indexes-content" class="content-area"></div>
                </div>

                <div id="constraints" class="tab-pane">
                    <h2>Constraints</h2>
                    <div id="constraints-content" class="content-area"></div>
                </div>

                <div id="foreign-keys" class="tab-pane">
                    <h2>Foreign Keys</h2>
                    <div id="foreign-keys-content" class="content-area"></div>
                </div>

                <div id="explain" class="tab-pane">
                    <h2>Query Explain</h2>
                    <div class="explain-controls">
                        <textarea id="query-input" placeholder="Enter SQL query to explain..."></textarea>
                        <div class="explain-options">
                            <label>
                                <input type="radio" name="explain-type" value="simple" checked> Simple
                            </label>
                            <label>
                                <input type="radio" name="explain-type" value="analyze"> Analyze
                            </label>
                            <label>
                                <input type="radio" name="explain-type" value="verbose"> Verbose
                            </label>
                            <label>
                                <input type="radio" name="explain-type" value="analyze_verbose"> Analyze + Verbose
                            </label>
                            <label>
                                <input type="radio" name="explain-type" value="json"> JSON
                            </label>
                        </div>
                        <button id="explain-button">Explain Query</button>
                    </div>
                    <div id="explain-content" class="content-area"></div>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2025 Metis RAG. All rights reserved.</p>
    </footer>

    <div id="loading-overlay" class="hidden">
        <div class="spinner"></div>
        <p>Loading...</p>
    </div>

    <div id="error-modal" class="modal hidden">
        <div class="modal-content">
            <span class="close">&times;</span>
            <h2>Error</h2>
            <p id="error-message"></p>
        </div>
    </div>
</body>
</html>

================
File: app/templates/system.html
================
{% extends "base.html" %}

{% block title %}System - Metis RAG{% endblock %}

{% block head %}
<style>
    .system-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }

    .stats-section {
        margin-bottom: 30px;
    }

    .stats-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 20px;
        margin-top: 20px;
    }

    .stat-card {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 15px;
        background-color: white;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        text-align: center;
    }

    .stat-card h3 {
        margin-top: 0;
        color: var(--secondary-color);
    }

    .stat-value {
        font-size: 2em;
        font-weight: bold;
        color: var(--primary-color);
        margin: 10px 0;
    }

    .models-section {
        margin-bottom: 30px;
    }

    .model-list {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
        gap: 20px;
        margin-top: 20px;
    }

    .model-card {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 15px;
        background-color: white;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    .model-card h3 {
        margin-top: 0;
        margin-bottom: 10px;
    }

    .model-meta {
        font-size: 0.9em;
        color: #777;
        margin-bottom: 10px;
    }

    .health-section {
        margin-bottom: 30px;
    }

    .health-status {
        display: flex;
        flex-direction: column;
        gap: 15px;
        margin-top: 20px;
    }

    .health-item {
        display: flex;
        align-items: center;
        padding: 15px;
        border-radius: 5px;
        background-color: white;
        border: 1px solid #ddd;
    }

    .health-indicator {
        width: 15px;
        height: 15px;
        border-radius: 50%;
        margin-right: 15px;
    }

    .health-indicator.healthy {
        background-color: var(--success-color);
    }

    .health-indicator.unhealthy {
        background-color: var(--error-color);
    }

    .health-indicator.unknown {
        background-color: var(--warning-color);
    }

    .refresh-btn {
        margin-left: auto;
    }
    
    .admin-section {
        margin-bottom: 30px;
        padding: 15px;
        border: 1px solid #ddd;
        border-radius: 5px;
        background-color: #f8f9fa;
    }
    
    .admin-section h2 {
        margin-top: 0;
    }
    
    .admin-link {
        display: inline-block;
        margin-top: 10px;
        padding: 8px 15px;
        background-color: var(--primary-color);
        color: white;
        text-decoration: none;
        border-radius: 4px;
    }
    
    .admin-link:hover {
        background-color: var(--primary-color-dark);
    }
</style>
{% endblock %}

{% block content %}
<div class="system-container">
    <div id="admin-section" class="admin-section" style="display: none;">
        <h2>Administration</h2>
        <p>Access the admin dashboard to manage users and system settings.</p>
        <a href="/admin" class="admin-link">Admin Dashboard</a>
    </div>

    <div class="stats-section">
        <h2>System Statistics</h2>
        <div class="stats-grid" id="stats-grid">
            <div class="stat-card">
                <h3>Documents</h3>
                <div class="stat-value" id="docs-count">-</div>
            </div>
            <div class="stat-card">
                <h3>Chunks</h3>
                <div class="stat-value" id="chunks-count">-</div>
            </div>
            <div class="stat-card">
                <h3>Vector Store Entries</h3>
                <div class="stat-value" id="vectors-count">-</div>
            </div>
            <div class="stat-card">
                <h3>Available Models</h3>
                <div class="stat-value" id="models-count">-</div>
            </div>
        </div>
    </div>

    <div class="health-section">
        <h2>System Health</h2>
        <div class="health-status" id="health-status">
            <div class="health-item">
                <div class="health-indicator unknown"></div>
                <div class="health-name">Overall Health</div>
                <div class="health-details" id="health-overall">Checking...</div>
                <button class="refresh-btn" id="refresh-health-btn">Refresh</button>
            </div>
            <div class="health-item">
                <div class="health-indicator unknown"></div>
                <div class="health-name">Ollama</div>
                <div class="health-details" id="health-ollama">Checking...</div>
            </div>
            <div class="health-item">
                <div class="health-indicator unknown"></div>
                <div class="health-name">Vector DB</div>
                <div class="health-details" id="health-vectordb">Checking...</div>
            </div>
        </div>
    </div>

    <div class="models-section">
        <h2>Available Models</h2>
        <div class="model-list" id="model-list">
            <div class="model-loading">Loading models...</div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const statsGrid = document.getElementById('stats-grid');
        const modelList = document.getElementById('model-list');
        const refreshHealthBtn = document.getElementById('refresh-health-btn');
        const adminSection = document.getElementById('admin-section');
        
        // Check if user is admin
        checkAdminStatus();
        
        // Load system stats
        loadSystemStats();
        
        // Load models
        loadModels();
        
        // Check health
        checkHealth();
        
        // Refresh health
        refreshHealthBtn.addEventListener('click', checkHealth);
        
        // Check if user is admin
        async function checkAdminStatus() {
            try {
                const token = localStorage.getItem('access_token');
                if (!token) {
                    return;
                }
                
                const response = await fetch('/api/auth/me', {
                    headers: {
                        'Authorization': `Bearer ${token}`
                    }
                });
                
                if (response.ok) {
                    const user = await response.json();
                    if (user.is_admin) {
                        adminSection.style.display = 'block';
                    }
                }
            } catch (error) {
                console.error('Error checking admin status:', error);
            }
        }
        
        // Load system stats
        function loadSystemStats() {
            fetch('/api/system/stats')
                .then(response => response.json())
                .then(stats => {
                    document.getElementById('docs-count').textContent = stats.documents_count;
                    document.getElementById('chunks-count').textContent = stats.total_chunks;
                    document.getElementById('vectors-count').textContent = stats.vector_store_size || '0';
                    document.getElementById('models-count').textContent = stats.available_models.length;
                })
                .catch(error => {
                    console.error('Error loading system stats:', error);
                });
        }
        
        // Load models
        function loadModels() {
            fetch('/api/system/models')
                .then(response => response.json())
                .then(models => {
                    modelList.innerHTML = '';
                    
                    if (models.length === 0) {
                        modelList.innerHTML = '<div class="model-empty">No models found</div>';
                        return;
                    }
                    
                    models.forEach(model => {
                        const modelEl = createModelElement(model);
                        modelList.appendChild(modelEl);
                    });
                })
                .catch(error => {
                    console.error('Error loading models:', error);
                    modelList.innerHTML = '<div class="model-error">Error loading models</div>';
                });
        }
        
        // Create model element
        function createModelElement(model) {
            const modelEl = document.createElement('div');
            modelEl.className = 'model-card';
            
            const modified = model.modified_at ? new Date(model.modified_at).toLocaleDateString() : 'Unknown';
            const size = model.size ? formatBytes(model.size) : 'Unknown';
            
            modelEl.innerHTML = `
                <h3>${model.name}</h3>
                <div class="model-meta">
                    <div>Size: ${size}</div>
                    <div>Modified: ${modified}</div>
                </div>
                <div>${model.description || ''}</div>
            `;
            
            return modelEl;
        }
        
        // Check health
        function checkHealth() {
            // Reset indicators
            const indicators = document.querySelectorAll('.health-indicator');
            indicators.forEach(ind => {
                ind.className = 'health-indicator unknown';
            });
            
            document.getElementById('health-overall').textContent = 'Checking...';
            document.getElementById('health-ollama').textContent = 'Checking...';
            document.getElementById('health-vectordb').textContent = 'Checking...';
            
            // Fetch health
            fetch('/api/system/health')
                .then(response => response.json())
                .then(health => {
                    // Update overall health
                    const overallIndicator = document.querySelector('.health-item:nth-child(1) .health-indicator');
                    overallIndicator.className = `health-indicator ${health.status}`;
                    document.getElementById('health-overall').textContent = health.status.charAt(0).toUpperCase() + health.status.slice(1);
                    
                    // Update Ollama health
                    const ollamaIndicator = document.querySelector('.health-item:nth-child(2) .health-indicator');
                    ollamaIndicator.className = `health-indicator ${health.ollama_status}`;
                    document.getElementById('health-ollama').textContent = health.ollama_status.charAt(0).toUpperCase() + health.ollama_status.slice(1);
                    
                    // Update Vector DB health
                    const vectordbIndicator = document.querySelector('.health-item:nth-child(3) .health-indicator');
                    vectordbIndicator.className = `health-indicator ${health.vector_db_status}`;
                    document.getElementById('health-vectordb').textContent = health.vector_db_status.charAt(0).toUpperCase() + health.vector_db_status.slice(1);
                })
                .catch(error => {
                    console.error('Error checking health:', error);
                    document.getElementById('health-overall').textContent = 'Error checking health';
                });
        }
        
        // Format bytes
        function formatBytes(bytes, decimals = 2) {
            if (bytes === 0) return '0 Bytes';
            
            const k = 1024;
            const dm = decimals < 0 ? 0 : decimals;
            const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'];
            
            const i = Math.floor(Math.log(bytes) / Math.log(k));
            
            return parseFloat((bytes / Math.pow(k, i)).toFixed(dm)) + ' ' + sizes[i];
        }
    });
</script>
{% endblock %}

================
File: app/templates/tasks.html
================
{% extends "base.html" %}

{% block title %}Background Tasks - Metis RAG{% endblock %}

{% block head %}
<link rel="stylesheet" href="{{ url_for('static', path='css/tasks.css') }}">
{% endblock %}

{% block sidebar %}
<div class="sidebar-nav">
    <div class="sidebar-section">
        <h3>Navigation</h3>
        <ul class="nav-list">
            <li><a href="/"><i class="fas fa-comment"></i> Chat</a></li>
            <li><a href="/documents"><i class="fas fa-file-alt"></i> Documents</a></li>
            <li><a href="/tasks" class="active"><i class="fas fa-tasks"></i> Background Tasks</a></li>
            <li><a href="/analytics"><i class="fas fa-chart-bar"></i> Analytics</a></li>
            <li><a href="/system"><i class="fas fa-cogs"></i> System</a></li>
        </ul>
    </div>
    
    <div class="sidebar-section">
        <h3>Task Filters</h3>
        <div class="filter-group">
            <label>Status</label>
            <select id="status-filter" class="form-control">
                <option value="">All Statuses</option>
                <option value="pending">Pending</option>
                <option value="scheduled">Scheduled</option>
                <option value="running">Running</option>
                <option value="completed">Completed</option>
                <option value="failed">Failed</option>
                <option value="cancelled">Cancelled</option>
            </select>
        </div>
        
        <div class="filter-group">
            <label>Task Type</label>
            <select id="type-filter" class="form-control">
                <option value="">All Types</option>
                <option value="document_processing">Document Processing</option>
                <option value="vector_store_update">Vector Store Update</option>
                <option value="report_generation">Report Generation</option>
                <option value="system_maintenance">System Maintenance</option>
            </select>
        </div>
        
        <div class="filter-actions">
            <button id="apply-filters" class="btn btn-primary">Apply Filters</button>
            <button id="clear-filters" class="btn btn-secondary">Clear</button>
        </div>
    </div>
    
    <div class="sidebar-section">
        <h3>Create Task</h3>
        <button id="create-task-btn" class="btn btn-success w-100" data-bs-toggle="modal" data-bs-target="#createTaskModal">
            <i class="fas fa-plus"></i> New Task
        </button>
    </div>
    
    <div class="sidebar-section">
        <h3>Auto-Refresh</h3>
        <div class="form-check form-switch">
            <input class="form-check-input" type="checkbox" id="auto-refresh" checked>
            <label class="form-check-label" for="auto-refresh">Enable auto-refresh</label>
        </div>
        <div class="refresh-interval">
            <label>Refresh interval:</label>
            <select id="refresh-interval" class="form-control">
                <option value="5000">5 seconds</option>
                <option value="10000">10 seconds</option>
                <option value="30000">30 seconds</option>
                <option value="60000">1 minute</option>
            </select>
        </div>
    </div>
</div>
{% endblock %}

{% block content %}
<div class="container-fluid">
    <div class="row mb-4">
        <div class="col-12">
            <h1 class="mb-4">Background Tasks</h1>
            
            <div class="card">
                <div class="card-header">
                    <h5 class="card-title mb-0">Task Statistics</h5>
                </div>
                <div class="card-body">
                    <div class="row">
                        <div class="col-md-6">
                            <div class="row">
                                <div class="col-6">
                                    <div class="card bg-light mb-3">
                                        <div class="card-body text-center">
                                            <h5 class="card-title">Pending</h5>
                                            <h2 id="pending-count">-</h2>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-6">
                                    <div class="card bg-info text-white mb-3">
                                        <div class="card-body text-center">
                                            <h5 class="card-title">Running</h5>
                                            <h2 id="running-count">-</h2>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-6">
                                    <div class="card bg-success text-white mb-3">
                                        <div class="card-body text-center">
                                            <h5 class="card-title">Completed</h5>
                                            <h2 id="completed-count">-</h2>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-6">
                                    <div class="card bg-danger text-white mb-3">
                                        <div class="card-body text-center">
                                            <h5 class="card-title">Failed</h5>
                                            <h2 id="failed-count">-</h2>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="card mb-3">
                                <div class="card-body">
                                    <h5 class="card-title">System Load</h5>
                                    <div class="progress mb-3" style="height: 25px;">
                                        <div id="system-load-bar" class="progress-bar" role="progressbar" style="width: 0%;" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
                                    </div>
                                    <div class="row">
                                        <div class="col-6">
                                            <p><strong>CPU:</strong> <span id="cpu-usage">-</span></p>
                                            <p><strong>Memory:</strong> <span id="memory-usage">-</span></p>
                                        </div>
                                        <div class="col-6">
                                            <p><strong>Disk:</strong> <span id="disk-usage">-</span></p>
                                            <p><strong>I/O Wait:</strong> <span id="io-wait">-</span></p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="row mb-4">
        <div class="col-12">
            <div class="card">
                <div class="card-header d-flex justify-content-between align-items-center">
                    <h5 class="card-title mb-0">Task List</h5>
                    <div>
                        <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#createTaskModal">
                            <i class="fas fa-plus"></i> New Task
                        </button>
                        <div class="btn-group ms-2">
                            <button type="button" class="btn btn-outline-secondary dropdown-toggle" data-bs-toggle="dropdown" aria-expanded="false">
                                <span id="status-filter-text">All Statuses</span>
                            </button>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item status-filter" href="#" data-status="">All Statuses</a></li>
                                <li><a class="dropdown-item status-filter" href="#" data-status="pending">Pending</a></li>
                                <li><a class="dropdown-item status-filter" href="#" data-status="scheduled">Scheduled</a></li>
                                <li><a class="dropdown-item status-filter" href="#" data-status="running">Running</a></li>
                                <li><a class="dropdown-item status-filter" href="#" data-status="completed">Completed</a></li>
                                <li><a class="dropdown-item status-filter" href="#" data-status="failed">Failed</a></li>
                                <li><a class="dropdown-item status-filter" href="#" data-status="cancelled">Cancelled</a></li>
                            </ul>
                        </div>
                    </div>
                </div>
                <div class="card-body">
                    <div class="table-responsive">
                        <table class="table table-striped table-hover">
                            <thead>
                                <tr>
                                    <th>ID</th>
                                    <th>Name</th>
                                    <th>Type</th>
                                    <th>Status</th>
                                    <th>Priority</th>
                                    <th>Progress</th>
                                    <th>Created</th>
                                    <th>Actions</th>
                                </tr>
                            </thead>
                            <tbody id="task-list">
                                <tr>
                                    <td colspan="8" class="text-center">Loading tasks...</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <nav>
                        <ul class="pagination justify-content-center" id="pagination">
                            <li class="page-item disabled">
                                <a class="page-link" href="#" tabindex="-1">Previous</a>
                            </li>
                            <li class="page-item active"><a class="page-link" href="#">1</a></li>
                            <li class="page-item disabled">
                                <a class="page-link" href="#">Next</a>
                            </li>
                        </ul>
                    </nav>
                </div>
            </div>
        </div>
    </div>

    <div class="row mb-4">
        <div class="col-12">
            <div class="card">
                <div class="card-header">
                    <h5 class="card-title mb-0">Resource Alerts</h5>
                </div>
                <div class="card-body">
                    <div class="table-responsive">
                        <table class="table table-striped">
                            <thead>
                                <tr>
                                    <th>Time</th>
                                    <th>Resource</th>
                                    <th>Value</th>
                                    <th>Threshold</th>
                                    <th>Message</th>
                                </tr>
                            </thead>
                            <tbody id="alerts-list">
                                <tr>
                                    <td colspan="5" class="text-center">No alerts</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Create Task Modal -->
<div class="modal fade" id="createTaskModal" tabindex="-1" aria-labelledby="createTaskModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="createTaskModalLabel">Create New Task</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <form id="create-task-form">
                    <div class="mb-3">
                        <label for="task-name" class="form-label">Task Name</label>
                        <input type="text" class="form-control" id="task-name" required>
                    </div>
                    <div class="mb-3">
                        <label for="task-type" class="form-label">Task Type</label>
                        <select class="form-select" id="task-type" required>
                            <option value="">Select Task Type</option>
                            <option value="document_processing">Document Processing</option>
                            <option value="vector_store_update">Vector Store Update</option>
                            <option value="report_generation">Report Generation</option>
                            <option value="system_maintenance">System Maintenance</option>
                        </select>
                    </div>
                    <div class="mb-3">
                        <label for="task-priority" class="form-label">Priority</label>
                        <select class="form-select" id="task-priority">
                            <option value="normal">Normal</option>
                            <option value="low">Low</option>
                            <option value="high">High</option>
                            <option value="critical">Critical</option>
                        </select>
                    </div>
                    <div class="mb-3">
                        <label for="task-params" class="form-label">Parameters (JSON)</label>
                        <textarea class="form-control" id="task-params" rows="5"></textarea>
                        <div class="form-text">Enter task parameters in JSON format.</div>
                    </div>
                </form>
            </div>
            <div class="modal-footer">
                <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Cancel</button>
                <button type="button" class="btn btn-primary" id="submit-task">Create Task</button>
            </div>
        </div>
    </div>
</div>

<!-- Task Details Modal -->
<div class="modal fade" id="taskDetailsModal" tabindex="-1" aria-labelledby="taskDetailsModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="taskDetailsModalLabel">Task Details</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <div class="row mb-3">
                    <div class="col-md-6">
                        <p><strong>ID:</strong> <span id="detail-id"></span></p>
                        <p><strong>Name:</strong> <span id="detail-name"></span></p>
                        <p><strong>Type:</strong> <span id="detail-type"></span></p>
                        <p><strong>Status:</strong> <span id="detail-status"></span></p>
                        <p><strong>Priority:</strong> <span id="detail-priority"></span></p>
                    </div>
                    <div class="col-md-6">
                        <p><strong>Created:</strong> <span id="detail-created"></span></p>
                        <p><strong>Started:</strong> <span id="detail-started"></span></p>
                        <p><strong>Completed:</strong> <span id="detail-completed"></span></p>
                        <p><strong>Execution Time:</strong> <span id="detail-execution-time"></span></p>
                        <p><strong>Retries:</strong> <span id="detail-retries"></span></p>
                    </div>
                </div>
                <div class="progress mb-3" style="height: 25px;">
                    <div id="detail-progress-bar" class="progress-bar" role="progressbar" style="width: 0%;" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
                </div>
                <div class="mb-3">
                    <h6>Parameters</h6>
                    <pre id="detail-params" class="bg-light p-3 rounded"></pre>
                </div>
                <div class="mb-3">
                    <h6>Result</h6>
                    <pre id="detail-result" class="bg-light p-3 rounded"></pre>
                </div>
                <div class="mb-3" id="detail-error-container" style="display: none;">
                    <h6>Error</h6>
                    <pre id="detail-error" class="bg-danger text-white p-3 rounded"></pre>
                </div>
            </div>
            <div class="modal-footer">
                <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
                <button type="button" class="btn btn-danger" id="cancel-task" style="display: none;">Cancel Task</button>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script src="{{ url_for('static', path='js/tasks.js') }}"></script>
{% endblock %}

================
File: app/templates/test_models.html
================
{% extends "base.html" %}

{% block title %}Test Models - Metis RAG{% endblock %}

{% block head %}
<style>
    .test-container {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
    }
    .model-list {
        margin-top: 20px;
        border: 1px solid #ccc;
        padding: 10px;
        border-radius: 5px;
    }
    .model-item {
        padding: 5px;
        margin: 5px 0;
        background-color: #f5f5f5;
        border-radius: 3px;
    }
    .test-dropdown {
        width: 100%;
        padding: 8px;
        margin-top: 10px;
        border-radius: 4px;
        border: 1px solid #ccc;
    }
</style>
{% endblock %}

{% block content %}
<div class="test-container">
    <h1>Test Models</h1>
    
    <div>
        <h2>Model Selection Dropdown</h2>
        <p>This dropdown should be populated with all available models:</p>
        <select id="test-model-select" class="test-dropdown">
            <option value="loading">Loading models...</option>
        </select>
    </div>
    
    <div>
        <h2>Model List</h2>
        <p>This list should show all available models:</p>
        <div id="model-list" class="model-list">
            <div>Loading models...</div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script src="{{ url_for('static', path='js/test_models.js') }}"></script>
{% endblock %}

================
File: app/utils/__init__.py
================
from app.utils.file_utils import validate_file, save_upload_file, delete_document_files
from app.utils.text_utils import extract_citations, truncate_text, clean_text

================
File: app/utils/email.py
================
import logging
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from app.core.config import SETTINGS

logger = logging.getLogger(__name__)

async def send_password_reset_email(to_email: str, username: str, reset_url: str):
    """
    Send a password reset email
    
    Args:
        to_email: Recipient email
        username: Recipient username
        reset_url: Password reset URL
    """
    try:
        # Create message
        message = MIMEMultipart()
        message["From"] = SETTINGS.smtp_sender
        message["To"] = to_email
        message["Subject"] = "Metis RAG - Password Reset"
        
        # Create HTML content
        html = f"""
        <html>
        <head>
            <style>
                body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }}
                .container {{ max-width: 600px; margin: 0 auto; padding: 20px; }}
                .header {{ background-color: #007bff; color: white; padding: 10px 20px; text-align: center; }}
                .content {{ padding: 20px; border: 1px solid #ddd; border-top: none; }}
                .button {{ display: inline-block; background-color: #007bff; color: white; text-decoration: none; padding: 10px 20px; border-radius: 5px; margin-top: 20px; }}
                .footer {{ margin-top: 20px; font-size: 12px; color: #777; text-align: center; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>Password Reset</h1>
                </div>
                <div class="content">
                    <p>Hello {username},</p>
                    <p>We received a request to reset your password for your Metis RAG account. If you didn't make this request, you can ignore this email.</p>
                    <p>To reset your password, click the button below:</p>
                    <p><a href="{reset_url}" class="button">Reset Password</a></p>
                    <p>Or copy and paste this URL into your browser:</p>
                    <p>{reset_url}</p>
                    <p>This link will expire in 24 hours.</p>
                    <p>Thank you,<br>The Metis RAG Team</p>
                </div>
                <div class="footer">
                    <p>This is an automated message, please do not reply to this email.</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # Attach HTML content
        message.attach(MIMEText(html, "html"))
        
        # Connect to SMTP server
        with smtplib.SMTP(SETTINGS.smtp_server, SETTINGS.smtp_port) as server:
            if SETTINGS.smtp_use_tls:
                server.starttls()
            
            if SETTINGS.smtp_username and SETTINGS.smtp_password:
                server.login(SETTINGS.smtp_username, SETTINGS.smtp_password)
            
            # Send email
            server.send_message(message)
        
        logger.info(f"Password reset email sent to {to_email}")
    except Exception as e:
        logger.error(f"Error sending password reset email: {str(e)}")
        # Don't raise the exception to prevent leaking information

================
File: app/utils/file_utils.py
================
import os
import logging
import shutil
from pathlib import Path
from typing import List, Optional, Set
from fastapi import UploadFile

from app.core.config import UPLOAD_DIR

logger = logging.getLogger("app.utils.file_utils")

# Set of allowed file extensions with max size in MB
ALLOWED_EXTENSIONS = {
    ".pdf": 20,    # 20MB max for PDFs
    ".txt": 10,    # 10MB max for text files
    ".csv": 15,    # 15MB max for CSV files
    ".md": 10,     # 10MB max for markdown files
    ".docx": 20,   # 20MB max for Word documents
    ".doc": 20,    # 20MB max for older Word documents
    ".rtf": 15,    # 15MB max for rich text files
    ".html": 10,   # 10MB max for HTML files
    ".json": 10,   # 10MB max for JSON files
    ".xml": 10     # 10MB max for XML files
}

# Default max file size in bytes (10MB)
DEFAULT_MAX_FILE_SIZE = 10 * 1024 * 1024

async def validate_file(file: UploadFile) -> tuple[bool, str]:
    """
    Enhanced file validation with detailed error messages
    Returns a tuple of (is_valid, error_message)
    """
    # Get file extension
    _, ext = os.path.splitext(file.filename.lower())
    
    # Check if extension is allowed
    if ext not in ALLOWED_EXTENSIONS:
        error_msg = f"File type {ext} is not allowed. Supported types: {', '.join(ALLOWED_EXTENSIONS)}"
        logger.warning(error_msg)
        return False, error_msg
    
    # Get max file size for this extension
    max_file_size = ALLOWED_EXTENSIONS.get(ext, DEFAULT_MAX_FILE_SIZE) * 1024 * 1024
    
    # Check file size
    try:
        # Save current position
        current_position = await file.tell()
        
        # Move to end to get size
        await file.seek(0, 2)  # Seek to end
        file_size = await file.tell()
        
        # Reset position
        await file.seek(current_position)
        
        if file_size > max_file_size:
            error_msg = f"File exceeds maximum size of {max_file_size/(1024*1024):.1f}MB"
            logger.warning(error_msg)
            return False, error_msg
            
        # Basic content validation for specific file types
        if ext == ".pdf":
            # Save current position
            current_position = await file.tell()
            
            # Check PDF header
            await file.seek(0)
            header = await file.read(5)
            
            # Reset position
            await file.seek(current_position)
            
            if header != b"%PDF-":
                error_msg = "Invalid PDF file format"
                logger.warning(error_msg)
                return False, error_msg
                
    except Exception as e:
        error_msg = f"Error validating file: {str(e)}"
        logger.error(error_msg)
        return False, error_msg
    
    return True, ""

async def save_upload_file(file: UploadFile, document_id: str) -> str:
    """
    Save an uploaded file to the upload directory
    """
    try:
        # Create directory for the document
        document_dir = os.path.join(UPLOAD_DIR, document_id)
        os.makedirs(document_dir, exist_ok=True)
        
        # Define file path
        file_path = os.path.join(document_dir, file.filename)
        
        # Save file
        with open(file_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        logger.info(f"File saved to {file_path}")
        return file_path
    except Exception as e:
        logger.error(f"Error saving uploaded file: {str(e)}")
        raise
    finally:
        # Make sure to close the file
        await file.close()

def delete_document_files(document_id: str) -> None:
    """
    Delete document files
    """
    try:
        # Get document directory
        document_dir = os.path.join(UPLOAD_DIR, document_id)
        
        # Check if directory exists
        if os.path.exists(document_dir):
            # Delete directory and all its contents
            shutil.rmtree(document_dir)
            logger.info(f"Deleted document files for {document_id}")
        else:
            logger.warning(f"Document directory for {document_id} does not exist")
    except Exception as e:
        logger.error(f"Error deleting document files: {str(e)}")
        raise

================
File: app/utils/text_processor.py
================
import re

def normalize_text(text):
    """
    Normalize text for better formatting and readability.
    
    This function fixes common formatting issues:
    - Adds proper spacing around punctuation
    - Replaces straight apostrophes with curly ones
    - Fixes hyphenation in compound words
    - Removes spaces in function/variable names
    
    Args:
        text: The input text to normalize
        
    Returns:
        Normalized text with improved formatting
    """
    if not text:
        return text
        
    # Fix spacing around punctuation
    text = re.sub(r'([.!?,:;])([A-Za-z0-9])', r'\1 \2', text)
    
    # Fix missing spaces after punctuation
    text = re.sub(r'([A-Za-z0-9])([.!?,:;])', r'\1\2 ', text)
    
    # Fix apostrophes
    text = text.replace("'", "'")
    
    # Fix hyphenation in common terms (remove spaces around hyphens)
    text = re.sub(r'(\w+) - (\w+)', r'\1-\2', text)
    text = re.sub(r'(\w+) - (\w+) - (\w+)', r'\1-\2-\3', text)
    
    # Fix function names with spaces
    text = re.sub(r'([a-z]+) _ ([a-z]+)', r'\1_\2', text)
    
    # Fix multiple spaces
    text = re.sub(r' +', ' ', text)
    
    # Fix spacing in code blocks (preserve indentation)
    lines = text.split('\n')
    for i, line in enumerate(lines):
        # Only process non-indented lines or lines that aren't code
        if not line.startswith('    ') and not line.startswith('\t'):
            lines[i] = line
    
    return '\n'.join(lines)

def format_code_blocks(text):
    """
    Properly format code blocks in text.
    
    This function:
    - Ensures proper indentation in code blocks
    - Fixes function and variable names with spaces
    - Maintains consistent formatting
    
    Args:
        text: The input text containing code blocks
        
    Returns:
        Text with properly formatted code blocks
    """
    if not text:
        return text
        
    # Identify code blocks (between triple backticks)
    code_block_pattern = r'```(?:python)?(.*?)```'
    
    def process_code_block(match):
        code = match.group(1)
        
        # Fix function names with spaces
        code = re.sub(r'([a-z]+) _ ([a-z]+)', r'\1_\2', code)
        
        # Fix variable names with spaces
        code = re.sub(r'([a-z]+) _ ([a-z]+)', r'\1_\2', code)
        
        return f'```python{code}```'
    
    # Process all code blocks
    processed_text = re.sub(code_block_pattern, process_code_block, text, flags=re.DOTALL)
    
    return processed_text

================
File: app/utils/text_utils.py
================
import re
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger("app.utils.text_utils")

def extract_citations(text: str) -> List[int]:
    """
    Extract citation numbers from text
    
    Example:
    "According to [1] and also mentioned in [3], the study shows..."
    Returns: [1, 3]
    """
    try:
        # Find all citations in the format [number]
        citations = re.findall(r'\[(\d+)\]', text)
        
        # Convert to integers and remove duplicates
        citation_numbers = list(set(int(c) for c in citations))
        
        # Sort by number
        citation_numbers.sort()
        
        return citation_numbers
    except Exception as e:
        logger.error(f"Error extracting citations: {str(e)}")
        return []

def truncate_text(text: str, max_length: int = 100) -> str:
    """
    Truncate text to a maximum length
    """
    if len(text) <= max_length:
        return text
    return text[:max_length] + "..."

def clean_text(text: str) -> str:
    """
    Clean text by removing extra whitespace, etc.
    """
    # Remove multiple newlines
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # Remove multiple spaces
    text = re.sub(r' {2,}', ' ', text)
    
    # Trim whitespace
    text = text.strip()
    
    return text

================
File: app/main.py
================
import logging
import os
from fastapi import FastAPI, Request, status
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

from app.core.config import API_V1_STR, PROJECT_NAME, SETTINGS
from app.core.security import setup_security
from app.core.logging import setup_logging
from app.core.rate_limit import setup_rate_limiting, ip_ban_middleware
from app.core.security_alerts import SecurityEvent, log_security_event
from app.middleware.auth import log_suspicious_requests, AuthMiddleware
from app.api.chat import router as chat_router
from app.api.documents import router as documents_router
from app.api.system import router as system_router
from app.api.analytics import router as analytics_router
from app.api.processing import router as processing_router
from app.api.query_analysis import router as query_analysis_router
from app.api.tasks import router as tasks_router
from app.api.auth import router as auth_router
from app.api.password_reset import router as password_reset_router
from app.api.admin import router as admin_router
from app.api.roles import router as roles_router
from app.api.document_sharing import router as document_sharing_router
from app.api.notifications import router as notifications_router
from app.api.organizations import router as organizations_router
from app.api.schema import router as schema_router
from app.db.session import init_db, get_session
from app.rag.tool_initializer import initialize_tools

# Setup logging
setup_logging()
logger = logging.getLogger("app.main")

# Create FastAPI app
app = FastAPI(
    title=PROJECT_NAME,
    description="Metis RAG API with JWT Authentication",
    version=SETTINGS.version,
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json"
)

# Add security middleware to log suspicious requests
app.middleware("http")(log_suspicious_requests)

# Add IP ban middleware only if rate limiting is enabled
if SETTINGS.rate_limiting_enabled:
    app.middleware("http")(ip_ban_middleware)

# Setup security
setup_security(app)

# Add authentication middleware
app.add_middleware(AuthMiddleware)

# Add database context middleware for Row Level Security
from app.middleware.db_context import DBContextMiddleware
app.add_middleware(DBContextMiddleware)

# Mount static files
app.mount("/static", StaticFiles(directory="app/static"), name="static")

# Setup templates
templates = Jinja2Templates(directory="app/templates")

# Include API routers
app.include_router(chat_router, prefix=f"{API_V1_STR}/chat", tags=["chat"])
app.include_router(documents_router, prefix=f"{API_V1_STR}/documents", tags=["documents"])
app.include_router(system_router, prefix=f"{API_V1_STR}/system", tags=["system"])
app.include_router(analytics_router, prefix=f"{API_V1_STR}/analytics", tags=["analytics"])
app.include_router(processing_router, prefix=f"{API_V1_STR}/processing", tags=["processing"])
app.include_router(query_analysis_router, prefix=f"{API_V1_STR}/query", tags=["query"])
app.include_router(tasks_router, prefix=f"{API_V1_STR}/tasks", tags=["tasks"])
app.include_router(auth_router, prefix=f"{API_V1_STR}/auth", tags=["auth"])
app.include_router(password_reset_router, prefix=f"{API_V1_STR}/password-reset", tags=["password-reset"])
app.include_router(admin_router, prefix=f"{API_V1_STR}/admin", tags=["admin"])
app.include_router(roles_router, prefix=f"{API_V1_STR}/roles", tags=["roles"])
app.include_router(document_sharing_router, prefix=f"{API_V1_STR}/sharing", tags=["sharing"])
app.include_router(notifications_router, prefix=f"{API_V1_STR}/notifications", tags=["notifications"])
app.include_router(organizations_router, prefix=f"{API_V1_STR}/organizations", tags=["organizations"])
app.include_router(schema_router, tags=["schema"])  # Schema router has its own prefix

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """
    Root endpoint that returns the main HTML page
    """
    return templates.TemplateResponse("chat.html", {"request": request})

@app.get("/documents", response_class=HTMLResponse)
async def documents_page(request: Request):
    """
    Documents management page
    """
    return templates.TemplateResponse("documents.html", {"request": request})

@app.get("/system", response_class=HTMLResponse)
async def system_page(request: Request):
    """
    System management page
    """
    return templates.TemplateResponse("system.html", {"request": request})

@app.get("/analytics", response_class=HTMLResponse)
async def analytics_page(request: Request):
    """
    Analytics dashboard page
    """
    return templates.TemplateResponse("analytics.html", {"request": request})

@app.get("/tasks", response_class=HTMLResponse)
async def tasks_page(request: Request):
    """
    Background tasks management page
    """
    return templates.TemplateResponse("tasks.html", {"request": request})

@app.get("/admin", response_class=HTMLResponse)
async def admin_page(request: Request):
    """
    Admin page
    """
    return templates.TemplateResponse("admin.html", {"request": request})

@app.get("/schema", response_class=HTMLResponse)
async def schema_page(request: Request):
    """
    Database schema viewer page
    """
    return templates.TemplateResponse("schema.html", {"request": request})

@app.get("/test-models", response_class=HTMLResponse)
async def test_models_page(request: Request):
    """
    Test models page for debugging
    """
    return templates.TemplateResponse("test_models.html", {"request": request})

@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    """
    Login page
    """
    # Check for credentials in URL params (security vulnerability)
    params = request.query_params
    has_credentials = "username" in params or "password" in params
    
    if has_credentials:
        # Log security event (without logging the actual credentials)
        client_host = request.client.host if request.client else "unknown"
        user_agent = request.headers.get("user-agent", "unknown")
        logger.warning(
            f"Security alert: Credentials detected in URL parameters. "
            f"IP: {client_host}, "
            f"User-Agent: {user_agent}"
        )
        
        # Create and log security event
        security_event = SecurityEvent(
            event_type="credentials_in_url",
            severity="high",
            source_ip=client_host,
            username=params.get("username", "unknown"),
            user_agent=user_agent,
            details={
                "path": request.url.path,
                "query_params": str(request.url.query),
                "has_username": "username" in params,
                "has_password": "password" in params
            }
        )
        log_security_event(security_event)
        
        # Get redirect param if it exists
        redirect_param = params.get("redirect", "")
        # Create clean URL (without credentials)
        clean_url = "/login" + (f"?redirect={redirect_param}" if redirect_param else "")
        
        # Redirect to clean URL with warning flag
        return RedirectResponse(
            url=clean_url + ("&" if redirect_param else "?") + "security_warning=credentials_in_url",
            status_code=status.HTTP_303_SEE_OTHER
        )
    
    # Normal login page rendering
    return templates.TemplateResponse("login.html", {
        "request": request,
        "security_warning": params.get("security_warning", "")
    })

@app.get("/register", response_class=HTMLResponse)
async def register_page(request: Request):
    """
    Registration page
    """
    return templates.TemplateResponse("register.html", {"request": request})

@app.get("/forgot-password", response_class=HTMLResponse)
async def forgot_password_page(request: Request):
    """
    Forgot password page
    """
    return templates.TemplateResponse("forgot_password.html", {"request": request})

@app.get("/reset-password", response_class=HTMLResponse)
async def reset_password_page(request: Request):
    """
    Reset password page
    """
    token = request.query_params.get("token", "")
    return templates.TemplateResponse("reset_password.html", {"request": request, "token": token})

@app.on_event("startup")
async def startup_event():
    """
    Actions to run on application startup
    """
    logger.info("Starting up Metis RAG application")
    
    # Print out the SECRET_KEY for debugging
    logger.info(f"Using SECRET_KEY: {SETTINGS.secret_key[:5]}...")
    
    # Initialize database
    try:
        logger.info("Initializing database connection")
        await init_db()
        logger.info("Database connection initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        raise
    
    # Initialize rate limiting if enabled
    if SETTINGS.rate_limiting_enabled:
        try:
            logger.info("Initializing rate limiting")
            rate_limiting_success = await setup_rate_limiting()
            if rate_limiting_success:
                logger.info("Rate limiting initialized successfully")
            else:
                logger.warning("Rate limiting initialization failed, continuing without rate limiting")
        except Exception as e:
            logger.error(f"Error initializing rate limiting: {str(e)}")
    
    # Initialize tools
    try:
        logger.info("Initializing tools")
        initialize_tools()
        logger.info("Tools initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing tools: {str(e)}")

@app.on_event("shutdown")
async def shutdown_event():
    """
    Actions to run on application shutdown
    """
    logger.info("Shutting down Metis RAG application")

================
File: config/.env.docker
================
# API settings
API_V1_STR=/api
PROJECT_NAME=Metis RAG

# Ollama settings
OLLAMA_BASE_URL=http://ollama:11434
DEFAULT_MODEL=gemma3:4b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# LLM Judge settings
CHUNKING_JUDGE_MODEL=gemma3:4b
RETRIEVAL_JUDGE_MODEL=gemma3:4b
USE_CHUNKING_JUDGE=True
USE_RETRIEVAL_JUDGE=True

# LangGraph RAG Agent settings
LANGGRAPH_RAG_MODEL=gemma3:4b
USE_LANGGRAPH_RAG=True
USE_ENHANCED_LANGGRAPH_RAG=True

# Document settings
UPLOAD_DIR=/app/data/uploads
CHROMA_DB_DIR=/app/data/chroma_db
CHUNK_SIZE=1500
CHUNK_OVERLAP=150

# Database settings
DATABASE_TYPE=postgresql
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_HOST=postgres
DATABASE_PORT=5432
DATABASE_NAME=metis_rag
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/metis_rag
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# Security settings
CORS_ORIGINS=*

# Mem0 settings
MEM0_ENDPOINT=http://mem0:8050
MEM0_API_KEY=${MEM0_ADMIN_API_KEY:-default_dev_key}
USE_MEM0=True

================
File: config/.env.example
================
# API Settings
API_HOST=0.0.0.0
API_PORT=8000

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=gemma3:12b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# LLM Judge Settings
CHUNKING_JUDGE_MODEL=gemma3:12b
RETRIEVAL_JUDGE_MODEL=gemma3:12b
USE_CHUNKING_JUDGE=True
USE_RETRIEVAL_JUDGE=True

# LangGraph RAG Agent Settings
LANGGRAPH_RAG_MODEL=gemma3:12b
USE_LANGGRAPH_RAG=True

# Document Settings
UPLOAD_DIR=./uploads
CHROMA_DB_DIR=./chroma_db
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Security Settings
CORS_ORIGINS=*

================
File: config/.env.test
================
# Database settings
DATABASE_TYPE=sqlite
DATABASE_URL=sqlite+aiosqlite:///./test.db

# Ollama settings
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=gemma3:4b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# LLM Judge settings
CHUNKING_JUDGE_MODEL=gemma3:4b
RETRIEVAL_JUDGE_MODEL=gemma3:4b
USE_CHUNKING_JUDGE=True
USE_RETRIEVAL_JUDGE=True

# Security settings
SECRET_KEY=test-secret-key-for-testing-only

================
File: config/docker-compose.mem0.yml
================
version: '3'

services:
  postgres:
    image: postgres:15
    container_name: mem0-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: mem0
    ports:
      - "5433:5432"  # Use 5433 to avoid conflict with the main database
    volumes:
      - mem0-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  mem0:
    image: mem0ai/mem0:latest
    container_name: mem0-server
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgres://postgres:postgres@postgres:5432/mem0
    ports:
      - "8050:8050"
    restart: unless-stopped

volumes:
  mem0-postgres-data:

================
File: config/docker-compose.yml
================
version: '3.8'

services:
  metis-rag:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./uploads:/app/uploads
      - ./chroma_db:/app/chroma_db
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MEM0_ENDPOINT=http://mem0:8050
      - USE_MEM0=true
    depends_on:
      - ollama
      - mem0

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
              
  mem0:
    image: mem0ai/mem0:latest
    ports:
      - "8050:8050"
    depends_on:
      - mem0-postgres
    environment:
      - DATABASE_URL=postgres://postgres:postgres@mem0-postgres:5432/mem0
      
  mem0-postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=mem0
    volumes:
      - mem0-postgres-data:/var/lib/postgresql/data
    ports:
      - "5433:5432"  # Use different port to avoid conflict

volumes:
  ollama_data:
  mem0-postgres-data:

================
File: config/Dockerfile
================
FROM python:3.10-slim as base

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p data/uploads data/chroma_db data/cache

# Expose port
EXPOSE 8000

# Set health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

# Set entrypoint
ENTRYPOINT ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

================
File: config/requirements.txt
================
# Core dependencies
fastapi~=0.111.0
uvicorn>=0.22.0,<0.23.0
python-multipart>=0.0.6
httpx>=0.24.0,<0.25.0
# Pydantic v2
pydantic~=2.7.0
# LangChain and LangGraph
langchain~=0.3.0
langgraph~=0.1.0
langchain-core>=0.1.0
langchain-community~=0.3.0
# ChromaDB and dependencies
chromadb>=0.4.13,<0.4.15
# Document processing dependencies
pypdf>=3.15.1
unstructured>=0.10.16
# Other dependencies
python-dotenv>=1.0.0
jinja2>=3.1.2
sse-starlette>=1.6.5
psutil>=5.9.5
# Security dependencies
fastapi-limiter>=0.1.5
redis>=4.2.0
passlib[bcrypt]>=1.7.4
python-jose[cryptography]>=3.3.0
bcrypt>=4.0.1

# Database dependencies
sqlalchemy>=2.0.0
alembic>=1.12.0
psycopg2-binary>=2.9.9

# Memory management
mem0ai>=0.1.0
# Added for Pydantic v2 compatibility
pydantic-settings~=2.0

================
File: data/cache/llm_response/stats.json
================
{
  "name": "llm_response",
  "size": 12,
  "max_size": 2000,
  "hits": 0,
  "misses": 2,
  "hit_ratio": 0.0,
  "ttl_seconds": 86400,
  "persist": true
}

================
File: data/cache/vector_search/stats.json
================
{
  "name": "vector_search",
  "size": 12,
  "max_size": 1000,
  "hits": 2,
  "misses": 1,
  "hit_ratio": 0.6666666666666666,
  "ttl_seconds": 3600,
  "persist": true
}

================
File: data/demo_cache/perf_cache/stats.json
================
{
  "name": "perf_cache",
  "size": 10,
  "max_size": 1000,
  "hits": 0,
  "misses": 10,
  "hit_ratio": 0.0,
  "ttl_seconds": 60,
  "persist": true
}

================
File: data/test_docs/create_sample_files.py
================
#!/usr/bin/env python3
"""
Script to create sample PDF and DOC files for testing Metis_RAG.
This script creates binary files that simulate PDF and DOC formats.
"""

import os
import random
import base64

def create_sample_pdf():
    """Create a sample PDF file with some binary content."""
    # This is a minimal valid PDF structure
    pdf_content = b'''%PDF-1.4
1 0 obj
<< /Type /Catalog /Pages 2 0 R >>
endobj
2 0 obj
<< /Type /Pages /Kids [3 0 R] /Count 1 >>
endobj
3 0 obj
<< /Type /Page /Parent 2 0 R /Resources << /Font << /F1 4 0 R >> >> /MediaBox [0 0 612 792] /Contents 5 0 R >>
endobj
4 0 obj
<< /Type /Font /Subtype /Type1 /BaseFont /Helvetica >>
endobj
5 0 obj
<< /Length 68 >>
stream
BT
/F1 24 Tf
100 700 Td
(Metis RAG Performance Benchmarks) Tj
ET
endstream
endobj
xref
0 6
0000000000 65535 f
0000000010 00000 n
0000000059 00000 n
0000000118 00000 n
0000000251 00000 n
0000000319 00000 n
trailer
<< /Size 6 /Root 1 0 R >>
startxref
439
%%EOF'''
    
    # Add more content to make it at least 1000 bytes
    additional_content = b'\n'
    for i in range(50):
        line = f"Line {i}: This is sample content for testing Metis RAG with PDF files. " * 5
        additional_content += line.encode('utf-8') + b'\n'
    
    pdf_content += additional_content
    
    with open('data/test_docs/performance_benchmarks.pdf', 'wb') as f:
        f.write(pdf_content)
    
    print(f"Created sample PDF file: performance_benchmarks.pdf ({len(pdf_content)} bytes)")

def create_sample_doc():
    """Create a sample DOC file with some binary content."""
    # This is a simplified binary structure that looks like a DOC file
    doc_header = b'\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1\x00\x00\x00\x00\x00\x00\x00\x00'
    doc_header += b'\x00\x00\x00\x00\x00\x00\x00\x00\x3E\x00\x03\x00\xFE\xFF\x09\x00'
    
    # Add document content
    doc_content = b"Metis RAG Implementation Guide\r\n\r\n"
    
    # Add more content to make it at least 1000 bytes
    for i in range(50):
        line = f"Section {i}: This document provides implementation guidance for Metis RAG systems. " * 5
        doc_content += line.encode('utf-8') + b'\r\n'
    
    # Combine header and content
    doc_data = doc_header + doc_content
    
    with open('data/test_docs/implementation_guide.doc', 'wb') as f:
        f.write(doc_data)
    
    print(f"Created sample DOC file: implementation_guide.doc ({len(doc_data)} bytes)")

if __name__ == "__main__":
    create_sample_pdf()
    create_sample_doc()
    print("Sample files created successfully!")

================
File: data/test_docs/document_metrics.csv
================
document_id,title,file_type,word_count,character_count,chunk_count,embedding_dimensions,processing_time_ms,retrieval_score,relevance_score,creation_date,last_modified_date,author,department,tags
DOC001,Annual Financial Report,pdf,4532,28765,12,768,1245,0.87,0.92,2024-01-15,2024-02-10,John Smith,Finance,"financial, annual, report"
DOC002,Customer Satisfaction Survey Results,csv,2156,13450,6,768,876,0.76,0.81,2024-02-03,2024-02-03,Maria Johnson,Marketing,"survey, customer, satisfaction"
DOC003,Product Development Roadmap,docx,3245,19876,9,768,1032,0.82,0.88,2024-01-20,2024-02-15,David Chen,Product,"roadmap, development, strategy"
DOC004,Employee Handbook 2024,pdf,5678,34567,15,768,1567,0.79,0.85,2024-01-05,2024-01-30,Sarah Williams,HR,"handbook, policy, employees"
DOC005,Technical Documentation v2.1,md,4321,26789,11,768,1123,0.91,0.94,2024-02-10,2024-02-20,Michael Brown,Engineering,"technical, documentation, api"
DOC006,Market Analysis Q1 2024,pptx,1876,11234,5,768,765,0.84,0.89,2024-01-25,2024-02-05,Jennifer Lee,Marketing,"market, analysis, quarterly"
DOC007,Research Paper: AI Applications,pdf,6789,42345,18,768,1876,0.93,0.96,2024-02-01,2024-02-18,Robert Johnson,Research,"research, AI, applications"
DOC008,Sales Forecast 2024-2025,xlsx,2345,14567,7,768,897,0.81,0.87,2024-01-10,2024-02-12,Thomas Wilson,Sales,"sales, forecast, financial"
DOC009,Project Alpha Implementation Plan,docx,4123,25678,11,768,1234,0.88,0.91,2024-01-18,2024-02-08,Amanda Garcia,Project Management,"project, implementation, plan"
DOC010,Compliance Guidelines Update,pdf,3456,21234,9,768,1087,0.85,0.90,2024-01-22,2024-02-14,Daniel Martinez,Legal,"compliance, guidelines, legal"
DOC011,Customer Journey Mapping Results,pptx,2134,13456,6,768,854,0.79,0.84,2024-02-05,2024-02-15,Emily Taylor,UX Design,"customer, journey, mapping"
DOC012,Quarterly Budget Review,xlsx,1987,12345,5,768,765,0.82,0.86,2024-01-08,2024-02-01,Christopher Lee,Finance,"budget, quarterly, review"
DOC013,API Documentation v3.0,md,5432,32456,14,768,1345,0.94,0.97,2024-02-08,2024-02-22,Jessica Wang,Engineering,"api, documentation, technical"
DOC014,Marketing Campaign Results,csv,2543,15678,7,768,932,0.83,0.88,2024-01-30,2024-02-16,Andrew Smith,Marketing,"marketing, campaign, results"
DOC015,Security Protocol Update,pdf,3876,23456,10,768,1156,0.89,0.93,2024-01-12,2024-02-07,Olivia Brown,IT Security,"security, protocol, cybersecurity"
DOC016,User Feedback Analysis,csv,3245,19876,9,768,1078,0.81,0.85,2024-02-02,2024-02-17,Ryan Johnson,Product,"feedback, user, analysis"
DOC017,Strategic Plan 2024-2026,docx,6543,39876,17,768,1765,0.90,0.94,2024-01-15,2024-02-10,Sophia Martinez,Executive,"strategic, plan, long-term"
DOC018,Code Documentation: Backend Services,md,4876,29876,13,768,1287,0.92,0.95,2024-01-28,2024-02-18,Ethan Wilson,Engineering,"code, documentation, backend"
DOC019,HR Policy Updates 2024,pdf,3987,24567,10,768,1176,0.84,0.89,2024-01-05,2024-01-25,Isabella Garcia,HR,"policy, hr, updates"
DOC020,Competitive Analysis Report,pptx,4123,25432,11,768,1234,0.86,0.91,2024-01-20,2024-02-12,Matthew Taylor,Strategy,"competitive, analysis, market"
DOC021,Product Launch Plan: Q2 2024,docx,3654,22345,10,768,1132,0.87,0.92,2024-02-05,2024-02-20,Emma Johnson,Marketing,"product, launch, plan"
DOC022,Financial Projections 2024-2025,xlsx,2987,18765,8,768,987,0.85,0.90,2024-01-10,2024-02-05,Noah Brown,Finance,"financial, projections, forecast"
DOC023,Customer Support Metrics,csv,2345,14321,6,768,876,0.78,0.83,2024-01-25,2024-02-15,Ava Wilson,Customer Support,"support, metrics, customer"
DOC024,Infrastructure Upgrade Proposal,pdf,5432,32876,14,768,1432,0.89,0.93,2024-01-18,2024-02-08,Liam Garcia,IT,"infrastructure, upgrade, proposal"
DOC025,UX Research Findings,pptx,3876,23987,10,768,1176,0.91,0.95,2024-02-01,2024-02-18,Charlotte Martinez,UX Design,"ux, research, findings"
DOC026,Onboarding Process Documentation,md,4321,26543,11,768,1234,0.83,0.88,2024-01-15,2024-02-10,Benjamin Taylor,HR,"onboarding, process, documentation"
DOC027,Sales Performance Analysis Q1,xlsx,2765,17654,7,768,954,0.82,0.87,2024-01-30,2024-02-16,Amelia Johnson,Sales,"sales, performance, analysis"
DOC028,Risk Assessment Report,pdf,4987,30765,13,768,1345,0.88,0.92,2024-01-12,2024-02-07,William Brown,Risk Management,"risk, assessment, report"
DOC029,Content Strategy 2024,docx,3654,22345,10,768,1132,0.84,0.89,2024-02-03,2024-02-18,Harper Wilson,Content,"content, strategy, marketing"
DOC030,System Architecture Overview,md,5876,35432,15,768,1543,0.93,0.96,2024-01-20,2024-02-15,James Garcia,Engineering,"architecture, system, technical"
DOC031,Customer Segmentation Analysis,csv,3245,19876,9,768,1087,0.85,0.90,2024-01-25,2024-02-12,Evelyn Martinez,Marketing,"segmentation, customer, analysis"
DOC032,Budget Allocation 2024,xlsx,2543,15678,7,768,932,0.81,0.86,2024-01-08,2024-02-01,Alexander Taylor,Finance,"budget, allocation, financial"
DOC033,Product Specifications v2.0,pdf,4765,29345,12,768,1287,0.90,0.94,2024-02-05,2024-02-20,Sofia Johnson,Product,"specifications, product, technical"
DOC034,Employee Engagement Survey Results,csv,3456,21234,9,768,1123,0.79,0.84,2024-01-15,2024-02-08,Sebastian Brown,HR,"engagement, survey, employee"
DOC035,API Performance Metrics,md,4123,25432,11,768,1234,0.92,0.95,2024-01-28,2024-02-15,Scarlett Wilson,Engineering,"api, performance, metrics"
DOC036,Market Trends Report Q1 2024,pptx,3987,24567,10,768,1176,0.86,0.91,2024-02-01,2024-02-18,Jack Garcia,Marketing,"market, trends, report"
DOC037,Legal Compliance Checklist,docx,2876,17654,8,768,987,0.84,0.89,2024-01-10,2024-02-05,Victoria Martinez,Legal,"legal, compliance, checklist"
DOC038,Data Migration Plan,pdf,5432,32876,14,768,1432,0.88,0.93,2024-01-20,2024-02-10,Leo Taylor,IT,"data, migration, plan"
DOC039,User Interface Guidelines,md,4321,26543,11,768,1234,0.91,0.95,2024-02-03,2024-02-18,Penelope Johnson,UX Design,"ui, guidelines, design"
DOC040,Quarterly Sales Report,xlsx,3245,19876,9,768,1087,0.83,0.88,2024-01-30,2024-02-16,Henry Brown,Sales,"sales, quarterly, report"

================
File: data/test_docs/implementation_guide.txt
================
# Metis RAG Implementation Guide

## Introduction

This implementation guide provides detailed instructions for deploying and configuring the Metis Retrieval-Augmented Generation (RAG) system. Metis RAG is designed to enhance large language model capabilities by integrating external knowledge sources, improving response accuracy, and reducing hallucinations. This guide covers all aspects of implementation from initial setup to advanced configuration and optimization.

## System Requirements

### Hardware Requirements

The Metis RAG system can be deployed on various hardware configurations depending on the expected workload and document collection size. Minimum recommended specifications:

- **CPU**: 4+ cores (8+ cores recommended for production)
- **RAM**: 16GB minimum (32GB+ recommended for production)
- **Storage**: 100GB+ SSD storage
- **Network**: 1Gbps Ethernet

For larger deployments supporting multiple concurrent users and extensive document collections, consider:

- **CPU**: 16+ cores
- **RAM**: 64GB+
- **Storage**: 500GB+ SSD in RAID configuration
- **Network**: 10Gbps Ethernet

### Software Requirements

- **Operating System**: Ubuntu 20.04 LTS or later, CentOS 8+, or macOS 12+
- **Python**: Version 3.10 or later
- **Database**: PostgreSQL 14+ (recommended) or SQLite 3.35+ (for development)
- **Vector Database**: ChromaDB, Qdrant, or Milvus
- **Container Platform**: Docker and Docker Compose (for containerized deployment)

### Dependencies

The following major dependencies are required:

- **LangChain**: For orchestrating the RAG workflow
- **Sentence Transformers**: For generating embeddings
- **FastAPI**: For API endpoints
- **SQLAlchemy**: For database ORM
- **Pydantic**: For data validation
- **Alembic**: For database migrations
- **Uvicorn**: For ASGI server

## Installation

### Option 1: Docker Deployment (Recommended)

1. Clone the repository:
   ```bash
   git clone https://github.com/organization/metis-rag.git
   cd metis-rag
   ```

2. Configure environment variables:
   ```bash
   cp config/.env.example config/.env
   # Edit config/.env with your configuration
   ```

3. Build and start the containers:
   ```bash
   docker-compose up -d
   ```

4. Run database migrations:
   ```bash
   docker-compose exec app alembic upgrade head
   ```

5. Verify the installation:
   ```bash
   curl http://localhost:8000/api/health
   ```

### Option 2: Local Development Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/organization/metis-rag.git
   cd metis-rag
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Configure environment variables:
   ```bash
   cp config/.env.example config/.env
   # Edit config/.env with your configuration
   ```

5. Run database migrations:
   ```bash
   alembic upgrade head
   ```

6. Start the development server:
   ```bash
   uvicorn app.main:app --reload
   ```

## Configuration

### Environment Variables

The following key environment variables control the system behavior:

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | Database connection string | `sqlite:///./test.db` |
| `VECTOR_DB_TYPE` | Vector database type (chroma, qdrant, milvus) | `chroma` |
| `VECTOR_DB_PATH` | Path to vector database files | `./chroma_db` |
| `EMBEDDING_MODEL` | Model name for embeddings | `all-MiniLM-L6-v2` |
| `CHUNK_SIZE` | Document chunk size in tokens | `512` |
| `CHUNK_OVERLAP` | Overlap between chunks in tokens | `128` |
| `MAX_WORKERS` | Maximum number of worker processes | `4` |
| `CACHE_ENABLED` | Enable response caching | `true` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `API_KEY_ENABLED` | Enable API key authentication | `false` |
| `LLM_PROVIDER` | LLM provider (openai, anthropic, local) | `openai` |
| `LLM_MODEL` | LLM model name | `gpt-3.5-turbo` |

### Configuration Files

Additional configuration is available through JSON configuration files:

- `config/chunking_strategies.json`: Define custom chunking strategies
- `config/embedding_models.json`: Configure embedding model parameters
- `config/retrieval_settings.json`: Adjust retrieval parameters
- `config/llm_settings.json`: Configure LLM behavior and prompts

## Document Processing

### Supported Document Types

Metis RAG supports the following document types:

- **Text**: .txt, .md, .csv
- **Office Documents**: .docx, .xlsx, .pptx
- **PDF**: .pdf
- **Web Content**: HTML, URLs
- **Code**: .py, .js, .java, .cpp, etc.
- **Structured Data**: .json, .yaml, .xml

### Document Ingestion

Documents can be ingested through multiple methods:

1. **API Upload**:
   ```bash
   curl -X POST http://localhost:8000/api/documents \
     -F "file=@/path/to/document.pdf" \
     -F "metadata={\"source\":\"manual\",\"category\":\"technical\"}"
   ```

2. **Batch Processing**:
   ```bash
   curl -X POST http://localhost:8000/api/processing/batch \
     -H "Content-Type: application/json" \
     -d '{"directory": "/path/to/documents", "recursive": true}'
   ```

3. **Web Crawler**:
   ```bash
   curl -X POST http://localhost:8000/api/processing/crawl \
     -H "Content-Type: application/json" \
     -d '{"url": "https://example.com", "max_depth": 2}'
   ```

### Processing Pipeline

The document processing pipeline consists of the following stages:

1. **Document Loading**: Parse and extract text from various file formats
2. **Text Cleaning**: Remove irrelevant content, normalize text
3. **Chunking**: Split documents into manageable chunks
4. **Metadata Extraction**: Extract and enrich document metadata
5. **Embedding Generation**: Create vector embeddings for each chunk
6. **Indexing**: Store chunks and embeddings in the vector database

Processing progress can be monitored through the `/api/processing/status` endpoint.

## Query Processing

### Basic Querying

To query the system, send a POST request to the query endpoint:

```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{"query": "How does the chunking strategy affect retrieval performance?"}'
```

### Advanced Query Parameters

The query API supports several parameters for fine-tuning retrieval:

| Parameter | Description | Default |
|-----------|-------------|---------|
| `top_k` | Number of chunks to retrieve | `5` |
| `similarity_threshold` | Minimum similarity score (0-1) | `0.7` |
| `filter` | Metadata filter criteria | `{}` |
| `rerank` | Enable cross-encoder reranking | `false` |
| `include_sources` | Include source references | `true` |
| `max_tokens` | Maximum response length | `1024` |

Example with advanced parameters:

```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the performance implications of different vector databases?",
    "top_k": 8,
    "similarity_threshold": 0.75,
    "filter": {"category": "technical"},
    "rerank": true,
    "include_sources": true
  }'
```

## Performance Optimization

### Embedding Model Selection

The choice of embedding model significantly impacts both retrieval quality and performance. Consider the following trade-offs:

- **Smaller models** (e.g., all-MiniLM-L6-v2): Faster processing, lower memory usage, slightly reduced accuracy
- **Larger models** (e.g., all-mpnet-base-v2): Higher accuracy, increased memory usage, slower processing

To change the embedding model:

1. Update the `EMBEDDING_MODEL` environment variable
2. Reindex existing documents:
   ```bash
   curl -X POST http://localhost:8000/api/processing/reindex
   ```

### Chunking Strategy Optimization

Optimal chunking strategies depend on your document types and query patterns:

- **Smaller chunks** (256-512 tokens): Better for precise factual retrieval
- **Larger chunks** (1024+ tokens): Better for contextual understanding
- **Semantic chunking**: Better for documents with clear topical boundaries

Custom chunking strategies can be defined in `config/chunking_strategies.json`.

### Database Optimization

For PostgreSQL deployments:

1. Increase shared buffers:
   ```
   shared_buffers = 4GB  # 25% of RAM for dedicated servers
   ```

2. Optimize work memory:
   ```
   work_mem = 64MB
   maintenance_work_mem = 256MB
   ```

3. Adjust autovacuum settings:
   ```
   autovacuum_vacuum_scale_factor = 0.05
   autovacuum_analyze_scale_factor = 0.02
   ```

### Vector Database Tuning

For large document collections:

1. Implement vector database sharding
2. Configure appropriate index types (HNSW, IVF, etc.)
3. Adjust search parameters for the optimal speed/accuracy trade-off

## Monitoring and Maintenance

### Logging

Logs are written to:
- Console output
- `logs/app.log` (rotated daily)
- Syslog (if configured)

Log levels can be adjusted via the `LOG_LEVEL` environment variable.

### Metrics

System metrics are available at the `/api/metrics` endpoint, including:
- Query latency
- Retrieval statistics
- Document processing throughput
- Cache hit rates
- Resource utilization

### Backup and Recovery

Regular backups are recommended for both the SQL database and vector store:

1. Database backup:
   ```bash
   docker-compose exec db pg_dump -U postgres metis_rag > backup_$(date +%Y%m%d).sql
   ```

2. Vector store backup:
   ```bash
   docker-compose exec app python -m scripts.backup_vector_store
   ```

## Troubleshooting

### Common Issues

1. **Slow document processing**:
   - Reduce embedding model size
   - Increase worker count
   - Check for disk I/O bottlenecks

2. **High memory usage**:
   - Reduce batch sizes
   - Implement document processing queues
   - Consider horizontal scaling

3. **Poor retrieval quality**:
   - Adjust chunking strategy
   - Try different embedding models
   - Implement reranking

4. **API timeouts**:
   - Increase timeout settings
   - Optimize database queries
   - Consider caching frequent queries

### Diagnostic Tools

The system includes several diagnostic endpoints:

- `/api/health`: Overall system health
- `/api/diagnostics/embedding`: Test embedding generation
- `/api/diagnostics/retrieval`: Test retrieval performance
- `/api/diagnostics/processing`: Test document processing

## Conclusion

This implementation guide covers the essential aspects of deploying and configuring the Metis RAG system. For additional support, consult the following resources:

- API Documentation: `/docs` endpoint
- GitHub Repository: https://github.com/organization/metis-rag
- Community Forum: https://community.metis-rag.org

Regular updates and security patches are released monthly. Subscribe to the mailing list for notifications about new releases and features.

================
File: data/test_docs/performance_benchmarks.txt
================
# RAG System Performance Benchmarks
## Executive Summary

This document presents comprehensive performance benchmarks for our Retrieval-Augmented Generation (RAG) system across various configurations and workloads. The benchmarks evaluate key performance metrics including latency, throughput, accuracy, and resource utilization. These results provide valuable insights for system optimization and capacity planning.

## Test Environment

### Hardware Configuration
- **CPU**: 8-core Intel Xeon E5-2686 v4 @ 2.30GHz
- **RAM**: 32GB DDR4
- **Storage**: 500GB SSD (NVMe)
- **Network**: 10 Gbps

### Software Stack
- **Operating System**: Ubuntu 22.04 LTS
- **Vector Database**: Chroma v0.4.15
- **Embedding Model**: all-MiniLM-L6-v2 (384 dimensions)
- **LLM**: GPT-3.5-Turbo (4K context window)
- **Python Version**: 3.10.12
- **Framework Version**: 1.2.3

## Methodology

Performance tests were conducted using a standardized test harness that simulates various user workloads and query patterns. Each test was run multiple times to ensure statistical significance, with outliers removed using standard deviation analysis. The following metrics were collected:

1. **Query Latency**: Time from query submission to response delivery
2. **Throughput**: Number of queries processed per minute
3. **Retrieval Precision**: Relevance of retrieved documents
4. **Generation Quality**: Accuracy and relevance of generated responses
5. **Resource Utilization**: CPU, memory, and I/O usage during operation

## Benchmark Results

### 1. Query Latency

| Configuration | P50 (ms) | P90 (ms) | P95 (ms) | P99 (ms) |
|---------------|----------|----------|----------|----------|
| Base Config   | 245      | 387      | 456      | 612      |
| Optimized     | 187      | 298      | 342      | 489      |
| Cached        | 78       | 124      | 156      | 203      |
| Distributed   | 156      | 267      | 312      | 423      |

The optimized configuration shows a 24% improvement in median latency compared to the base configuration. Caching provides the most significant performance boost, reducing median latency by 68%.

### 2. Throughput

| Document Count | Queries Per Minute | CPU Utilization | Memory Usage (GB) |
|----------------|-------------------|-----------------|-------------------|
| 1,000          | 342               | 45%             | 4.2               |
| 10,000         | 287               | 62%             | 7.8               |
| 100,000        | 213               | 78%             | 12.5              |
| 1,000,000      | 156               | 86%             | 18.7              |

Throughput decreases as the document collection grows, with a 54% reduction when scaling from 1,000 to 1,000,000 documents. This indicates the need for horizontal scaling for very large document collections.

### 3. Chunking Strategy Impact

| Chunk Size | Overlap | Retrieval Precision | Query Latency (ms) | Memory Usage (GB) |
|------------|---------|---------------------|-------------------|-------------------|
| 256        | 0       | 0.72                | 187               | 8.4               |
| 256        | 64      | 0.78                | 195               | 9.2               |
| 512        | 0       | 0.81                | 203               | 7.6               |
| 512        | 128     | 0.87                | 218               | 8.3               |
| 1024       | 0       | 0.76                | 234               | 6.8               |
| 1024       | 256     | 0.83                | 256               | 7.5               |

A chunk size of 512 with 128-token overlap provides the best balance between retrieval precision and performance. Smaller chunks improve memory efficiency but reduce context availability for the LLM.

### 4. Embedding Model Comparison

| Model                  | Dimensions | Embedding Time (ms) | Retrieval Precision | Storage (GB/1M docs) |
|------------------------|------------|---------------------|---------------------|----------------------|
| all-MiniLM-L6-v2       | 384        | 12                  | 0.82                | 1.5                  |
| all-mpnet-base-v2      | 768        | 28                  | 0.89                | 3.0                  |
| text-embedding-ada-002 | 1536       | 45                  | 0.91                | 6.0                  |
| e5-large-v2            | 1024       | 36                  | 0.90                | 4.0                  |

Higher-dimensional models provide better retrieval precision but increase storage requirements and embedding time. The all-mpnet-base-v2 model offers a good balance for most applications.

### 5. Vector Database Scaling

| Shard Count | Query Latency (ms) | Indexing Time (min) | Memory Usage (GB) |
|-------------|-------------------|---------------------|-------------------|
| 1           | 312               | 45                  | 18.7              |
| 2           | 187               | 28                  | 10.2              |
| 4           | 124               | 18                  | 6.1               |
| 8           | 98                | 12                  | 3.8               |

Horizontal scaling with multiple shards significantly improves query performance and reduces memory pressure on individual nodes. A 4-shard configuration provides a good balance for most workloads.

## Optimization Recommendations

Based on the benchmark results, we recommend the following optimizations:

1. **Chunking Strategy**: Use 512-token chunks with 128-token overlap for optimal retrieval precision and performance.

2. **Embedding Model Selection**: Use all-mpnet-base-v2 for production workloads requiring high precision, or all-MiniLM-L6-v2 for applications with stricter latency requirements.

3. **Caching Implementation**: Implement a two-tier caching strategy:
   - L1: In-memory cache for frequent queries (LRU policy)
   - L2: Persistent cache for embedding vectors (time-based expiration)

4. **Horizontal Scaling**: Deploy a 4-shard vector database configuration for collections exceeding 100,000 documents.

5. **Asynchronous Processing**: Implement asynchronous document processing with a queue-based architecture to smooth ingestion workloads.

6. **Resource Allocation**: Allocate at least 4GB of RAM per million documents for optimal performance.

## Conclusion

The RAG system demonstrates good performance characteristics across various workloads, with several opportunities for optimization. By implementing the recommended configurations, we expect to achieve a 40-60% improvement in query latency and a 30-50% increase in throughput for typical workloads.

Future benchmarking efforts will focus on:
- Multi-modal retrieval performance
- Cross-encoder re-ranking impact
- Hybrid search configurations
- LLM context window optimization

These benchmarks provide a solid foundation for capacity planning and system optimization as we scale the RAG system to support growing document collections and user bases.

## Appendix: Test Queries

The following query sets were used for benchmarking:

1. **Simple Factual Queries**: Single-fact retrieval questions
2. **Complex Analytical Queries**: Multi-hop reasoning questions
3. **Domain-Specific Queries**: Technical questions in specific domains
4. **Ambiguous Queries**: Questions with multiple possible interpretations
5. **Long-form Queries**: Requests for detailed explanations or summaries

Each query set contains 100 questions designed to test different aspects of the RAG system's performance and accuracy.

================
File: data/test_docs/quarterly_report.txt
================
Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs

================
File: data/test_docs/rag_system_overview.txt
================
# Retrieval-Augmented Generation (RAG) System Overview

## Introduction

Retrieval-Augmented Generation (RAG) represents a significant advancement in natural language processing and AI systems. By combining the strengths of retrieval-based and generation-based approaches, RAG systems deliver more accurate, contextually relevant, and factually grounded responses. This document provides a comprehensive overview of RAG systems, their architecture, implementation considerations, and best practices.

## Core Components

### 1. Document Processing Pipeline

The document processing pipeline is responsible for ingesting, processing, and indexing documents for efficient retrieval. Key steps include:

- Document ingestion from various sources (files, databases, APIs)
- Text extraction and cleaning
- Chunking strategies to divide documents into manageable segments
- Metadata extraction and enrichment
- Embedding generation using vector models
- Vector database indexing for efficient similarity search

Effective document processing ensures that the knowledge base is properly structured and optimized for retrieval operations. The chunking strategy is particularly important, as it affects both retrieval precision and computational efficiency.

### 2. Vector Database

The vector database stores document chunks and their corresponding embeddings. It supports:

- Fast similarity search operations
- Metadata filtering
- Hybrid search combining vector similarity and keyword matching
- Scalability to handle large document collections

Popular vector database options include Chroma, Pinecone, Weaviate, and Milvus, each with different performance characteristics and feature sets.

### 3. Query Processing

Query processing transforms user queries into effective search parameters:

- Query understanding and intent classification
- Query expansion and reformulation
- Embedding generation for vector similarity search
- Hybrid search parameter optimization

Advanced query processing may include techniques like query decomposition for complex questions and query routing to specialized retrievers.

### 4. Retrieval System

The retrieval system is responsible for finding the most relevant document chunks based on the processed query:

- Vector similarity search
- Metadata filtering
- Re-ranking of initial results
- Hybrid retrieval combining different search strategies
- Context window optimization

Retrieval quality directly impacts the overall system performance, making this component critical to RAG effectiveness.

### 5. Generation System

The generation system produces responses based on the retrieved context and user query:

- Context integration with the query
- Prompt engineering and optimization
- Response generation using LLMs
- Post-processing and formatting
- Citation and source attribution

The generation system must effectively leverage the retrieved information while maintaining coherence and relevance to the original query.

## Implementation Considerations

### Performance Optimization

Performance optimization is crucial for RAG systems, particularly as the document collection grows:

- Embedding model selection balancing quality and speed
- Chunking strategy optimization
- Vector database indexing and sharding
- Caching frequently accessed embeddings and results
- Asynchronous processing for document ingestion
- Batch processing for efficiency

Monitoring system performance metrics helps identify bottlenecks and optimization opportunities.

### Quality Evaluation

Evaluating RAG system quality requires multiple dimensions:

- Retrieval precision and recall
- Response relevance and completeness
- Factual accuracy and hallucination reduction
- Response coherence and readability
- End-user satisfaction metrics

Establishing a comprehensive evaluation framework helps guide system improvements and measure progress.

### Scalability

Scalability considerations ensure the system can grow with increasing demands:

- Horizontal scaling of vector databases
- Distributed processing for document ingestion
- Load balancing for query handling
- Resource allocation optimization
- Database partitioning strategies

Planning for scalability from the beginning prevents performance degradation as usage increases.

## Advanced Features

### Multi-Modal RAG

Multi-modal RAG extends beyond text to include:

- Image understanding and retrieval
- Audio content processing
- Video content analysis
- Chart and graph interpretation
- Combined multi-modal context integration

These capabilities enable more comprehensive information retrieval across different content types.

### Adaptive Retrieval

Adaptive retrieval dynamically adjusts retrieval parameters based on:

- Query complexity analysis
- User feedback and interaction patterns
- Session context and history
- Task-specific optimization
- Learning from successful retrievals

This approach improves system performance over time through continuous adaptation.

### Knowledge Graph Integration

Knowledge graph integration enhances RAG systems with:

- Structured relationship understanding
- Entity resolution and linking
- Reasoning capabilities
- Hierarchical knowledge representation
- Complex query handling

Combining vector search with knowledge graph traversal creates more powerful retrieval capabilities.

## Best Practices

### Security and Privacy

Security and privacy considerations include:

- Access control for sensitive documents
- Personally identifiable information (PII) detection and handling
- Encryption for data at rest and in transit
- Audit logging for system access and usage
- Compliance with relevant regulations (GDPR, HIPAA, etc.)

Implementing robust security measures protects both the system and its users.

### Monitoring and Maintenance

Effective monitoring and maintenance ensure system reliability:

- Performance metric tracking
- Error detection and alerting
- Regular index optimization
- Content freshness monitoring
- User feedback collection and analysis

Proactive monitoring prevents issues before they impact users.

### User Experience Design

User experience considerations improve system adoption:

- Intuitive query interfaces
- Transparent source attribution
- Confidence indicators for responses
- Feedback mechanisms
- Progressive disclosure of complex information

A well-designed user experience increases trust and system effectiveness.

## Conclusion

Retrieval-Augmented Generation represents a powerful approach to building knowledge-intensive AI systems. By effectively combining retrieval and generation capabilities, RAG systems deliver more accurate, contextually relevant, and trustworthy responses. Implementing a successful RAG system requires careful attention to architecture, performance optimization, quality evaluation, and user experience design.

As the field continues to evolve, we can expect further advancements in retrieval techniques, multi-modal capabilities, and integration with other AI approaches. Organizations that effectively implement RAG systems will benefit from more capable and reliable AI assistants that can leverage their specific knowledge bases.

================
File: data/test_docs/rag_test_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "answer": "Based on the provided context, the architecture of Metis RAG can be summarized as follows:\n\nAccording to [1] and [2], Metis RAG has a modular architecture consisting of three main components: Frontend Layer, API Layer, and RAG Engine.\n\nThe Frontend Layer is built with HTML, CSS, and JavaScript, providing an intuitive interface for document management, chat interactions, system configuration, and analytics and monitoring. This is stated in both [1] and [2].\n\nThe API Layer is implemented using FastAPI and provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval. Again, this information is present in both [1] and [2].\n\nThe RAG Engine consists of three main parts: Document Processing, Vector Store, and LLM Integration.\n\nDocument Processing handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction, as described in [1] and [2].\n\nVector Store is responsible for storing document embeddings, efficient similarity search, and metadata filtering. This information can be found in both [1] and [2].\n\nLLM Integration connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization. This is also stated in both [1] and [2].\n\nNo additional information beyond what's provided in the context is necessary to answer this question.\n\nSources:\n[1] Source: technical_documentation.md, Tags: [], Folder: /test\n[2] Source: technical_documentation.md, Tags: [], Folder: /test",
    "sources": [
      {
        "document_id": "b09f6566-22ad-4d5f-ad6e-1c8a5dda1ef8",
        "chunk_id": "f86ba6ad-8036-44b4-a647-d6044fc42cdc",
        "relevance_score": 0.6145516890095938,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "2fec744c-af0d-40da-b8b4-b22fbf53ca8c",
        "chunk_id": "465de177-2f2c-4f02-bc13-b45e30970fcb",
        "relevance_score": 0.6145516890095938,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "4cb81e59-d0cd-427d-9478-3db915be355c",
        "relevance_score": 0.5401424824966378,
        "excerpt": "Product ID: P003\nName: MetisRAG Basic\nCategory: Software\nPrice: $499\nFeatures: Basic RAG capabilitie..."
      }
    ]
  },
  {
    "query": "What was the revenue reported in Q1 2025?",
    "answer": "I'll do my best to provide an accurate and well-informed response based on the provided context.\n\nAccording to the Quarterly Business Report [1], the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year.\n\nSource: [1]\n\nPlease let me know if you'd like me to provide more information or clarification!",
    "sources": [
      {
        "document_id": "3404bc5a-fe9f-4d36-8d19-6a8a4fd8216a",
        "chunk_id": "1561ae99-4450-4d13-8460-9cf188d7d7ff",
        "relevance_score": 0.6630528548563427,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "173e7a09-66e5-4523-a883-6916e339d6a2",
        "chunk_id": "7d141d75-a409-472d-9529-74d94d89f742",
        "relevance_score": 0.6630528548563427,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "016005ef-97e5-405b-8360-ea5501fb15d6",
        "relevance_score": 0.5167652143787004,
        "excerpt": "Product ID: P001\nName: MetisRAG Enterprise\nCategory: Software\nPrice: $4999\nFeatures: Advanced RAG ca..."
      }
    ]
  },
  {
    "query": "What are the components of the RAG engine?",
    "answer": "Based on the provided context, I can answer the user's question.\n\nThe components of the RAG engine are described in the Architecture Overview section [1] and [2]. According to this information, the core RAG engine consists of three main components:\n\n* Document Processing: This component handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction.\n* Vector Store: This component is responsible for storing document embeddings, efficient similarity search, and metadata filtering.\n* LLM Integration: This component connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization.\n\nThese components work together to enable the Metis RAG system's Retrieval-Augmented Generation capabilities.",
    "sources": [
      {
        "document_id": "b09f6566-22ad-4d5f-ad6e-1c8a5dda1ef8",
        "chunk_id": "f86ba6ad-8036-44b4-a647-d6044fc42cdc",
        "relevance_score": 0.57311767290417,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "2fec744c-af0d-40da-b8b4-b22fbf53ca8c",
        "chunk_id": "465de177-2f2c-4f02-bc13-b45e30970fcb",
        "relevance_score": 0.57311767290417,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "36fe23f2-875b-4b97-a697-b531f3e063a2",
        "relevance_score": 0.5202960339765587,
        "excerpt": "Product ID: P002\nName: MetisRAG Professional\nCategory: Software\nPrice: $1999\nFeatures: Standard RAG ..."
      }
    ]
  },
  {
    "query": "What are the strategic initiatives for Q2?",
    "answer": "Based on the provided context, it appears that there is no specific information regarding strategic initiatives for Q2 (Quarter 2). The context primarily discusses the potential and current state of Quantum Process Synchronization (QPS) research, emphasizing its developmental phase.\n\nWhile we don't have direct information about strategic initiatives for Q2, we can infer from the general tone and themes in the provided context that any strategic plans for this quarter would likely involve continued research, testing, and refinement of QPS-based technologies. The fact that investors and governments worldwide are showing strong interest in quantum research suggests that there may be ongoing efforts to accelerate the development of practical applications.\n\nHowever, without more specific information, it's difficult to provide concrete details about what these strategic initiatives might entail. If you're looking for guidance on potential strategic plans or initiatives related to QPS, I would suggest exploring other sources that provide more context or insights into the research and development landscape.",
    "sources": [
      {
        "document_id": "c8f04dec-4895-4bc0-ade4-c2c1ce465f18",
        "chunk_id": "2ed5746d-8fba-4f8d-9287-553b93aec2a0",
        "relevance_score": 0.5260502976127305,
        "excerpt": "While the potential of QPS is immense, the research team emphasizes that practical applications are ..."
      },
      {
        "document_id": "94db96c8-d51e-4c2a-8bbf-5e78cf655596",
        "chunk_id": "774daf50-a734-49f4-b473-c19fc94801ee",
        "relevance_score": 0.5260502976127305,
        "excerpt": "While the potential of QPS is immense, the research team emphasizes that practical applications are ..."
      },
      {
        "document_id": "d9c026c6-5313-4a7e-8d9e-60ee34a816c9",
        "chunk_id": "b10fa892-9ae1-438f-8870-559d363db1df",
        "relevance_score": 0.5260502976127305,
        "excerpt": "While the potential of QPS is immense, the research team emphasizes that practical applications are ..."
      }
    ]
  },
  {
    "query": "How does the vector store work?",
    "answer": "Based on the provided context, specifically [1] and [2], I can provide information about the Vector Store component of the Metis RAG system.\n\nAccording to the documentation, the Vector Store is responsible for:\n\n* Storing document embeddings\n* Efficient similarity search\n* Metadata filtering\n\nThis suggests that the Vector Store plays a crucial role in enabling efficient retrieval and comparison of documents within the Metis RAG system. The vector store likely uses techniques such as latent semantic analysis (LSA) or word embeddings to create dense representations of documents, allowing for fast and accurate matching between queries and relevant documents.\n\nHowever, I must note that the context does not provide detailed information on how the Vector Store component works in practice, such as specific algorithms used or parameters for tuning its performance. For more comprehensive understanding of this component's functionality, additional documentation or expert knowledge may be necessary.\n\nIn any case, based on the provided context, it appears that the Vector Store is designed to facilitate efficient and effective document retrieval within the Metis RAG system.",
    "sources": [
      {
        "document_id": "b09f6566-22ad-4d5f-ad6e-1c8a5dda1ef8",
        "chunk_id": "f86ba6ad-8036-44b4-a647-d6044fc42cdc",
        "relevance_score": 0.5206977405774185,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "2fec744c-af0d-40da-b8b4-b22fbf53ca8c",
        "chunk_id": "465de177-2f2c-4f02-bc13-b45e30970fcb",
        "relevance_score": 0.5206977405774185,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "4cb81e59-d0cd-427d-9478-3db915be355c",
        "relevance_score": 0.49338299939463015,
        "excerpt": "Product ID: P003\nName: MetisRAG Basic\nCategory: Software\nPrice: $499\nFeatures: Basic RAG capabilitie..."
      }
    ]
  },
  {
    "query": "What was the customer retention rate?",
    "answer": "Based on the provided context, the customer retention rate is mentioned in [1] and [2]. According to these sources, the customer retention rate remained strong at 94%.\n\nSince this information is present in the provided context, I will not attempt to provide an alternative answer or make any assumptions. Instead, I will reiterate the directly stated information.\n\nCustomer Retention Rate: 94% [1], [2]",
    "sources": [
      {
        "document_id": "3404bc5a-fe9f-4d36-8d19-6a8a4fd8216a",
        "chunk_id": "1561ae99-4450-4d13-8460-9cf188d7d7ff",
        "relevance_score": 0.5500964726882089,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "173e7a09-66e5-4523-a883-6916e339d6a2",
        "chunk_id": "7d141d75-a409-472d-9529-74d94d89f742",
        "relevance_score": 0.5500964726882089,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "955afe4e-e33d-4cef-abdb-f02504119b3d",
        "relevance_score": 0.4963214209080996,
        "excerpt": "Product ID: P005\nName: MetisRAG Mobile\nCategory: Mobile App\nPrice: $9.99 per month\nFeatures: iOS and..."
      }
    ]
  }
]

================
File: data/test_docs/sample_analysis.py
================
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Sample Python file for testing Metis_RAG document processing capabilities.
This file demonstrates various Python constructs and NLP-related functions
that might be relevant to a RAG system.
"""

import os
import sys
import json
import re
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Union
from datetime import datetime, timedelta
from collections import Counter, defaultdict

class DocumentProcessor:
    """A sample document processor class for RAG systems."""
    
    def __init__(self, chunk_size: int = 1000, overlap: int = 200):
        """Initialize the document processor with chunking parameters.
        
        Args:
            chunk_size: The size of each text chunk in characters
            overlap: The overlap between consecutive chunks in characters
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.processed_docs = 0
        self.total_chunks = 0
        self.processing_stats = {
            "start_time": None,
            "end_time": None,
            "total_docs": 0,
            "total_chunks": 0,
            "avg_chunks_per_doc": 0,
            "processing_errors": 0
        }
    
    def process_document(self, document_text: str, metadata: Dict = None) -> List[Dict]:
        """Process a document by chunking it and preparing for embedding.
        
        Args:
            document_text: The text content of the document
            metadata: Optional metadata about the document
            
        Returns:
            A list of chunks with their metadata
        """
        if not document_text:
            raise ValueError("Document text cannot be empty")
            
        if not self.processing_stats["start_time"]:
            self.processing_stats["start_time"] = datetime.now()
            
        chunks = self._chunk_text(document_text)
        result = []
        
        for i, chunk in enumerate(chunks):
            chunk_metadata = metadata.copy() if metadata else {}
            chunk_metadata.update({
                "chunk_id": i,
                "total_chunks": len(chunks),
                "chunk_size": len(chunk),
                "processing_timestamp": datetime.now().isoformat()
            })
            
            result.append({
                "text": chunk,
                "metadata": chunk_metadata
            })
            
        self.processed_docs += 1
        self.total_chunks += len(chunks)
        self.processing_stats["total_docs"] = self.processed_docs
        self.processing_stats["total_chunks"] = self.total_chunks
        self.processing_stats["avg_chunks_per_doc"] = self.total_chunks / self.processed_docs
        
        return result
    
    def _chunk_text(self, text: str) -> List[str]:
        """Split text into chunks with specified overlap.
        
        Args:
            text: The text to chunk
            
        Returns:
            A list of text chunks
        """
        if len(text) <= self.chunk_size:
            return [text]
            
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # Try to find a good breaking point (newline or period)
            if end < len(text):
                # Look for newline
                newline_pos = text.rfind('\n', start, end)
                period_pos = text.rfind('. ', start, end)
                
                if newline_pos > start + self.chunk_size // 2:
                    end = newline_pos + 1
                elif period_pos > start + self.chunk_size // 2:
                    end = period_pos + 2
            
            chunks.append(text[start:end])
            start = end - self.overlap
            
        return chunks
    
    def get_processing_stats(self) -> Dict:
        """Get statistics about document processing.
        
        Returns:
            A dictionary with processing statistics
        """
        stats = self.processing_stats.copy()
        stats["end_time"] = datetime.now()
        
        if stats["start_time"]:
            duration = stats["end_time"] - stats["start_time"]
            stats["processing_duration_seconds"] = duration.total_seconds()
            
            if stats["total_docs"] > 0:
                stats["seconds_per_document"] = duration.total_seconds() / stats["total_docs"]
                
        return stats


def calculate_embedding_similarity(embedding1: List[float], embedding2: List[float]) -> float:
    """Calculate cosine similarity between two embeddings.
    
    Args:
        embedding1: First embedding vector
        embedding2: Second embedding vector
        
    Returns:
        Cosine similarity score
    """
    # Convert to numpy arrays
    vec1 = np.array(embedding1)
    vec2 = np.array(embedding2)
    
    # Calculate cosine similarity
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    return dot_product / (norm1 * norm2)


if __name__ == "__main__":
    # Sample usage
    processor = DocumentProcessor(chunk_size=500, overlap=100)
    
    sample_text = """
    Retrieval-Augmented Generation (RAG) is a technique that enhances large language models
    by incorporating external knowledge retrieval. This approach allows models to access and
    utilize information beyond their training data, improving factuality and reducing hallucinations.
    
    The RAG architecture typically consists of two main components:
    1. A retrieval system that identifies relevant documents or passages from a knowledge base
    2. A generation system that produces responses based on both the query and the retrieved information
    
    Benefits of RAG include:
    - Improved accuracy and factuality
    - Reduced hallucinations
    - Ability to access up-to-date information
    - Enhanced domain-specific knowledge
    - Greater transparency and explainability
    
    Common challenges in implementing RAG systems include:
    - Ensuring retrieval quality and relevance
    - Managing computational efficiency
    - Balancing between retrieved information and model knowledge
    - Handling contradictory information
    - Addressing potential biases in the knowledge base
    """
    
    chunks = processor.process_document(sample_text, {"source": "sample_script", "filetype": "python"})
    
    print(f"Generated {len(chunks)} chunks from the sample text")
    print(f"Processing stats: {processor.get_processing_stats()}")

================
File: data/test_docs/smart_home_developer_reference.md
================
# SmartHome Developer Reference

## API Overview

The SmartHome system provides a RESTful API that allows developers to integrate with and extend the functionality of their SmartHome installation.

### Base URL

All API requests should be made to:

```
https://api.smarthome.example.com/v1
```

### Authentication

The API uses OAuth 2.0 for authentication. To obtain an access token:

1. Register your application at the [SmartHome Developer Portal](https://developer.smarthome.example.com)
2. Implement the OAuth 2.0 authorization code flow
3. Request the appropriate scopes for your application

Example authorization request:

```http
GET https://auth.smarthome.example.com/authorize?
  response_type=code&
  client_id=YOUR_CLIENT_ID&
  redirect_uri=YOUR_REDIRECT_URI&
  scope=devices.read devices.write
```

After the user authorizes your application, they will be redirected to your `redirect_uri` with an authorization code:

```
https://your-app.example.com/callback?code=AUTHORIZATION_CODE
```

Exchange this code for an access token:

```http
POST https://auth.smarthome.example.com/token
Content-Type: application/x-www-form-urlencoded

grant_type=authorization_code&
code=AUTHORIZATION_CODE&
client_id=YOUR_CLIENT_ID&
client_secret=YOUR_CLIENT_SECRET&
redirect_uri=YOUR_REDIRECT_URI
```

Response:

```json
{
  "access_token": "ACCESS_TOKEN",
  "token_type": "bearer",
  "expires_in": 3600,
  "refresh_token": "REFRESH_TOKEN",
  "scope": "devices.read devices.write"
}
```

Include the access token in the Authorization header of all API requests:

```http
Authorization: Bearer ACCESS_TOKEN
```

## API Endpoints

### Devices

#### List all devices

```http
GET /devices
```

Parameters:
- `type`: Filter devices by type (optional)
- `room`: Filter devices by room (optional)
- `status`: Filter devices by status (online/offline) (optional)

Response:

```json
{
  "devices": [
    {
      "id": "device_123456",
      "name": "Living Room Motion Sensor",
      "type": "motion_sensor",
      "model": "SH-MS100",
      "connected": true,
      "battery": 87,
      "last_event": "2025-03-20T15:30:45Z"
    },
    {
      "id": "device_789012",
      "name": "Front Door Sensor",
      "type": "door_sensor",
      "model": "SH-DS100",
      "connected": true,
      "battery": 92,
      "last_event": "2025-03-21T08:15:22Z"
    }
  ]
}
```

#### Get device details

```http
GET /devices/{device_id}
```

Response:

```json
{
  "id": "device_123456",
  "name": "Living Room Motion Sensor",
  "type": "motion_sensor",
  "model": "SH-MS100",
  "firmware_version": "1.2.5",
  "connected": true,
  "battery": 87,
  "last_event": "2025-03-20T15:30:45Z",
  "room": "living_room",
  "capabilities": ["motion", "temperature", "light"],
  "attributes": {
    "motion": false,
    "temperature": 72.3,
    "light": 65
  },
  "created_at": "2024-12-15T08:30:00Z",
  "updated_at": "2025-03-20T15:30:45Z"
}
```

#### Update device

```http
PATCH /devices/{device_id}
```

Request body:

```json
{
  "name": "Updated Device Name",
  "room": "bedroom"
}
```

Response:

```json
{
  "id": "device_123456",
  "name": "Updated Device Name",
  "room": "bedroom"
  // ... other device properties
}
```

#### Control device

```http
POST /devices/{device_id}/commands
```

Request body:

```json
{
  "command": "set_level",
  "arguments": {
    "level": 75
  }
}
```

Response:

```json
{
  "status": "success",
  "command_id": "cmd_987654",
  "executed_at": "2025-03-22T14:25:30Z"
}
```

### Events

#### Get recent events

```http
GET /events
```

Parameters:
- `limit`: Maximum number of events to return (default: 50, max: 500)
- `device_id`: Filter events by device ID
- `event_type`: Filter events by type (motion, door, button, etc.)
- `start_time`: ISO 8601 formatted timestamp
- `end_time`: ISO 8601 formatted timestamp

Response:

```json
{
  "events": [
    {
      "id": "evt_123456",
      "device_id": "device_123456",
      "type": "motion",
      "value": "active",
      "timestamp": "2025-03-20T15:30:45Z"
    },
    {
      "id": "evt_123457",
      "device_id": "device_789012",
      "type": "door",
      "value": "open",
      "timestamp": "2025-03-21T08:15:22Z"
    }
  ]
}
```

### Rooms

#### List all rooms

```http
GET /rooms
```

Response:

```json
{
  "rooms": [
    {
      "id": "room_123456",
      "name": "Living Room",
      "device_count": 5
    },
    {
      "id": "room_789012",
      "name": "Kitchen",
      "device_count": 3
    }
  ]
}
```

### Automations

#### List all automations

```http
GET /automations
```

Response:

```json
{
  "automations": [
    {
      "id": "auto_123456",
      "name": "Motion-activated lights",
      "enabled": true,
      "trigger": {
        "type": "device",
        "device_id": "device_123456",
        "event": "motion.active"
      },
      "conditions": [
        {
          "type": "time",
          "operation": "after_sunset"
        }
      ],
      "actions": [
        {
          "type": "device_command",
          "device_id": "device_654321",
          "command": "turn_on"
        }
      ]
    }
  ]
}
```

#### Create automation

```http
POST /automations
```

Request body:

```json
{
  "name": "Motion-activated lights",
  "enabled": true,
  "trigger": {
    "type": "device",
    "device_id": "device_123456",
    "event": "motion.active"
  },
  "conditions": [
    {
      "type": "time",
      "operation": "after_sunset"
    }
  ],
  "actions": [
    {
      "type": "device_command",
      "device_id": "device_654321",
      "command": "turn_on"
    }
  ]
}
```

## Webhook Integration

You can register webhook URLs to receive real-time notifications when events occur in the SmartHome system.

To register a webhook:

```http
POST /webhooks
```

Request body:

```json
{
  "url": "https://your-server.example.com/smarthome-webhook",
  "events": ["motion", "door", "button"],
  "secret": "your_webhook_secret"
}
```

Response:

```json
{
  "id": "webhook_123456",
  "url": "https://your-server.example.com/smarthome-webhook",
  "events": ["motion", "door", "button"],
  "created_at": "2025-03-22T12:00:00Z"
}
```

The secret will be used to sign webhook requests with an HMAC, allowing you to verify that requests come from the SmartHome system.

When an event occurs, the SmartHome system will send an HTTP POST request to your webhook URL:

```http
POST https://your-server.example.com/smarthome-webhook
Content-Type: application/json
X-SmartHome-Signature: sha256=...

{
  "event_type": "motion",
  "device_id": "device_123456",
  "value": "active",
  "timestamp": "2025-03-22T12:05:30Z"
}
```

To verify the signature:

```python
import hmac
import hashlib

def verify_webhook_signature(request_body, signature_header, webhook_secret):
    expected_signature = hmac.new(
        webhook_secret.encode('utf-8'),
        request_body.encode('utf-8'),
        hashlib.sha256
    ).hexdigest()
    
    signature = signature_header.split('=')[1]
    
    return hmac.compare_digest(signature, expected_signature)
```

## Real-time API

In addition to webhooks, you can use the SmartHome WebSocket API for real-time updates.

Connect to:

```
wss://api.smarthome.example.com/v1/ws
```

Authentication is done by including the access token as a query parameter:

```
wss://api.smarthome.example.com/v1/ws?access_token=YOUR_ACCESS_TOKEN
```

After connection, subscribe to device events:

```json
{
  "type": "subscribe",
  "topics": ["device.device_123456", "device.device_789012"]
}
```

You'll receive messages when events occur:

```json
{
  "topic": "device.device_123456",
  "type": "event",
  "data": {
    "event_type": "motion",
    "value": "active",
    "timestamp": "2025-03-22T12:05:30Z"
  }
}
```

## Error Handling

The API uses standard HTTP status codes to indicate success or failure of requests.

Common status codes:

- 200 OK: Request succeeded
- 201 Created: Resource was successfully created
- 400 Bad Request: Invalid request parameters
- 401 Unauthorized: Authentication failed
- 403 Forbidden: Permission denied
- 404 Not Found: Resource not found
- 429 Too Many Requests: Rate limit exceeded
- 500 Internal Server Error: Server encountered an error

Error response format:

```json
{
  "error": {
    "code": "invalid_request",
    "message": "The request is missing a required parameter",
    "details": {
      "missing_parameter": "name"
    }
  }
}
```

## Rate Limiting

The API implements rate limiting to protect against abuse. Rate limits are applied on a per-token basis.

Current rate limits:
- 60 requests per minute
- 1000 requests per hour

Rate limit headers are included in all responses:

```
X-RateLimit-Limit: 60
X-RateLimit-Remaining: 59
X-RateLimit-Reset: 1648164342
```

If you exceed the rate limit, you'll receive a 429 Too Many Requests response.

## SDK Libraries

Official SmartHome SDK libraries are available for several programming languages:

- [JavaScript/Node.js](https://github.com/smarthome-example/smarthome-node)
- [Python](https://github.com/smarthome-example/smarthome-python)
- [Ruby](https://github.com/smarthome-example/smarthome-ruby)
- [Java](https://github.com/smarthome-example/smarthome-java)
- [Go](https://github.com/smarthome-example/smarthome-go)

Example using the Node.js SDK:

```javascript
const SmartHome = require('smarthome-sdk');

const client = new SmartHome.Client({
  clientId: 'YOUR_CLIENT_ID',
  clientSecret: 'YOUR_CLIENT_SECRET',
  redirectUri: 'YOUR_REDIRECT_URI'
});

// If you already have an access token
client.setAccessToken('ACCESS_TOKEN');

// Get all devices
client.devices.list()
  .then(devices => {
    console.log(devices);
  })
  .catch(error => {
    console.error(error);
  });

// Control a device
client.devices.sendCommand('device_123456', 'turn_on')
  .then(result => {
    console.log(result);
  })
  .catch(error => {
    console.error(error);
  });
```

## Integration Examples

### Motion-activated lighting

This example demonstrates how to create an automation that turns on lights when motion is detected, but only after sunset.

```javascript
const SmartHome = require('smarthome-sdk');
const client = new SmartHome.Client({ /* auth details */ });

async function createMotionLightingAutomation() {
  const automation = {
    name: "Motion-activated lights",
    enabled: true,
    trigger: {
      type: "device",
      device_id: "device_123456", // Motion sensor
      event: "motion.active"
    },
    conditions: [
      {
        type: "time",
        operation: "after_sunset"
      }
    ],
    actions: [
      {
        type: "device_command",
        device_id: "device_654321", // Light
        command: "turn_on"
      }
    ]
  };
  
  try {
    const result = await client.automations.create(automation);
    console.log("Automation created:", result);
  } catch (error) {
    console.error("Failed to create automation:", error);
  }
}

createMotionLightingAutomation();
```

### Security monitoring

This example shows how to set up a webhook to receive alerts when doors or windows are opened:

```python
from flask import Flask, request, jsonify
import hmac
import hashlib

app = Flask(__name__)

WEBHOOK_SECRET = "your_webhook_secret"

@app.route('/smarthome-webhook', methods=['POST'])
def webhook():
    # Verify signature
    signature_header = request.headers.get('X-SmartHome-Signature')
    if not signature_header:
        return jsonify({"error": "Missing signature"}), 401
    
    request_body = request.get_data().decode('utf-8')
    expected_signature = hmac.new(
        WEBHOOK_SECRET.encode('utf-8'),
        request_body.encode('utf-8'),
        hashlib.sha256
    ).hexdigest()
    
    signature = signature_header.split('=')[1]
    
    if not hmac.compare_digest(signature, expected_signature):
        return jsonify({"error": "Invalid signature"}), 401
    
    # Process the event
    event = request.json
    
    if event['event_type'] == 'door' and event['value'] == 'open':
        # Send alert, log event, etc.
        print(f"Door opened: {event['device_id']} at {event['timestamp']}")
    
    return jsonify({"status": "received"}), 200

if __name__ == '__main__':
    app.run(port=5000)
```

## Troubleshooting

### Common Issues

1. **Authentication failures**
   - Check that your client ID and secret are correct
   - Verify that your redirect URI exactly matches the one registered
   - Ensure your access token hasn't expired

2. **Rate limiting**
   - Implement exponential backoff for retries
   - Cache responses where appropriate
   - Optimize your code to reduce unnecessary API calls

3. **Webhook delivery issues**
   - Make sure your server is publicly accessible
   - Check that you're correctly verifying the signature
   - Implement proper error handling in your webhook endpoint

### Debugging Tools

The SmartHome Developer Portal provides several debugging tools:

1. **API Explorer**: Test API endpoints interactively
2. **Webhook Tester**: Send test events to your webhook endpoints
3. **Event Logs**: View recent API requests and responses
4. **Token Debugger**: Inspect your access tokens and permissions

## API Changelog

### v1.5 (2025-02-15)
- Added support for room grouping and hierarchy
- Expanded automation capabilities with more condition types
- Improved webhook reliability with delivery receipts
- Enhanced rate limiting with more granular controls

### v1.4 (2024-12-10)
- Added WebSocket API for real-time updates
- Introduced device categories and improved filtering
- Added batch operations for device control
- Expanded SDK support to include Go

### v1.3 (2024-10-05)
- Added historical data endpoints for device states
- Improved authentication with PKCE support
- Added support for geofencing triggers
- Enhanced error reporting with more detailed information

### v1.2 (2024-08-20)
- Added webhook signature verification
- Introduced automation API
- Expanded device capabilities
- Added SDK libraries for JavaScript, Python, and Ruby

### v1.1 (2024-06-15)
- Added support for device rooms and groups
- Improved rate limiting with header information
- Enhanced event filtering capabilities
- Added support for device commands

### v1.0 (2024-04-01)
- Initial release
- Basic device management
- Event history
- User authentication

================
File: data/test_docs/smart_home_device_comparison.csv
================
Device ID,Device Name,Category,Protocol,Battery Life,Price,Indoor Range,Outdoor Range,Water Resistant,Voice Control,Hub Required
SH-MS100,Motion Sensor,Sensor,Zigbee,2 years,$24.99,40 ft,25 ft,No,No,Yes
SH-DS100,Door Sensor,Sensor,Zigbee,2 years,$19.99,50 ft,30 ft,No,No,Yes
SH-LS100,Light Switch,Switch,Z-Wave,N/A,$34.99,100 ft,75 ft,No,Yes,Yes
SH-PL100,Smart Plug,Plug,Wi-Fi,N/A,$29.99,150 ft,100 ft,No,Yes,No
SH-BL100,Smart Bulb,Lighting,Bluetooth,N/A,$15.99,30 ft,N/A,No,Yes,No
SH-KB100,Smart Keypad,Security,SHC,1 year,$79.99,N/A,N/A,Yes,No,Yes
SH-CM100,Smart Camera,Security,Wi-Fi,8 hours,$129.99,N/A,100 ft,Yes,Yes,No
SH-TH100,Temperature/Humidity Sensor,Sensor,Zigbee,18 months,$39.99,60 ft,45 ft,Yes,No,Yes
SH-WV100,Water Valve Controller,Plumbing,Z-Wave,N/A,$89.99,50 ft,N/A,Yes,Yes,Yes
SH-RC100,Remote Control,Controller,RF,1 year,$79.99,75 ft,50 ft,No,Yes,Yes
SH-SW100,Siren/Warning Device,Security,Zigbee,Backup: 24 hours,$49.99,100 ft,75 ft,Yes,No,Yes
SH-LD100,Leak Detector,Sensor,Zigbee,3 years,$29.99,40 ft,N/A,Yes,No,Yes
SH-CO100,CO Detector,Safety,Zigbee,5 years,$59.99,50 ft,N/A,No,No,Yes
SH-SM100,Smoke Detector,Safety,Zigbee,5 years,$59.99,50 ft,N/A,No,No,Yes
SH-GS100,Glass Break Sensor,Security,Zigbee,3 years,$39.99,30 ft,N/A,No,No,Yes
SH-VB100,Vibration Sensor,Security,Zigbee,2 years,$24.99,30 ft,N/A,No,No,Yes
SH-LK100,Smart Lock,Security,Z-Wave,1 year,$149.99,50 ft,N/A,Yes,Yes,Yes
SH-GD100,Garage Door Controller,Security,Z-Wave,N/A,$79.99,75 ft,50 ft,No,Yes,Yes
SH-TH101,Outdoor Temperature Sensor,Sensor,Zigbee,18 months,$49.99,N/A,75 ft,Yes,No,Yes
SH-IR100,IR Blaster,Controller,Wi-Fi,N/A,$39.99,30 ft,N/A,No,Yes,No

================
File: data/test_docs/smart_home_technical_specs.txt
================
# SmartHome Technical Specifications

## System Architecture

The SmartHome system follows a hub-and-spoke architecture with the following components:

### SmartHome Hub (Model SH-100)
- Central processing unit: ARM Cortex-A53 quad-core @ 1.4GHz
- Memory: 2GB RAM
- Storage: 16GB eMMC
- Connectivity: Wi-Fi (802.11ac), Bluetooth 5.0, Zigbee 3.0, Z-Wave
- Power: 5V DC, 2A
- Dimensions: 4.5" x 4.5" x 1.2" (114mm x 114mm x 30mm)
- Weight: 8.5 oz (240g)

### Communication Protocols
The SmartHome system uses the following protocols for device communication:
- SmartHome Connect (SHC) - proprietary protocol for secure device communication
- MQTT for lightweight messaging
- CoAP for constrained devices
- HTTP/HTTPS for cloud connectivity

### Security Features
- End-to-end encryption (AES-256)
- Secure boot
- Automatic security updates
- Certificate-based device authentication
- OAuth 2.0 for API authentication
- Local processing of sensitive commands
- Secure element for cryptographic operations
- Regular security audits and penetration testing

## Hardware Components

### Main Board
- SoC: Broadcom BCM2711
- GPU: VideoCore VI
- Thermal management: Passive cooling with aluminum heat spreader
- Watchdog timer for automatic recovery

### Network Interface
- Wi-Fi: 2.4GHz and 5GHz IEEE 802.11ac wireless
- Ethernet: 10/100/1000 BaseT Gigabit Ethernet port
- Bluetooth: Bluetooth 5.0, BLE
- Zigbee: Built-in Zigbee 3.0 coordinator
- Z-Wave: Z-Wave Plus v2 certified

### I/O Interfaces
- USB: 2x USB 3.0 ports
- HDMI: 1x micro HDMI port (for setup and diagnostics)
- Power: USB-C power input
- Buttons: Reset, pairing, and power

### Power Management
- Input: 5V DC, 2A minimum (USB-C)
- Power consumption: 2.5W typical, 5W peak
- Backup power: Optional UPS accessory with 4-hour backup
- Power monitoring and management features

## Software Architecture

### Core Software Stack
- Operating System: Custom Linux distribution (SmartHomeOS)
- Kernel: Linux 5.10 LTS
- Runtime: Node.js 16.x and Python 3.9
- Database: SQLite (local), PostgreSQL (cloud)
- Message broker: Mosquitto MQTT

### Software Components
- Device Manager: Manages device discovery, pairing, and lifecycle
- Rule Engine: Processes automations and rules
- Event Bus: Manages event distribution and handling
- State Manager: Maintains current and historical device states
- Security Manager: Handles authentication, authorization, and encryption
- Update Manager: Manages software and firmware updates
- API Gateway: Exposes RESTful and WebSocket APIs

### Cloud Services
- Account Management: User accounts and authentication
- Remote Access: Secure remote access to local hub
- Backup & Restore: Configuration and automation backup
- Analytics: Anonymous usage statistics (opt-in)
- Voice Assistant Integration: Cloud-based integration with third-party voice services

## Performance Specifications

### Device Capacity
- Maximum devices: 200 devices per hub
- Recommended devices: 50-100 for optimal performance
- Event processing: Up to 100 events per second
- Automation limit: 200 automation rules per hub

### Networking
- Local network bandwidth: <1 Mbps typical usage
- Internet bandwidth: 50-100 MB per day typical usage
- Local latency: <50ms for device commands
- Remote latency: Dependent on internet connection (typically <500ms)

### Reliability
- MTBF (Mean Time Between Failures): >50,000 hours
- Automatic recovery from most error conditions
- Daily automatic diagnostics and health checks
- Optional heartbeat monitoring

## Upgrade and Expansion

### Software Updates
- Automatic over-the-air (OTA) updates
- Update frequency: Monthly for regular updates, immediate for security patches
- Update window: Configurable by user (default: 2:00 AM - 4:00 AM local time)
- Rollback capability for failed updates

### Hardware Expansion
- USB expansion for additional protocols (e.g., Insteon, KNX)
- Support for multiple hubs in mesh configuration
- External antenna options for extended range

## Compatibility

### Third-Party Integration
- Voice assistants: Amazon Alexa, Google Assistant, Apple HomeKit
- Smart platforms: IFTTT, Samsung SmartThings, Home Assistant
- Custom systems: REST API and WebHooks

### Certified Device Partners
- Philips Hue lighting
- Schlage and Yale smart locks
- Ecobee and Nest thermostats
- Ring and Arlo cameras
- GE, Leviton, and Lutron switches and dimmers

## Development Tools

### SmartHome Developer Kit
- APIs: REST, WebSocket, and local Node.js
- SDKs: JavaScript, Python, Java, Swift
- Development environment: VSCode with SmartHome extension
- Simulator: Virtual device simulator for testing
- Documentation: Comprehensive API reference and tutorials

### Certification Program
- Self-certification tools for partners
- Certification requirements and guidelines
- Compatibility testing framework
- Performance and security testing tools

## Environmental Specifications

### Operating Environment
- Temperature: 32°F to 104°F (0°C to 40°C)
- Humidity: 10% to 90% non-condensing
- Altitude: Up to 10,000 feet (3,000 meters)

### Regulatory Compliance
- FCC certified (USA)
- CE marked (Europe)
- UL listed
- RoHS compliant
- WEEE registered
- California Energy Commission (CEC) compliant

## Warranty and Support

### Warranty
- 2-year limited hardware warranty
- 90-day software support
- Extended warranty options available

### Support Options
- Online knowledge base and community forums
- Email support with 24-hour response time
- Phone support (available 7 AM - 9 PM ET, 7 days a week)
- Premium support plans for priority assistance

## Future Roadmap (Planned for 2025-2026)

### Hardware Enhancements
- Thread protocol support
- Matter protocol certification
- Improved energy monitoring and management
- Enhanced security features

### Software Enhancements
- Advanced AI-based automation suggestions
- Improved anomaly detection for security
- Enhanced energy management and optimization
- Advanced scene creation and management
- Expanded developer APIs and tools

================
File: data/test_docs/smart_home_user_guide.txt
================
SmartHome User Guide
====================

Getting Started
--------------

1. Unbox your SmartHome Hub (Model SH-100) and connect it to power using the provided adapter.
2. Download the SmartHome mobile app from the App Store or Google Play.
3. Launch the app and follow the on-screen instructions to create an account.
4. Connect your hub to your home Wi-Fi network through the app.
5. Once connected, the hub will automatically check for and install the latest firmware.

Adding Devices
-------------

To add a new device to your SmartHome system:

1. Press the "Add Device" button in the mobile app.
2. Select the device type from the list or scan the QR code on the device.
3. Put the device in pairing mode according to its instructions (usually by holding a button for 5 seconds).
4. Follow the app instructions to complete the pairing process.

Troubleshooting
--------------

Hub LED Status Indicators:
- Solid Blue: Hub is powered and working normally
- Blinking Blue: Hub is in pairing mode
- Solid Red: Hub is starting up
- Blinking Red: Hub has no internet connection
- Alternating Red/Blue: Hub is updating firmware

Common Issues:

1. Devices won't connect:
   - Ensure the device is within range of the hub (30-50 feet for most devices)
   - Check that the device is in pairing mode
   - Verify the device is compatible with SmartHome (see compatible devices list)

2. Hub offline:
   - Check your internet connection
   - Restart your router
   - Power cycle the hub by unplugging it for 10 seconds, then plugging it back in

3. App can't find hub during setup:
   - Make sure your phone is connected to the same Wi-Fi network
   - Ensure the hub's power LED is solid blue
   - Try moving your phone closer to the hub
   - Reset the hub by pressing and holding the reset button for 10 seconds

Voice Integration
---------------

The SmartHome system works with the following voice assistants:

1. Amazon Alexa
   - Open the Alexa app
   - Go to Skills & Games
   - Search for "SmartHome"
   - Enable the SmartHome skill
   - Link your SmartHome account

2. Google Assistant
   - Open the Google Home app
   - Tap the "+" icon
   - Select "Set up device"
   - Choose "Works with Google"
   - Find and select SmartHome
   - Link your SmartHome account

3. Apple HomeKit
   - Open the Home app on your iOS device
   - Tap the "+" icon
   - Select "Add Accessory"
   - Scan the HomeKit setup code found on the bottom of your SmartHome Hub
   - Follow the prompts to complete setup

Maintenance
----------

Firmware Updates:
- The SmartHome Hub checks for updates automatically every 24 hours
- Updates are installed between 2:00 AM and 4:00 AM local time to minimize disruption
- You can manually check for updates in the SmartHome app by going to Settings > Hub > Check for Updates

Battery-powered devices:
- The SmartHome app will notify you when device batteries are running low
- Most sensors use standard CR2032 or CR2450 coin cell batteries
- The SmartHome Remote Control uses 2 AAA batteries
- Average battery life is detailed in the product specifications (typically 1-2 years for sensors)

Factory Reset:
To reset your SmartHome Hub to factory settings:
1. Locate the small reset button on the back of the hub
2. Press and hold for 15 seconds until all LEDs flash
3. The hub will restart and enter setup mode
4. Follow the initial setup process in the app

WARNING: Factory reset will delete all your devices, routines, and settings!

Safety Information
----------------

- The SmartHome Hub and plug-in devices are designed for indoor use only
- Keep all devices away from water and excessive moisture
- Do not disassemble any SmartHome devices
- If a device becomes damaged, discontinue use immediately
- The SmartHub operates on 5V DC power and presents no electrical hazard
- Some sensors contain small batteries which should be kept away from children

Customer Support
--------------

For additional assistance:
- Visit our support website: https://support.smarthome.example.com
- Email us: support@smarthome.example.com
- Call our support line: 1-800-SMART-HOME (1-800-762-7846)
- Support hours: Monday-Friday 8AM-8PM, Saturday 9AM-5PM Eastern Time

================
File: data/test_docs/technical_documentation.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: data/test_quality_docs/product_specifications.csv
================
Product ID,Name,Category,Price,Features,Release Date
P001,MetisRAG Enterprise,Software,$4999,Advanced RAG capabilities|Multi-document retrieval|Enterprise security,2025-01-15
P002,MetisRAG Professional,Software,$1999,Standard RAG capabilities|Document management|API access,2025-01-15
P003,MetisRAG Basic,Software,$499,Basic RAG capabilities|Limited document storage|Web interface,2025-01-15
P004,MetisRAG API,Service,$0.10 per query,Pay-per-use|REST API|Developer documentation,2025-02-01
P005,MetisRAG Mobile,Mobile App,$9.99 per month,iOS and Android|Offline capabilities|Voice interface,2025-03-15

================
File: data/test_quality_docs/quarterly_report.txt
================
Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs

================
File: data/test_quality_docs/technical_documentation.md
================
# Metis RAG Technical Documentation

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: data/cookies.txt
================
# Netscape HTTP Cookie File
# https://curl.se/docs/http-cookies.html
# This file was generated by libcurl! Edit at your own risk.

================
File: data/metis_rag_visualization.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis_RAG System Visualization</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        h1 {
            text-align: center;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #3498db;
        }
        h2 {
            margin-top: 20px;
            padding-bottom: 5px;
            border-bottom: 1px solid #bdc3c7;
            cursor: pointer;
        }
        h2:hover {
            color: #3498db;
        }
        h2::before {
            content: "▶ ";
            font-size: 0.8em;
        }
        h2.expanded::before {
            content: "▼ ";
        }
        .diagram-container {
            background-color: white;
            padding: 15px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin-bottom: 20px;
            overflow: hidden;
            max-height: 500px;
            transition: max-height 0.3s ease;
        }
        .description {
            margin-bottom: 10px;
        }
        .mermaid {
            margin: 0 auto;
            font-size: 12px;
            transform-origin: top left;
        }
        .section {
            margin-bottom: 20px;
            display: none;
        }
        .section.expanded {
            display: block;
        }
        .key-component {
            background-color: white;
            padding: 15px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin-bottom: 15px;
        }
        .key-component h3 {
            margin-top: 0;
            color: #3498db;
        }
        .key-component ul {
            padding-left: 20px;
        }
        .controls {
            margin-bottom: 10px;
            text-align: right;
        }
        .zoom-control {
            display: inline-block;
            margin-left: 10px;
        }
        .zoom-btn {
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 3px;
            padding: 5px 10px;
            cursor: pointer;
            font-size: 14px;
            margin: 0 2px;
        }
        .zoom-btn:hover {
            background-color: #2980b9;
        }
        .zoom-level {
            display: inline-block;
            width: 40px;
            text-align: center;
        }
        #toc {
            background-color: white;
            padding: 15px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }
        #toc h2 {
            margin-top: 0;
            cursor: default;
        }
        #toc h2::before {
            content: "";
        }
        #toc ul {
            list-style-type: none;
            padding-left: 10px;
        }
        #toc li {
            margin-bottom: 5px;
        }
        #toc a {
            color: #3498db;
            text-decoration: none;
        }
        #toc a:hover {
            text-decoration: underline;
        }
        .expand-all-btn {
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 3px;
            padding: 5px 10px;
            cursor: pointer;
            font-size: 14px;
            margin-left: 10px;
        }
        .expand-all-btn:hover {
            background-color: #2980b9;
        }
        .legend {
            background-color: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 15px;
            font-size: 12px;
        }
        .legend-item {
            display: inline-block;
            margin-right: 15px;
            margin-bottom: 5px;
        }
        .legend-color {
            display: inline-block;
            width: 12px;
            height: 12px;
            margin-right: 5px;
            border-radius: 2px;
            vertical-align: middle;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Initialize Mermaid
            mermaid.initialize({
                startOnLoad: true,
                theme: 'default',
                securityLevel: 'loose',
                flowchart: {
                    useMaxWidth: true,
                    htmlLabels: true,
                    curve: 'basis'
                },
                er: {
                    useMaxWidth: true
                },
                sequence: {
                    useMaxWidth: true,
                    showSequenceNumbers: true
                }
            });
            
            // Make sections collapsible
            const headings = document.querySelectorAll('h2:not(#toc-heading)');
            headings.forEach(heading => {
                heading.addEventListener('click', function() {
                    this.classList.toggle('expanded');
                    const section = this.nextElementSibling;
                    section.classList.toggle('expanded');
                });
            });
            
            // Expand first section by default
            if (headings.length > 0) {
                headings[0].classList.add('expanded');
                headings[0].nextElementSibling.classList.add('expanded');
            }
            
            // Add zoom functionality
            const containers = document.querySelectorAll('.diagram-container');
            containers.forEach((container, index) => {
                const mermaidDiv = container.querySelector('.mermaid');
                if (mermaidDiv) {
                    let scale = 0.8; // Initial scale
                    
                    // Create zoom controls
                    const controls = document.createElement('div');
                    controls.className = 'controls';
                    
                    const zoomControl = document.createElement('div');
                    zoomControl.className = 'zoom-control';
                    
                    const zoomOutBtn = document.createElement('button');
                    zoomOutBtn.className = 'zoom-btn';
                    zoomOutBtn.textContent = '-';
                    zoomOutBtn.addEventListener('click', function() {
                        scale = Math.max(0.3, scale - 0.1);
                        mermaidDiv.style.transform = `scale(${scale})`;
                        zoomLevel.textContent = `${Math.round(scale * 100)}%`;
                    });
                    
                    const zoomLevel = document.createElement('span');
                    zoomLevel.className = 'zoom-level';
                    zoomLevel.textContent = `${Math.round(scale * 100)}%`;
                    
                    const zoomInBtn = document.createElement('button');
                    zoomInBtn.className = 'zoom-btn';
                    zoomInBtn.textContent = '+';
                    zoomInBtn.addEventListener('click', function() {
                        scale = Math.min(2, scale + 0.1);
                        mermaidDiv.style.transform = `scale(${scale})`;
                        zoomLevel.textContent = `${Math.round(scale * 100)}%`;
                    });
                    
                    const fitBtn = document.createElement('button');
                    fitBtn.className = 'zoom-btn';
                    fitBtn.textContent = 'Fit';
                    fitBtn.addEventListener('click', function() {
                        scale = 0.8;
                        mermaidDiv.style.transform = `scale(${scale})`;
                        zoomLevel.textContent = `${Math.round(scale * 100)}%`;
                    });
                    
                    zoomControl.appendChild(zoomOutBtn);
                    zoomControl.appendChild(zoomLevel);
                    zoomControl.appendChild(zoomInBtn);
                    zoomControl.appendChild(fitBtn);
                    
                    controls.appendChild(zoomControl);
                    container.insertBefore(controls, mermaidDiv);
                    
                    // Set initial scale
                    mermaidDiv.style.transform = `scale(${scale})`;
                }
            });
            
            // Add expand all button
            const expandAllBtn = document.createElement('button');
            expandAllBtn.className = 'expand-all-btn';
            expandAllBtn.textContent = 'Expand All';
            expandAllBtn.addEventListener('click', function() {
                const allExpanded = document.querySelectorAll('h2.expanded').length === headings.length;
                
                if (allExpanded) {
                    // Collapse all except the first one
                    headings.forEach((heading, index) => {
                        if (index > 0) {
                            heading.classList.remove('expanded');
                            heading.nextElementSibling.classList.remove('expanded');
                        }
                    });
                    this.textContent = 'Expand All';
                } else {
                    // Expand all
                    headings.forEach(heading => {
                        heading.classList.add('expanded');
                        heading.nextElementSibling.classList.add('expanded');
                    });
                    this.textContent = 'Collapse All';
                }
            });
            
            const title = document.querySelector('h1');
            title.appendChild(expandAllBtn);
        });
    </script>
</head>
<body>
    <h1>Metis_RAG System Visualization</h1>
    
    <div id="toc">
        <h2 id="toc-heading">Table of Contents</h2>
        <ul>
            <li><a href="#system-architecture">System Architecture</a></li>
            <li><a href="#database-schema">Database Schema</a></li>
            <li><a href="#query-processing">Query Processing Flow</a></li>
            <li><a href="#document-processing">Document Processing Flow</a></li>
            <li><a href="#tool-system">Tool System</a></li>
            <li><a href="#langgraph">LangGraph Integration</a></li>
            <li><a href="#key-components">Key Components</a></li>
        </ul>
    </div>
    
    <h2 id="system-architecture">System Architecture</h2>
    <div class="section">
        <div class="description">
            <p>This diagram shows the overall architecture of the Metis_RAG system, including the main components and their interactions. The system is organized in layers, with data flowing from the user through the API layer to the core RAG engine and back.</p>
        </div>
        <div class="legend">
            <div class="legend-item"><span class="legend-color" style="background-color: #c4e3f3;"></span> User Interface</div>
            <div class="legend-item"><span class="legend-color" style="background-color: #d9edf7;"></span> API Layer</div>
            <div class="legend-item"><span class="legend-color" style="background-color: #dff0d8;"></span> Core RAG Engine</div>
            <div class="legend-item"><span class="legend-color" style="background-color: #fcf8e3;"></span> Tool System</div>
            <div class="legend-item"><span class="legend-color" style="background-color: #f2dede;"></span> Document Processing</div>
            <div class="legend-item"><span class="legend-color" style="background-color: #e8e8e8;"></span> Data Storage</div>
            <div class="legend-item"><span class="legend-color" style="background-color: #d9d9d9;"></span> Vector Store</div>
        </div>
        <div class="diagram-container">
            <pre class="mermaid">
graph TD
    %% Define styles for different components
    classDef userInterface fill:#c4e3f3,stroke:#9acfea,color:#31708f
    classDef apiLayer fill:#d9edf7,stroke:#9acfea,color:#31708f
    classDef coreEngine fill:#dff0d8,stroke:#d6e9c6,color:#3c763d
    classDef toolSystem fill:#fcf8e3,stroke:#faebcc,color:#8a6d3b
    classDef docProcessing fill:#f2dede,stroke:#ebccd1,color:#a94442
    classDef dataStorage fill:#e8e8e8,stroke:#ccc,color:#333
    classDef vectorStore fill:#d9d9d9,stroke:#bbb,color:#333

    %% User Interface Layer
    WebUI["Web UI"]:::userInterface
    APIClient["API Client"]:::userInterface
    
    %% API Layer
    API["FastAPI Endpoints"]:::apiLayer
    Auth["Authentication"]:::apiLayer
    
    %% Core RAG Engine
    QA["Query Analyzer"]:::coreEngine
    QP["Query Planner"]:::coreEngine
    PE["Plan Executor"]:::coreEngine
    RS["Response Synthesizer"]:::coreEngine
    RE["Response Evaluator"]:::coreEngine
    PL["Process Logger"]:::coreEngine
    
    %% Tool System
    TR["Tool Registry"]:::toolSystem
    RAGTool["RAG Tool"]:::toolSystem
    CalcTool["Calculator Tool"]:::toolSystem
    DBTool["Database Tool"]:::toolSystem
    
    %% Document Processing
    DAS["Document Analysis Service"]:::docProcessing
    WP["Worker Pool"]:::docProcessing
    PJ["Processing Job"]:::docProcessing
    
    %% Database Layer
    PostgreSQL[(PostgreSQL)]:::dataStorage
    DocRepo["Document Repository"]:::dataStorage
    ConvRepo["Conversation Repository"]:::dataStorage
    AnalyticsRepo["Analytics Repository"]:::dataStorage
    
    %% Vector Store
    ChromaDB[(ChromaDB)]:::vectorStore
    
    %% Connections
    WebUI --> APIClient
    APIClient --> API
    API --> Auth
    
    API --> QA
    API --> DAS
    API --> DocRepo
    API --> ConvRepo
    API --> AnalyticsRepo
    
    QA --> QP
    QP --> PE
    PE --> RS
    RS --> RE
    
    PL -.-> QA
    PL -.-> QP
    PL -.-> PE
    PL -.-> RS
    PL -.-> RE
    
    PE --> TR
    TR --> RAGTool
    TR --> CalcTool
    TR --> DBTool
    
    DAS --> WP
    WP --> PJ
    
    DocRepo --> PostgreSQL
    ConvRepo --> PostgreSQL
    AnalyticsRepo --> PostgreSQL
    
    RAGTool --> ChromaDB
    RAGTool --> DocRepo
    PJ --> DocRepo
            </pre>
        </div>
    </div>

    <h2 id="database-schema">Database Schema</h2>
    <div class="section">
        <div class="description">
            <p>This entity-relationship diagram shows the database schema used by the Metis_RAG system.</p>
        </div>
        <div class="diagram-container">
            <pre class="mermaid">
erDiagram
    Documents ||--o{ Chunks : contains
    Documents ||--o{ DocumentTags : has
    Tags ||--o{ DocumentTags : used_in
    Documents }|--|| Folders : stored_in
    Conversations ||--o{ Messages : contains
    Messages ||--o{ Citations : references
    Citations }o--|| Documents : cites
    Citations }o--|| Chunks : cites_specific
    ProcessingJobs ||--o{ Documents : processes
    AnalyticsQueries }o--|| Documents : uses
</pre>
        </div>
        <div class="key-component">
            <h3>Key Database Tables</h3>
            <ul>
                <li><strong>Documents</strong>: Stores document metadata and content</li>
                <li><strong>Chunks</strong>: Contains document fragments for retrieval</li>
                <li><strong>Tags</strong>: Categorizes documents</li>
                <li><strong>Folders</strong>: Organizes documents in a hierarchy</li>
                <li><strong>Conversations</strong>: Tracks user interactions</li>
                <li><strong>Messages</strong>: Stores individual messages in conversations</li>
                <li><strong>Citations</strong>: Links responses to source documents</li>
                <li><strong>ProcessingJobs</strong>: Manages document processing tasks</li>
                <li><strong>AnalyticsQueries</strong>: Records query performance metrics</li>
            </ul>
        </div>
    </div>

    <h2 id="query-processing">Query Processing Flow</h2>
    <div class="section">
        <div class="description">
            <p>This sequence diagram illustrates the flow of a query through the Metis_RAG system.</p>
        </div>
        <div class="diagram-container">
            <pre class="mermaid">
sequenceDiagram
    participant User
    participant API as API Layer
    participant QA as Query Analyzer
    participant QP as Query Planner
    participant PE as Plan Executor
    participant TR as Tool Registry
    participant RT as RAG Tool
    participant VS as Vector Store
    participant RS as Response Synthesizer
    participant RE as Response Evaluator
    participant PL as Process Logger

    User->>API: Submit Query
    API->>QA: Analyze Query
    QA->>PL: Log Analysis
    QA->>QP: Create Plan
    QP->>PL: Log Plan
    QP->>PE: Execute Plan
    
    loop For Each Step
        PE->>TR: Get Tool
        TR->>RT: Execute RAG Tool
        RT->>VS: Retrieve Chunks
        VS-->>RT: Return Chunks
        RT-->>PE: Return Results
        PE->>PL: Log Step Results
    end
    
    PE->>RS: Synthesize Response
    RS->>PL: Log Synthesis
    RS->>RE: Evaluate Response
    RE->>PL: Log Evaluation
    
    alt Response Needs Refinement
        RE->>RS: Refine Response
        RS->>PL: Log Refinement
    end
    
    RE-->>API: Return Final Response
    API-->>User: Display Response
            </pre>
        </div>
    </div>

    <h2 id="document-processing">Document Processing Flow</h2>
    <div class="section">
        <div class="description">
            <p>This sequence diagram shows how documents are processed in the Metis_RAG system.</p>
        </div>
        <div class="diagram-container">
            <pre class="mermaid">
sequenceDiagram
    participant User
    participant API as API Layer
    participant DAS as Document Analysis Service
    participant WP as Worker Pool
    participant PJ as Processing Job
    participant DR as Document Repository
    participant VS as Vector Store

    User->>API: Upload Document
    API->>DR: Store Document
    API->>DAS: Analyze Document
    DAS->>WP: Create Processing Job
    WP->>PJ: Execute Job
    
    PJ->>DR: Retrieve Document
    DR-->>PJ: Return Document
    PJ->>PJ: Chunk Document
    PJ->>VS: Store Chunks & Embeddings
    PJ->>DR: Update Document Status
    
    DR-->>API: Return Success
    API-->>User: Confirm Processing
            </pre>
        </div>
    </div>

    <h2 id="tool-system">Tool System</h2>
    <div class="section">
        <div class="description">
            <p>This class diagram shows the structure of the tool system in Metis_RAG.</p>
        </div>
        <div class="diagram-container">
            <pre class="mermaid">
classDiagram
    class Tool {
        <<abstract>>
        +String name
        +String description
        +execute(input_data)
        +get_description()
        +get_input_schema()
        +get_output_schema()
        +get_examples()
    }
    
    class ToolRegistry {
        -Dict tools
        +register_tool(tool)
        +get_tool(name)
        +list_tools()
        +get_tool_examples(name)
        +get_tool_count()
    }
    
    class RAGTool {
        -RAGEngine rag_engine
        +execute(input_data)
        +get_input_schema()
        +get_output_schema()
        +get_examples()
    }
    
    class DatabaseTool {
        -DatabaseConnection db_connection
        +execute(input_data)
        +get_input_schema()
        +get_output_schema()
        +get_examples()
    }
    
    class CalculatorTool {
        +execute(input_data)
        +get_input_schema()
        +get_output_schema()
        +get_examples()
    }
    
    Tool <|-- RAGTool
    Tool <|-- DatabaseTool
    Tool <|-- CalculatorTool
    ToolRegistry o-- Tool
            </pre>
        </div>
    </div>

    <h2 id="langgraph">LangGraph Integration</h2>
    <div class="section">
        <div class="description">
            <p>This diagram shows how LangGraph is integrated into the Metis_RAG system for complex query processing.</p>
        </div>
        <div class="diagram-container">
            <pre class="mermaid">
flowchart TD
    QA[Query Analysis]
    QP[Query Planning]
    PE[Plan Execution]
    RT[Retrieval]
    QR[Query Refinement]
    CO[Context Optimization]
    GN[Generation]
    RE[Response Evaluation]
    RR[Response Refinement]
    CP[Complete]
    
    QA -->|Simple Query| RT
    QA -->|Complex Query| QP
    QP --> PE
    PE --> RT
    RT -->|Needs Refinement| QR
    RT -->|Good Results| CO
    QR --> RT
    CO --> GN
    GN --> RE
    RE -->|Needs Refinement| RR
    RE -->|Good Response| CP
    RR --> RE
            </pre>
        </div>
    </div>

    <h2 id="key-components">Key Components</h2>
    <div class="section">
        <div class="key-component">
            <h3>Core RAG Engine</h3>
            <ul>
                <li><strong>Query Analyzer</strong>: Determines query complexity and required tools</li>
                <li><strong>Query Planner</strong>: Creates execution plans for complex queries</li>
                <li><strong>Plan Executor</strong>: Executes query plans using appropriate tools</li>
                <li><strong>Response Synthesizer</strong>: Generates coherent responses from execution results</li>
                <li><strong>Response Evaluator</strong>: Assesses response quality and determines if refinement is needed</li>
                <li><strong>Process Logger</strong>: Records the entire query processing workflow for auditing</li>
            </ul>
        </div>
        
        <div class="key-component">
            <h3>Tool System</h3>
            <ul>
                <li><strong>Tool Registry</strong>: Manages available tools</li>
                <li><strong>RAG Tool</strong>: Retrieves information from documents using vector search</li>
                <li><strong>Calculator Tool</strong>: Performs mathematical calculations</li>
                <li><strong>Database Tool</strong>: Queries structured data</li>
            </ul>
        </div>
        
        <div class="key-component">
            <h3>Document Processing</h3>
            <ul>
                <li><strong>Document Analysis Service</strong>: Analyzes documents to determine optimal processing strategies</li>
                <li><strong>Worker Pool</strong>: Manages parallel document processing</li>
                <li><strong>Processing Job</strong>: Represents a document processing task</li>
            </ul>
        </div>
        
        <div class="key-component">
            <h3>Database Layer</h3>
            <ul>
                <li><strong>Document Repository</strong>: Manages document storage and retrieval</li>
                <li><strong>Conversation Repository</strong>: Stores conversation history</li>
                <li><strong>Analytics Repository</strong>: Records query analytics</li>
            </ul>
        </div>
        
        <div class="key-component">
            <h3>Vector Store</h3>
            <ul>
                <li><strong>ChromaDB</strong>: Stores document chunks and embeddings for semantic search</li>
            </ul>
        </div>
    </div>
</body>
</html>

================
File: docs/bug_fixes/2025-03-27-column-name-and-method-signature-fixes.md
================
# Bug Fixes: Column Name and Method Signature Issues (2025-03-27)

## Issues Fixed

### 1. Column Name Issue in Document Listing API Endpoint

**Problem:**
The document listing API endpoint was failing with a `UndefinedColumnError: column "metadata" does not exist` error. This was due to a previous database schema change where the column was renamed from `metadata` to `doc_metadata` to avoid conflicts with SQLAlchemy's reserved `metadata` attribute, but the SQL queries in the API endpoints were not updated to reflect this change.

**Files Modified:**
- `app/api/documents.py`

**Changes Made:**
- Updated SQL queries in `list_documents` function to use `doc_metadata` instead of `metadata AS doc_metadata`
- Updated SQL queries in `filter_documents` function to use `doc_metadata` instead of `metadata AS doc_metadata`
- Fixed the SQL query in `process_document_background` function to use `doc_metadata` instead of `metadata`
- Updated the Document creation in `process_document_background` to use `doc_row.doc_metadata` instead of `doc_row.metadata`

### 2. Conversation Repository Method Signature Issue

**Problem:**
The chat functionality was failing with a `ConversationRepository.create_conversation() got an unexpected keyword argument 'user_id'` error. This was because the `create_conversation` method in the `ConversationRepository` class was updated to use `self.user_id` from the constructor instead of accepting it as a parameter, but the calls to this method in the chat API endpoints were still passing the `user_id` parameter.

**Files Modified:**
- `app/api/chat.py`

**Changes Made:**
- Removed the `user_id` parameter from all calls to `conversation_repository.create_conversation()` in the chat API endpoints
- This includes calls in the `query_chat`, `langgraph_query_chat`, and `enhanced_langgraph_query_chat` functions

## Impact

These fixes resolve the errors that were preventing the document listing and chat functionality from working properly:
- The document listing endpoint can now correctly query the database using the renamed column
- The chat functionality can now create conversations without passing the user_id parameter

## Testing

After applying these fixes, the following functionality should be tested:
1. Document listing and filtering
2. Chat functionality with different models
3. RAG-enabled chat queries
4. LangGraph RAG Agent queries (if enabled)
5. Enhanced LangGraph RAG Agent queries (if enabled)

================
File: docs/bug_fixes/2025-03-27-user-id-consistency-fix.md
================
# User ID Consistency Fix

## Issue
A 403 Forbidden error was occurring on subsequent queries within the same conversation. The root cause was inconsistent handling of user IDs between requests:

1. In the first request (without a valid JWT token), a temporary session ID (e.g., "session_54471745") was generated and stored in the conversation's metadata.
2. In subsequent requests (with a valid JWT token), a UUID user ID was extracted from the token.
3. When checking if the conversation belonged to the user, the system compared the UUID from the current request with the string session ID stored with the conversation, resulting in a mismatch and a 403 error.

## Root Causes

1. **Inconsistent User ID Handling**:
   - The RAG engine generated temporary session IDs for unauthenticated requests
   - These string IDs were stored in conversation metadata
   - Later authenticated requests used UUID user IDs, causing comparison mismatches

2. **Missing User Context in Repository**:
   - The `get_conversation_repository` function didn't pass the user ID to the repository constructor
   - This meant the repository didn't have consistent user context when creating conversations

3. **Direct Metadata Comparison**:
   - Permission checks directly compared `conversation.conv_metadata.get("user_id")` with `user_id`
   - This comparison failed when comparing a string session ID with a UUID

## Fixes Implemented

1. **Added Optional User Authentication**:
   - Created `get_current_user_optional` function in `security.py` that returns the user if authenticated, or None if not
   - This allows handling both authenticated and unauthenticated users consistently

2. **Improved Repository Initialization**:
   - Updated `get_conversation_repository` to pass the current user's ID to the repository constructor
   - This ensures the repository has consistent user context throughout the request lifecycle

3. **Enhanced Permission Checking**:
   - Added an override of the `get_by_id` method in `ConversationRepository` that includes permission checking
   - This ensures that users can only access their own conversations

4. **Consistent API Endpoint Handling**:
   - Updated all chat API endpoints to use the repository's permission checking
   - Removed direct metadata comparisons in favor of repository methods

## Benefits

1. **Consistent User Identification**: The system now handles both authenticated and unauthenticated users consistently
2. **Proper Permission Enforcement**: Permission checks are now handled at the repository level
3. **Reduced Code Duplication**: Permission logic is centralized in the repository
4. **Improved Security**: Users can only access their own conversations

## Files Modified

1. `app/core/security.py` - Added `get_current_user_optional` function
2. `app/db/dependencies.py` - Updated `get_conversation_repository` to pass user context
3. `app/db/repositories/conversation_repository.py` - Added `get_by_id` override with permission checking
4. `app/api/chat.py` - Updated all endpoints to use repository permission checking

================
File: docs/bug_fixes/2025-03-31-chat-memory-fix.md
================
# Chat Memory Fix

**Date:** March 31, 2025  
**Issue:** Metis RAG system not maintaining conversation memory between turns  
**Fix Type:** Enhancement  
**Components Affected:** Query Planner, Query Analyzer, Enhanced LangGraph RAG Agent  

## Issue Description

The Metis RAG system was not maintaining conversation memory between turns, causing it to "forget" previous parts of the conversation. This was observed in the following behaviors:

1. When asked to reformat a previous response, Metis provided a new response instead of reformatting the original one.
2. When asked follow-up questions that referenced previous turns, Metis lost context.
3. Metis explicitly stated it "does not have a persistent memory of our entire conversation" in response to a direct question.

## Root Cause Analysis

After investigating the codebase, we identified that the system had two parallel mechanisms for handling conversation history:

1. **Database-based Conversation Storage**: A robust SQL database schema with `Conversation`, `Message`, and `Citation` models.
2. **Mem0-based Memory System**: An external memory service integration via `mem0_client.py`.

While both systems stored conversation history correctly, this history wasn't being passed to the query planning and analysis components:

1. The `QueryPlanner.create_plan()` method didn't accept chat history as a parameter.
2. The `QueryAnalyzer.analyze()` method processed each query in isolation.
3. The agent called `query_planner.create_plan(query_id=query_id, query=query)` without passing history.

This created a disconnect where conversation history was stored but not used in the query planning process.

## Solution Implemented

We implemented the **Context Augmentation** approach, where:

1. The conversation history is passed through the system.
2. Retrieval is still based primarily on the current query (for efficiency and focus).
3. The final LLM prompt includes both the retrieved documents AND the conversation history.

### Changes Made

1. **QueryPlan Class (`app/rag/query_planner.py`)**:
   - Added `chat_history` parameter to store conversation history
   - Updated serialization methods to include chat history

2. **QueryPlanner Class (`app/rag/query_planner.py`)**:
   - Modified `create_plan` to accept chat history
   - Updated the synthesize step to indicate it should use history

3. **QueryAnalyzer Class (`app/rag/query_analyzer.py`)**:
   - Added chat history parameter to `analyze` method
   - Enhanced the analysis prompt to include conversation history
   - Added instructions to consider previous turns when analyzing queries

4. **EnhancedLangGraphRAGAgent (`app/rag/agents/enhanced_langgraph_rag_agent.py`)**:
   - Updated `_analyze_query` to extract chat history from conversation context
   - Modified `_plan_query` to pass chat history to the query planner

## Testing

The fix was tested with various conversation scenarios:

1. Simple follow-up questions ("Tell me more about that")
2. Requests to reformulate previous responses ("Give me that as a list")
3. Questions that reference entities from previous turns
4. Explicit questions about conversation memory

## Related Documentation

For a detailed implementation plan, see [Chat Memory Augmentation Plan](../implementation/chat_memory_augmentation_plan.md).

================
File: docs/demos/Metis_RAG_Technical_Demo.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG Technical Demonstration</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        h2 {
            color: #2980b9;
            border-left: 4px solid #3498db;
            padding-left: 10px;
            margin-top: 30px;
        }
        ul {
            padding-left: 20px;
        }
        li {
            margin-bottom: 8px;
        }
        .feature {
            font-weight: bold;
            color: #2c3e50;
        }
        .section {
            background-color: white;
            border-radius: 5px;
            padding: 15px 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .highlight {
            background-color: #f1f8ff;
            border-left: 4px solid #3498db;
            padding: 10px 15px;
            margin: 15px 0;
        }
        ol {
            padding-left: 20px;
        }
        ol li {
            margin-bottom: 10px;
        }
        .strategy {
            margin-bottom: 20px;
        }
        .strategy-name {
            font-weight: bold;
            color: #2980b9;
        }
        .future-feature {
            background-color: #e8f4fc;
            border-radius: 5px;
            padding: 10px 15px;
            margin-bottom: 15px;
        }
        .future-feature h3 {
            color: #2c3e50;
            margin-top: 0;
        }
    </style>
</head>
<body>
    <h1>Metis RAG Technical Demonstration</h1>
    
    <div class="section">
        <h2>What is Metis RAG?</h2>
        <p>
            Metis RAG is an advanced Retrieval-Augmented Generation platform that combines conversational AI with document-based knowledge retrieval. It enables users to chat with large language models while providing relevant context from uploaded documents, enhancing response accuracy and factual grounding.
        </p>
    </div>

    <div class="section">
        <h2>Key Features</h2>
        <ul>
            <li><span class="feature">Document Intelligence:</span> Supports multiple file formats (PDF, TXT, CSV, MD) with advanced processing capabilities</li>
            <li><span class="feature">Advanced RAG Engine:</span> Implements multiple chunking strategies (recursive, token-based, markdown header) for optimal document representation</li>
            <li><span class="feature">Vector Database Integration:</span> Uses ChromaDB with optimized caching for efficient similarity search</li>
            <li><span class="feature">LLM Integration:</span> Connects to Ollama for local LLM inference with enhanced prompt engineering</li>
            <li><span class="feature">Document Management:</span> Provides tagging, organization, and folder hierarchy for documents</li>
            <li><span class="feature">Analytics Dashboard:</span> Tracks system usage, performance metrics, and document utilization</li>
            <li><span class="feature">Responsive UI:</span> Features modern interface with light/dark mode and mobile optimization</li>
        </ul>
    </div>

    <div class="section">
        <h2>Technical Architecture</h2>
        <ul>
            <li><span class="feature">Frontend:</span> HTML/CSS/JavaScript with responsive design and streaming response handling</li>
            <li><span class="feature">Backend:</span> FastAPI (Python) with modular component design</li>
            <li><span class="feature">RAG Engine:</span> Custom implementation with document processor, vector store, and LLM integration</li>
            <li><span class="feature">Vector Database:</span> ChromaDB with performance optimizations and caching</li>
            <li><span class="feature">Testing Framework:</span> Comprehensive test suite for quality, performance, and edge cases</li>
        </ul>
    </div>

    <div class="section">
        <h2>Advanced Chunking Strategies</h2>
        
        <div class="strategy">
            <p><span class="strategy-name">1. Recursive Chunking</span> (Default)</p>
            <ul>
                <li>Recursively splits text by characters with configurable chunk size and overlap</li>
                <li>Maintains semantic coherence by respecting natural text boundaries</li>
                <li>Optimal for general-purpose documents with varied content</li>
                <li>Configurable parameters: chunk size (default: 500), chunk overlap (default: 50)</li>
            </ul>
        </div>
        
        <div class="strategy">
            <p><span class="strategy-name">2. Token-based Chunking</span></p>
            <ul>
                <li>Splits text based on token count rather than character count</li>
                <li>Preserves semantic meaning by respecting token boundaries</li>
                <li>Better suited for technical content where token-level precision matters</li>
                <li>Avoids mid-word splits that can occur with character-based chunking</li>
                <li>Optimizes for LLM context window utilization</li>
            </ul>
        </div>
        
        <div class="strategy">
            <p><span class="strategy-name">3. Markdown Header Chunking</span></p>
            <ul>
                <li>Intelligently splits markdown documents based on header structure</li>
                <li>Preserves document hierarchy and organizational structure</li>
                <li>Creates chunks that align with logical document sections</li>
                <li>Maintains header context for improved retrieval relevance</li>
                <li>Particularly effective for technical documentation and knowledge bases</li>
            </ul>
        </div>
        
        <div class="highlight">
            The chunking strategy selection is automatically determined based on document type but can be manually overridden. This multi-strategy approach significantly improves retrieval accuracy by ensuring that document chunks maintain semantic coherence and structural integrity.
        </div>
    </div>

    <div class="section">
        <h2>Basic Operation</h2>
        
        <ol>
            <li>
                <strong>Document Upload & Processing:</strong>
                <ul>
                    <li>Upload documents through the document management interface</li>
                    <li>Documents are processed through configurable chunking strategies</li>
                    <li>Text is embedded and stored in the vector database with metadata</li>
                </ul>
            </li>
            
            <li>
                <strong>Chat Interaction:</strong>
                <ul>
                    <li>User sends a query through the chat interface</li>
                    <li>System retrieves relevant document chunks based on semantic similarity</li>
                    <li>Retrieved context is combined with the query in a prompt to the LLM</li>
                    <li>Response is generated with citations to source documents</li>
                </ul>
            </li>
            
            <li>
                <strong>System Management:</strong>
                <ul>
                    <li>Monitor system performance through the analytics dashboard</li>
                    <li>Configure models and system parameters</li>
                    <li>View document usage statistics and query patterns</li>
                </ul>
            </li>
        </ol>
    </div>

    <div class="section">
        <h2>Performance & Testing</h2>
        <ul>
            <li>Comprehensive testing framework with quality, performance, and edge case tests</li>
            <li>Current response time averaging ~9.8s (optimization target: <3s)</li>
            <li>Support for up to 10,000 documents with efficient retrieval</li>
            <li>Automated test suite for RAG functionality verification</li>
            <li>Performance benchmarking for response time, throughput, and resource utilization</li>
        </ul>
    </div>

    <div class="section">
        <h2>Future Advances: Agentic RAG</h2>
        
        <div class="future-feature">
            <h3>1. Autonomous Information Gathering</h3>
            <ul>
                <li>Self-directed exploration of document corpus beyond initial retrieval</li>
                <li>Iterative search refinement based on information gaps</li>
                <li>Multi-hop reasoning across document boundaries</li>
                <li>Autonomous query decomposition and recomposition</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>2. Tool Integration</h3>
            <ul>
                <li>Integration with external tools and APIs for data enrichment</li>
                <li>Ability to execute code for data analysis within responses</li>
                <li>Database querying capabilities for structured data integration</li>
                <li>Web search integration for supplementing internal knowledge</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>3. Adaptive Retrieval</h3>
            <ul>
                <li>Dynamic adjustment of retrieval parameters based on query complexity</li>
                <li>Automatic chunking strategy selection based on document content</li>
                <li>Self-tuning relevance thresholds based on feedback</li>
                <li>Context-aware retrieval that considers conversation history</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>4. Reasoning and Synthesis</h3>
            <ul>
                <li>Enhanced multi-document synthesis with cross-reference analysis</li>
                <li>Contradiction detection and resolution across documents</li>
                <li>Temporal awareness for handling time-sensitive information</li>
                <li>Uncertainty quantification in responses</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>5. Feedback Loop Integration</h3>
            <ul>
                <li>Learning from user interactions to improve retrieval</li>
                <li>Automated fine-tuning of embeddings based on usage patterns</li>
                <li>Continuous improvement of prompt templates</li>
                <li>Self-evaluation of response quality</li>
            </ul>
        </div>
        
        <div class="highlight">
            These Agentic RAG capabilities will transform Metis RAG from a passive retrieval system to an active knowledge assistant that can autonomously navigate complex information landscapes, reason across documents, and continuously improve its performance.
        </div>
    </div>

    <div class="section">
        <h2>Demonstration Points</h2>
        <ol>
            <li><strong>Document Upload:</strong> Show support for multiple file types and processing options</li>
            <li><strong>RAG in Action:</strong> Demonstrate how responses incorporate document knowledge</li>
            <li><strong>Source Citations:</strong> Highlight how the system cites sources for factual claims</li>
            <li><strong>Multiple Document Synthesis:</strong> Show how the system combines information across documents</li>
            <li><strong>Analytics:</strong> Display system performance metrics and document utilization</li>
            <li><strong>Testing:</strong> Run the test script to demonstrate RAG retrieval verification</li>
        </ol>
        
        <div class="highlight">
            Metis RAG represents a significant advancement in combining local LLM capabilities with enterprise knowledge management, providing accurate, contextual responses grounded in organizational documents.
        </div>
    </div>
</body>
</html>

================
File: docs/demos/Metis_RAG_Technical_Demo.md
================
# Metis RAG Technical Demonstration

## What is Metis RAG?

Metis RAG is an advanced Retrieval-Augmented Generation platform that combines conversational AI with document-based knowledge retrieval. It enables users to chat with large language models while providing relevant context from uploaded documents, enhancing response accuracy and factual grounding.

## Key Features

- **Document Intelligence**: Supports multiple file formats (PDF, TXT, CSV, MD) with advanced processing capabilities
- **Advanced RAG Engine**: Implements multiple chunking strategies (recursive, token-based, markdown header) for optimal document representation
- **Vector Database Integration**: Uses ChromaDB with optimized caching for efficient similarity search
- **LLM Integration**: Connects to Ollama for local LLM inference with enhanced prompt engineering
- **Document Management**: Provides tagging, organization, and folder hierarchy for documents
- **Analytics Dashboard**: Tracks system usage, performance metrics, and document utilization
- **Responsive UI**: Features modern interface with light/dark mode and mobile optimization

## Technical Architecture

- **Frontend**: HTML/CSS/JavaScript with responsive design and streaming response handling
- **Backend**: FastAPI (Python) with modular component design
- **RAG Engine**: Custom implementation with document processor, vector store, and LLM integration
- **Vector Database**: ChromaDB with performance optimizations and caching
- **Testing Framework**: Comprehensive test suite for quality, performance, and edge cases

## Advanced Chunking Strategies

Metis RAG implements three sophisticated chunking strategies to optimize document representation for different content types:

1. **Recursive Chunking** (Default)
   - Recursively splits text by characters with configurable chunk size and overlap
   - Maintains semantic coherence by respecting natural text boundaries
   - Optimal for general-purpose documents with varied content
   - Configurable parameters: chunk size (default: 500), chunk overlap (default: 50)

2. **Token-based Chunking**
   - Splits text based on token count rather than character count
   - Preserves semantic meaning by respecting token boundaries
   - Better suited for technical content where token-level precision matters
   - Avoids mid-word splits that can occur with character-based chunking
   - Optimizes for LLM context window utilization

3. **Markdown Header Chunking**
   - Intelligently splits markdown documents based on header structure
   - Preserves document hierarchy and organizational structure
   - Creates chunks that align with logical document sections
   - Maintains header context for improved retrieval relevance
   - Particularly effective for technical documentation and knowledge bases

The chunking strategy selection is automatically determined based on document type but can be manually overridden. This multi-strategy approach significantly improves retrieval accuracy by ensuring that document chunks maintain semantic coherence and structural integrity.

## Basic Operation

1. **Document Upload & Processing**:
   - Upload documents through the document management interface
   - Documents are processed through configurable chunking strategies
   - Text is embedded and stored in the vector database with metadata

2. **Chat Interaction**:
   - User sends a query through the chat interface
   - System retrieves relevant document chunks based on semantic similarity
   - Retrieved context is combined with the query in a prompt to the LLM
   - Response is generated with citations to source documents

3. **System Management**:
   - Monitor system performance through the analytics dashboard
   - Configure models and system parameters
   - View document usage statistics and query patterns

## Performance & Testing

- Comprehensive testing framework with quality, performance, and edge case tests
- Current response time averaging ~9.8s (optimization target: <3s)
- Support for up to 10,000 documents with efficient retrieval
- Automated test suite for RAG functionality verification
- Performance benchmarking for response time, throughput, and resource utilization

## Future Advances: Agentic RAG

The roadmap for Metis RAG includes significant advancements in Agentic RAG capabilities:

1. **Autonomous Information Gathering**
   - Self-directed exploration of document corpus beyond initial retrieval
   - Iterative search refinement based on information gaps
   - Multi-hop reasoning across document boundaries
   - Autonomous query decomposition and recomposition

2. **Tool Integration**
   - Integration with external tools and APIs for data enrichment
   - Ability to execute code for data analysis within responses
   - Database querying capabilities for structured data integration
   - Web search integration for supplementing internal knowledge

3. **Adaptive Retrieval**
   - Dynamic adjustment of retrieval parameters based on query complexity
   - Automatic chunking strategy selection based on document content
   - Self-tuning relevance thresholds based on feedback
   - Context-aware retrieval that considers conversation history

4. **Reasoning and Synthesis**
   - Enhanced multi-document synthesis with cross-reference analysis
   - Contradiction detection and resolution across documents
   - Temporal awareness for handling time-sensitive information
   - Uncertainty quantification in responses

5. **Feedback Loop Integration**
   - Learning from user interactions to improve retrieval
   - Automated fine-tuning of embeddings based on usage patterns
   - Continuous improvement of prompt templates
   - Self-evaluation of response quality

These Agentic RAG capabilities will transform Metis RAG from a passive retrieval system to an active knowledge assistant that can autonomously navigate complex information landscapes, reason across documents, and continuously improve its performance.

## Demonstration Points

1. **Document Upload**: Show support for multiple file types and processing options
2. **RAG in Action**: Demonstrate how responses incorporate document knowledge
3. **Source Citations**: Highlight how the system cites sources for factual claims
4. **Multiple Document Synthesis**: Show how the system combines information across documents
5. **Analytics**: Display system performance metrics and document utilization
6. **Testing**: Run the test script to demonstrate RAG retrieval verification

Metis RAG represents a significant advancement in combining local LLM capabilities with enterprise knowledge management, providing accurate, contextual responses grounded in organizational documents.

================
File: docs/deployment/docker_deployment.md
================
# Docker Deployment Guide for Metis_RAG

This guide explains how to deploy the Metis_RAG system using Docker and Docker Compose.

## Prerequisites

- [Docker](https://docs.docker.com/get-docker/) (version 20.10.0 or higher)
- [Docker Compose](https://docs.docker.com/compose/install/) (version 2.0.0 or higher)
- At least 8GB of RAM and 20GB of disk space

## Quick Start

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/Metis_RAG.git
   cd Metis_RAG
   ```

2. Copy the Docker environment file:
   ```bash
   cp config/.env.docker config/.env
   ```

3. Start the services:
   ```bash
   cd config
   docker-compose up -d
   ```

4. Access the application at http://localhost:8000

## Services

The Docker Compose configuration includes the following services:

- **metis-rag**: The main application
- **postgres**: PostgreSQL database for storing documents, conversations, and analytics
- **ollama**: Local LLM service for text generation and embeddings
- **mem0**: Memory layer for enhanced context in conversations

## Configuration

### Environment Variables

You can customize the deployment by modifying the environment variables in the `.env` file. The most important variables are:

- `METIS_PORT`: The port to expose the application on (default: 8000)
- `METIS_POSTGRES_PASSWORD`: The password for the PostgreSQL database
- `DEFAULT_MODEL`: The default LLM model to use (default: gemma3:4b)

### Volumes

The following volumes are used for data persistence:

- `postgres-data`: PostgreSQL data
- `../data/ollama`: Ollama models and data
- `../data/mem0`: Mem0 data
- `../data`: Application data (uploads, vector store, cache)

## Scaling

### Resource Requirements

- **Small deployment**: 8GB RAM, 4 CPU cores, 20GB disk
- **Medium deployment**: 16GB RAM, 8 CPU cores, 50GB disk
- **Large deployment**: 32GB RAM, 16 CPU cores, 100GB disk

### Performance Tuning

For better performance, you can adjust the following settings:

1. Increase PostgreSQL connection pool size in `.env`:
   ```
   DATABASE_POOL_SIZE=10
   DATABASE_MAX_OVERFLOW=20
   ```

2. Adjust the number of workers in the Docker Compose file:
   ```yaml
   metis-rag:
     # ...
     command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
   ```

## Monitoring

The Docker Compose configuration includes health checks for all services. You can monitor the health of the services using:

```bash
docker-compose ps
```

For more detailed monitoring, you can check the logs:

```bash
docker-compose logs -f
```

## Backup and Restore

### Database Backup

```bash
docker exec metis-postgres pg_dump -U postgres metis_rag > backup.sql
```

### Database Restore

```bash
cat backup.sql | docker exec -i metis-postgres psql -U postgres metis_rag
```

## Troubleshooting

### Common Issues

1. **Service fails to start**: Check the logs with `docker-compose logs <service-name>`
2. **Database connection issues**: Ensure PostgreSQL is running and the connection string is correct
3. **Out of memory errors**: Increase the memory allocated to Docker

### Logs

To view logs for a specific service:

```bash
docker-compose logs -f metis-rag
```

## Updating

To update to a new version:

1. Pull the latest changes:
   ```bash
   git pull
   ```

2. Rebuild and restart the services:
   ```bash
   docker-compose down
   docker-compose build
   docker-compose up -d
   ```

## Security Considerations

1. Change default passwords in the `.env` file
2. Use a reverse proxy (like Nginx) with HTTPS for production deployments
3. Restrict access to the Docker API
4. Regularly update all components

## Production Deployment

For production deployments, additional steps are recommended:

1. Use a dedicated PostgreSQL server with proper backup procedures
2. Set up a load balancer for high availability
3. Configure proper logging and monitoring
4. Use Docker Swarm or Kubernetes for orchestration

================
File: docs/implementation/chat_memory_augmentation_plan.md
================
# Chat Memory Augmentation Plan for Metis RAG

## Problem Statement

The Metis RAG system currently lacks persistent memory of conversation history between turns. This results in the system appearing to "forget" previous parts of the conversation, as observed in the following behaviors:

1. When asked to reformat a previous response, Metis provides a new response instead of reformatting the original one.
2. When asked follow-up questions that reference previous turns, Metis loses context.
3. Metis explicitly states it "does not have a persistent memory of our entire conversation" in response to a direct question.

## Current System Analysis

After examining the codebase, we've identified that the lack of conversational memory stems from the following architectural limitations:

1. **Agent Layer (`enhanced_langgraph_rag_agent.py`)**: 
   - Only passes the current query to the QueryPlanner, without any conversation history.
   - Line 331: `plan = await self.query_planner.create_plan(query_id=query_id, query=query)`

2. **Query Planner (`query_planner.py`)**: 
   - `create_plan` method only accepts `query_id` and `query` parameters.
   - No mechanism to store or utilize conversation history.

3. **Query Analyzer (`query_analyzer.py`)**: 
   - `analyze` method only considers the current query in isolation.
   - LLM prompt doesn't include any conversation context.

4. **RAG Tool (`rag_tool.py`)**: 
   - Retrieval is based solely on the current query.
   - No consideration of conversation history during retrieval or response generation.

## Solution: Context Augmentation Approach

We will implement the **Context Augmentation** approach, where:

1. The conversation history will be passed through the system.
2. Retrieval will still be based primarily on the current query (for efficiency and focus).
3. The final LLM prompt will include both the retrieved documents AND the conversation history.
4. This allows the LLM to generate responses that maintain conversational context while still grounding answers in retrieved information.

```mermaid
graph TD
    A[User Query + Chat History] --> B{Agent};
    B -- Query + History --> C{Query Planner};
    C -- Analyzes Query + History --> D[Plan Steps];
    D -- Includes History Context --> E{RAG Tool / Other Tools};
    F[Vector DB] --> G{Retriever};
    E -- Query --> G;
    G --> H[Retrieved Docs];
    H -- Docs + History --> I[LLM Prompt Builder];
    I --> J{LLM};
    J --> K[Metis Response];

    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style I fill:#ccf,stroke:#333,stroke-width:2px
```

## Implementation Plan

### 1. Modify QueryPlan Class (`query_planner.py`)

Add support for storing conversation history in the QueryPlan:

```python
class QueryPlan:
    def __init__(self, query_id: str, query: str, steps: List[Dict[str, Any]], 
                 chat_history: Optional[List[Tuple[str, str]]] = None):
        self.query_id = query_id
        self.query = query
        self.steps = steps
        self.current_step = 0
        self.results = []
        self.completed = False
        self.chat_history = chat_history  # Add this line
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "query_id": self.query_id,
            "query": self.query,
            "steps": self.steps,
            "current_step": self.current_step,
            "results": self.results,
            "completed": self.completed,
            "chat_history": self.chat_history  # Add this line
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'QueryPlan':
        plan = cls(
            query_id=data["query_id"],
            query=data["query"],
            steps=data["steps"],
            chat_history=data.get("chat_history")  # Add this line
        )
        plan.current_step = data.get("current_step", 0)
        plan.results = data.get("results", [])
        plan.completed = data.get("completed", False)
        return plan
```

### 2. Modify QueryPlanner Class (`query_planner.py`)

Update the `create_plan` method to accept and utilize chat history:

```python
async def create_plan(self, query_id: str, query: str, 
                     chat_history: Optional[List[Tuple[str, str]]] = None) -> QueryPlan:
    """
    Create a plan for executing a query
    
    Args:
        query_id: Unique query ID
        query: Query string
        chat_history: Optional list of (user_message, ai_message) tuples
        
    Returns:
        QueryPlan instance
    """
    self.logger.info(f"Creating plan for query: {query}")
    
    # Analyze the query with chat history context
    analysis = await self.query_analyzer.analyze(query, chat_history)
    
    # Determine if the query is simple or complex
    complexity = analysis.get("complexity", "simple")
    required_tools = analysis.get("requires_tools", [])
    sub_queries = analysis.get("sub_queries", [])
    
    # Create plan steps
    steps = []
    
    if complexity == "simple":
        # Simple query - just use RAG
        steps.append({
            "type": "tool",
            "tool": "rag",
            "input": {
                "query": query,
                "top_k": 5
            },
            "description": "Retrieve information using RAG"
        })
    else:
        # Complex query - may require multiple steps
        # ... existing code for complex queries ...
    
    # Add a final step to synthesize the results with chat history
    steps.append({
        "type": "synthesize",
        "description": "Synthesize results from previous steps with conversation history",
        "with_history": True  # Flag to indicate this step should use history
    })
    
    # Create the plan with chat history
    plan = QueryPlan(
        query_id=query_id,
        query=query,
        steps=steps,
        chat_history=chat_history  # Pass chat history to the plan
    )
    
    self.logger.info(f"Created plan with {len(steps)} steps for query: {query}")
    return plan
```

### 3. Modify QueryAnalyzer Class (`query_analyzer.py`)

Update the `analyze` method and prompt creation to consider chat history:

```python
async def analyze(self, query: str, 
                 chat_history: Optional[List[Tuple[str, str]]] = None) -> Dict[str, Any]:
    """
    Analyze a query to determine its complexity and requirements
    
    Args:
        query: Query string
        chat_history: Optional list of (user_message, ai_message) tuples
        
    Returns:
        Dict with analysis results
    """
    start_time = time.time()
    self.logger.info(f"Analyzing query: {query}")
    
    prompt = self._create_analysis_prompt(query, chat_history)
    response = await self.llm_provider.generate(prompt=prompt)
    analysis = self._parse_analysis(response.get("response", ""))
    
    elapsed_time = time.time() - start_time
    self.logger.info(f"Query analysis completed in {elapsed_time:.2f}s. Complexity: {analysis.get('complexity')}")
    
    return analysis

def _create_analysis_prompt(self, query: str, 
                           chat_history: Optional[List[Tuple[str, str]]] = None) -> str:
    """
    Create a prompt for query analysis
    
    Args:
        query: Query string
        chat_history: Optional list of (user_message, ai_message) tuples
        
    Returns:
        Prompt string
    """
    # Format chat history if available
    history_str = ""
    if chat_history:
        history_lines = []
        for i, (user_msg, ai_msg) in enumerate(chat_history):
            history_lines.append(f"Turn {i+1}:")
            history_lines.append(f"User: {user_msg}")
            history_lines.append(f"AI: {ai_msg}")
        history_str = "\n".join(history_lines)

    return f"""
You are an expert query analyzer for a RAG (Retrieval-Augmented Generation) system. Your task is to analyze the following query, considering the preceding conversation history, and determine its complexity, required tools, and potential sub-queries.

Conversation History:
{history_str if history_str else "None"}

Current Query: "{query}"

Available tools:
1. rag - Retrieves information from documents using RAG
2. calculator - Performs mathematical calculations
3. database - Queries structured data from databases

Please analyze the query and provide your assessment in the following JSON format:
{{
  "complexity": "simple" or "complex",
  "requires_tools": ["tool1", "tool2", ...],
  "sub_queries": ["sub-query1", "sub-query2", ...],
  "reasoning": "Detailed explanation of your analysis, considering the history"
}}

Where:
- "complexity" indicates whether the query is simple (can be answered with a single RAG lookup) or complex (requires multiple steps or tools)
- "requires_tools" lists the tools needed to answer the query
- "sub_queries" lists potential sub-queries if the main query needs to be broken down
- "reasoning" explains your analysis in detail

Analyze the query carefully, considering:
1. Does it require factual information retrieval? (use rag tool)
2. Does it involve calculations? (use calculator tool)
3. Does it need structured data lookup? (use database tool)
4. Does it require multiple steps or a combination of tools?
5. Would breaking it into sub-queries improve the response quality?
6. How does the conversation history affect the interpretation of the current query? Does the query refer back to previous turns (e.g., "like you mentioned before", "tell me more about that")?

Provide your analysis in valid JSON format.
"""
```

### 4. Modify Agent Implementation (`enhanced_langgraph_rag_agent.py`)

The agent needs to:
1. Maintain chat history in its state
2. Pass chat history to the QueryPlanner
3. Ensure the final response generation uses both retrieved information and chat history

```python
# Pseudocode for the agent implementation changes
async def process_query(self, query_id: str, query: str, session_id: str):
    # Retrieve chat history for this session
    chat_history = await self.get_chat_history(session_id)
    
    # Create a plan using the query planner with chat history
    plan = await self.query_planner.create_plan(
        query_id=query_id, 
        query=query,
        chat_history=chat_history
    )
    
    # Execute the plan steps
    while not plan.is_completed():
        step = plan.get_next_step()
        
        if step["type"] == "tool":
            # Execute tool (e.g., RAG)
            result = await self.execute_tool(step)
        elif step["type"] == "synthesize":
            # Generate final response using both retrieved info and chat history
            result = await self.synthesize_response(plan, step)
        
        plan.record_step_result(result)
    
    # Extract the final response
    response = self.extract_response(plan)
    
    # Update chat history with this turn
    await self.update_chat_history(session_id, query, response)
    
    return response

async def synthesize_response(self, plan: QueryPlan, step: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate a final response using retrieved information and chat history
    """
    # Get results from previous steps (e.g., RAG tool output)
    retrieved_chunks = []
    for result in plan.results:
        if "chunks" in result:
            retrieved_chunks.extend(result["chunks"])
    
    # Format retrieved chunks as context
    context = "\n\n".join([chunk["content"] for chunk in retrieved_chunks])
    
    # Format chat history
    history_str = ""
    if plan.chat_history:
        history_lines = []
        for i, (user_msg, ai_msg) in enumerate(plan.chat_history):
            history_lines.append(f"User: {user_msg}")
            history_lines.append(f"AI: {ai_msg}")
        history_str = "\n".join(history_lines)
    
    # Create prompt for response generation
    prompt = f"""
You are Metis, a helpful AI assistant with RAG capabilities. Generate a response to the user's query using the retrieved information and conversation history.

Conversation History:
{history_str if history_str else "None"}

Current Query: "{plan.query}"

Retrieved Information:
{context}

Generate a comprehensive, accurate response that:
1. Directly addresses the user's query
2. Maintains continuity with the conversation history
3. Is grounded in the retrieved information
4. Cites sources when appropriate using [1], [2], etc.

Your response:
"""
    
    # Generate response using LLM
    llm_response = await self.llm_provider.generate(prompt=prompt)
    
    return {
        "response": llm_response.get("response", ""),
        "sources": [chunk["metadata"].get("document_id") for chunk in retrieved_chunks if "metadata" in chunk]
    }
```

## Testing Plan

1. **Unit Tests**:
   - Update existing tests for QueryPlanner, QueryAnalyzer, and Agent to handle chat history
   - Add new tests specifically for conversation memory scenarios

2. **Integration Tests**:
   - Test the full conversation flow with multi-turn dialogues
   - Verify that references to previous turns are handled correctly

3. **Test Cases**:
   - Simple follow-up questions ("Tell me more about that")
   - Requests to reformulate previous responses ("Give me that as a list")
   - Questions that reference entities from previous turns
   - Explicit questions about conversation memory

## Implementation Phases

1. **Phase 1**: Modify data structures (QueryPlan) to store chat history
2. **Phase 2**: Update QueryPlanner and QueryAnalyzer to accept and use chat history
3. **Phase 3**: Implement the synthesize_response method in the Agent
4. **Phase 4**: Add chat history persistence (if needed)
5. **Phase 5**: Testing and refinement

## Conclusion

This implementation will add conversational memory to the Metis RAG system using the Context Augmentation approach. By passing chat history through the system and incorporating it in the final response generation, Metis will be able to maintain context across conversation turns while still grounding its responses in retrieved information.

================
File: docs/implementation/llm_enhanced_rag_implementation_plan_updated.md
================
# LLM-Enhanced RAG System Implementation Plan (Updated)

## Overview

This document outlines a comprehensive plan for implementing an enhanced Retrieval Augmented Generation (RAG) system using LLM-based "judges" to improve two critical aspects of the RAG pipeline:

1. **Dynamic Chunking Strategy Selection**: An LLM agent ("Chunking Judge") that analyzes documents and selects the most appropriate chunking strategy and parameters.

2. **Query Refinement and Retrieval Enhancement**: An LLM agent ("Retrieval Judge") that analyzes queries and retrieved chunks to improve retrieval quality through query refinement, relevance evaluation, and potential re-retrieval.

## Implementation Progress

### Completed:

- [x] Created git branch `llm-enhanced-rag` for the implementation
- [x] Updated configuration to use gemma3:12b as the default model
- [x] Added configuration variables for LLM judges
- [x] Created the agents directory structure
- [x] Implemented the Chunking Judge class with enhanced document sample extraction
- [x] Integrated Chunking Judge with DocumentProcessor
- [x] Added unit tests for the Chunking Judge
- [x] Added integration tests for the Chunking Judge
- [x] Updated documentation in README.md
- [x] Conducted testing of Chunking Judge with real Ollama client
- [x] Analyzed test results and verified Chunking Judge functionality
- [x] Implemented the Retrieval Judge class
- [x] Integrated Retrieval Judge with RAGEngine
- [x] Added unit tests for the Retrieval Judge
- [x] Added integration tests for the Retrieval Judge
- [x] Conducted performance analysis of Retrieval Judge vs. standard retrieval
- [x] Tested Retrieval Judge with edge cases (ambiguous, typo, domain-specific, multi-part, short, and long queries)
- [x] Analyzed test results and verified Retrieval Judge functionality
- [x] Updated technical documentation with Retrieval Judge capabilities and performance metrics
- [x] Updated implementation plan with test results and progress

### Pending:

- [x] Implement advanced semantic chunking
- [x] Integrate LangGraph for agentic RAG
- [ ] Create benchmarking scripts
- [ ] Conduct comprehensive performance evaluation with real-world queries

## Current System Analysis

The current RAG implementation has several strengths:

- Multiple chunking strategies (recursive, token-based, markdown) with file type-specific handling
- Relevance filtering with a threshold of 0.4 to filter out less relevant chunks
- Robust vector store using ChromaDB with caching for performance
- Well-structured document model with chunks, metadata, tags, and folder organization
- Resilient Ollama client with retry logic and error handling

However, the system currently uses static rules for chunking strategy selection and fixed parameters, which limits adaptability to diverse document types. The retrieval process is also relatively simple, without query refinement or iterative retrieval.

## Implementation Approach

We are taking a phased approach to implementation, starting with the Chunking Judge and then moving on to the Retrieval Judge. Both judges use the gemma3:12b model for its multimodal capabilities, with the system designed to allow changing the model in the future.

## Phase 1: Chunking Judge Implementation (COMPLETED)

### 1.1 Create Agent Architecture (COMPLETED)

```mermaid
flowchart TD
    A[Document Upload] --> B[Document Processor]
    B --> C{Chunking Judge}
    C -->|Analyze Document| D[Select Strategy]
    D --> E[Determine Parameters]
    E --> F[Apply Chunking]
    F --> G[Store Chunks]
```

### 1.2 Create the Chunking Judge Class (COMPLETED)

The Chunking Judge class has been implemented in `app/rag/agents/chunking_judge.py`. The implementation includes:

- Analysis of document structure, content type, and formatting
- Selection of the most appropriate chunking strategy (recursive, token, markdown)
- Recommendation of optimal chunk size and overlap parameters
- Enhanced document sample extraction that prioritizes headers, introduction, and conclusion
- Robust error handling and fallback mechanisms

### 1.3 Update Configuration (COMPLETED)

Configuration variables have been added to `app/core/config.py`:

```python
# LLM Judge settings
CHUNKING_JUDGE_MODEL = os.getenv("CHUNKING_JUDGE_MODEL", "gemma3:12b")
RETRIEVAL_JUDGE_MODEL = os.getenv("RETRIEVAL_JUDGE_MODEL", "gemma3:12b")
USE_CHUNKING_JUDGE = os.getenv("USE_CHUNKING_JUDGE", "True").lower() == "true"
USE_RETRIEVAL_JUDGE = os.getenv("USE_RETRIEVAL_JUDGE", "True").lower() == "true"
```

### 1.4 Integrate with DocumentProcessor (COMPLETED)

The DocumentProcessor has been updated to use the Chunking Judge when processing documents. The integration includes:

- Using the Chunking Judge if enabled via configuration
- Updating chunking strategy and parameters based on the judge's recommendation
- Storing the chunking analysis in document metadata
- Logging the judge's recommendations and justification

### 1.5 Testing (COMPLETED)

Unit and integration tests have been created for the Chunking Judge:

- Unit tests in `tests/unit/test_chunking_judge.py`
- Integration tests in `tests/integration/test_chunking_judge_integration.py`

### 1.6 Test Results with Real Ollama (COMPLETED)

The Chunking Judge was tested with the real Ollama client (gemma3:12b model) using three different document types:

#### Test Results Summary:

1. **Plain Text Document (test_document.txt)**
   - **Recommended Strategy**: markdown
   - **Parameters**: chunk_size=1500, chunk_overlap=200
   - **Justification**: "The document is formatted with Markdown headers, indicating a clear hierarchical structure. Using the 'markdown' strategy will preserve this structure, ensuring that related sections and subsections are grouped together. A chunk size of 1500 characters allows for substantial content within each chunk while maintaining reasonable size. An overlap of 200 characters helps to avoid abrupt transitions between chunks and ensures context is carried over."

2. **Markdown Document (technical_documentation.md)**
   - **Recommended Strategy**: markdown
   - **Parameters**: chunk_size=1500, chunk_overlap=200
   - **Justification**: "The document is a markdown file with a well-defined structure using headers (e.g., Introduction, Architecture Overview, Deployment Options, Configuration, API Reference, Performance Considerations). Using the 'markdown' strategy will allow us to split the document along these logical sections, preserving the context and relationships between different parts of the documentation. A chunk size of 1500 characters allows for a reasonable amount of content within each chunk while still allowing for meaningful sections. An overlap of 200 ensures that context isn't lost between chunks."

3. **CSV Document (test_data.csv)**
   - **Recommended Strategy**: recursive
   - **Parameters**: chunk_size=1000, chunk_overlap=200
   - **Justification**: "The document is a CSV file, which doesn't inherently have a clear header structure suitable for markdown chunking. While it's structured data, the content within each cell (particularly the 'skills' column) can be quite variable in length. Token-based chunking might be overly granular, as it could split skills lists mid-item. Recursive chunking provides a good balance. A chunk size of 1000 characters allows for capturing a reasonable amount of context from multiple rows, while an overlap of 200 helps maintain continuity between chunks, especially when a row's content spans across two chunks. This approach will allow the RAG system to understand the relationships between employees and their skills."

#### Analysis of Results:

The test results demonstrate that the Chunking Judge effectively:

1. **Recognizes Document Structure**: Correctly identifies document structures, even recognizing markdown-like formatting in the plain text file.

2. **Selects Appropriate Strategies**: Recommends chunking strategies that align with the document structure (markdown for header-based documents, recursive for tabular data).

3. **Optimizes Parameters**: Suggests appropriate chunk sizes and overlaps based on document characteristics.

4. **Provides Detailed Justifications**: Offers clear, detailed explanations that demonstrate understanding of document structure and the implications of different chunking strategies.

The real Ollama LLM provides more nuanced and detailed recommendations than the mock client used in initial testing, demonstrating the value of using a sophisticated language model for this task.

## Phase 2: Retrieval Judge Implementation (COMPLETED)

### 2.1 Create Agent Architecture

```mermaid
flowchart TD
    A[User Query] --> B[RAG Engine]
    B --> C[Initial Retrieval]
    C --> D{Retrieval Judge}
    D -->|Analyze Query & Chunks| E[Refine Query]
    D -->|Evaluate Relevance| F[Filter/Re-rank]
    D -->|Insufficient Results| G[Request More]
    E --> H[Additional Retrieval]
    F --> I[Final Context]
    G --> H
    H --> I
    I --> J[Generate Response]
```

### 2.2 Create the Retrieval Judge Class (COMPLETED)

The Retrieval Judge class has been implemented in `app/rag/agents/retrieval_judge.py`. The implementation includes:

- Query analysis to determine complexity, specificity, and intent
- Dynamic recommendation of retrieval parameters (k, threshold, reranking)
- Evaluation of retrieved chunks for relevance to the query
- Query refinement to improve retrieval precision
- Context optimization for better response generation
- Robust error handling and fallback mechanisms

The class provides the following key methods:

```python
class RetrievalJudge:
    """
    LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = RETRIEVAL_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Analyze a query and recommend retrieval parameters
        
        Returns:
            Dict with keys:
            - complexity: The assessed complexity of the query (simple, moderate, complex)
            - parameters: Dict of recommended retrieval parameters (k, threshold, etc.)
            - justification: Explanation of the recommendation
        """
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Evaluate retrieved chunks for relevance to the query
        
        Returns:
            Dict with keys:
            - relevance_scores: Dict mapping chunk IDs to relevance scores (0-1)
            - needs_refinement: Boolean indicating if query refinement is needed
            - justification: Explanation of the evaluation
        """
    
    async def refine_query(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """
        Refine a query based on retrieved chunks to improve retrieval precision
        
        Returns:
            Refined query string
        """
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Optimize the assembly of chunks into a context for the LLM
        
        Returns:
            Reordered and filtered list of chunks optimized for context assembly
        """
```

Each method uses carefully crafted prompts to guide the LLM in performing its specific task, with robust parsing of the LLM responses and fallback mechanisms for error handling.

### 2.3 Integrate with RAGEngine (COMPLETED)

The Retrieval Judge has been integrated with the RAGEngine to enhance the retrieval process. The integration includes:

- Initialization of the RetrievalJudge in the RAGEngine constructor if enabled via configuration
- A new `_enhanced_retrieval` method that implements the enhanced retrieval process
- Updates to the main `query` method to use enhanced retrieval when the judge is enabled

The enhanced retrieval process includes:

1. Query analysis to determine complexity and optimal retrieval parameters
2. Initial retrieval with recommended parameters
3. Evaluation of retrieved chunks for relevance
4. Query refinement when initial results are insufficient
5. Additional retrieval with refined query when needed
6. Re-ranking and filtering of chunks based on relevance scores
7. Context optimization for better response generation

The implementation in `rag_engine.py` includes:

```python
# In rag_engine.py

async def _enhanced_retrieval(
    self,
    query: str,
    conversation_context: str = "",
    top_k: int = 10,
    metadata_filters: Optional[Dict[str, Any]] = None
) -> Tuple[str, List[Dict[str, Any]], List[str]]:
    """
    Enhanced retrieval using the Retrieval Judge
    """
    # Step 1: Analyze the query using the Retrieval Judge
    query_analysis = await self.retrieval_judge.analyze_query(query)
    
    # Extract recommended parameters
    recommended_k = query_analysis.get("parameters", {}).get("k", top_k)
    relevance_threshold = query_analysis.get("parameters", {}).get("threshold", 0.4)
    apply_reranking = query_analysis.get("parameters", {}).get("reranking", True)
    
    # Step 2: Initial retrieval with recommended parameters
    search_results = await self.vector_store.search(
        query=search_query,
        top_k=max(15, recommended_k + 5),  # Get a few extra for filtering
        filter_criteria=metadata_filters
    )
    
    # Step 3: Evaluate chunks with the Retrieval Judge
    evaluation = await self.retrieval_judge.evaluate_chunks(query, search_results)
    
    # Step 4: Refine query if needed and perform additional retrieval
    if evaluation.get("needs_refinement", False):
        refined_query = await self.retrieval_judge.refine_query(query, search_results)
        additional_results = await self.vector_store.search(
            query=refined_query,
            top_k=recommended_k,
            filter_criteria=metadata_filters
        )
        # Combine results, avoiding duplicates
        # ...
    
    # Step 5: Filter and re-rank chunks based on relevance scores
    # ...
    
    # Step 6: Optimize context assembly
    if len(relevant_results) > 3 and apply_reranking:
        optimized_results = await self.retrieval_judge.optimize_context(query, relevant_results)
        if optimized_results:
            relevant_results = optimized_results
    
    # Step 7: Format context with source information
    # ...
    
    return context, sources, document_ids
```

The integration is designed to be seamless, with the Retrieval Judge being enabled or disabled via configuration. This allows for flexibility in deployment, where the enhanced retrieval can be used for complex queries while falling back to standard retrieval for simpler queries or when performance is a concern.

## Phase 3: Advanced Semantic Chunking (COMPLETED)

Building on the success of the Chunking Judge, we have implemented advanced semantic chunking strategies:

### 3.1 Implement Semantic Chunking (COMPLETED)

Created a SemanticChunker class in `app/rag/chunkers/semantic_chunker.py` that uses the LLM to identify natural semantic boundaries in text. The implementation includes:

- Intelligent boundary detection based on topic transitions and subject matter shifts
- Handling of long documents by processing in sections
- Caching for performance optimization
- Fallback mechanisms for error handling
- Both synchronous and asynchronous interfaces

### 3.2 Integrate Semantic Chunking with DocumentProcessor (COMPLETED)

Updated the DocumentProcessor to support semantic chunking as a new chunking strategy. The integration includes:

- Adding semantic chunking as a strategy option in the DocumentProcessor
- Updating the Chunking Judge to recommend semantic chunking when appropriate
- Creating comprehensive unit and integration tests

## Phase 4: LangGraph Integration for Agentic RAG (FUTURE PHASE)

Building on the success of the individual judges, we will implement a more sophisticated agentic RAG system using LangGraph:

### 4.1 Install Dependencies

Add LangGraph to the project dependencies.

### 4.2 Create LangGraph RAG Agent

Implement a LangGraphRAGAgent class that uses a state machine to orchestrate the RAG process.

### 4.3 Integrate LangGraph Agent with API

Add a new endpoint to the API for the LangGraph RAG agent.

## Implementation Timeline

### Phase 1: Chunking Judge (COMPLETED - March 5, 2025)
- Implement Chunking Judge class and integration with DocumentProcessor
- Testing and refinement of Chunking Judge
- Analysis of test results with real documents

### Phase 2: Retrieval Judge (COMPLETED - March 18, 2025)
- Implemented Retrieval Judge class and integration with RAGEngine
- Added unit and integration tests for the Retrieval Judge
- Conducted performance analysis comparing judge-enhanced vs. standard retrieval
- Tested with edge cases (ambiguous, typo, domain-specific, multi-part, short, and long queries)
- Verified functionality with comprehensive test coverage

### Phase 3: Advanced Semantic Chunking (COMPLETED - March 18, 2025)
- Implemented SemanticChunker class in app/rag/chunkers/semantic_chunker.py
- Integrated with DocumentProcessor and Chunking Judge
- Added unit and integration tests for the SemanticChunker
- Updated technical documentation with semantic chunking capabilities

### Phase 4: LangGraph Integration (COMPLETED - March 18, 2025)
- Implemented LangGraphRAGAgent class with state machine for orchestrating the RAG process
- Integrated with existing components (Chunking Judge, Semantic Chunker, Retrieval Judge)
- Added API endpoint for the LangGraph RAG Agent
- Created integration tests for the LangGraph RAG Agent

## Potential Challenges and Mitigations

1. **Increased Latency**
   - **Challenge**: Adding LLM calls will increase response time (observed ~30 seconds for Chunking Judge analysis with gemma3:12b)
   - **Mitigation**: Implement caching for judge decisions (already showing 33.33% vector store cache hit rate and 16.67% LLM cache hit rate), make judges optional via configuration, optimize when to use judges, consider using smaller models for faster inference
   - **Update**: Our testing shows that while initial queries with the Retrieval Judge are slower, subsequent queries are significantly faster (89.26% improvement) due to caching effects

2. **Cost Considerations**
   - **Challenge**: Additional LLM calls increase computational costs
   - **Mitigation**: Make judges optional, implement usage tracking, optimize when to use judges, cache results for similar documents/queries
   - **Update**: The Retrieval Judge's context optimization reduces the number of chunks by 76.4% on average, which can reduce the token count for the final LLM call, potentially offsetting some of the additional costs

3. **Error Handling**
   - **Challenge**: LLM responses may not always be parseable (observed occasional timeout errors during testing)
   - **Mitigation**: Robust fallback mechanisms in the parsing functions, implement retry logic with exponential backoff (already implemented in OllamaClient)
   - **Update**: Our testing revealed occasional timeout errors with the Retrieval Judge, but the retry logic in OllamaClient successfully handled these cases

4. **Cold Start Problem**
   - **Challenge**: Judges may perform poorly on new document types
   - **Mitigation**: Implement a feedback loop to improve judge performance over time, store successful analyses for reference
   - **Update**: The Retrieval Judge showed strong performance across different query types, with domain-specific and long queries being handled particularly well

5. **Model Quality Variations**
   - **Challenge**: Different LLMs may provide varying quality of recommendations
   - **Mitigation**: Test multiple models and select the best performing one, allow configuration of model per judge
   - **Update**: The gemma3:4b model used for testing the Retrieval Judge provided high-quality recommendations, suggesting that even smaller models can be effective for this task

6. **Justification Quality**
   - **Challenge**: Ensuring justifications are detailed and helpful for debugging and understanding
   - **Mitigation**: Include specific prompting for detailed justifications, as demonstrated in the Chunking Judge test results
   - **Update**: The Retrieval Judge provided detailed justifications for its decisions, particularly in the chunk evaluation component, which helps in understanding and debugging the system

## Conclusion

The LLM-enhanced RAG system with "judges" for chunking and retrieval has significant potential to improve the adaptability and accuracy of the RAG pipeline. By dynamically selecting chunking strategies and refining queries, the system can better handle diverse document types and complex queries.

Both Phase 1 (Chunking Judge) and Phase 2 (Retrieval Judge) have been successfully implemented and tested, providing a solid foundation for the remaining phases.

### Phase 1 Results (Chunking Judge)

The Chunking Judge effectively:

1. Recognizes document structure and formatting, even identifying markdown-like elements in plain text files
2. Selects appropriate chunking strategies based on document characteristics
3. Recommends optimized parameters (chunk size and overlap) tailored to each document type
4. Provides detailed, insightful justifications for its recommendations

The real Ollama LLM (gemma3:12b) provided more nuanced and detailed recommendations than our initial mock implementation, demonstrating the value of using a sophisticated language model for this task. The Chunking Judge makes the system more adaptable to different document types without manual configuration, which improves retrieval quality and user experience.

### Phase 2 Results (Retrieval Judge)

The Retrieval Judge implementation enhances the RAG pipeline by:

1. Analyzing queries to determine complexity, specificity, and intent
2. Dynamically adjusting retrieval parameters based on query characteristics
3. Evaluating retrieved chunks for relevance with detailed scoring
4. Refining queries when initial results are insufficient
5. Optimizing the order and selection of chunks for context assembly

### Phase 3 Results (Semantic Chunker)

The Semantic Chunker implementation enhances the document processing pipeline by:

1. Identifying natural semantic boundaries in text based on topic transitions
2. Preserving semantic meaning and context in chunks
3. Creating more coherent, self-contained chunks than traditional methods
4. Respecting the logical flow of information in documents
5. Providing caching and fallback mechanisms for robust performance

### Phase 4 Results (LangGraph RAG Agent)

The LangGraph RAG Agent implementation provides a sophisticated orchestration layer for the RAG process:

1. Uses a state machine to coordinate the entire RAG workflow
2. Integrates all existing components (Chunking Judge, Semantic Chunker, Retrieval Judge)
3. Provides a clear, modular architecture with well-defined states and transitions
4. Enables more complex, multi-step reasoning during the RAG process
5. Supports both streaming and non-streaming responses
6. Includes robust error handling and fallback mechanisms
7. Exposes a dedicated API endpoint for direct access

The LangGraph integration represents a significant advancement in our RAG system's architecture, moving from a linear pipeline to a more flexible, state-based approach that can adapt to different query types and document characteristics.

Our comprehensive testing of the Retrieval Judge with edge cases revealed:

- **Query Analysis**: The judge classifies most queries as "moderate" complexity, with only very short queries as "simple" and long multi-part queries as "complex". It provides detailed justifications for parameter recommendations.

- **Query Refinement**: The judge dramatically expands and clarifies queries:
  - Short queries expanded by 887.5% on average
  - Ambiguous queries gain 650.6% more specificity
  - Typos are automatically corrected
  - Query refinement is very fast (2.08s average)

- **Chunk Evaluation**: The judge effectively evaluates chunk relevance:
  - 7/11 test queries identified as needing refinement
  - Domain-specific and long queries rarely need refinement (0%)
  - Ambiguous, multi-part, and short queries always need refinement (100%)

- **Context Optimization**: The judge dramatically reduces context size:
  - 76.4% average reduction in chunks (typically from 5 to 1)
  - Long queries maintain more chunks (5 to 3) for comprehensive answers

- **Performance Improvement**: The judge-enhanced retrieval is 89.26% faster than standard retrieval (18.41s vs 171.47s on average) due to effective caching of both vector store queries and LLM responses.

The integration with the RAGEngine provides a seamless experience, with the Retrieval Judge being enabled or disabled via configuration. This allows for flexibility in deployment, where the enhanced retrieval can be used for complex queries while falling back to standard retrieval for simpler queries or when performance is a concern.

### Next Steps

With all four phases (Chunking Judge, Retrieval Judge, Semantic Chunker, and LangGraph RAG Agent) successfully implemented and tested, the next steps will focus on:

1. Comprehensive benchmarking and performance evaluation with real-world queries
2. Implementing a feedback loop to continuously improve the system over time
3. Exploring additional optimizations and enhancements to the LangGraph state machine
4. Developing user interface components to visualize the agentic RAG process

Our testing has demonstrated that the LLM-enhanced RAG system with judges for chunking and retrieval, semantic chunking, and LangGraph orchestration significantly improves the adaptability, performance, and accuracy of the RAG pipeline:

1. The Chunking Judge intelligently selects the most appropriate chunking strategy and parameters
2. The Semantic Chunker creates more coherent, meaningful chunks that preserve context
3. The Retrieval Judge optimizes query processing and context assembly

Together, these components form a powerful, intelligent RAG system that:
- Adapts to different document types without manual configuration
- Preserves semantic meaning throughout the pipeline
- Transforms ambiguous queries into specific, detailed requests
- Evaluates and filters content for maximum relevance
- Orchestrates complex, multi-step reasoning processes
- Provides a flexible, state-based architecture for the RAG workflow
- Optimizes performance through effective caching

These enhancements have made the RAG system more effective for a wider range of use cases, particularly for complex, domain-specific queries. The LangGraph integration has successfully built on the foundation of the judges and semantic chunker to create a sophisticated, agentic RAG system that can adapt to different query types and document characteristics.

================
File: docs/implementation/llm_enhanced_rag_implementation_plan.md
================
# LLM-Enhanced RAG System Implementation Plan

## Overview

This document outlines a comprehensive plan for implementing an enhanced Retrieval Augmented Generation (RAG) system using LLM-based "judges" to improve two critical aspects of the RAG pipeline:

1. **Dynamic Chunking Strategy Selection**: An LLM agent ("Chunking Judge") that analyzes documents and selects the most appropriate chunking strategy and parameters.

2. **Query Refinement and Retrieval Enhancement**: An LLM agent ("Retrieval Judge") that analyzes queries and retrieved chunks to improve retrieval quality through query refinement, relevance evaluation, and potential re-retrieval.

## Current System Analysis

The current RAG implementation has several strengths:

- Multiple chunking strategies (recursive, token-based, markdown) with file type-specific handling
- Relevance filtering with a threshold of 0.4 to filter out less relevant chunks
- Robust vector store using ChromaDB with caching for performance
- Well-structured document model with chunks, metadata, tags, and folder organization
- Resilient Ollama client with retry logic and error handling

However, the system currently uses static rules for chunking strategy selection and fixed parameters, which limits adaptability to diverse document types. The retrieval process is also relatively simple, without query refinement or iterative retrieval.

## Implementation Approach

We will take a phased approach to implementation, starting with the Chunking Judge and then moving on to the Retrieval Judge. Both judges will use the gemma3:12b model for its multimodal capabilities, with the system designed to allow changing the model in the future.

## Phase 1: Chunking Judge Implementation

### 1.1 Create Agent Architecture

```mermaid
flowchart TD
    A[Document Upload] --> B[Document Processor]
    B --> C{Chunking Judge}
    C -->|Analyze Document| D[Select Strategy]
    D --> E[Determine Parameters]
    E --> F[Apply Chunking]
    F --> G[Store Chunks]
```

### 1.2 Create the Chunking Judge Class

```python
# app/rag/agents/chunking_judge.py
import logging
import json
import re
from typing import Dict, Any, List, Optional
from app.models.document import Document
from app.rag.ollama_client import OllamaClient
from app.core.config import CHUNKING_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.chunking_judge")

class ChunkingJudge:
    """
    LLM-based agent that analyzes documents and recommends optimal chunking strategies
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = CHUNKING_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_document(self, document: Document) -> Dict[str, Any]:
        """
        Analyze a document and recommend the best chunking strategy and parameters
        
        Returns:
            Dict with keys:
            - strategy: The recommended chunking strategy
            - parameters: Dict of parameters for the chosen strategy
            - justification: Explanation of the recommendation
        """
        # Extract a sample of the document content (to avoid exceeding context window)
        content_sample = self._extract_representative_sample(document.content)
        
        # Create prompt for the LLM
        prompt = self._create_analysis_prompt(document.filename, content_sample)
        
        # Get recommendation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        recommendation = self._parse_recommendation(response.get("response", ""))
        
        logger.info(f"Chunking Judge recommended strategy '{recommendation['strategy']}' for document {document.filename}")
        
        return recommendation
    
    def _extract_representative_sample(self, content: str, max_length: int = 5000) -> str:
        """Extract a representative sample of the document content"""
        if len(content) <= max_length:
            return content
        
        # Take beginning, middle and end sections
        third = max_length // 3
        beginning = content[:third]
        middle_start = (len(content) - third) // 2
        middle = content[middle_start:middle_start + third]
        end = content[-third:]
        
        return f"{beginning}\n\n[...]\n\n{middle}\n\n[...]\n\n{end}"
    
    def _create_analysis_prompt(self, filename: str, content_sample: str) -> str:
        """Create a prompt for the LLM to analyze the document"""
        return f"""You are a document analysis expert. Your task is to analyze the following document and recommend the best chunking strategy for a RAG (Retrieval Augmented Generation) system.

Available Strategies:
- recursive: Splits text recursively by characters. Good for general text with natural separators.
- token: Splits text by tokens. Good for preserving semantic units in technical content.
- markdown: Splits markdown documents by headers. Good for structured documents with clear sections.

Document Filename: {filename}

Document Sample:
{content_sample}

Analyze the document structure, content type, and formatting. Consider:
1. Is this a structured document with clear sections or headers?
2. Does it contain code, tables, or other special formatting?
3. What's the typical paragraph and sentence length?
4. Are there natural breaks in the content?
5. Would semantic chunking be more appropriate than fixed-size chunking?

Output your recommendation in JSON format:
{{
    "strategy": "...",  // One of: recursive, token, markdown
    "parameters": {{
        "chunk_size": ...,  // Recommended chunk size (characters or tokens)
        "chunk_overlap": ...  // Recommended overlap size
    }},
    "justification": "..." // Explanation of your reasoning
}}
"""
    
    def _parse_recommendation(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM response to extract the recommendation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                recommendation = json.loads(json_str)
                
                # Validate the recommendation
                if "strategy" not in recommendation:
                    raise ValueError("Missing 'strategy' in recommendation")
                
                # Set defaults if missing
                if "parameters" not in recommendation:
                    recommendation["parameters"] = {}
                if "chunk_size" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_size"] = 500
                if "chunk_overlap" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_overlap"] = 50
                
                return recommendation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing chunking recommendation: {str(e)}")
            # Return default recommendation
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 500,
                    "chunk_overlap": 50
                },
                "justification": "Failed to parse LLM recommendation, using default strategy."
            }
```

### 1.3 Update Configuration

```python
# app/core/config.py
# Add new configuration variables
CHUNKING_JUDGE_MODEL = os.getenv("CHUNKING_JUDGE_MODEL", DEFAULT_MODEL)
RETRIEVAL_JUDGE_MODEL = os.getenv("RETRIEVAL_JUDGE_MODEL", DEFAULT_MODEL)
USE_CHUNKING_JUDGE = os.getenv("USE_CHUNKING_JUDGE", "True").lower() == "true"
USE_RETRIEVAL_JUDGE = os.getenv("USE_RETRIEVAL_JUDGE", "True").lower() == "true"
```

### 1.4 Integrate with DocumentProcessor

```python
# app/rag/document_processor.py
# Update the process_document method

from app.rag.agents.chunking_judge import ChunkingJudge
from app.core.config import USE_CHUNKING_JUDGE

async def process_document(self, document: Document) -> Document:
    """
    Process a document by splitting it into chunks
    """
    try:
        logger.info(f"Processing document: {document.filename}")
        
        # Get the document path
        file_path = os.path.join(UPLOAD_DIR, document.id, document.filename)
        
        # Get file extension for specialized handling
        _, ext = os.path.splitext(file_path.lower())
        
        # Use Chunking Judge if enabled
        if USE_CHUNKING_JUDGE:
            chunking_judge = ChunkingJudge()
            analysis_result = await chunking_judge.analyze_document(document)
            
            # Update chunking strategy and parameters
            self.chunking_strategy = analysis_result["strategy"]
            if "chunk_size" in analysis_result["parameters"]:
                self.chunk_size = analysis_result["parameters"]["chunk_size"]
            if "chunk_overlap" in analysis_result["parameters"]:
                self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
            
            # Store the chunking analysis in document metadata
            document.metadata["chunking_analysis"] = analysis_result
            
            logger.info(f"Using Chunking Judge recommendation: strategy={self.chunking_strategy}, " +
                       f"chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
        else:
            logger.info(f"Chunking Judge disabled, using default strategy: {self.chunking_strategy}")
        
        # Get appropriate text splitter for this file type and strategy
        self.text_splitter = self._get_text_splitter(ext)
        
        # Rest of the existing code...
```

### 1.5 Enhance Text Splitter Selection

```python
# app/rag/document_processor.py
# Update the _get_text_splitter method to support more advanced chunking strategies

def _get_text_splitter(self, file_ext=None):
    """Get the appropriate text splitter based on chunking strategy and file type"""
    logger.info(f"Using chunking strategy: {self.chunking_strategy} for file type: {file_ext}")
    
    # If we have a chunking analysis, log it
    if hasattr(self, 'document') and self.document and 'chunking_analysis' in self.document.metadata:
        logger.info(f"Chunking analysis: {self.document.metadata['chunking_analysis']['justification']}")
    
    # Text file handling - use paragraph-based splitting for more natural chunks
    if file_ext == ".txt":
        # Use a larger chunk size for text files to preserve more context
        larger_chunk_size = self.chunk_size * 3  # Increase from 500 to 1500
        logger.info(f"Using paragraph-based splitting for text file with increased chunk size {larger_chunk_size}")
        return RecursiveCharacterTextSplitter(
            chunk_size=larger_chunk_size,
            chunk_overlap=self.chunk_overlap * 2,  # Increase overlap as well
            separators=["\n\n", "\n", ".", " ", ""]
        )
    
    # PDF-specific handling
    if file_ext == ".pdf":
        logger.info(f"Using PDF-specific splitting with chunk size {self.chunk_size}")
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
    
    # Markdown-specific handling
    if file_ext == ".md" and self.chunking_strategy == "markdown":
        logger.info("Using header-based splitting for markdown")
        # First split by headers
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "header1"),
                ("##", "header2"),
                ("###", "header3"),
                ("####", "header4"),
            ]
        )
        # Then apply recursive splitting to each section
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
    
    # CSV-specific handling
    if file_ext == ".csv":
        logger.info(f"Using larger chunks for CSV with chunk size {self.chunk_size}")
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size * 2,  # Double chunk size for CSVs
            chunk_overlap=self.chunk_overlap
        )
    
    # Standard strategies
    if self.chunking_strategy == "recursive":
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
    elif self.chunking_strategy == "token":
        return TokenTextSplitter(
            chunk_size=self.chunk_size // 4,  # Adjust for tokens vs characters
            chunk_overlap=self.chunk_overlap // 4
        )
    elif self.chunking_strategy == "markdown":
        return MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "header1"),
                ("##", "header2"),
                ("###", "header3"),
                ("####", "header4"),
            ]
        )
    else:
        logger.warning(f"Unknown chunking strategy: {self.chunking_strategy}, falling back to recursive")
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
```

## Phase 2: Retrieval Judge Implementation

### 2.1 Create Agent Architecture

```mermaid
flowchart TD
    A[User Query] --> B[RAG Engine]
    B --> C[Initial Retrieval]
    C --> D{Retrieval Judge}
    D -->|Analyze Query & Chunks| E[Refine Query]
    D -->|Evaluate Relevance| F[Filter/Re-rank]
    D -->|Insufficient Results| G[Request More]
    E --> H[Additional Retrieval]
    F --> I[Final Context]
    G --> H
    H --> I
    I --> J[Generate Response]
```

### 2.2 Create the Retrieval Judge Class

```python
# app/rag/agents/retrieval_judge.py
import logging
import json
import re
from typing import Dict, Any, List, Optional
from app.rag.ollama_client import OllamaClient
from app.core.config import RETRIEVAL_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.retrieval_judge")

class RetrievalJudge:
    """
    LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = RETRIEVAL_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def evaluate_retrieval(
        self, 
        query: str, 
        chunks: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate the retrieved chunks for a query and recommend improvements
        
        Returns:
            Dict with keys:
            - refined_query: Reformulated query (or original if no change)
            - relevant_chunks: List of relevant chunk IDs
            - re_rank_chunks: List of chunk IDs in re-ranked order
            - request_more: Boolean indicating if more retrieval is needed
            - justification: Explanation of the evaluation
        """
        # Create prompt for the LLM
        prompt = self._create_evaluation_prompt(query, chunks, conversation_history)
        
        # Get evaluation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        evaluation = self._parse_evaluation(response.get("response", ""), chunks)
        
        logger.info(f"Retrieval Judge evaluated query: '{query}', refined to: '{evaluation['refined_query']}'")
        logger.info(f"Found {len(evaluation['relevant_chunks'])} relevant chunks out of {len(chunks)}")
        
        return evaluation
    
    def _create_evaluation_prompt(
        self, 
        query: str, 
        chunks: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """Create a prompt for the LLM to evaluate the retrieval"""
        # Format chunks for the prompt
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            # Extract a preview of the chunk content
            content = chunk.get("content", "")
            content_preview = content[:300] + "..." if len(content) > 300 else content
            
            # Format chunk with metadata
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            
            chunks_text += f"Chunk {i+1} (ID: {chunk.get('chunk_id', 'unknown')}):\n"
            chunks_text += f"Source: {filename}\n"
            chunks_text += f"Content: {content_preview}\n\n"
        
        # Format conversation history if provided
        history_text = ""
        if conversation_history and len(conversation_history) > 0:
            history_text = "Previous conversation:\n"
            for msg in conversation_history:
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                history_text += f"{role.capitalize()}: {content}\n"
            history_text += "\n"
        
        return f"""You are a retrieval evaluation expert. Your task is to analyze a user query and a set of retrieved document chunks, and determine if the retrieval is sufficient and relevant.

User Query: {query}

{history_text}Retrieved Chunks:
{chunks_text}

Your task:
1. Evaluate the relevance of each chunk to the query
2. Determine if the retrieval is sufficient to answer the query
3. If needed, reformulate the query to improve retrieval
4. Identify which chunks are most relevant
5. Re-rank the chunks in order of relevance to the query

Output your evaluation in JSON format:
{{
    "refined_query": "...",  // Reformulated query (or original if no change needed)
    "relevant_chunks": [...],  // List of relevant chunk IDs (or indices)
    "re_rank_chunks": [...],  // List of chunk IDs in order of relevance (most to least)
    "request_more": true/false,  // Whether additional retrieval is needed
    "justification": "..."  // Explanation of your evaluation
}}
"""
    
    def _parse_evaluation(self, response_text: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Parse the LLM response to extract the evaluation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                evaluation = json.loads(json_str)
                
                # Set defaults if missing
                if "refined_query" not in evaluation:
                    evaluation["refined_query"] = ""
                if "relevant_chunks" not in evaluation:
                    evaluation["relevant_chunks"] = []
                if "re_rank_chunks" not in evaluation:
                    evaluation["re_rank_chunks"] = []
                if "request_more" not in evaluation:
                    evaluation["request_more"] = False
                
                return evaluation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing retrieval evaluation: {str(e)}")
            # Return default evaluation
            return {
                "refined_query": "",
                "relevant_chunks": [chunk.get("chunk_id") for chunk in chunks],
                "re_rank_chunks": [],
                "request_more": False,
                "justification": "Failed to parse LLM evaluation, using all chunks."
            }
```

### 2.3 Integrate with RAGEngine

```python
# app/rag/rag_engine.py
# Update the query method

from app.rag.agents.retrieval_judge import RetrievalJudge
from app.core.config import USE_RETRIEVAL_JUDGE

async def query(
    self,
    query: str,
    model: str = DEFAULT_MODEL,
    use_rag: bool = True,
    top_k: int = 10,
    system_prompt: Optional[str] = None,
    stream: bool = False,
    model_parameters: Dict[str, Any] = None,
    conversation_history: Optional[List[Message]] = None,
    metadata_filters: Optional[Dict[str, Any]] = None,
    max_retrieval_iterations: int = 2  # New parameter
) -> Dict[str, Any]:
    """
    Query the RAG engine with optional conversation history and metadata filtering
    """
    start_time = time.time()
    document_ids = []
    
    try:
        logger.info(f"RAG query: {query[:50]}...")
        
        # Get context from vector store if RAG is enabled
        context = ""
        sources = []
        
        # Format conversation history if provided
        conversation_context = ""
        formatted_history = []
        
        if conversation_history and len(conversation_history) > 1:
            # Format the conversation history (existing code)
            # ...
            
            # Also prepare a version for the retrieval judge
            for msg in conversation_history[:-1]:  # Exclude current query
                formatted_history.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        if use_rag:
            # Check if there are any documents in the vector store
            stats = self.vector_store.get_stats()
            if stats["count"] == 0:
                # Handle empty vector store (existing code)
                # ...
            else:
                # Iterative retrieval with Retrieval Judge if enabled
                search_query = query
                iteration = 0
                all_results = []
                
                while iteration < max_retrieval_iterations:
                    # Log the search query for this iteration
                    logger.info(f"Retrieval iteration {iteration+1}: searching with query: {search_query[:100]}...")
                    
                    # Perform search with current query
                    search_results = await self.vector_store.search(
                        query=search_query,
                        top_k=15,  # Get more results for the judge to evaluate
                        filter_criteria=metadata_filters
                    )
                    
                    if not search_results:
                        logger.warning(f"No results found for query in iteration {iteration+1}")
                        break
                    
                    # Add results to our collection
                    all_results.extend(search_results)
                    
                    # Use Retrieval Judge if enabled and not the last iteration
                    if USE_RETRIEVAL_JUDGE and iteration < max_retrieval_iterations - 1:
                        retrieval_judge = RetrievalJudge()
                        evaluation = await retrieval_judge.evaluate_retrieval(
                            query=query,  # Original query
                            chunks=search_results,
                            conversation_history=formatted_history
                        )
                        
                        # Log the evaluation
                        logger.info(f"Retrieval Judge evaluation: {evaluation['justification']}")
                        
                        # Check if we need more retrieval
                        if evaluation["request_more"] and evaluation["refined_query"]:
                            # Use the refined query for the next iteration
                            search_query = evaluation["refined_query"]
                            iteration += 1
                            logger.info(f"Retrieval Judge requested more retrieval with refined query: {search_query}")
                        else:
                            # No more retrieval needed
                            break
                    else:
                        # No retrieval judge or last iteration
                        break
                
                # After retrieval iterations, process all results
                if all_results:
                    logger.info(f"Retrieved {len(all_results)} total chunks from vector store")
                    
                    # If we used the retrieval judge, filter and re-rank results
                    if USE_RETRIEVAL_JUDGE and 'evaluation' in locals():
                        # Filter to relevant chunks if specified
                        if evaluation["relevant_chunks"]:
                            relevant_ids = set(evaluation["relevant_chunks"])
                            all_results = [r for r in all_results if r.get("chunk_id") in relevant_ids]
                            logger.info(f"Filtered to {len(all_results)} relevant chunks based on Retrieval Judge")
                        
                        # Re-rank chunks if specified
                        if evaluation["re_rank_chunks"]:
                            # Create a mapping of chunk_id to result
                            result_map = {r.get("chunk_id"): r for r in all_results}
                            
                            # Re-order results based on the re-ranking
                            reranked_results = []
                            for chunk_id in evaluation["re_rank_chunks"]:
                                if chunk_id in result_map:
                                    reranked_results.append(result_map[chunk_id])
                            
                            # Add any remaining results not in the re-ranking
                            remaining = [r for r in all_results if r.get("chunk_id") not in evaluation["re_rank_chunks"]]
                            all_results = reranked_results + remaining
                            logger.info(f"Re-ranked chunks based on Retrieval Judge")
                    
                    # Format context with source information (existing code, but using all_results)
                    # ...
                
                # Rest of the existing code for context formatting
                # ...
        
        # Rest of the existing code for prompt construction and response generation
        # ...
```

## Phase 3: Advanced Semantic Chunking

Building on the success of the Chunking Judge, we can implement more advanced semantic chunking strategies:

### 3.1 Implement Semantic Chunking

```python
# app/rag/semantic_chunker.py
import logging
from typing import List, Dict, Any
from langchain.text_splitter import TextSplitter
from app.rag.ollama_client import OllamaClient
from app.models.document import Document

logger = logging.getLogger("app.rag.semantic_chunker")

class SemanticChunker(TextSplitter):
    """
    LLM-based semantic chunker that splits text based on semantic boundaries
    """
    def __init__(
        self,
        ollama_client: OllamaClient = None,
        model: str = "gemma3:12b",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        super().__init__()
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    async def split_text(self, text: str) -> List[str]:
        """Split text into semantically meaningful chunks"""
        # For very short texts, just return as is
        if len(text) < self.chunk_size:
            return [text]
        
        # Create prompt for the LLM
        prompt = self._create_chunking_prompt(text)
        
        # Get chunking from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response to get chunks
        chunks = self._parse_chunks(response.get("response", ""), text)
        
        logger.info(f"Semantic chunker created {len(chunks)} chunks")
        
        return chunks
    
    def _create_chunking_prompt(self, text: str) -> str:
        """Create a prompt for the LLM to chunk the text"""
        # Use a sample if the text is too long
        if len(text) > 10000:
            text = text[:10000] + "...\n[Text truncated for prompt length]"
        
        return f"""You are a document chunking expert. Your task is to split the following text into semantically meaningful chunks.

Each chunk should:
1. Be a coherent, self-contained unit of information
2. Preserve the semantic meaning of the content
3. Be approximately {self.chunk_size} characters in length (but prioritize semantic coherence over exact size)
4. Not break in the middle of a sentence or logical unit

Text to chunk:
{text}

Output format:
CHUNK 1:
[Content of first chunk]
END CHUNK 1

CHUNK 2:
[Content of second chunk]
END CHUNK 2

... and so on.

Make sure each chunk is semantically meaningful and can stand on its own as much as possible.
"""
    
    def _parse_chunks(self, response_text: str, original_text: str) -> List[str]:
        """Parse the LLM response to extract chunks"""
        try:
            # Extract chunks using regex
            import re
            chunk_pattern = r"CHUNK \d+:\n(.*?)END CHUNK \d+"
            chunks = re.findall(chunk_pattern, response_text, re.DOTALL)
            
            # Clean up chunks
            chunks = [chunk.strip() for chunk in chunks]
            
            # If no chunks were found, fall back to a simple split
            if not chunks:
                logger.warning("No chunks found in LLM response, falling back to simple splitting")
                return self._fallback_split(original_text)
            
            return chunks
        except Exception as e:
            logger.error(f"Error parsing chunks: {str(e)}")
            return self._fallback_split(original_text)
    
    def _fallback_split(self, text: str) -> List[str]:
        """Fallback method to split text if LLM chunking fails"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
        return splitter.split_text(text)
```

### 3.2 Integrate Semantic Chunking with DocumentProcessor

```python
# app/rag/document_processor.py
# Update the _get_text_splitter method to support semantic chunking

from app.rag.semantic_chunker import SemanticChunker

def _get_text_splitter(self, file_ext=None):
    """Get the appropriate text splitter based on chunking strategy and file type"""
    logger.info(f"Using chunking strategy: {self.chunking_strategy} for file type: {file_ext}")
    
    # Add semantic chunking strategy
    if self.chunking_strategy == "semantic":
        logger.info(f"Using semantic chunking with chunk size {self.chunk_size}")
        return SemanticChunker(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
    
    # Rest of the existing code...
```

## Phase 4: LangGraph Integration for Agentic RAG

Building on the success of the individual judges, we can implement a more sophisticated agentic RAG system using LangGraph:

### 4.1 Install Dependencies

```bash
pip install langgraph
```

### 4.2 Create LangGraph RAG Agent

```python
# app/rag/agents/langgraph_rag_agent.py
import logging
from typing import Dict, Any, List, Optional, TypedDict, Annotated, Literal
from enum import Enum
import json

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore
from app.core.config import DEFAULT_MODEL

logger = logging.getLogger("app.rag.agents.langgraph_rag_agent")

# Define state types
class AgentState(TypedDict):
    query: str
    conversation_history: List[Dict[str, str]]
    retrieved_chunks: List[Dict[str, Any]]
    refined_query: Optional[str]
    relevant_chunks: List[Dict[str, Any]]
    final_context: str
    answer: Optional[str]
    sources: List[Dict[str, Any]]
    needs_more_retrieval: bool
    needs_query_refinement: bool

class RAGAgentAction(str, Enum):
    RETRIEVE = "retrieve"
    EVALUATE = "evaluate"
    REFINE_QUERY = "refine_query"
    GENERATE = "generate"
    END = "end"

class LangGraphRAGAgent:
    """
    Agentic RAG system using LangGraph
    """
    def __init__(
        self,
        ollama_client: Optional[OllamaClient] = None,
        vector_store: Optional[VectorStore] = None,
        model: str = DEFAULT_MODEL,
        top_k: int = 10
    ):
        self.ollama_client = ollama_client or OllamaClient()
        self.vector_store = vector_store or VectorStore()
        self.model = model
        self.top_k = top_k
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph for the RAG agent"""
        # Create the graph
        graph = StateGraph(AgentState)
        
        # Add nodes
        graph.add_node("retrieve", self._retrieve)
        graph.add_node("evaluate", self._evaluate_retrieval)
        graph.add_node("refine_query", self._refine_query)
        graph.add_node("generate", self._generate_response)
        
        # Add edges
        graph.add_edge("retrieve", "evaluate")
        graph.add_conditional_edges(
            "evaluate",
            self._route_after_evaluation,
            {
                RAGAgentAction.REFINE_QUERY: "refine_query",
                RAGAgentAction.GENERATE: "generate",
            }
        )
        graph.add_edge("refine_query", "retrieve")
        graph.add_edge("generate", END)
        
        # Set entry point
        graph.set_entry_point("retrieve")
        
        return graph.compile()
    
    async def _retrieve(self, state: AgentState) -> AgentState:
        """Retrieve chunks from the vector store"""
        query = state.get("refined_query") or state.get("query")
        logger.info(f"Retrieving chunks for query: {query[:100]}...")
        
        search_results = await self.vector_store.search(
            query=query,
            top_k=self.top_k
        )
        
        logger.info(f"Retrieved {len(search_results)} chunks")
        
        return {
            **state,
            "retrieved_chunks": search_results
        }
    
    async def _evaluate_retrieval(self, state: AgentState) -> AgentState:
        """Evaluate the retrieved chunks"""
        query = state.get("query")
        chunks = state.get("retrieved_chunks", [])
        
        if not chunks:
            logger.warning("No chunks retrieved to evaluate")
            return {
                **state,
                "needs_more_retrieval": False,
                "needs_query_refinement": False,
                "relevant_chunks": [],
                "final_context": "No relevant information found."
            }
        
        # Create prompt for evaluation
        prompt = f"""You are a retrieval evaluation expert. Analyze these retrieved chunks for the query: "{query}"

Retrieved Chunks:
{self._format_chunks_for_prompt(chunks)}

Evaluate:
1. Are these chunks relevant to the query?
2. Do they contain sufficient information to answer the query?
3. Is query refinement needed to get better results?

Output your evaluation in JSON format:
{{
    "needs_query_refinement": true/false,
    "needs_more_retrieval": true/false,
    "relevant_chunk_ids": [...],
    "justification": "..."
}}
"""
        
        # Get evaluation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        evaluation = self._parse_json_response(response.get("response", ""))
        
        # Get relevant chunks
        relevant_chunk_ids = evaluation.get("relevant_chunk_ids", [])
        relevant_chunks = [c for c in chunks if c.get("chunk_id") in relevant_chunk_ids]
        
        # If no relevant chunks specified, use all chunks
        if not relevant_chunks and chunks:
            relevant_chunks = chunks
        
        # Format final context
        final_context = self._format_context(relevant_chunks)
        
        return {
            **state,
            "needs_more_retrieval": evaluation.get("needs_more_retrieval", False),
            "needs_query_refinement": evaluation.get("needs_query_refinement", False),
            "relevant_chunks": relevant_chunks,
            "final_context": final_context
        }
    
    def _route_after_evaluation(self, state: AgentState) -> RAGAgentAction:
        """Determine next action based on evaluation"""
        if state.get("needs_query_refinement", False):
            return RAGAgentAction.REFINE_QUERY
        else:
            return RAGAgentAction.GENERATE
    
    async def _refine_query(self, state: AgentState) -> AgentState:
        """Refine the query based on evaluation"""
        original_query = state.get("query", "")
        chunks = state.get("retrieved_chunks", [])
        
        # Create prompt for query refinement
        prompt = f"""You are a query refinement expert. The original query was: "{original_query}"

The retrieved chunks were:
{self._format_chunks_for_prompt(chunks)}

These chunks were not sufficient to answer the query. Please refine the query to get better results.
The refined query should be more specific and help retrieve more relevant information.

Output your refined query in JSON format:
{{
    "refined_query": "..."
}}
"""
        
        # Get refinement from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        refinement = self._parse_json_response(response.get("response", ""))
        refined_query = refinement.get("refined_query", original_query)
        
        logger.info(f"Refined query: '{refined_query}'")
        
        return {
            **state,
            "refined_query": refined_query
        }
    
    async def _generate_response(self, state: AgentState) -> AgentState:
        """Generate a response based on the retrieved context"""
        query = state.get("query", "")
        context = state.get("final_context", "")
        
        # Create prompt for response generation
        prompt = f"""Answer the following question based on the provided context:

Context:
{context}

Question: {query}

Provide a comprehensive answer based only on the information in the context. If the context doesn't contain enough information to answer the question, say so clearly.
"""
        
        # Get response from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Extract sources for citations
        sources = []
        for chunk in state.get("relevant_chunks", []):
            metadata = chunk.get("metadata", {})
            sources.append({
                "document_id": metadata.get("document_id", ""),
                "chunk_id": chunk.get("chunk_id", ""),
                "filename": metadata.get("filename", "Unknown"),
                "excerpt": chunk.get("content", "")[:200] + "..." if len(chunk.get("content", "")) > 200 else chunk.get("content", "")
            })
        
        return {
            **state,
            "answer": response.get("response", ""),
            "sources": sources
        }
    
    def _format_chunks_for_prompt(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks for inclusion in a prompt"""
        formatted_chunks = []
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            content_preview = content[:300] + "..." if len(content) > 300 else content
            
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            
            formatted_chunks.append(
                f"Chunk {i+1} (ID: {chunk.get('chunk_id', 'unknown')}):\n"
                f"Source: {filename}\n"
                f"Content: {content_preview}"
            )
        
        return "\n\n".join(formatted_chunks)
    
    def _format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks into a context for the LLM"""
        context_pieces = []
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            
            context_pieces.append(
                f"[{i+1}] Source: {filename}\n\n{content}"
            )
        
        return "\n\n".join(context_pieces)
    
    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """Parse JSON from LLM response"""
        try:
            import re
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                return json.loads(json_str)
            else:
                logger.warning("Could not find JSON in response")
                return {}
        except Exception as e:
            logger.error(f"Error parsing JSON response: {str(e)}")
            return {}
    
    async def query(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Process a query through the agentic RAG system
        """
        # Initialize state
        initial_state = {
            "query": query,
            "conversation_history": conversation_history or [],
            "retrieved_chunks": [],
            "refined_query": None,
            "relevant_chunks": [],
            "final_context": "",
            "answer": None,
            "sources": [],
            "needs_more_retrieval": False,
            "needs_query_refinement": False
        }
        
        # Run the graph
        result = await self.graph.ainvoke(initial_state)
        
        # Return the result
        return {
            "query": query,
            "answer": result.get("answer", ""),
            "sources": result.get("sources", [])
        }
```

### 4.3 Integrate LangGraph Agent with API

```python
# app/api/chat.py
# Add a new endpoint for the LangGraph RAG agent

from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent

@router.post("/chat/langgraph_rag", response_model=ChatResponse)
async def langgraph_rag_chat(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    rag_agent: LangGraphRAGAgent = Depends(get_langgraph_rag_agent)
):
    """
    Chat with the LangGraph RAG agent
    """
    try:
        # Format conversation history
        formatted_history = []
        if request.conversation_history:
            for msg in request.conversation_history:
                formatted_history.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # Process query through the agent
        result = await rag_agent.query(
            query=request.query,
            conversation_history=formatted_history
        )
        
        # Format response
        response = ChatResponse(
            query=request.query,
            answer=result.get("answer", ""),
            sources=[Citation(**source) for source in result.get("sources", [])]
        )
        
        # Record analytics in the background
        background_tasks.add_task(
            record_chat_analytics,
            query=request.query,
            model=request.model,
            use_rag=True,
            document_ids=[s.document_id for s in response.sources]
        )
        
        return response
    except Exception as e:
        logger.error(f"Error in LangGraph RAG chat: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
```

## Testing and Evaluation

### Unit Tests for Chunking Judge

```python
# tests/unit/test_chunking_judge.py
import pytest
from unittest.mock import AsyncMock, patch
from app.rag.agents.chunking_judge import ChunkingJudge
from app.models.document import Document

@pytest.fixture
def mock_ollama_client():
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "strategy": "markdown",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is a structured markdown document with clear headers."
        }
        """
    }
    return client

@pytest.mark.asyncio
async def test_chunking_judge_analysis(mock_ollama_client):
    # Create a test document
    doc = Document(
        id="test-id",
        filename="test.md",
        content="# Header 1\nContent\n## Header 2\nMore content"
    )
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(doc)
    
    # Verify result
    assert result["strategy"] == "markdown"
    assert result["parameters"]["chunk_size"] == 800
    assert result["parameters"]["chunk_overlap"] == 100
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "document analysis expert" in call_args["prompt"].lower()
    assert "test.md" in call_args["prompt"]
```

### Unit Tests for Retrieval Judge

```python
# tests/unit/test_retrieval_judge.py
import pytest
from unittest.mock import AsyncMock, patch
from app.rag.agents.retrieval_judge import RetrievalJudge

@pytest.fixture
def mock_ollama_client():
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "refined_query": "advanced machine learning techniques for time series data",
            "relevant_chunks": ["chunk1", "chunk3"],
            "re_rank_chunks": ["chunk3", "chunk1"],
            "request_more": true,
            "justification": "The query needs to be more specific about time series data."
        }
        """
    }
    return client

@pytest.fixture
def test_chunks():
    return [
        {
            "chunk_id": "chunk1",
            "content": "Machine learning is a field of study...",
            "metadata": {"filename": "ml.txt"}
        },
        {
            "chunk_id": "chunk2",
            "content": "Data visualization techniques include...",
            "metadata": {"filename": "viz.txt"}
        },
        {
            "chunk_id": "chunk3",
            "content": "Time series analysis requires specialized algorithms...",
            "metadata": {"filename": "timeseries.txt"}
        }
    ]

@pytest.mark.asyncio
async def test_retrieval_judge_evaluation(mock_ollama_client, test_chunks):
    # Create retrieval judge with mock client
    judge = RetrievalJudge(ollama_client=mock_ollama_client)
    
    # Test evaluation
    result = await judge.evaluate_retrieval(
        query="machine learning for time series",
        chunks=test_chunks
    )
    
    # Verify result
    assert "advanced machine learning techniques" in result["refined_query"]
    assert "chunk1" in result["relevant_chunks"]
    assert "chunk3" in result["relevant_chunks"]
    assert result["re_rank_chunks"][0] == "chunk3"  # Re-ranked with chunk3 first
    assert result["request_more"] is True
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "retrieval evaluation expert" in call_args["prompt"].lower()
    assert "machine learning for time series" in call_args["prompt"]
```

### Performance Benchmarking

```python
# scripts/benchmark_judges.py
import asyncio
import time
import json
import argparse
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore

async def run_benchmark(queries, use_judges=True):
    """Run benchmark tests with and without judges"""
    vector_store = VectorStore()
    rag_engine = RAGEngine(vector_store=vector_store)
    
    results = []
    
    for query in queries:
        start_time = time.time()
        
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            max_retrieval_iterations=2 if use_judges else 1
        )
        
        elapsed_time = time.time() - start_time
        
        results.append({
            "query": query,
            "time_seconds": elapsed_time,
            "sources_count": len(response.get("sources", [])),
            "answer_length": len(response.get("answer", ""))
        })
    
    return results

async def main():
    parser = argparse.ArgumentParser(description="Benchmark RAG with and without judges")
    parser.add_argument("--queries_file", type=str, default="benchmark_queries.json", help="JSON file with test queries")
    parser.add_argument("--output_file", type=str, default="benchmark_results.json", help="Output file for results")
    args = parser.parse_args()
    
    # Load test queries
    with open(args.queries_file, "r") as f:
        queries = json.load(f)
    
    # Run benchmarks
    print(f"Running benchmark with {len(queries)} queries...")
    
    print("Testing with judges enabled...")
    with_judges = await run_benchmark(queries, use_judges=True)
    
    print("Testing with judges disabled...")
    without_judges = await run_benchmark(queries, use_judges=False)
    
    # Compute averages
    avg_time_with = sum(r["time_seconds"] for r in with_judges) / len(with_judges)
    avg_time_without = sum(r["time_seconds"] for r in without_judges) / len(without_judges)
    
    avg_sources_with = sum(r["sources_count"] for r in with_judges) / len(with_judges)
    avg_sources_without = sum(r["sources_count"] for r in without_judges) / len(without_judges)
    
    # Save results
    results = {
        "with_judges": with_judges,
        "without_judges": without_judges,
        "summary": {
            "avg_time_with_judges": avg_time_with,
            "avg_time_without_judges": avg_time_without,
            "time_difference_percent": (avg_time_with - avg_time_without) / avg_time_without * 100,
            "avg_sources_with_judges": avg_sources_with,
            "avg_sources_without_judges": avg_sources_without
        }
    }
    
    with open(args.output_file, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"Benchmark complete. Results saved to {args.output_file}")
    print(f"Average time with judges: {avg_time_with:.2f}s")
    print(f"Average time without judges: {avg_time_without:.2f}s")
    print(f"Time difference: {(avg_time_with - avg_time_without) / avg_time_without * 100:.2f}%")

if __name__ == "__main__":
    asyncio.run(main())
```

## Implementation Timeline

### Phase 1: Chunking Judge (Weeks 1-2)
- Week 1: Implement Chunking Judge class and integration with DocumentProcessor
- Week 2: Testing and refinement of Chunking Judge

### Phase 2: Retrieval Judge (Weeks 3-4)
- Week 3: Implement Retrieval Judge class and integration with RAGEngine
- Week 4: Testing and refinement of Retrieval Judge

### Phase 3: Advanced Semantic Chunking (Weeks 5-6)
- Week 5: Implement SemanticChunker class
- Week 6: Integration and testing of semantic chunking

### Phase 4: LangGraph Integration (Weeks 7-8)
- Week 7: Implement LangGraphRAGAgent
- Week 8: Integration with API and testing

## Potential Challenges and Mitigations

1. **Increased Latency**
   - **Challenge**: Adding LLM calls will increase response time
   - **Mitigation**: Implement caching for judge decisions, make judges optional via configuration, optimize when to use judges

2. **Cost Considerations**
   - **Challenge**: Additional LLM calls increase computational costs
   - **Mitigation**: Make judges optional, implement usage tracking, optimize when to use judges

3. **Error Handling**
   - **Challenge**: LLM responses may not always be parseable
   - **Mitigation**: Robust fallback mechanisms in the parsing functions

4. **Cold Start Problem**
   - **Challenge**: Judges may perform poorly on new document types
   - **Mitigation**: Implement a feedback loop to improve judge performance over time

## Conclusion

The proposed LLM-enhanced RAG system with "judges" for chunking and retrieval has significant potential to improve the adaptability and accuracy of the RAG pipeline. By dynamically selecting chunking strategies and refining queries, the system can better handle diverse document types and complex queries.

The phased implementation approach allows for incremental testing and refinement, with each phase building on the success of the previous one. The use of gemma3:12b as the model for both judges leverages its multimodal capabilities, with the system designed to allow changing the model in the future.

This implementation plan draws on best practices and techniques from LangChain, LangGraph, and recent research in semantic chunking and query refinement. The result will be a more intelligent and adaptable RAG system that can provide more accurate and relevant responses to user queries.

================
File: docs/implementation/Mem0_Docker_Integration_Plan.md
================
# Comprehensive Plan for Enhancing Mem0 Docker Integration in Metis_RAG

This document outlines a detailed plan for enhancing the Mem0 Docker integration in Metis_RAG, addressing key areas of database separation, performance optimization, API key management, and Docker configuration.

## Table of Contents

1. [Docker Configuration Enhancement](#1-docker-configuration-enhancement)
   1. [Updated Docker Compose Configuration](#11-updated-docker-compose-configuration)
   2. [Create a .dockerignore File](#12-create-a-dockerignore-file)
2. [Performance Optimization](#2-performance-optimization)
   1. [Enhanced Mem0 Client with Caching](#21-enhanced-mem0-client-with-caching)
   2. [Database Connection Pooling](#22-database-connection-pooling)
3. [Environment Configuration](#3-environment-configuration)
   1. [Update .env.example](#31-update-envexample)
   2. [Update .env.docker](#32-update-envdocker)
4. [Documentation Enhancement](#4-documentation-enhancement)
   1. [Update README_MEM0_INTEGRATION.md](#41-update-readme_mem0_integrationmd)
5. [Testing Plan](#5-testing-plan)
   1. [Unit Tests](#51-unit-tests)
6. [Prioritized Implementation Steps](#6-prioritized-implementation-steps)
7. [Monitoring and Maintenance](#7-monitoring-and-maintenance)

## 1. Docker Configuration Enhancement

### 1.1 Updated Docker Compose Configuration

Create an enhanced `docker-compose.yml` with detailed comments:

```yaml
version: '3.8'

services:
  # Main Metis RAG application service
  metis-rag:
    build:
      context: ..
      dockerfile: config/Dockerfile
      target: ${METIS_BUILD_TARGET:-base}  # Allows switching between development and production builds
    image: metis-rag:${METIS_VERSION:-latest}
    container_name: metis-rag
    restart: unless-stopped
    ports:
      - "${METIS_PORT:-8000}:8000"  # Expose the API port, configurable via environment
    volumes:
      - ../data:/app/data  # Mount data directory for persistent storage
      - ../config:/app/config  # Mount config directory for easy configuration
    environment:
      # Core application settings
      - METIS_CONFIG_FILE=/app/config/settings.json
      # Database connection settings
      - METIS_DB_TYPE=postgresql
      - METIS_POSTGRES_DSN=postgresql://postgres:postgres@metis-postgres:5432/metis
      # LLM provider settings
      - METIS_LLM_PROVIDER_TYPE=ollama
      - METIS_OLLAMA_BASE_URL=http://ollama:11434
      # Mem0 settings
      - MEM0_ENDPOINT=http://mem0:8050
      - MEM0_API_KEY=${MEM0_ADMIN_API_KEY:-default_dev_key}  # Use env var or default
      - USE_MEM0=True
    networks:
      - metis-network  # Use custom network for service discovery
    depends_on:
      metis-postgres:
        condition: service_healthy  # Wait for PostgreSQL to be ready
      ollama:
        condition: service_healthy  # Wait for Ollama to be ready
      mem0:
        condition: service_healthy  # Wait for Mem0 to be ready
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '2'  # Limit CPU usage
          memory: 2G  # Limit memory usage

  # PostgreSQL database for Metis RAG
  metis-postgres:
    image: postgres:15-alpine  # Use Alpine for smaller image size
    container_name: metis-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=metis
    volumes:
      - metis-postgres-data:/var/lib/postgresql/data  # Persistent volume for database
    ports:
      - "5432:5432"  # Expose PostgreSQL port
    networks:
      - metis-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]  # Check if PostgreSQL is ready
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1'  # Limit CPU usage
          memory: 1G  # Limit memory usage

  # Ollama LLM service
  ollama:
    image: ollama/ollama:latest
    container_name: metis-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"  # Expose Ollama API port
    volumes:
      - ../data/ollama:/root/.ollama  # Persistent volume for models
    networks:
      - metis-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s  # Longer start period as models may take time to load
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]  # Use GPU for inference

  # PostgreSQL database for Mem0 (separate from Metis RAG database)
  mem0-postgres:
    image: postgres:15-alpine
    container_name: mem0-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=mem0
    volumes:
      - mem0-postgres-data:/var/lib/postgresql/data  # Persistent volume for database
    ports:
      - "5433:5432"  # Use different port to avoid conflict with metis-postgres
    networks:
      - metis-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1'  # Limit CPU usage
          memory: 1G  # Limit memory usage

  # Mem0 memory service
  mem0:
    image: mem0ai/mem0:latest
    container_name: metis-mem0
    restart: unless-stopped
    ports:
      - "8050:8050"  # Expose Mem0 API port
    environment:
      - DATABASE_URL=postgres://postgres:postgres@mem0-postgres:5432/mem0  # Connect to mem0-postgres
      - MEM0_ADMIN_API_KEY=${MEM0_ADMIN_API_KEY:-default_dev_key}  # Use env var or default
    volumes:
      - ../data/mem0:/app/data  # Persistent volume for Mem0 data
    networks:
      - metis-network
    depends_on:
      mem0-postgres:
        condition: service_healthy  # Wait for PostgreSQL to be ready
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8050/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1'  # Limit CPU usage
          memory: 1G  # Limit memory usage

# Define custom network for service discovery
networks:
  metis-network:
    driver: bridge

# Define named volumes for persistent data
volumes:
  metis-postgres-data:  # Persistent volume for Metis RAG database
  mem0-postgres-data:   # Persistent volume for Mem0 database
```

### 1.2 Create a .dockerignore File

Create a `.dockerignore` file to exclude unnecessary files from the Docker build context:

```
# Version control
.git
.gitignore

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg
venv/
venv_py310/

# Data directories
data/
uploads/
chroma_db/

# IDE files
.idea/
.vscode/
*.swp
*.swo

# Logs
logs/
*.log

# Test files
.coverage
htmlcov/
.pytest_cache/
```

## 2. Performance Optimization

### 2.1 Enhanced Mem0 Client with Caching

Update the `app/rag/mem0_client.py` file to include caching and a clear_cache method:

```python
"""
Mem0 client for Metis_RAG with enhanced caching
"""
import os
import logging
from typing import Optional, Dict, Any, List, Union
from cachetools import TTLCache

from app.core.config import SETTINGS

logger = logging.getLogger("app.rag.mem0_client")

# Default agent ID for Metis RAG
METIS_AGENT_ID = "metis_rag_agent"

# Default persona for Metis RAG agent
METIS_PERSONA = """
You are Metis RAG, a helpful assistant that answers questions based on provided documents.
You provide accurate, concise, and helpful responses based on the information in your knowledge base.
When you don't know the answer, you clearly state that you don't have enough information.
"""

# Check if Mem0 is enabled and the package is available
MEM0_AVAILABLE = False
if SETTINGS.use_mem0:
    try:
        from mem0ai import Mem0Client
        MEM0_AVAILABLE = True
    except ImportError:
        logger.warning("mem0ai package not found. Mem0 integration will be disabled.")

# Singleton instance of Mem0Client
_mem0_client: Optional["EnhancedMem0Client"] = None

class EnhancedMem0Client:
    """
    Enhanced Mem0 client with caching capabilities
    """
    def __init__(self, api_key: Optional[str] = None, endpoint: str = "http://localhost:8050"):
        """
        Initialize the enhanced Mem0 client
        
        Args:
            api_key: Optional API key for authentication
            endpoint: Mem0 API endpoint
        """
        from mem0ai import Mem0Client
        
        # Initialize the base client
        if api_key:
            self.client = Mem0Client(api_key=api_key, endpoint=endpoint)
        else:
            self.client = Mem0Client(endpoint=endpoint)
            
        # Initialize caches with appropriate TTLs
        self.human_cache = TTLCache(maxsize=1000, ttl=300)  # 5 minutes
        self.recall_cache = TTLCache(maxsize=1000, ttl=60)  # 1 minute
        self.archival_cache = TTLCache(maxsize=2000, ttl=300)  # 5 minutes
        self.search_cache = TTLCache(maxsize=2000, ttl=120)  # 2 minutes
        
        # Store endpoint for logging
        self.endpoint = endpoint
        
    def clear_cache(self, cache_type: Optional[str] = None):
        """
        Clear the cache
        
        Args:
            cache_type: Type of cache to clear (human, recall, archival, search, or None for all)
        """
        if cache_type == "human" or cache_type is None:
            self.human_cache.clear()
            logger.debug("Cleared human cache")
            
        if cache_type == "recall" or cache_type is None:
            self.recall_cache.clear()
            logger.debug("Cleared recall cache")
            
        if cache_type == "archival" or cache_type is None:
            self.archival_cache.clear()
            logger.debug("Cleared archival cache")
            
        if cache_type == "search" or cache_type is None:
            self.search_cache.clear()
            logger.debug("Cleared search cache")
            
        if cache_type is None:
            logger.info("Cleared all caches")
        else:
            logger.info(f"Cleared {cache_type} cache")
            
    def get_agent(self, agent_id: str) -> Optional[Dict[str, Any]]:
        """
        Get agent information
        
        Args:
            agent_id: Agent ID
            
        Returns:
            Agent information or None if not found
        """
        cache_key = f"agent:{agent_id}"
        if cache_key in self.human_cache:
            return self.human_cache[cache_key]
            
        try:
            agent = self.client.get_agent(agent_id)
            if agent:
                self.human_cache[cache_key] = agent
            return agent
        except Exception as e:
            logger.error(f"Error getting agent {agent_id}: {str(e)}")
            return None
            
    def create_agent(self, agent_id: str, name: str, persona: str) -> bool:
        """
        Create a new agent
        
        Args:
            agent_id: Agent ID
            name: Agent name
            persona: Agent persona
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.client.create_agent(agent_id=agent_id, name=name, persona=persona)
            # Invalidate cache
            cache_key = f"agent:{agent_id}"
            if cache_key in self.human_cache:
                del self.human_cache[cache_key]
            return True
        except Exception as e:
            logger.error(f"Error creating agent {agent_id}: {str(e)}")
            return False
            
    def get_human(self, human_id: str) -> Optional[Dict[str, Any]]:
        """
        Get human information
        
        Args:
            human_id: Human ID
            
        Returns:
            Human information or None if not found
        """
        cache_key = f"human:{human_id}"
        if cache_key in self.human_cache:
            return self.human_cache[cache_key]
            
        try:
            human = self.client.get_human(human_id)
            if human:
                self.human_cache[cache_key] = human
            return human
        except Exception as e:
            logger.error(f"Error getting human {human_id}: {str(e)}")
            return None
            
    def create_human(self, human_id: str, name: str) -> bool:
        """
        Create a new human
        
        Args:
            human_id: Human ID
            name: Human name
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.client.create_human(human_id=human_id, name=name)
            # Invalidate cache
            cache_key = f"human:{human_id}"
            if cache_key in self.human_cache:
                del self.human_cache[cache_key]
            return True
        except Exception as e:
            logger.error(f"Error creating human {human_id}: {str(e)}")
            return False
            
    def append_message(self, agent_id: str, human_id: str, message: Dict[str, str]) -> bool:
        """
        Append a message to recall memory
        
        Args:
            agent_id: Agent ID
            human_id: Human ID
            message: Message to append
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.client.append_message(agent_id=agent_id, human_id=human_id, message=message)
            # Invalidate recall cache for this conversation
            cache_prefix = f"recall:{agent_id}:{human_id}"
            keys_to_delete = [k for k in self.recall_cache.keys() if k.startswith(cache_prefix)]
            for k in keys_to_delete:
                del self.recall_cache[k]
            return True
        except Exception as e:
            logger.error(f"Error appending message for {human_id}: {str(e)}")
            return False
            
    def get_recall_memory(self, agent_id: str, human_id: str, limit: int = 10) -> List[Dict[str, str]]:
        """
        Get recall memory
        
        Args:
            agent_id: Agent ID
            human_id: Human ID
            limit: Maximum number of messages to retrieve
            
        Returns:
            List of messages
        """
        cache_key = f"recall:{agent_id}:{human_id}:{limit}"
        if cache_key in self.recall_cache:
            return self.recall_cache[cache_key]
            
        try:
            recall = self.client.get_recall_memory(agent_id=agent_id, human_id=human_id, limit=limit)
            self.recall_cache[cache_key] = recall
            return recall
        except Exception as e:
            logger.error(f"Error getting recall memory for {human_id}: {str(e)}")
            return []
            
    def create_archival_memory(self, agent_id: str, human_id: str, data: Dict[str, Any], kind: str, replace: bool = False) -> bool:
        """
        Create archival memory
        
        Args:
            agent_id: Agent ID
            human_id: Human ID
            data: Memory data
            kind: Memory kind
            replace: Whether to replace existing memory
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.client.create_archival_memory(
                agent_id=agent_id,
                human_id=human_id,
                data=data,
                kind=kind,
                replace=replace
            )
            # Invalidate archival cache for this kind
            cache_prefix = f"archival:{agent_id}:{human_id}:{kind}"
            keys_to_delete = [k for k in self.archival_cache.keys() if k.startswith(cache_prefix)]
            for k in keys_to_delete:
                del self.archival_cache[k]
                
            # Also invalidate search cache as it might be affected
            search_prefix = f"search:{agent_id}:{human_id}:{kind}"
            keys_to_delete = [k for k in self.search_cache.keys() if k.startswith(search_prefix)]
            for k in keys_to_delete:
                del self.search_cache[k]
                
            return True
        except Exception as e:
            logger.error(f"Error creating archival memory for {human_id}: {str(e)}")
            return False
            
    def get_archival_memory(self, agent_id: str, human_id: str, kind: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get archival memory
        
        Args:
            agent_id: Agent ID
            human_id: Human ID
            kind: Memory kind
            limit: Maximum number of memories to retrieve
            
        Returns:
            List of memories
        """
        cache_key = f"archival:{agent_id}:{human_id}:{kind}:{limit}"
        if cache_key in self.archival_cache:
            return self.archival_cache[cache_key]
            
        try:
            archival = self.client.get_archival_memory(
                agent_id=agent_id,
                human_id=human_id,
                kind=kind,
                limit=limit
            )
            self.archival_cache[cache_key] = archival
            return archival
        except Exception as e:
            logger.error(f"Error getting archival memory for {human_id}: {str(e)}")
            return []
            
    def search_archival_memory(self, agent_id: str, human_id: str, query: Optional[str], kind: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Search archival memory
        
        Args:
            agent_id: Agent ID
            human_id: Human ID
            query: Search query
            kind: Memory kind
            limit: Maximum number of results
            
        Returns:
            List of search results
        """
        cache_key = f"search:{agent_id}:{human_id}:{kind}:{query}:{limit}"
        if cache_key in self.search_cache:
            return self.search_cache[cache_key]
            
        try:
            results = self.client.search_archival_memory(
                agent_id=agent_id,
                human_id=human_id,
                query=query,
                kind=kind,
                limit=limit
            )
            self.search_cache[cache_key] = results
            return results
        except Exception as e:
            logger.error(f"Error searching archival memory for {human_id}: {str(e)}")
            return []

def get_mem0_client() -> Optional[EnhancedMem0Client]:
    """
    Get the EnhancedMem0Client instance
    
    Returns:
        EnhancedMem0Client instance or None if not configured
    """
    global _mem0_client
    
    if _mem0_client is None:
        try:
            # Check if Mem0 is enabled and available
            if not SETTINGS.use_mem0 or not MEM0_AVAILABLE:
                logger.info("Mem0 integration is disabled or not available")
                return None
                
            # Initialize the client (API key is optional for local development)
            if SETTINGS.mem0_api_key:
                _mem0_client = EnhancedMem0Client(api_key=SETTINGS.mem0_api_key, endpoint=SETTINGS.mem0_endpoint)
                logger.info(f"Initialized Mem0 client with API key and endpoint: {SETTINGS.mem0_endpoint}")
            else:
                _mem0_client = EnhancedMem0Client(endpoint=SETTINGS.mem0_endpoint)
                logger.info(f"Initialized Mem0 client without API key at endpoint: {SETTINGS.mem0_endpoint}")
            
            # Ensure the Metis RAG agent exists
            if not _mem0_client.get_agent(METIS_AGENT_ID):
                _mem0_client.create_agent(
                    agent_id=METIS_AGENT_ID,
                    name="Metis RAG Agent",
                    persona=METIS_PERSONA
                )
                logger.info(f"Created Metis RAG agent with ID: {METIS_AGENT_ID}")
            
            logger.info(f"Initialized Mem0 client with endpoint: {SETTINGS.mem0_endpoint}")
        except Exception as e:
            logger.error(f"Error initializing Mem0 client: {str(e)}")
            return None
    
    return _mem0_client

# Keep the existing functions but update them to use the enhanced client
# (get_or_create_human, store_message, get_conversation_history, etc.)
```

### 2.2 Database Connection Pooling

Ensure proper connection pooling in `app/db/session.py`:

```python
"""
Database session management
"""
import logging
from typing import AsyncGenerator
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import declarative_base

from app.core.config import SETTINGS

logger = logging.getLogger("app.db.session")

# Create SQLAlchemy base
Base = declarative_base()

# Create async engine with connection pooling
engine = create_async_engine(
    SETTINGS.database_url,
    echo=SETTINGS.sql_echo,
    pool_size=SETTINGS.database_pool_size,
    max_overflow=SETTINGS.database_max_overflow,
    pool_pre_ping=True,  # Verify connections before using them
    pool_recycle=3600,   # Recycle connections after 1 hour
)

# Create async session factory
AsyncSessionLocal = async_sessionmaker(
    bind=engine,
    expire_on_commit=False,
    autoflush=False,
    autocommit=False,
)

async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Get database session
    
    Yields:
        AsyncSession: Database session
    """
    session = AsyncSessionLocal()
    try:
        yield session
    finally:
        await session.close()
```

## 3. Environment Configuration

### 3.1 Update .env.example

Update the `.env.example` file to include Mem0 API key:

```
# API settings
API_V1_STR=/api
PROJECT_NAME=Metis RAG

# Ollama settings
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=gemma3:12b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# LLM Judge settings
CHUNKING_JUDGE_MODEL=gemma3:12b
RETRIEVAL_JUDGE_MODEL=gemma3:12b
USE_CHUNKING_JUDGE=True
USE_RETRIEVAL_JUDGE=True

# LangGraph RAG Agent settings
LANGGRAPH_RAG_MODEL=gemma3:12b
USE_LANGGRAPH_RAG=True
USE_ENHANCED_LANGGRAPH_RAG=True

# Document settings
UPLOAD_DIR=./data/uploads
CHROMA_DB_DIR=./data/chroma_db
CHUNK_SIZE=1500
CHUNK_OVERLAP=150

# Database settings
DATABASE_TYPE=postgresql
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=metis_rag
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/metis_rag
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# Security settings
CORS_ORIGINS=*

# Mem0 settings
MEM0_ENDPOINT=http://localhost:8050
MEM0_API_KEY=your_mem0_api_key_here
USE_MEM0=True
```

### 3.2 Update .env.docker

Update the `.env.docker` file to include Mem0 API key:

```
# API settings
API_V1_STR=/api
PROJECT_NAME=Metis RAG

# Ollama settings
OLLAMA_BASE_URL=http://ollama:11434
DEFAULT_MODEL=gemma3:12b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# LLM Judge settings
CHUNKING_JUDGE_MODEL=gemma3:12b
RETRIEVAL_JUDGE_MODEL=gemma3:12b
USE_CHUNKING_JUDGE=True
USE_RETRIEVAL_JUDGE=True

# LangGraph RAG Agent settings
LANGGRAPH_RAG_MODEL=gemma3:12b
USE_LANGGRAPH_RAG=True
USE_ENHANCED_LANGGRAPH_RAG=True

# Document settings
UPLOAD_DIR=/app/data/uploads
CHROMA_DB_DIR=/app/data/chroma_db
CHUNK_SIZE=1500
CHUNK_OVERLAP=150

# Database settings
DATABASE_TYPE=postgresql
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_HOST=metis-postgres
DATABASE_PORT=5432
DATABASE_NAME=metis
DATABASE_URL=postgresql://postgres:postgres@metis-postgres:5432/metis
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# Security settings
CORS_ORIGINS=*

# Mem0 settings
MEM0_ENDPOINT=http://mem0:8050
MEM0_API_KEY=${MEM0_ADMIN_API_KEY:-default_dev_key}
USE_MEM0=True
```

## 4. Documentation Enhancement

### 4.1 Update README_MEM0_INTEGRATION.md

Update the `app/rag/README_MEM0_INTEGRATION.md` file:

```markdown
# Mem0 Integration for Metis RAG

This document describes how to set up and use the Mem0 integration for Metis RAG.

## Overview

Mem0 is a memory layer for AI applications that provides a way to store and retrieve information related to users, sessions, and documents. It enables more personalized and context-aware interactions in the Metis RAG system.

The integration provides the following features:

- Conversation history storage and retrieval
- User preferences storage and retrieval
- Document interaction tracking
- Enhanced context for RAG queries

## Why Docker for Mem0?

Running Mem0 in Docker is the recommended approach for the following reasons:

### Dependency Isolation
Docker containers encapsulate Mem0 and its dependencies (including PostgreSQL) in an isolated environment. This prevents conflicts with your project's Python environment and other system-level packages.

### Reproducibility
The Docker setup ensures that everyone on your team (and your deployment environment) is using the exact same Mem0 and PostgreSQL configuration. This eliminates "works on my machine" problems.

### Easy Setup
The provided docker-compose.yml file makes it incredibly easy to start and stop Mem0 and its database. You don't need to manually install and configure PostgreSQL.

### Cleanliness
When you're done with Mem0, you can simply stop and remove the containers, leaving your host system clean.

### Production-Like Environment
Using Docker for development brings your development environment closer to a production environment, which often uses containers.

## Setup

### 1. Start Mem0 with Docker Compose

The easiest way to run Mem0 is using Docker Compose:

```bash
docker-compose up -d
```

This will start the entire Metis RAG stack, including:
- Metis RAG application
- PostgreSQL database for Metis RAG
- PostgreSQL database for Mem0
- Mem0 server
- Ollama for LLM inference

The Mem0 server will be accessible at http://localhost:8050.

### 2. Configure Metis RAG

Update your `.env` file to include the Mem0 configuration:

```
# Mem0 settings
MEM0_ENDPOINT=http://localhost:8050
MEM0_API_KEY=your_mem0_api_key_here
USE_MEM0=True
```

For Docker deployment, the API key is set in the docker-compose.yml file and passed to the application as an environment variable.

### 3. Restart Metis RAG

Restart your Metis RAG application to apply the changes.

## Usage

The Mem0 integration is used automatically by the RAG engine when enabled. It provides the following functionality:

### Conversation History

Conversation history is stored in Mem0's recall memory. This allows the system to maintain context across sessions and provide more coherent responses.

```python
# Store a user message
store_message(human_id="user123", role="user", content="What is RAG?")

# Store an assistant message
store_message(human_id="user123", role="assistant", content="RAG stands for Retrieval-Augmented Generation...")

# Get conversation history
history = get_conversation_history(human_id="user123", limit=10)
```

### User Preferences

User preferences are stored in Mem0's archival memory. This allows the system to personalize responses based on user preferences, such as preferred models or response styles.

```python
# Store user preferences
store_user_preferences(human_id="user123", preferences={
    "preferred_model": "gemma3:12b",
    "response_style": "concise",
    "language": "en"
})

# Get user preferences
preferences = get_user_preferences(human_id="user123")
```

### Document Interactions

Document interactions are stored in Mem0's archival memory. This allows the system to track which documents a user has interacted with and provide more relevant responses.

```python
# Store document interaction
store_document_interaction(
    human_id="user123",
    document_id="doc456",
    interaction_type="view",
    data={"timestamp": "2023-01-01T00:00:00Z"}
)

# Get document interactions
interactions = get_document_interactions(
    human_id="user123",
    document_id="doc456",
    interaction_type="view",
    limit=10
)
```

## Performance Considerations

The Mem0 integration includes several performance optimizations:

### Caching

The Mem0 client includes caching for frequently accessed data, reducing the number of API calls to the Mem0 server.

```python
# The EnhancedMem0Client automatically caches:
# - Human and agent information
# - Recall memory (conversation history)
# - Archival memory (user preferences, document interactions)
# - Search results

# You can manually clear the cache if needed:
client = get_mem0_client()
client.clear_cache()  # Clear all caches
client.clear_cache("human")  # Clear only human cache
client.clear_cache("recall")  # Clear only recall cache
```

### Connection Pooling

The PostgreSQL connection uses connection pooling to efficiently manage database connections.

### Batch Operations

Document processing uses batch operations to efficiently process large numbers of documents.

## API

The Mem0 integration provides the following API:

### `get_mem0_client()`

Get the Mem0 client instance.

### `store_message(human_id, role, content)`

Store a message in recall memory.

### `get_conversation_history(human_id, limit=10)`

Get conversation history from recall memory.

### `store_user_preferences(human_id, preferences)`

Store user preferences in archival memory.

### `get_user_preferences(human_id)`

Get user preferences from archival memory.

### `store_document_interaction(human_id, document_id, interaction_type, data)`

Store document interaction in archival memory.

### `get_document_interactions(human_id, document_id=None, interaction_type=None, limit=10)`

Get document interactions from archival memory.

## Troubleshooting

If you encounter issues with the Mem0 integration, check the following:

1. Make sure the Mem0 server is running and accessible at the configured endpoint.
2. Check the logs for any error messages related to Mem0.
3. Make sure the `USE_MEM0` setting is set to `True` in your `.env` file.
4. If you're using an API key, make sure it's correctly configured.
5. For Docker deployments, check that the Mem0 container is healthy using `docker ps`.
6. Check the Mem0 logs using `docker logs metis-mem0`.

## References

- [Mem0 Documentation](https://docs.mem0.ai)
- [Mem0 GitHub Repository](https://github.com/mem0ai/mem0)
```

## 5. Testing Plan

### 5.1 Unit Tests

Create unit tests for the Mem0 client:

```python
"""
Unit tests for Mem0 client
"""
import pytest
from unittest.mock import patch, MagicMock
from app.rag.mem0_client import get_mem0_client, store_message, get_conversation_history

@pytest.fixture
def mock_mem0_client():
    """
    Mock Mem0 client
    """
    with patch("app.rag.mem0_client.EnhancedMem0Client") as mock_client:
        mock_instance = MagicMock()
        mock_client.return_value = mock_instance
        yield mock_instance

@pytest.fixture
def mock_settings():
    """
    Mock settings
    """
    with patch("app.rag.mem0_client.SETTINGS") as mock_settings:
        mock_settings.use_mem0 = True
        mock_settings.mem0_endpoint = "http://localhost:8050"
        mock_settings.mem0_api_key = "test_key"
        yield mock_settings

def test_get_mem0_client(mock_mem0_client, mock_settings):
    """
    Test get_mem0_client
    """
    # Reset singleton
    import app.rag.mem0_client
    app.rag.mem0_client._mem0_client = None
    
    # Mock MEM0_AVAILABLE
    with patch("app.rag.mem0_client.MEM0_AVAILABLE", True):
        client = get_mem0_client()
        assert client is not None
        assert client == mock_mem0_client

def test_store_message(mock_mem0_client, mock_settings):
    """
    Test store_message
    """
    # Reset singleton
    import app.rag.mem0_client
    app.rag.mem0_client._mem0_client = None
    
    # Mock MEM0_AVAILABLE and get_or_create_human
    with patch("app.rag.mem0_client.MEM0_AVAILABLE", True):
        with patch("app.rag.mem0_client.get_or_create_human", return_value=True):
            result = store_message("user123", "user", "Hello")
            assert result is True
            mock_mem0_client.append_message.assert_called_once()

def test_get_conversation_history(mock_mem0_client, mock_settings):
    """
    Test get_conversation_history
    """
    # Reset singleton
    import app.rag.mem0_client
    app.rag.mem0_client._mem0_client = None
    
    # Mock MEM0_AVAILABLE
    with patch("app.rag.mem0_client.MEM0_AVAILABLE", True):
        mock_mem0_client.get_recall_memory.return_value = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there"}
        ]
        
        history = get_conversation_history("user123")
        assert len(history) == 2
        assert history[0]["role"] == "user"
        assert history[1]["role"] == "assistant"
        mock_mem0_client.get_recall_memory.assert_called_once()

def test_clear_cache(mock_mem0_client, mock_settings):
    """
    Test clear_cache
    """
    # Reset singleton
    import app.rag.mem0_client
    app.rag.mem0_client._mem0_client = None
    
    # Mock MEM0_AVAILABLE
    with patch("app.rag.mem0_client.MEM0_AVAILABLE", True):
        client = get_mem0_client()
        
        # Test clearing all caches
        client.clear_cache()
        mock_mem0_client.human_cache.clear.assert_called_once()
        mock_mem0_client.recall_cache.clear.assert_called_once()
        mock_mem0_client.archival_cache.clear.assert_called_once()
        mock_mem0_client.search_cache.clear.assert_called_once()
        
        # Reset mock
        mock_mem0_client.reset_mock()
        
        # Test clearing specific cache
        client.clear_cache("human")
        mock_mem0_client.human_cache.clear.assert_called_once()
        mock_mem0_client.recall_cache.clear.assert_not_called()
        mock_mem0_client.archival_cache.clear.assert_not_called()
        mock_mem0_client.search_cache.clear.assert_not_called()
```

## 6. Prioritized Implementation Steps

Based on your feedback, here are the prioritized implementation steps:

1. **Implement the Mem0Client changes**
   - Update `mem0_client.py` to include caching logic
   - Add the `clear_cache` method
   - Implement the `get_or_create_human` method

2. **Integrate with RAGEngine**
   - Update the `RAGEngine.query` method to use the Mem0Client for storing/retrieving conversation history, user preferences, and document interactions
   - Incorporate user preferences and document interaction data into the retrieval process

3. **Update API Endpoints**
   - Modify the `app/api/chat.py` endpoints to use the updated RAGEngine
   - Pass the user_id to the RAGEngine.query method
   - Add a new endpoint for retrieving conversation history

4. **Update Docker Configuration**
   - Use the provided, improved docker-compose.yml file
   - Create the .dockerignore file
   - Update environment configuration files

5. **Write Tests**
   - Write unit tests for the Mem0Client
   - Write integration tests for the Mem0 integration
   - Write performance tests for the Mem0 integration

6. **Update Documentation**
   - Update README_MEM0_INTEGRATION.md with clear instructions
   - Add code examples for using the Mem0 integration

7. **Deploy and Test**
   - Deploy the updated Docker configuration
   - Run tests to verify functionality and performance
   - Monitor system performance and make adjustments as needed

## 7. Monitoring and Maintenance

1. **Monitor Mem0 Performance**
   - Track API call latency
   - Monitor cache hit rates
   - Adjust cache TTLs based on usage patterns

2. **Monitor PostgreSQL Performance**
   - Track query performance
   - Monitor connection pool usage
   - Adjust pool size and max overflow based on usage patterns

3. **Regular Maintenance**
   - Update Mem0 and PostgreSQL versions
   - Backup Mem0 data
   - Clean up old data to prevent database growth

================
File: docs/implementation/mem0_integration_plan.md
================
# Mem0 Integration Plan for Metis_RAG

## Overview
This document outlines the plan for integrating mem0, a memory layer for AI applications, into the Metis_RAG system. The integration will enhance Metis_RAG with persistent memory capabilities, enabling more personalized and context-aware responses.

## Architecture Diagram
```mermaid
graph TD
    A[Metis_RAG] --> B[mem0 Integration]
    B --> C1[User Memory]
    B --> C2[Session Memory]
    B --> C3[Document Memory]
    C1 --> D1[User Preferences]
    C1 --> D2[Conversation History]
    C2 --> D3[Current Session Context]
    C3 --> D4[Document Interactions]
    D1 --> E1[Personalized Responses]
    D2 --> E2[Contextual Continuity]
    D3 --> E3[Real-time Adaptation]
    D4 --> E4[Improved Document Relevance]
```

## Integration Steps

### 1. Memory Layer Addition
- Add mem0 as a memory layer between the RAG system and the user interface
- Implement three memory types:
  * User Memory: Store long-term user preferences and conversation history
  * Session Memory: Maintain context for the current conversation
  * Document Memory: Track user interactions with specific documents

### 2. API Integration
- Add mem0 API calls in the FastAPI backend
- Modify the chat endpoint to:
  * Retrieve relevant memories before generating responses
  * Store new memories after each interaction

### 3. Document Processing Enhancement
- Integrate mem0 with LangChain document processing
- Store document-specific memories (e.g., frequently referenced sections)

### 4. User Interface Updates
- Add memory management controls
- Show memory-related information in the chat interface

### 5. Testing and Validation
- Add tests for memory integration
- Verify memory persistence across sessions
- Ensure proper memory retrieval and usage

## Implementation Timeline
1. Week 1: Memory layer setup and API integration
2. Week 2: Document processing enhancement
3. Week 3: User interface updates
4. Week 4: Testing and validation

## Dependencies
- mem0 Python package
- Additional memory storage capacity
- Testing infrastructure for memory-related features

## Risks and Mitigation
- **Risk**: Increased latency due to memory operations
  - **Mitigation**: Implement caching and optimize memory queries
- **Risk**: Memory storage growth
  - **Mitigation**: Implement memory pruning and archiving strategies

================
File: docs/implementation/Metis_RAG_Access_Control_Implementation_Plan.md
================
# Metis RAG Access Control Implementation Plan

## Overview

This document outlines the implementation plan for enhancing Metis RAG's access control, user roles, and permission management capabilities. The goal is to create a secure, flexible, and collaborative system that respects data boundaries while enabling appropriate sharing of knowledge across teams and organizations.

## Current State Analysis

Based on the analysis of the codebase, the following observations were made:

1. **Authentication System**: The system has JWT-based authentication with basic user/admin roles.
2. **Resource Ownership**: Documents and conversations are linked to users via a `user_id` field.
3. **Data Access Gaps**:
   - Vector Store queries don't consistently filter by user ownership
   - No mechanism for document sharing between users
   - Limited role granularity (only user/admin)
   - Tools lack user-specific access controls
   - No team or organization structure for group permissions

## Implementation Plan

### 1. Data Model Enhancements

#### 1.1 Document Sharing Structure

Create new database tables to support document sharing:

```sql
CREATE TABLE document_sharing (
    id UUID PRIMARY KEY,
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    shared_with_user_id UUID REFERENCES users(id) NULL,
    shared_with_team_id UUID REFERENCES teams(id) NULL,
    shared_with_organization_id UUID REFERENCES organizations(id) NULL,
    permission_level VARCHAR NOT NULL, -- 'read', 'write', 'admin'
    created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW(),
    created_by UUID REFERENCES users(id),
    CHECK (
        (shared_with_user_id IS NOT NULL AND shared_with_team_id IS NULL AND shared_with_organization_id IS NULL) OR
        (shared_with_user_id IS NULL AND shared_with_team_id IS NOT NULL AND shared_with_organization_id IS NULL) OR
        (shared_with_user_id IS NULL AND shared_with_team_id IS NULL AND shared_with_organization_id IS NOT NULL)
    )
);

CREATE INDEX ix_document_sharing_document_id ON document_sharing(document_id);
CREATE INDEX ix_document_sharing_user_id ON document_sharing(shared_with_user_id);
CREATE INDEX ix_document_sharing_team_id ON document_sharing(shared_with_team_id);
CREATE INDEX ix_document_sharing_organization_id ON document_sharing(shared_with_organization_id);
```

#### 1.2 Organization and Team Structure

Create tables for organization and team management:

```sql
CREATE TABLE organizations (
    id UUID PRIMARY KEY,
    name VARCHAR NOT NULL,
    description TEXT,
    created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW(),
    created_by UUID REFERENCES users(id)
);

CREATE TABLE teams (
    id UUID PRIMARY KEY,
    name VARCHAR NOT NULL,
    description TEXT,
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW(),
    created_by UUID REFERENCES users(id)
);

CREATE TABLE user_organizations (
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    role VARCHAR NOT NULL, -- 'member', 'admin'
    joined_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (user_id, organization_id)
);

CREATE TABLE user_teams (
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    team_id UUID REFERENCES teams(id) ON DELETE CASCADE,
    role VARCHAR NOT NULL, -- 'member', 'admin'
    joined_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (user_id, team_id)
);
```

#### 1.3 Enhanced User Roles

Update the `users` table to support more granular roles:

```sql
ALTER TABLE users ADD COLUMN role VARCHAR NOT NULL DEFAULT 'user';
-- Possible roles: 'user', 'power_user', 'team_admin', 'org_admin', 'super_admin', 'read_only'
```

#### 1.4 Document Visibility Field

Add a visibility field to documents:

```sql
ALTER TABLE documents ADD COLUMN visibility VARCHAR NOT NULL DEFAULT 'private';
-- Possible values: 'private', 'shared', 'team', 'organization', 'public'
```

### 2. Backend Implementation

#### 2.1 Update RAG Engine for Access Control

Modify the RAG engine to include access control in vector store searches:

```python
async def search(self, query: str, user_id: str, top_k: int = 10, **kwargs):
    """
    Search for documents with access control
    """
    # Get document IDs the user can access
    accessible_docs = await self._get_accessible_documents(user_id)
    
    # Add filter to only retrieve from accessible documents
    filter_criteria = kwargs.get("filter_criteria", {})
    filter_criteria["document_id"] = {"$in": accessible_docs}
    
    # Perform the search with access filters
    search_results = await self.vector_store.search(
        query=query,
        top_k=top_k,
        filter_criteria=filter_criteria
    )
    
    return search_results

async def _get_accessible_documents(self, user_id: str) -> List[str]:
    """
    Get documents that the user can access:
    - Documents owned by the user
    - Documents shared with the user
    - Documents shared with user's teams
    - Documents shared with user's organization
    - Public documents
    """
    # Implementation details here
```

#### 2.2 Repository Access Control Layer

Create a base access control service for repositories:

```python
class AccessControlService:
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def check_document_access(self, user_id: str, document_id: str, 
                                     required_permission: str = 'read') -> bool:
        """
        Check if a user has access to a document
        
        Args:
            user_id: User ID
            document_id: Document ID
            required_permission: Required permission level ('read', 'write', 'admin')
            
        Returns:
            True if user has access, False otherwise
        """
        # Check if user owns the document
        stmt = select(Document).where(Document.id == document_id, Document.user_id == user_id)
        result = await self.session.execute(stmt)
        if result.scalar_one_or_none():
            return True
            
        # Check if document is public
        stmt = select(Document).where(
            Document.id == document_id,
            Document.visibility == 'public'
        )
        result = await self.session.execute(stmt)
        if result.scalar_one_or_none():
            return True
            
        # Check individual sharing
        stmt = select(DocumentSharing).where(
            DocumentSharing.document_id == document_id,
            DocumentSharing.shared_with_user_id == user_id,
            self._permission_check(DocumentSharing.permission_level, required_permission)
        )
        result = await self.session.execute(stmt)
        if result.scalar_one_or_none():
            return True
            
        # Check team sharing
        # Get user's teams
        user_teams_stmt = select(UserTeam.team_id).where(UserTeam.user_id == user_id)
        user_teams = await self.session.execute(user_teams_stmt)
        team_ids = [team_id for (team_id,) in user_teams]
        
        if team_ids:
            stmt = select(DocumentSharing).where(
                DocumentSharing.document_id == document_id,
                DocumentSharing.shared_with_team_id.in_(team_ids),
                self._permission_check(DocumentSharing.permission_level, required_permission)
            )
            result = await self.session.execute(stmt)
            if result.scalar_one_or_none():
                return True
                
        # Check organization sharing
        # Get user's organizations
        user_orgs_stmt = select(UserOrganization.organization_id).where(UserOrganization.user_id == user_id)
        user_orgs = await self.session.execute(user_orgs_stmt)
        org_ids = [org_id for (org_id,) in user_orgs]
        
        if org_ids:
            stmt = select(DocumentSharing).where(
                DocumentSharing.document_id == document_id,
                DocumentSharing.shared_with_organization_id.in_(org_ids),
                self._permission_check(DocumentSharing.permission_level, required_permission)
            )
            result = await self.session.execute(stmt)
            if result.scalar_one_or_none():
                return True
                
        return False
        
    def _permission_check(self, permission_level, required_permission):
        """Map permission hierarchy for comparison"""
        permission_map = {
            'read': ['read', 'write', 'admin'],
            'write': ['write', 'admin'],
            'admin': ['admin']
        }
        return permission_level.in_(permission_map[required_permission])
```

#### 2.3 Tool Registry with Permission Checks

Update the Tool Registry to check permissions:

```python
class ToolRegistry:
    # ...existing code...
    
    async def execute_tool(self, tool_name: str, input_data: Dict[str, Any], 
                           user_id: str = None) -> Dict[str, Any]:
        """
        Execute a tool with permission checks
        
        Args:
            tool_name: Name of the tool to execute
            input_data: Tool input data
            user_id: User ID for permission checking
            
        Returns:
            Tool execution result
        """
        # Get the tool
        tool = self.get_tool(tool_name)
        if not tool:
            return {"error": f"Tool not found: {tool_name}"}
            
        # Check permissions if user_id provided
        if user_id:
            can_use_tool = await self._check_tool_permission(tool_name, user_id)
            if not can_use_tool:
                return {"error": f"Permission denied for tool: {tool_name}"}
                
        # Execute the tool
        return await tool.execute(input_data)
        
    async def _check_tool_permission(self, tool_name: str, user_id: str) -> bool:
        """
        Check if a user can use a tool
        
        Args:
            tool_name: Tool name
            user_id: User ID
            
        Returns:
            True if permitted, False otherwise
        """
        # Get user role
        session = await get_db_session()
        try:
            stmt = select(User.role).where(User.id == user_id)
            result = await session.execute(stmt)
            role = result.scalar_one_or_none()
            
            # Tool permission mapping
            tool_permissions = {
                # Basic tools available to all users
                'rag': ['user', 'power_user', 'team_admin', 'org_admin', 'super_admin'],
                
                # Advanced tools with restricted access
                'database': ['power_user', 'team_admin', 'org_admin', 'super_admin'],
                'calculator': ['user', 'power_user', 'team_admin', 'org_admin', 'super_admin'],
                
                # Admin-only tools
                'system': ['org_admin', 'super_admin']
            }
            
            # Check if role has permission to use the tool
            return role in tool_permissions.get(tool_name, ['super_admin'])
        finally:
            await session.close()
```

#### 2.4 Query Planner with Permission Awareness

Update the Query Planner to incorporate permissions:

```python
class QueryPlanner:
    # ...existing code...
    
    async def create_plan(self, query_id: str, query: str, user_id: str) -> QueryPlan:
        """
        Create a plan for executing a query
        
        Args:
            query_id: Unique query ID
            query: Query string
            user_id: User ID for permission checking
            
        Returns:
            QueryPlan instance
        """
        self.logger.info(f"Creating plan for query: {query}")
        
        # Store user ID for permission checking during execution
        self.current_user_id = user_id
        
        # Analyze the query
        analysis = await self.query_analyzer.analyze(query)
        
        # Determine if the query is simple or complex
        complexity = analysis.get("complexity", "simple")
        required_tools = analysis.get("requires_tools", [])
        sub_queries = analysis.get("sub_queries", [])
        
        # Create plan steps
        steps = []
        
        # Filter required tools based on user permissions
        permitted_tools = await self._filter_permitted_tools(required_tools, user_id)
        
        # Create steps with user context
        if complexity == "simple":
            # Simple query - just use RAG
            steps.append({
                "type": "tool",
                "tool": "rag",
                "input": {
                    "query": query,
                    "top_k": 5,
                    "user_id": user_id  # Include user ID for access control
                },
                "description": "Retrieve information using RAG"
            })
        else:
            # Complex query - may require multiple steps
            
            # First, add steps for any required tools
            for tool_name in permitted_tools:
                tool = self.tool_registry.get_tool(tool_name)
                if not tool:
                    self.logger.warning(f"Required tool not found: {tool_name}")
                    continue
                
                # Create a step for this tool with user context
                tool_input = self._create_tool_input(tool_name, query)
                tool_input["user_id"] = user_id  # Add user ID for access control
                steps.append({
                    "type": "tool",
                    "tool": tool_name,
                    "input": tool_input,
                    "description": f"Execute {tool_name} tool"
                })
            
            # Handle sub-queries with user context
            for sub_query in sub_queries:
                steps.append({
                    "type": "tool",
                    "tool": "rag",
                    "input": {
                        "query": sub_query,
                        "top_k": 3,
                        "user_id": user_id  # Include user ID for access control
                    },
                    "description": f"Retrieve information for sub-query: {sub_query}"
                })
            
            # Add a final step to synthesize the results
            steps.append({
                "type": "synthesize",
                "description": "Synthesize results from previous steps"
            })
        
        # Create the plan
        plan = QueryPlan(
            query_id=query_id,
            query=query,
            steps=steps,
            user_id=user_id  # Store user ID in the plan
        )
        
        self.logger.info(f"Created plan with {len(steps)} steps for query: {query}")
        return plan
        
    async def _filter_permitted_tools(self, tools: List[str], user_id: str) -> List[str]:
        """Filter tool list based on user permissions"""
        permitted_tools = []
        for tool_name in tools:
            can_use = await self.tool_registry._check_tool_permission(tool_name, user_id)
            if can_use:
                permitted_tools.append(tool_name)
            else:
                self.logger.warning(f"User {user_id} doesn't have permission to use tool: {tool_name}")
        return permitted_tools
```

### 3. Frontend Implementation

#### 3.1 Document Management Page Enhancements

Create a dedicated document management interface with the following features:

1. Document list with visibility indicators
2. Sharing controls for each document
3. Filter options for owned vs. shared documents
4. Batch operations for sharing with teams/organizations

#### 3.2 Sharing Interface

Design a new sharing modal/panel that appears when clicking a "Share" button:

1. User search functionality
2. Team/Organization selection
3. Permission level selection (read/write/admin)
4. Current sharing status display
5. Option to make document public

#### 3.3 Permission Management Pages

Create new administrative pages for permission management:

1. User management page for admins
2. Team creation and management page
3. Organization structure management
4. Role assignment interface
5. Bulk permission operations

#### 3.4 Navigation and Information Architecture

Update the application's navigation and information architecture:

1. Separate "My Documents" and "Shared With Me" sections
2. Add "Teams" and "Organization" sections in the sidebar
3. Create an "Admin" section for permission management
4. Add document ownership and sharing indicators throughout the UI

## Implementation Checklist

### Phase 1: Data Model and Core Backend

- [ ] **Database Schema Updates**
  - [ ] Create migration for `document_sharing` table
  - [ ] Create migration for `organizations` table
  - [ ] Create migration for `teams` table
  - [ ] Create migration for `user_organizations` table
  - [ ] Create migration for `user_teams` table
  - [ ] Add `role` column to `users` table
  - [ ] Add `visibility` column to `documents` table

- [ ] **Core Access Control Service**
  - [ ] Implement `AccessControlService` class
  - [ ] Create document access permission logic
  - [ ] Add organization & team membership checks
  - [ ] Create permission validation utilities

- [ ] **Repository Layer Updates**
  - [ ] Update `DocumentRepository` to use access control
  - [ ] Update other repositories to check permissions
  - [ ] Add sharing management methods
  - [ ] Create tests for access control logic

### Phase 2: RAG and Tool Integration

- [ ] **RAG Engine Modifications**
  - [ ] Update vector search to filter by access permissions
  - [ ] Add user context to all search operations
  - [ ] Implement document visibility filtering
  - [ ] Update query processing for permission checks

- [ ] **Tool Registry Enhancements**
  - [ ] Add permission checks to tool execution
  - [ ] Create role-based permission mappings
  - [ ] Update tool input schemas to include user context
  - [ ] Create tests for tool permission controls

- [ ] **Query Planner Updates**
  - [ ] Add user context to query plans
  - [ ] Filter tools based on user permissions
  - [ ] Enhance plan execution with permission checks
  - [ ] Create tests for permission-aware query planning

### Phase 3: Frontend Implementation

- [ ] **Document Management UI**
  - [ ] Design document list with sharing indicators
  - [ ] Implement filter tabs for different document types
  - [ ] Create document card components with ownership info
  - [ ] Add multi-select and batch operations

- [ ] **Sharing Interface**
  - [ ] Design sharing modal component
  - [ ] Implement user search functionality
  - [ ] Create team/organization selector
  - [ ] Implement permission level controls
  - [ ] Create sharing status display

- [ ] **Administrative Pages**
  - [ ] Design user management interface
  - [ ] Create team management pages
  - [ ] Implement organization structure UI
  - [ ] Design role assignment interface
  - [ ] Create permission audit reports

- [ ] **Navigation and Information Architecture**
  - [ ] Update sidebar navigation
  - [ ] Create document ownership badges
  - [ ] Implement sharing status indicators
  - [ ] Add permission-based conditional rendering

### Phase 4: Testing and Deployment

- [ ] **Unit and Integration Testing**
  - [ ] Create test cases for access control logic
  - [ ] Test document sharing functionality
  - [ ] Verify permission-based filtering
  - [ ] Test all UI components

- [ ] **End-to-End Testing**
  - [ ] Create test scenarios for different user roles
  - [ ] Test sharing workflows
  - [ ] Verify permission inheritance
  - [ ] Test edge cases and error handling

- [ ] **Documentation**
  - [ ] Update API documentation
  - [ ] Create user guide for sharing features
  - [ ] Document administrative workflows
  - [ ] Create developer documentation for permission system

- [ ] **Deployment**
  - [ ] Create database migration scripts
  - [ ] Plan data backups before migration
  - [ ] Schedule deployment window
  - [ ] Prepare rollback plan

## Frontend Design Mockups

### Document Management Page

```
+----------------------------------------------+
| Documents                                    |
+----------------------------------------------+
| [My Documents] [Shared With Me] [Team Docs]  |
+----------------------------------------------+
| [+ Upload] [+ New Folder] [Search...]        |
+----------------------------------------------+
| Filters: [All ▼] [Recent ▼] [Tags ▼]         |
+----------------------------------------------+
| ┌────────────────────────────────────────┐   |
| │ 📄 Document_1.pdf                       │   |
| │ [Private] Last modified: 2 days ago     │   |
| │ [View] [Share] [Edit] [Delete]          │   |
| └────────────────────────────────────────┘   |
| ┌────────────────────────────────────────┐   |
| │ 📄 Document_2.docx                      │   |
| │ [Shared with 3 users] Modified: Today   │   |
| │ [View] [Share] [Edit] [Delete]          │   |
| └────────────────────────────────────────┘   |
| ┌────────────────────────────────────────┐   |
| │ 📄 Team_Report.pdf                      │   |
| │ [Shared with Marketing Team] Yesterday  │   |
| │ [View] [Share] [Edit] [Delete]          │   |
| └────────────────────────────────────────┘   |
+----------------------------------------------+
```

### Sharing Modal

```
+----------------------------------------------+
| Share "Document_1.pdf"                     X |
+----------------------------------------------+
| Current visibility: Private                  |
+----------------------------------------------+
| Change visibility:                           |
| (•) Private  ( ) Team  ( ) Organization      |
| ( ) Public                                   |
+----------------------------------------------+
| Share with specific users:                   |
| [Search users...] [Add ⊕]                    |
+----------------------------------------------+
| Currently shared with:                       |
|                                              |
| No users                                     |
|                                              |
+----------------------------------------------+
| Share with teams:                            |
| [Select team ▼] [Permission ▼] [Add ⊕]       |
+----------------------------------------------+
| Currently shared with:                       |
|                                              |
| No teams                                     |
|                                              |
+----------------------------------------------+
| [Generate sharing link]  [Save Changes]      |
+----------------------------------------------+
```

### Team Management Page

```
+----------------------------------------------+
| Team Management                              |
+----------------------------------------------+
| [+ Create New Team]  [Search teams...]       |
+----------------------------------------------+
| ┌────────────────────────────────────────┐   |
| │ Marketing Team                          │   |
| │ 15 members · 28 documents               │   |
| │ [View Members] [View Documents] [Edit]  │   |
| └────────────────────────────────────────┘   |
| ┌────────────────────────────────────────┐   |
| │ Engineering                             │   |
| │ 8 members · 42 documents                │   |
| │ [View Members] [View Documents] [Edit]  │   |
| └────────────────────────────────────────┘   |
| ┌────────────────────────────────────────┐   |
| │ Executive                               │   |
| │ 3 members · 16 documents                │   |
| │ [View Members] [View Documents] [Edit]  │   |
| └────────────────────────────────────────┘   |
+----------------------------------------------+
```

## Conclusion

This implementation plan provides a comprehensive approach to enhancing Metis RAG with robust access control, document sharing, and permission management capabilities. By implementing these features, we'll create a more collaborative and secure environment that enables teams to share knowledge effectively while maintaining appropriate data boundaries.

The plan is structured into logical phases to enable incremental implementation and testing. Each phase builds on the previous one, ensuring a smooth progression from data model changes to backend implementation and finally to frontend enhancements.

================
File: docs/implementation/metis_rag_agentic_enhancement_plan.md
================
# Metis_RAG Agentic Enhancement Plan

## 1. Overview

This document outlines a plan to enhance Metis_RAG with agentic capabilities, allowing it to not just retrieve information but also analyze it, reason about it, and provide the best possible responses to users through active planning and tool use.

## 2. What Does "Agentic" Mean for RAG?

An agentic RAG system goes beyond simple retrieval and response generation. It:

- Actively reasons about the user's query
- Determines what information is needed
- Formulates a plan to gather that information
- Executes the plan (potentially using tools or taking multiple steps)
- Analyzes the gathered information
- Synthesizes a comprehensive response

It's more like an autonomous assistant that can take initiative rather than a passive question-answering system.

## 3. Proposed Architecture

```mermaid
graph TD
    Query[User Query] --> QueryAnalyzer[Query Analyzer]
    QueryAnalyzer --> QueryType{Query Type}
    QueryType -->|Simple| DirectRAG[Standard RAG Pipeline]
    QueryType -->|Complex| QueryPlanner[Query Planner]
    
    QueryPlanner --> Plan[Execution Plan]
    Plan --> Executor[Plan Executor]
    
    Executor --> RAGTool[RAG Tool]
    Executor --> CalculatorTool[Calculator Tool]
    Executor --> CodeTool[Code Execution Tool]
    Executor --> SearchTool[Search Tool]
    Executor --> DatabaseTool[Database Tool]
    
    RAGTool --> Results[Intermediate Results]
    CalculatorTool --> Results
    CodeTool --> Results
    SearchTool --> Results
    DatabaseTool --> Results
    DirectRAG --> Results
    
    Results --> ResponseSynthesizer[Response Synthesizer]
    ResponseSynthesizer --> DraftResponse[Draft Response]
    
    DraftResponse --> ResponseEvaluator[Response Evaluator]
    ResponseEvaluator --> QualityCheck{Quality Check}
    
    QualityCheck -->|Pass| FinalResponse[Final Response]
    QualityCheck -->|Fail| ResponseRefiner[Response Refiner]
    ResponseRefiner --> DraftResponse
    
    Query --> ProcessLogger[Process Logger]
    QueryAnalyzer --> ProcessLogger
    QueryPlanner --> ProcessLogger
    Executor --> ProcessLogger
    Results --> ProcessLogger
    ResponseSynthesizer --> ProcessLogger
    ResponseEvaluator --> ProcessLogger
    ResponseRefiner --> ProcessLogger
    FinalResponse --> ProcessLogger
    
    ProcessLogger --> AuditReport[Audit Report]
```

This architecture combines three key approaches:
1. **Query Planning and Decomposition** as the first step
2. **Tool-using RAG Agent** as the core framework
3. **Self-critique and Refinement** as the final step

Additionally, it includes a comprehensive **Process Logger** that records every step of the process to generate an audit report that guarantees the response is not hallucinated.

## 4. Key Components

### 4.1 Query Analyzer

The Query Analyzer determines if a query is simple (can be answered directly with RAG) or complex (requires planning and potentially tools).

```python
class QueryAnalyzer:
    """
    Analyzes queries to determine their complexity and requirements
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        
    async def analyze(self, query: str) -> Dict[str, Any]:
        """
        Analyze a query to determine its complexity and requirements
        
        Returns:
            Dict with keys:
            - complexity: simple or complex
            - requires_tools: list of required tools
            - sub_queries: list of potential sub-queries
            - reasoning: explanation of the analysis
        """
        prompt = self._create_analysis_prompt(query)
        response = await self.llm_provider.generate(prompt=prompt)
        return self._parse_analysis(response.get("response", ""))
        
    def _create_analysis_prompt(self, query: str) -> str:
        return f"""Analyze the following query to determine its complexity and requirements:

Query: {query}

Consider:
1. Is this a simple factual question or a complex analytical query?
2. Does it require calculations, code execution, or external information?
3. Can it be broken down into simpler sub-queries?
4. What tools might be needed to answer it fully?

Output your analysis in JSON format:
{{
    "complexity": "simple" or "complex",
    "requires_tools": ["calculator", "code_execution", "search", etc.],
    "sub_queries": ["sub-query 1", "sub-query 2", etc.],
    "reasoning": "Explanation of your analysis"
}}
"""
```

### 4.2 Query Planner

For complex queries, the Query Planner creates a plan that may involve multiple steps and tools.

```python
class QueryPlanner:
    """
    Creates execution plans for complex queries
    """
    def __init__(self, llm_provider, tool_registry):
        self.llm_provider = llm_provider
        self.tool_registry = tool_registry
        
    async def create_plan(self, query: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Create an execution plan for a complex query
        
        Returns:
            Dict with keys:
            - steps: list of execution steps
            - tools: list of required tools
            - reasoning: explanation of the plan
        """
        available_tools = self.tool_registry.list_tools()
        prompt = self._create_planning_prompt(query, analysis, available_tools)
        response = await self.llm_provider.generate(prompt=prompt)
        return self._parse_plan(response.get("response", ""))
        
    def _create_planning_prompt(self, query: str, analysis: Dict[str, Any], available_tools: List[str]) -> str:
        return f"""Create an execution plan for the following query:

Query: {query}

Analysis:
{json.dumps(analysis, indent=2)}

Available Tools:
{json.dumps(available_tools, indent=2)}

Create a step-by-step plan to answer this query. Each step should specify:
1. The action to take (e.g., retrieve information, calculate, execute code)
2. The tool to use (if any)
3. The input to the tool
4. How to use the output of this step

Output your plan in JSON format:
{{
    "steps": [
        {{
            "step_id": 1,
            "action": "action description",
            "tool": "tool name or null",
            "input": "input to the tool",
            "output_usage": "how to use the output"
        }},
        ...
    ],
    "reasoning": "Explanation of your plan"
}}
"""
```

### 4.3 Tool Registry and Tool Interface

The Tool Registry manages available tools, while the Tool interface defines the contract for tool implementations.

```python
class Tool(ABC):
    """
    Abstract base class for tools
    """
    @abstractmethod
    async def execute(self, input_data: Any) -> Any:
        """
        Execute the tool with the given input
        
        Args:
            input_data: Tool-specific input
            
        Returns:
            Tool-specific output
        """
        pass
        
    @abstractmethod
    def get_description(self) -> str:
        """
        Get a description of the tool
        
        Returns:
            Tool description
        """
        pass
        
    @abstractmethod
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the tool
        
        Returns:
            JSON Schema for tool input
        """
        pass
        
    @abstractmethod
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the tool
        
        Returns:
            JSON Schema for tool output
        """
        pass

class ToolRegistry:
    """
    Registry for available tools
    """
    def __init__(self):
        self.tools: Dict[str, Tool] = {}
        
    def register_tool(self, name: str, tool: Tool) -> None:
        """
        Register a tool
        
        Args:
            name: Tool name
            tool: Tool implementation
        """
        self.tools[name] = tool
        
    def get_tool(self, name: str) -> Optional[Tool]:
        """
        Get a tool by name
        
        Args:
            name: Tool name
            
        Returns:
            Tool implementation or None if not found
        """
        return self.tools.get(name)
        
    def list_tools(self) -> List[Dict[str, Any]]:
        """
        List all available tools
        
        Returns:
            List of tool information dictionaries
        """
        return [
            {
                "name": name,
                "description": tool.get_description(),
                "input_schema": tool.get_input_schema(),
                "output_schema": tool.get_output_schema()
            }
            for name, tool in self.tools.items()
        ]
```

### 4.4 Plan Executor

The Plan Executor executes the plan, using various tools as needed.

```python
class PlanExecutor:
    """
    Executes query plans
    """
    def __init__(self, tool_registry: ToolRegistry):
        self.tool_registry = tool_registry
        
    async def execute_plan(self, plan: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        Execute a query plan
        
        Args:
            plan: Query plan
            query: Original query
            
        Returns:
            Dict with execution results
        """
        results = {}
        
        for step in plan["steps"]:
            step_id = step["step_id"]
            tool_name = step.get("tool")
            
            if tool_name:
                # Execute tool
                tool = self.tool_registry.get_tool(tool_name)
                if not tool:
                    results[step_id] = {"error": f"Tool not found: {tool_name}"}
                    continue
                    
                try:
                    tool_input = self._prepare_input(step["input"], results)
                    tool_output = await tool.execute(tool_input)
                    results[step_id] = {"output": tool_output}
                except Exception as e:
                    results[step_id] = {"error": str(e)}
            else:
                # No tool, just record the action
                results[step_id] = {"action": step["action"]}
                
        return {
            "query": query,
            "plan": plan,
            "results": results
        }
        
    def _prepare_input(self, input_template: str, results: Dict[str, Any]) -> Any:
        """
        Prepare tool input by replacing references to previous results
        
        Args:
            input_template: Input template with possible references
            results: Results from previous steps
            
        Returns:
            Prepared input
        """
        # Replace references like {step_1.output} with actual values
        # This is a simplified implementation
        for step_id, step_result in results.items():
            if "output" in step_result:
                input_template = input_template.replace(f"{{step_{step_id}.output}}", json.dumps(step_result["output"]))
                
        return json.loads(input_template)
```

### 4.5 Response Synthesizer

The Response Synthesizer combines the results from all tools into a coherent response.

```python
class ResponseSynthesizer:
    """
    Synthesizes a response from execution results
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        
    async def synthesize(self, execution_result: Dict[str, Any]) -> str:
        """
        Synthesize a response from execution results
        
        Args:
            execution_result: Result of plan execution
            
        Returns:
            Synthesized response
        """
        prompt = self._create_synthesis_prompt(execution_result)
        response = await self.llm_provider.generate(prompt=prompt)
        return response.get("response", "")
        
    def _create_synthesis_prompt(self, execution_result: Dict[str, Any]) -> str:
        return f"""Synthesize a comprehensive response to the following query based on the execution results:

Query: {execution_result["query"]}

Execution Results:
{json.dumps(execution_result["results"], indent=2)}

Your task is to create a clear, coherent response that:
1. Directly answers the user's query
2. Incorporates all relevant information from the execution results
3. Explains any calculations or reasoning
4. Acknowledges any limitations or uncertainties
5. Is well-structured and easy to understand

Your response should be in natural language, as if you're speaking directly to the user. Do not include JSON formatting, step numbers, or other technical details unless they're relevant to the answer.
"""
```

### 4.6 Response Evaluator and Refiner

The Response Evaluator checks the quality of the response, and the Response Refiner improves it if needed.

```python
class ResponseEvaluator:
    """
    Evaluates response quality
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        
    async def evaluate(self, query: str, response: str, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate response quality
        
        Args:
            query: Original query
            response: Synthesized response
            execution_result: Result of plan execution
            
        Returns:
            Dict with evaluation results
        """
        prompt = self._create_evaluation_prompt(query, response, execution_result)
        eval_response = await self.llm_provider.generate(prompt=prompt)
        return self._parse_evaluation(eval_response.get("response", ""))
        
    def _create_evaluation_prompt(self, query: str, response: str, execution_result: Dict[str, Any]) -> str:
        return f"""Evaluate the quality of the following response to a query:

Query: {query}

Response:
{response}

Execution Results (for reference):
{json.dumps(execution_result["results"], indent=2)}

Evaluate the response on the following criteria:
1. Factual accuracy: Does it correctly use the information from the execution results?
2. Completeness: Does it address all aspects of the query?
3. Relevance: Is it focused on answering the query?
4. Clarity: Is it clear and easy to understand?
5. Coherence: Is it well-structured and logically organized?
6. Hallucination: Does it contain any information not supported by the execution results?

Output your evaluation in JSON format:
{{
    "scores": {{
        "accuracy": 1-10,
        "completeness": 1-10,
        "relevance": 1-10,
        "clarity": 1-10,
        "coherence": 1-10,
        "hallucination": 1-10  // 10 = no hallucination, 1 = severe hallucination
    }},
    "overall_score": 1-10,
    "passes_threshold": true/false,
    "hallucinated_content": ["item 1", "item 2", ...],
    "issues": ["issue 1", "issue 2", ...],
    "suggestions": ["suggestion 1", "suggestion 2", ...]
}}
"""

class ResponseRefiner:
    """
    Refines responses based on evaluation
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        
    async def refine(self, query: str, response: str, evaluation: Dict[str, Any], execution_result: Dict[str, Any]) -> str:
        """
        Refine a response based on evaluation
        
        Args:
            query: Original query
            response: Original response
            evaluation: Evaluation results
            execution_result: Result of plan execution
            
        Returns:
            Refined response
        """
        prompt = self._create_refinement_prompt(query, response, evaluation, execution_result)
        refined_response = await self.llm_provider.generate(prompt=prompt)
        return refined_response.get("response", "")
        
    def _create_refinement_prompt(self, query: str, response: str, evaluation: Dict[str, Any], execution_result: Dict[str, Any]) -> str:
        return f"""Refine the following response to address the identified issues:

Query: {query}

Original Response:
{response}

Evaluation:
{json.dumps(evaluation, indent=2)}

Execution Results (for reference):
{json.dumps(execution_result["results"], indent=2)}

Your task is to create an improved version of the response that:
1. Addresses all the issues identified in the evaluation
2. Removes any hallucinated content not supported by the execution results
3. Incorporates the suggestions from the evaluation
4. Maintains the strengths of the original response
5. Is clear, coherent, and directly answers the query

Focus particularly on improving the areas with the lowest scores in the evaluation.
"""
```

### 4.7 Process Logger and Audit Report Generator

The Process Logger records every step of the query processing, and the Audit Report Generator creates a detailed report that guarantees the response is not hallucinated.

```python
class ProcessLogger:
    """
    Logs the entire query processing workflow
    """
    def __init__(self, db_connection=None):
        self.db_connection = db_connection
        self.process_log = {}
        
    def start_process(self, query_id: str, query: str) -> None:
        """
        Start logging a new process
        
        Args:
            query_id: Unique query ID
            query: User query
        """
        self.process_log[query_id] = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "steps": [],
            "final_response": None,
            "audit_report": None
        }
        
    def log_step(self, query_id: str, step_name: str, step_data: Dict[str, Any]) -> None:
        """
        Log a step in the process
        
        Args:
            query_id: Query ID
            step_name: Name of the step
            step_data: Data from the step
        """
        if query_id not in self.process_log:
            raise ValueError(f"Unknown query ID: {query_id}")
            
        self.process_log[query_id]["steps"].append({
            "step_name": step_name,
            "timestamp": datetime.now().isoformat(),
            "data": step_data
        })
        
        # If database connection is available, save to database
        if self.db_connection:
            self._save_to_db(query_id)
            
    def log_final_response(self, query_id: str, response: str) -> None:
        """
        Log the final response
        
        Args:
            query_id: Query ID
            response: Final response
        """
        if query_id not in self.process_log:
            raise ValueError(f"Unknown query ID: {query_id}")
            
        self.process_log[query_id]["final_response"] = {
            "content": response,
            "timestamp": datetime.now().isoformat()
        }
        
        # If database connection is available, save to database
        if self.db_connection:
            self._save_to_db(query_id)
            
    def generate_audit_report(self, query_id: str) -> Dict[str, Any]:
        """
        Generate an audit report for the process
        
        Args:
            query_id: Query ID
            
        Returns:
            Audit report
        """
        if query_id not in self.process_log:
            raise ValueError(f"Unknown query ID: {query_id}")
            
        process = self.process_log[query_id]
        
        # Extract information from steps
        query_analysis = self._find_step_data(process["steps"], "query_analysis")
        execution_plan = self._find_step_data(process["steps"], "query_planning")
        execution_results = self._find_step_data(process["steps"], "plan_execution")
        retrieval_results = self._find_step_data(process["steps"], "retrieval")
        response_evaluation = self._find_step_data(process["steps"], "response_evaluation")
        
        # Generate report
        audit_report = {
            "query_id": query_id,
            "query": process["query"],
            "timestamp": datetime.now().isoformat(),
            "process_summary": {
                "total_steps": len(process["steps"]),
                "process_duration_ms": self._calculate_duration(process),
                "steps_executed": [step["step_name"] for step in process["steps"]]
            },
            "information_sources": self._extract_information_sources(process),
            "reasoning_trace": self._extract_reasoning_trace(process),
            "hallucination_assessment": self._assess_hallucination(process),
            "verification_status": self._determine_verification_status(process)
        }
        
        # Store the audit report
        self.process_log[query_id]["audit_report"] = audit_report
        
        # If database connection is available, save to database
        if self.db_connection:
            self._save_to_db(query_id)
            
        return audit_report
        
    def _find_step_data(self, steps: List[Dict[str, Any]], step_name: str) -> Optional[Dict[str, Any]]:
        """Find data for a specific step"""
        for step in steps:
            if step["step_name"] == step_name:
                return step["data"]
        return None
        
    def _calculate_duration(self, process: Dict[str, Any]) -> int:
        """Calculate process duration in milliseconds"""
        if not process["steps"]:
            return 0
            
        start_time = datetime.fromisoformat(process["timestamp"])
        
        if process["final_response"]:
            end_time = datetime.fromisoformat(process["final_response"]["timestamp"])
        else:
            end_time = datetime.fromisoformat(process["steps"][-1]["timestamp"])
            
        return int((end_time - start_time).total_seconds() * 1000)
        
    def _extract_information_sources(self, process: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract information sources used in the process"""
        sources = []
        
        # Look for retrieval results
        for step in process["steps"]:
            if step["step_name"] == "retrieval":
                if "chunks" in step["data"]:
                    for chunk in step["data"]["chunks"]:
                        if "metadata" in chunk:
                            sources.append({
                                "document_id": chunk["metadata"].get("document_id", "unknown"),
                                "filename": chunk["metadata"].get("filename", "unknown"),
                                "relevance_score": chunk.get("relevance_score", 0),
                                "excerpt": chunk["content"][:200] + "..." if len(chunk["content"]) > 200 else chunk["content"]
                            })
            elif step["step_name"] == "plan_execution":
                if "results" in step["data"]:
                    for step_id, result in step["data"]["results"].items():
                        if "output" in result and isinstance(result["output"], dict) and "sources" in result["output"]:
                            for source in result["output"]["sources"]:
                                sources.append(source)
                                
        return sources
        
    def _extract_reasoning_trace(self, process: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract reasoning trace from the process"""
        reasoning_trace = []
        
        # Extract reasoning from each step
        for step in process["steps"]:
            if step["step_name"] == "query_analysis":
                reasoning_trace.append({
                    "stage": "Query Analysis",
                    "reasoning": step["data"].get("reasoning", "No reasoning provided")
                })
            elif step["step_name"] == "query_planning":
                reasoning_trace.append({
                    "stage": "Query Planning",
                    "reasoning": step["data"].get("reasoning", "No reasoning provided")
                })
            elif step["step_name"] == "response_evaluation":
                reasoning_trace.append({
                    "stage": "Response Evaluation",
                    "reasoning": step["data"].get("justification", "No reasoning provided")
                })
                
        return reasoning_trace
        
    def _assess_hallucination(self, process: Dict[str, Any]) -> Dict[str, Any]:
        """Assess hallucination risk in the response"""
        # Look for response evaluation
        for step in process["steps"]:
            if step["step_name"] == "response_evaluation":
                if "scores" in step["data"] and "hallucination" in step["data"]["scores"]:
                    hallucination_score = step["data"]["scores"]["hallucination"]
                    hallucinated_content = step["data"].get("hallucinated_content", [])
                    
                    return {
                        "hallucination_score": hallucination_score,
                        "hallucination_risk": "Low" if hallucination_score >= 8 else "Medium" if hallucination_score >= 5 else "High",
                        "hallucinated_content": hallucinated_content,
                        "assessment_method": "LLM-based evaluation"
                    }
                    
        # If no evaluation found, return default assessment
        return {
            "hallucination_score": None,
            "hallucination_risk": "Unknown",
            "hallucinated_content": [],
            "assessment_method": "No formal assessment"
        }
        
    def _determine_verification_status(self, process: Dict[str, Any]) -> Dict[str, Any]:
        """Determine verification status of the response"""
        # Check if all information in the response is traceable to sources
        hallucination_assessment = self._assess_hallucination(process)
        
        if hallucination_assessment["hallucination_risk"] == "Low":
            status = "Verified"
            confidence = "High"
        elif hallucination_assessment["hallucination_risk"] == "Medium":
            status = "Partially Verified"
            confidence = "Medium"
        elif hallucination_assessment["hallucination_risk"] == "High":
            status = "Not Verified"
            confidence = "Low"
        else:
            status = "Unknown"
            confidence = "Unknown"
            
        return {
            "status": status,
            "confidence": confidence,
            "verification_method": "Source tracing and LLM evaluation"
        }
        
    def _save_to_db(self, query_id: str) -> None:
        """Save process log to database"""
        if not self.db_connection:
            return
            
        process = self.process_log[query_id]
        
        # Save to database (implementation depends on database type)
        # This is a placeholder for the actual implementation
        pass
        
    def get_process_log(self, query_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the process log for a query
        
        Args:
            query_id: Query ID
            
        Returns:
            Process log or None if not found
        """
        return self.process_log.get(query_id)
        
    def get_audit_report(self, query_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the audit report for a query
        
        Args:
            query_id: Query ID
            
        Returns:
            Audit report or None if not found
        """
        process = self.process_log.get(query_id)
        if not process:
            return None
            
        return process.get("audit_report")
```

### 4.8 Agentic RAG Controller

The Agentic RAG Controller orchestrates the entire process, including logging and audit report generation.

```python
class AgenticRAGController:
    """
    Controller for the Agentic RAG system
    """
    def __init__(
        self,
        llm_provider,
        query_analyzer: QueryAnalyzer,
        query_planner: QueryPlanner,
        plan_executor: PlanExecutor,
        response_synthesizer: ResponseSynthesizer,
        response_evaluator: ResponseEvaluator,
        response_refiner: ResponseRefiner,
        process_logger: ProcessLogger,
        rag_engine: RAGEngine
    ):
        self.llm_provider = llm_provider
        self.query_analyzer = query_analyzer
        self.query_planner = query_planner
        self.plan_executor = plan_executor
        self.response_synthesizer = response_synthesizer
        self.response_evaluator = response_evaluator
        self.response_refiner = response_refiner
        self.process_logger = process_logger
        self.rag_engine = rag_engine
        
    async def process_query(self, query: str) -> Dict[str, Any]:
        """
        Process a query using the Agentic RAG system
        
        Args:
            query: User query
            
        Returns:
            Dict with response and metadata
        """
        # Generate query ID
        query_id = str(uuid.uuid4())
        
        # Start process logging
        self.process_logger.start_process(query_id, query)
        
        # Step 1: Analyze query
        analysis = await self.query_analyzer.analyze(query)
        self.process_logger.log_step(query_id, "query_analysis", analysis)
        
        # Step 2: Process based on complexity
        if analysis["complexity"] == "simple":
            # Use standard RAG for simple queries
            rag_response = await self.rag_engine.query(query=query)
            self.process_logger.log_step(query_id, "retrieval", rag_response)
            
            response = rag_response.get("answer", "")
            execution_result = {"query": query, "results": {"rag": rag_response}}
        else:
            # Use agentic approach for complex queries
            plan = await self.query_planner.create_plan(query, analysis)
            self.process_logger.log_step(query_id, "query_planning", plan)
            
            execution_result = await self.plan_executor.execute_plan(plan, query)
            self.process_logger.log_step(query_id, "plan_execution", execution_result)
            
            response = await self.response_synthesizer.synthesize(execution_result)
            self.process_logger.log_step(query_id, "response_synthesis", {"response": response})
            
        # Step 3: Evaluate and refine response
        evaluation = await self.response_evaluator.evaluate(query, response, execution_result)
        self.process_logger.log_step(query_id, "response_evaluation", evaluation)
        
        if not evaluation["passes_threshold"]:
            # Refine response if it doesn't pass quality threshold
            response = await self.response_refiner.refine(query, response, evaluation, execution_result)
            self.process_logger.log_step(query_id, "response_refinement", {"refined_response": response})
            
            # Re-evaluate refined response
            evaluation = await self.response_evaluator.evaluate(query, response, execution_result)
            self.process_logger.log_step(query_id, "response_evaluation", evaluation)
            
        # Log final response
        self.process_logger.log_final_response(query_id, response)
        
        # Generate audit report
        audit_report = self.process_logger.generate_audit_report(query_id)
        
        return {
            "query_id": query_id,
            "query": query,
            "response": response,
            "analysis": analysis,
            "evaluation": evaluation,
            "execution_result": execution_result,
            "audit_report": audit_report
        }
```

## 5. Tool Implementations

### 5.1 RAG Tool

```python
class RAGTool(Tool):
    """
    Tool for retrieving information using RAG
    """
    def __init__(self, rag_engine: RAGEngine):
        self.rag_engine = rag_engine
        
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute RAG retrieval
        
        Args:
            input_data: Dict with keys:
                - query: Query string
                - metadata_filters: Optional filters
                
        Returns:
            RAG response
        """
        query = input_data["query"]
        metadata_filters = input_data.get("metadata_filters")
        
        response = await self.rag_engine.query(
            query=query,
            metadata_filters=metadata_filters
        )
        
        return response
        
    def get_description(self) -> str:
        return "Retrieves information from the document store using RAG"
        
    def get_input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Query string"
                },
                "metadata_filters": {
                    "type": "object",
                    "description": "Optional filters for retrieval"
                }
            },
            "required": ["query"]
        }
        
    def get_output_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "answer": {
                    "type": "string",
                    "description": "Retrieved information"
                },
                "sources": {
                    "type": "array",
                    "description": "Sources of the information"
                }
            }
        }
```

### 5.2 Calculator Tool

```python
class CalculatorTool(Tool):
    """
    Tool for performing mathematical calculations
    """
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute calculation
        
        Args:
            input_data: Dict with keys:
                - expression: Mathematical expression
                
        Returns:
            Calculation result
        """
        expression = input_data["expression"]
        
        try:
            # Use a safe evaluation method
            import ast
            import math
            import operator
            
            # Define allowed operators
            operators = {
                ast.Add: operator.add,
                ast.Sub: operator.sub,
                ast.Mult: operator.mul,
                ast.Div: operator.truediv,
                ast.Pow: operator.pow,
                ast.BitXor: operator.xor,
                ast.USub: operator.neg
            }
            
            def safe_eval(node):
                if isinstance(node, ast.Num):
                    return node.n
                elif isinstance(node, ast.BinOp):
                    return operators[type(node.op)](safe_eval(node.left), safe_eval(node.right))
                elif isinstance(node, ast.UnaryOp):
                    return operators[type(node.op)](safe_eval(node.operand))
                elif isinstance(node, ast.Call):
                    if node.func.id == 'sqrt':
                        return math.sqrt(safe_eval(node.args[0]))
                    elif node.func.id == 'sin':
                        return math.sin(safe_eval(node.args[0]))
                    elif node.func.id == 'cos':
                        return math.cos(safe_eval(node.args[0]))
                    elif node.func.id == 'tan':
                        return math.tan(safe_eval(node.args[0]))
                    elif node.func.id == 'log':
                        return math.log(safe_eval(node.args[0]))
                    elif node.func.id == 'exp':
                        return math.exp(safe_eval(node.args[0]))
                    else:
                        raise ValueError(f"Function not allowed: {node.func.id}")
                else:
                    raise TypeError(f"Unsupported node type: {type(node)}")
            
            # Parse and evaluate expression
            node = ast.parse(expression, mode='eval').body
            result = safe_eval(node)
            
            return {
                "result": result,
                "expression": expression
            }
        except Exception as e:
            return {
                "error": str(e),
                "expression": expression
            }
        
    def get_description(self) -> str:
        return "Performs mathematical calculations"
        
    def get_input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string",
                    "description": "Mathematical expression to evaluate"
                }
            },
            "required": ["expression"]
        }
        
    def get_output_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "result": {
                    "type": "number",
                    "description": "Calculation result"
                },
                "expression": {
                    "type": "string",
                    "description": "Original expression"
                },
                "error": {
                    "type": "string",
                    "description": "Error message (if calculation failed)"
                }
            }
        }
```

### 5.3 Database Tool

```python
class DatabaseTool(Tool):
    """
    Tool for querying structured data
    """
    def __init__(self, db_connection):
        self.db_connection = db_connection
        
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute database query
        
        Args:
            input_data: Dict with keys:
                - query: SQL query
                - params: Query parameters
                
        Returns:
            Query results
        """
        query = input_data["query"]
        params = input_data.get("params", {})
        
        try:
            # Execute query
            cursor = await self.db_connection.execute(query, params)
            results = await cursor.fetchall()
            
            # Get column names
            column_names = [column[0] for column in cursor.description]
            
            # Format results as list of dictionaries
            formatted_results = [
                {column_names[i]: value for i, value in enumerate(row)}
                for row in results
            ]
            
            return {
                "results": formatted_results,
                "count": len(formatted_results),
                "query": query
            }
        except Exception as e:
            return {
                "error": str(e),
                "query": query
            }
        
    def get_description(self) -> str:
        return "Queries structured data from the database"
        
    def get_input_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "SQL query"
                },
                "params": {
                    "type": "object",
                    "description": "Query parameters"
                }
            },
            "required": ["query"]
        }
        
    def get_output_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "results": {
                    "type": "array",
                    "description": "Query results"
                },
                "count": {
                    "type": "integer",
                    "description": "Number of results"
                },
                "query": {
                    "type": "string",
                    "description": "Original query"
                },
                "error": {
                    "type": "string",
                    "description": "Error message (if query failed)"
                }
            }
        }
```

## 6. Audit Report Interface

The Audit Report Interface provides a way for users to view the audit report for a query, verifying that the response is not hallucinated.

```python
@router.get("/audit_report/{query_id}")
async def get_audit_report(
    query_id: str,
    process_logger: ProcessLogger = Depends(get_process_logger)
):
    """
    Get the audit report for a query
    """
    audit_report = process_logger.get_audit_report(query_id)
    
    if not audit_report:
        raise HTTPException(
            status_code=404,
            detail=f"Audit report not found for query {query_id}"
        )
        
    return audit_report

@router.get("/audit_report/{query_id}/html")
async def get_audit_report_html(
    query_id: str,
    request: Request,
    process_logger: ProcessLogger = Depends(get_process_logger)
):
    """
    Get the audit report for a query as HTML
    """
    audit_report = process_logger.get_audit_report(query_id)
    
    if not audit_report:
        raise HTTPException(
            status_code=404,
            detail=f"Audit report not found for query {query_id}"
        )
        
    return templates.TemplateResponse(
        "audit_report.html",
        {
            "request": request,
            "audit_report": audit_report
        }
    )
```

## 7. Integration with Existing Metis_RAG

### 7.1 LangGraph Integration

The agentic capabilities can be integrated with the existing LangGraph RAG Agent by extending the state graph:

```python
def _build_graph(self) -> StateGraph:
    """
    Build the state graph for the Agentic RAG process
    """
    # Create the state graph
    graph = StateGraph(AgenticRAGState)
    
    # Add nodes for each stage
    graph.add_node(RAGStage.QUERY_ANALYSIS, self._analyze_query)
    graph.add_node(RAGStage.QUERY_PLANNING, self._plan_query)
    graph.add_node(RAGStage.PLAN_EXECUTION, self._execute_plan)
    graph.add_node(RAGStage.RETRIEVAL, self._retrieve_chunks)
    graph.add_node(RAGStage.QUERY_REFINEMENT, self._refine_query)
    graph.add_node(RAGStage.CONTEXT_OPTIMIZATION, self._optimize_context)
    graph.add_node(RAGStage.GENERATION, self._generate_response)
    graph.add_node(RAGStage.RESPONSE_EVALUATION, self._evaluate_response)
    graph.add_node(RAGStage.RESPONSE_REFINEMENT, self._refine_response)
    graph.add_node(RAGStage.COMPLETE, self._finalize_response)
    
    # Define the edges between nodes
    # Start with query analysis
    graph.add_conditional_edges(
        RAGStage.QUERY_ANALYSIS,
        self._needs_planning,
        {
            True: RAGStage.QUERY_PLANNING,
            False: RAGStage.RETRIEVAL
        }
    )
    
    # After planning, execute the plan
    graph.add_edge(RAGStage.QUERY_PLANNING, RAGStage.PLAN_EXECUTION)
    
    # After plan execution, proceed to retrieval
    graph.add_edge(RAGStage.PLAN_EXECUTION, RAGStage.RETRIEVAL)
    
    # After retrieval, decide whether to refine the query or optimize the context
    graph.add_conditional_edges(
        RAGStage.RETRIEVAL,
        self._needs_refinement,
        {
            True: RAGStage.QUERY_REFINEMENT,
            False: RAGStage.CONTEXT_OPTIMIZATION
        }
    )
    
    # After query refinement, go back to retrieval with the refined query
    graph.add_edge(RAGStage.QUERY_REFINEMENT, RAGStage.RETRIEVAL)
    
    # After context optimization, proceed to generation
    graph.add_edge(RAGStage.CONTEXT_OPTIMIZATION, RAGStage.GENERATION)
    
    # After generation, evaluate the response
    graph.add_edge(RAGStage.GENERATION, RAGStage.RESPONSE_EVALUATION)
    
    # After evaluation, decide whether to refine the response or complete
    graph.add_conditional_edges(
        RAGStage.RESPONSE_EVALUATION,
        self._needs_response_refinement,
        {
            True: RAGStage.RESPONSE_REFINEMENT,
            False: RAGStage.COMPLETE
        }
    )
    
    # After response refinement, re-evaluate
    graph.add_edge(RAGStage.RESPONSE_REFINEMENT, RAGStage.RESPONSE_EVALUATION)
    
    # After completion, end the process
    graph.add_edge(RAGStage.COMPLETE, END)
    
    # Set the entry point
    graph.set_entry_point(RAGStage.QUERY_ANALYSIS)
    
    return graph
```

### 7.2 API Integration

The agentic capabilities can be exposed through the existing API:

```python
@router.post("/agentic_query", response_model=AgenticChatResponse)
async def agentic_query_chat(
    query: ChatQuery,
    agentic_rag_controller: AgenticRAGController = Depends(get_agentic_rag_controller)
):
    """
    Send a chat query to the Agentic RAG system and get a response
    """
    try:
        # Process query with Agentic RAG
        result = await agentic_rag_controller.process_query(query.message)
        
        # Return response
        return AgenticChatResponse(
            message=result["response"],
            conversation_id=query.conversation_id,
            analysis=result["analysis"],
            execution_details=result["execution_result"],
            evaluation=result["evaluation"],
            audit_report_id=result["query_id"]
        )
    except Exception as e:
        logger.error(f"Error generating agentic chat response: {str(e)}")
        
        # Create a user-friendly error message
        error_message = "Sorry, there was an error processing your request with the Agentic RAG system."
        
        # Return a 200 response with the error message
        return AgenticChatResponse(
            message=error_message,
            conversation_id=query.conversation_id
        )
```

## 8. Implementation Roadmap

### Phase 1: Foundation (Weeks 1-3)
- Implement Tool interface and basic tools (RAG, Calculator)
- Create ToolRegistry
- Implement QueryAnalyzer
- Implement ProcessLogger and basic audit reporting

### Phase 2: Planning and Execution (Weeks 4-6)
- Implement QueryPlanner
- Implement PlanExecutor
- Integrate with existing LangGraph framework
- Enhance audit reporting with execution details

### Phase 3: Response Quality (Weeks 7-9)
- Implement ResponseSynthesizer
- Implement ResponseEvaluator with hallucination detection
- Implement ResponseRefiner
- Complete audit report generation with hallucination assessment

### Phase 4: Advanced Tools and Integration (Weeks 10-12)
- Implement additional tools (Database, Code Execution)
- Create AgenticRAGController
- Integrate with API
- Add comprehensive testing
- Implement audit report interface and visualization

## 9. Conclusion

By implementing this agentic enhancement plan, Metis_RAG will evolve from a sophisticated RAG system to a truly intelligent assistant that can reason about queries, plan responses, use tools to gather information, and provide high-quality, comprehensive answers. The system will be able to handle a much wider range of queries, from simple fact retrieval to complex analytical questions that require multiple steps and tools.

The comprehensive process logging and audit report generation will provide transparency and accountability, allowing users to verify that responses are not hallucinated and to understand exactly how the system arrived at its answers. This will build trust in the system and make it suitable for applications where factual accuracy is critical.

The implementation leverages the existing strengths of Metis_RAG, particularly the LangGraph integration, while adding new capabilities in a modular, extensible way. The phased implementation approach allows for incremental development and testing, ensuring that each component works correctly before moving on to the next.

================
File: docs/implementation/Metis_RAG_Authentication_Implementation_Plan.md
================
# Metis RAG Authentication Implementation Plan

## Overview

This document outlines the plan for adding authentication to the Metis RAG application. The goal is to implement a user authentication system where each user gets a unique ID that will be used to track their chats and documents.

## Current State Analysis

Based on the analysis of the codebase and database, the following observations were made:

1. The database already has a `users` table with the following schema:
   ```sql
   CREATE TABLE users (
       id UUID NOT NULL PRIMARY KEY,
       username VARCHAR NOT NULL UNIQUE,
       email VARCHAR NOT NULL UNIQUE,
       password_hash VARCHAR NOT NULL,
       full_name VARCHAR,
       is_active BOOLEAN,
       is_admin BOOLEAN,
       created_at TIMESTAMP WITHOUT TIME ZONE,
       last_login TIMESTAMP WITHOUT TIME ZONE,
       metadata JSONB
   );
   ```

2. However, the `documents` and `conversations` tables do not have a `user_id` column yet, which is needed to associate these resources with users.

3. The system has some concept of `user_id` in the `Conversation` model's metadata, but it's not properly integrated with a user authentication system.

4. The application doesn't have a proper authentication system implemented yet.

## Implementation Plan

### 1. Database Schema Updates

Since the `users` table already exists in the database, we only need to add the `user_id` columns to the `documents` and `conversations` tables.

#### 1.1 Update Documents Table

Add a user_id column to the documents table to associate documents with users:

```sql
ALTER TABLE documents ADD COLUMN user_id UUID REFERENCES users(id);
CREATE INDEX ix_documents_user_id ON documents(user_id);
```

#### 1.2 Update Conversations Table

Add a user_id column to the conversations table (instead of storing it in metadata):

```sql
ALTER TABLE conversations ADD COLUMN user_id UUID REFERENCES users(id);
CREATE INDEX ix_conversations_user_id ON conversations(user_id);
```

We attempted to create a migration for these changes, but encountered permission issues with the database. The migration file has been created at `alembic/versions/add_user_id_columns.py`, but it will need to be run by a user with appropriate database permissions.

### 2. Model Updates

#### 2.1 Create User Model

Create a new User model in `app/models/user.py`:

```python
from typing import Optional, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class UserBase(BaseModel):
    """Base user model"""
    username: str
    email: str
    full_name: Optional[str] = None
    is_active: bool = True
    is_admin: bool = False
    
    class Config:
        arbitrary_types_allowed = True

class UserCreate(UserBase):
    """User creation model"""
    password: str
    
    class Config:
        arbitrary_types_allowed = True

class UserUpdate(BaseModel):
    """User update model"""
    username: Optional[str] = None
    email: Optional[str] = None
    full_name: Optional[str] = None
    password: Optional[str] = None
    is_active: Optional[bool] = None
    is_admin: Optional[bool] = None
    
    class Config:
        arbitrary_types_allowed = True

class User(UserBase):
    """User model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.now)
    last_login: Optional[datetime] = None
    metadata: Dict[str, Any] = {}
    
    class Config:
        arbitrary_types_allowed = True

class UserInDB(User):
    """User model with password hash (for internal use)"""
    password_hash: str
    
    class Config:
        arbitrary_types_allowed = True
```

#### 2.2 Update Database Models

Update the database models in `app/db/models.py` to include the User model and relationships:

```python
class User(Base):
    """User model for database"""
    __tablename__ = "users"

    id = Column(UUID(as_uuid=True) if DATABASE_TYPE == 'postgresql' else UUID, primary_key=True, default=uuid.uuid4)
    username = Column(String, unique=True, nullable=False)
    email = Column(String, unique=True, nullable=False)
    password_hash = Column(String, nullable=False)
    full_name = Column(String, nullable=True)
    is_active = Column(Boolean, default=True)
    is_admin = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime, nullable=True)
    user_metadata = Column('metadata', JSONType, default={})

    # Relationships
    documents = relationship("Document", back_populates="user")
    conversations = relationship("Conversation", back_populates="user")

    # Indexes
    __table_args__ = (
        Index('ix_users_username', username),
        Index('ix_users_email', email),
    )

    def __repr__(self):
        return f"<User(id={self.id}, username='{self.username}')>"
```

Update the Document model:

```python
class Document(Base):
    # Existing fields...
    user_id = Column(UUID(as_uuid=True) if DATABASE_TYPE == 'postgresql' else UUID, ForeignKey('users.id'), nullable=True)
    
    # Relationships
    # Existing relationships...
    user = relationship("User", back_populates="documents")
    
    # Indexes
    __table_args__ = (
        # Existing indexes...
        Index('ix_documents_user_id', user_id),
    )
```

Update the Conversation model:

```python
class Conversation(Base):
    # Existing fields...
    user_id = Column(UUID(as_uuid=True) if DATABASE_TYPE == 'postgresql' else UUID, ForeignKey('users.id'), nullable=True)
    
    # Relationships
    # Existing relationships...
    user = relationship("User", back_populates="conversations")
    
    # Indexes
    __table_args__ = (
        # Existing indexes...
        Index('ix_conversations_user_id', user_id),
    )
```

### 3. Security Implementation

#### 3.1 Update Security Module

Update the security module in `app/core/security.py` to include password hashing and JWT token generation:

```python
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from passlib.context import CryptContext
from pydantic import BaseModel

from app.core.config import SECRET_KEY, ALGORITHM, ACCESS_TOKEN_EXPIRE_MINUTES

# Password hashing
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# OAuth2 scheme
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="api/v1/auth/token")

# Token model
class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None
    user_id: Optional[str] = None

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """
    Verify a password against a hash
    """
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """
    Hash a password
    """
    return pwd_context.hash(password)

def create_access_token(data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
    """
    Create a JWT access token
    """
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    
    return encoded_jwt

async def get_current_user(token: str = Depends(oauth2_scheme)):
    """
    Get the current user from a JWT token
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        # Decode token
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        user_id: str = payload.get("user_id")
        
        if username is None or user_id is None:
            raise credentials_exception
        
        token_data = TokenData(username=username, user_id=user_id)
    except JWTError:
        raise credentials_exception
    
    # Get user from database
    from app.db.dependencies import get_user_repository
    from app.db.session import AsyncSessionLocal
    
    db = AsyncSessionLocal()
    try:
        user_repository = get_user_repository(db)
        user = await user_repository.get_by_username(token_data.username)
        
        if user is None:
            raise credentials_exception
        
        if not user.is_active:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Inactive user"
            )
        
        return user
    finally:
        await db.close()

async def get_current_active_user(current_user = Depends(get_current_user)):
    """
    Get the current active user
    """
    if not current_user.is_active:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Inactive user"
        )
    
    return current_user

async def get_current_admin_user(current_user = Depends(get_current_user)):
    """
    Get the current admin user
    """
    if not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not an admin user"
        )
    
    return current_user
```

#### 3.2 Update Config Module

Update the config module in `app/core/config.py` to include JWT settings:

```python
# JWT settings
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30
```

### 4. Repository Implementation

#### 4.1 Create User Repository

Create a new user repository in `app/db/repositories/user_repository.py`:

```python
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, or_, and_, select, text

from app.db.models import User as DBUser
from app.models.user import User as PydanticUser, UserCreate, UserUpdate, UserInDB
from app.db.repositories.base import BaseRepository
from app.core.security import get_password_hash, verify_password

class UserRepository(BaseRepository[DBUser]):
    """
    Repository for User model
    """
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, DBUser)
    
    async def create_user(self, user_data: UserCreate) -> PydanticUser:
        """
        Create a new user
        
        Args:
            user_data: User creation data
            
        Returns:
            Created user (Pydantic model)
        """
        # Check if username or email already exists
        existing_user = await self.get_by_username_or_email(user_data.username, user_data.email)
        if existing_user:
            raise ValueError("Username or email already exists")
        
        # Create password hash
        password_hash = get_password_hash(user_data.password)
        
        # Create user
        user = DBUser(
            username=user_data.username,
            email=user_data.email,
            password_hash=password_hash,
            full_name=user_data.full_name,
            is_active=user_data.is_active,
            is_admin=user_data.is_admin,
            created_at=datetime.utcnow()
        )
        
        self.session.add(user)
        await self.session.commit()
        await self.session.refresh(user)
        
        # Convert to Pydantic model
        return self._db_user_to_pydantic(user)
    
    async def get_by_username(self, username: str) -> Optional[PydanticUser]:
        """
        Get a user by username
        
        Args:
            username: Username
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(DBUser.username == username)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def get_by_email(self, email: str) -> Optional[PydanticUser]:
        """
        Get a user by email
        
        Args:
            email: Email
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(DBUser.email == email)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def get_by_username_or_email(self, username: str, email: str) -> Optional[PydanticUser]:
        """
        Get a user by username or email
        
        Args:
            username: Username
            email: Email
            
        Returns:
            User if found, None otherwise
        """
        stmt = select(DBUser).where(or_(DBUser.username == username, DBUser.email == email))
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        return self._db_user_to_pydantic(user)
    
    async def update_user(self, user_id: Union[UUID, str], user_data: UserUpdate) -> Optional[PydanticUser]:
        """
        Update a user
        
        Args:
            user_id: User ID
            user_data: User update data
            
        Returns:
            Updated user if found, None otherwise
        """
        user = await self.get_by_id(user_id)
        if not user:
            return None
        
        # Update fields if provided
        if user_data.username is not None:
            # Check if username already exists
            existing_user = await self.get_by_username(user_data.username)
            if existing_user and str(existing_user.id) != str(user_id):
                raise ValueError("Username already exists")
            user.username = user_data.username
        
        if user_data.email is not None:
            # Check if email already exists
            existing_user = await self.get_by_email(user_data.email)
            if existing_user and str(existing_user.id) != str(user_id):
                raise ValueError("Email already exists")
            user.email = user_data.email
        
        if user_data.full_name is not None:
            user.full_name = user_data.full_name
        
        if user_data.password is not None:
            user.password_hash = get_password_hash(user_data.password)
        
        if user_data.is_active is not None:
            user.is_active = user_data.is_active
        
        if user_data.is_admin is not None:
            user.is_admin = user_data.is_admin
        
        await self.session.commit()
        await self.session.refresh(user)
        
        return self._db_user_to_pydantic(user)
    
    async def authenticate_user(self, username: str, password: str) -> Optional[PydanticUser]:
        """
        Authenticate a user
        
        Args:
            username: Username
            password: Password
            
        Returns:
            User if authentication successful, None otherwise
        """
        # Get user by username
        stmt = select(DBUser).where(DBUser.username == username)
        result = await self.session.execute(stmt)
        user = result.scalars().first()
        
        if not user:
            return None
        
        # Verify password
        if not verify_password(password, user.password_hash):
            return None
        
        # Update last login
        user.last_login = datetime.utcnow()
        await self.session.commit()
        
        return self._db_user_to_pydantic(user)
    
    async def get_all_users(self, skip: int = 0, limit: int = 100) -> List[PydanticUser]:
        """
        Get all users with pagination
        
        Args:
            skip: Number of users to skip
            limit: Maximum number of users to return
            
        Returns:
            List of users
        """
        stmt = select(DBUser).offset(skip).limit(limit)
        result = await self.session.execute(stmt)
        users = result.scalars().all()
        
        return [self._db_user_to_pydantic(user) for user in users]
    
    def _db_user_to_pydantic(self, db_user: DBUser) -> PydanticUser:
        """
        Convert a database user to a Pydantic user
        
        Args:
            db_user: Database user
            
        Returns:
            Pydantic user
        """
        return PydanticUser(
            id=str(db_user.id),
            username=db_user.username,
            email=db_user.email,
            full_name=db_user.full_name,
            is_active=db_user.is_active,
            is_admin=db_user.is_admin,
            created_at=db_user.created_at,
            last_login=db_user.last_login,
            metadata=db_user.user_metadata or {}
        )
```

#### 4.2 Update Dependencies

Update the dependencies in `app/db/dependencies.py` to include the user repository:

```python
from app.db.repositories.user_repository import UserRepository

def get_user_repository(db: AsyncSession) -> UserRepository:
    """
    Get a user repository
    
    Args:
        db: Database session
        
    Returns:
        User repository
    """
    return UserRepository(db)
```

### 5. API Implementation

#### 5.1 Create Authentication API

Create a new authentication API in `app/api/auth.py`:

```python
from datetime import timedelta
from typing import Optional
from fastapi import APIRouter, HTTPException, Depends, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.user import User, UserCreate, UserUpdate
from app.core.security import create_access_token, get_current_user, get_current_active_user, Token
from app.core.config import ACCESS_TOKEN_EXPIRE_MINUTES
from app.db.dependencies import get_db, get_user_repository
from app.db.repositories.user_repository import UserRepository

# Create router
router = APIRouter()

@router.post("/token", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    db: AsyncSession = Depends(get_db),
    user_repository: UserRepository = Depends(get_user_repository)
):
    """
    Get an access token for a user
    """
    user = await user_repository.authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Create access token
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.username, "user_id": user.id},
        expires_delta=access_token_expires
    )
    
    return {"access_token": access_token, "token_type": "bearer"}

@router.post("/register", response_model=User)
async def register_user(
    user_data: UserCreate,
    db: AsyncSession = Depends(get_db),
    user_repository: UserRepository = Depends(get_user_repository)
):
    """
    Register a new user
    """
    try:
        user = await user_repository.create_user(user_data)
        return user
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.get("/me", response_model=User)
async def read_users_me(current_user: User = Depends(get_current_active_user)):
    """
    Get the current user
    """
    return current_user

@router.put("/me", response_model=User)
async def update_user_me(
    user_data: UserUpdate,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db),
    user_repository: UserRepository = Depends(get_user_repository)
):
    """
    Update the current user
    """
    try:
        updated_user = await user_repository.update_user(current_user.id, user_data)
        return updated_user
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
```

#### 5.2 Update Main App

Update the main app in `app/main.py` to include the authentication router:

```python
from app.api.auth import router as auth_router

# Include API routers
app.include_router(auth_router, prefix=f"{API_V1_STR}/auth", tags=["auth"])
```

### 6. Frontend Implementation

#### 6.1 Create Login Page

Create a login page template in `app/templates/login.html`:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG - Login</title>
    <link rel="stylesheet" href="/static/css/styles.css">
    <style>
        /* Styles for login form */
    </style>
</head>
<body>
    <div class="login-container">
        <h2>Login to Metis RAG</h2>
        <div id="error-message" class="error-message"></div>
        <form id="login-form">
            <div class="form-group">
                <label for="username">Username</label>
                <input type="text" id="username" name="username" required>
            </div>
            <div class="form-group">
                <label for="password">Password</label>
                <input type="password" id="password" name="password" required>
            </div>
            <button type="submit" class="btn">Login</button>
        </form>
        <div class="register-link">
            <p>Don't have an account? <a href="/register">Register</a></p>
        </div>
    </div>

    <script>
        // JavaScript for login form submission
    </script>
</body>
</html>
```

#### 6.2 Create Registration Page

Create a registration page template in `app/templates/register.html`:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG - Register</title>
    <link rel="stylesheet" href="/static/css/styles.css">
    <style>
        /* Styles for registration form */
    </style>
</head>
<body>
    <div class="register-container">
        <h2>Register for Metis RAG</h2>
        <div id="error-message" class="error-message"></div>
        <form id="register-form">
            <div class="form-group">
                <label for="username">Username</label>
                <input type="text" id="username" name="username" required>
            </div>
            <div class="form-group">
                <label for="email">Email</label>
                <input type="email" id="email" name="email" required>
            </div>
            <div class="form-group">
                <label for="full_name">Full Name</label>
                <input type="text" id="full_name" name="full_name">
            </div>
            <div class="form-group">
                <label for="password">Password</label>
                <input type="password" id="password" name="password" required>
            </div>
            <div class="form-group">
                <label for="confirm_password">Confirm Password</label>
                <input type="password" id="confirm_password" name="confirm_password" required>
            </div>
            <button type="submit" class="btn">Register</button>
        </form>
        <div class="login-link">
            <p>Already have an account? <a href="/login">Login</a></p>
        </div>
    </div>

    <script>
        // JavaScript for registration form submission
    </script>
</body>
</html>
```

#### 6.3 Update Main App Routes

Update the main app in `app/main.py` to include routes for the login and registration pages:

```python
@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    """
    Login page
    """
    return templates.TemplateResponse("login.html", {"request": request})

@app.get("/register", response_class=HTMLResponse)
async def register_page(request: Request):
    """
    Registration page
    """
    return templates.TemplateResponse("register.html", {"request": request})
```

### 7. Migration Implementation

Create a new Alembic migration in `alembic/versions/add_users_table.py`:

```python
"""Add users table and update documents and conversations tables

Revision ID: add_users_table
Revises: initial_schema
Create Date: 2025-03-21 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB


# revision identifiers, used by Alembic.
revision = 'add_users_table'
down_revision = 'initial_schema'
branch_labels = None
depends_on = None


def upgrade():
    # Create users table
    op.create_table(
        'users',
        sa.Column('id', UUID(), nullable=False),
        sa.Column('username', sa.String(), nullable=False),
        sa.Column('email', sa.String(), nullable=False),
        sa.Column('password_hash', sa.String(), nullable=False),
        sa.Column('full_name', sa.String(), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=True, default=True),
        sa.Column('is_admin', sa.Boolean(), nullable=True, default=False),
        sa.Column('created_at', sa.DateTime(), nullable=True, default=sa.func.now()),
        sa.Column('last_login', sa.DateTime(), nullable=True),
        sa.Column('metadata', JSONB(), nullable=True),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('username'),
        sa.UniqueConstraint('email')
    )
    op.create_index('ix_users_username', 'users', ['username'])
    op.create_index('ix_users_email', 'users', ['email'])
    
    # Add user_id column to documents table
    op.add_column('documents', sa.Column('user_id', UUID(), nullable=True))
    op.create_foreign_key('fk_documents_user_id', 'documents', 'users', ['user_id'], ['id'])
    op.create_index('ix_documents_user_id', 'documents', ['user_id'])
    
    # Add user_id column to conversations table
    op.add_column('conversations', sa.Column('user_id', UUID(), nullable=True))
    op.create_foreign_key('fk_conversations_user_id', 'conversations', 'users', ['user_id'], ['id'])
    op.create_index('ix_conversations_user_id', 'conversations', ['user_id'])


def downgrade():
    # Drop foreign keys and indexes
    op.drop_constraint('fk_documents_user_id', 'documents', type_='foreignkey')
    op.drop_index('ix_documents_user_id', table_name='documents')
    op.drop_constraint('fk_conversations_user_id', 'conversations', type_='foreignkey')
    op.drop_index('ix_conversations_user_id', table_name='conversations')
    
    # Drop columns
    op.drop_column('documents', 'user_id')
    op.drop_column('conversations', 'user_id')
    
    # Drop users table
    op.drop_index('ix_users_email', table_name='users')
    op.drop_index('ix_users_username', table_name='users')
    op.drop_table('users')
```

## Implementation Status

### Completed

1. ✅ Created the Pydantic User model in `app/models/user.py`
2. ✅ Updated the database models in `app/db/models.py` to include the User model relationships
3. ✅ Created the user repository in `app/db/repositories/user_repository.py`
4. ✅ Updated the security module in `app/core/security.py` (was already mostly implemented)
5. ✅ Created the authentication API in `app/api/auth.py`
6. ✅ Created login and registration templates in `app/templates/login.html` and `app/templates/register.html`
7. ✅ Updated the main app to include routes for login and registration pages
8. ✅ Created a migration file for adding user_id columns to documents and conversations tables

### Remaining Tasks

1. ⬜ Run the database migration with appropriate permissions
2. ⬜ Test the authentication system
3. ⬜ Update existing API endpoints to use the authentication system
4. ⬜ Update frontend components to use the authentication system

## Conclusion

This implementation plan provides a comprehensive approach to adding authentication to the Metis RAG application. We've discovered that the users table already exists in the database, which simplified part of our implementation. We've created all the necessary components for the authentication system, but we still need to run the database migration with appropriate permissions and integrate the authentication system with the existing application.

By completing this implementation, each user will get a unique ID that will be used to track their chats and documents, ensuring data privacy and personalized experiences.

================
File: docs/implementation/Metis_RAG_Database_Integration_Plan.md
================
# Metis_RAG Database Integration Plan

## Overview

This plan addresses the model compatibility issues between Pydantic and SQLAlchemy models, focusing on fixing the architecture first to ensure a robust foundation for the system. The implementation will prioritize PostgreSQL compatibility, performance optimization, and comprehensive testing.

```mermaid
graph TD
    A[Domain Layer<br>Pydantic Models] --> |Adapter Layer| B[Database Layer<br>SQLAlchemy Models]
    C[Document Processor] --> |Uses| A
    D[Database Repositories] --> |Uses| B
    E[API Layer] --> |Uses| A
    E --> |Uses| D
    F[Performance Tests] --> |Verify| G[Performance Requirements]
    H[Adapter Functions] --> |Convert| A
    H --> |Convert| B
    I[JSONB for Metadata] --> |Optimizes| B
```

## Current Issues

### 1. Model Compatibility Issues

- **Attribute Name Mismatch**: SQLAlchemy models use renamed attributes (`doc_metadata`, `chunk_metadata`, etc.) to avoid conflicts with SQLAlchemy's reserved keywords, while Pydantic models use `metadata`.
- **Type Conversion**: UUID objects need proper string conversion between PostgreSQL and SQLite.
- **Model Type Mismatch**: Document processor creates Pydantic `Chunk` objects, but database repositories expect SQLAlchemy `Chunk` objects.

### 2. Testing Framework Issues

- Performance tests fail when trying to use real components with incompatible models
- No clear separation between domain logic and database persistence
- Missing adapter functions to convert between model types

## Implementation Plan

### Phase 1: Create Standalone Adapter Layer

#### 1. Create Model Adapter Module

Create a new file `app/db/adapters.py` with standalone conversion functions:

```python
"""
Adapter functions to convert between Pydantic and SQLAlchemy models
"""
import uuid
from typing import List, Optional, Union, Dict, Any
from uuid import UUID

from app.models import document as pydantic_models
from app.db import models as db_models
from app.core.config import DATABASE_TYPE

def to_str_id(id_value: Union[str, UUID, None]) -> Optional[str]:
    """Convert ID to string format"""
    if id_value is None:
        return None
    return str(id_value)

def to_uuid_or_str(id_value: Union[str, UUID, None]) -> Optional[Union[str, UUID]]:
    """
    Convert ID to appropriate type based on database
    For PostgreSQL: UUID
    For SQLite: string
    """
    if id_value is None:
        return None
        
    if DATABASE_TYPE == 'postgresql':
        if isinstance(id_value, UUID):
            return id_value
        return UUID(id_value)
    else:
        return to_str_id(id_value)

def pydantic_document_to_sqlalchemy(doc: pydantic_models.Document) -> db_models.Document:
    """Convert Pydantic Document to SQLAlchemy Document"""
    # Handle UUID conversion based on database type
    doc_id = to_uuid_or_str(doc.id)
    
    sqlalchemy_doc = db_models.Document(
        id=doc_id,
        filename=doc.filename,
        content=doc.content,
        doc_metadata=doc.metadata,  # Note the attribute name change
        folder=doc.folder,
        uploaded=doc.uploaded,
        processing_status=doc.metadata.get("processing_status", "pending"),
        processing_strategy=doc.metadata.get("processing_strategy", None),
        file_size=doc.metadata.get("file_size", None),
        file_type=doc.metadata.get("file_type", None),
        last_accessed=doc.metadata.get("last_accessed", doc.uploaded)
    )
    
    # Convert chunks if available
    if hasattr(doc, 'chunks') and doc.chunks:
        sqlalchemy_doc.chunks = [
            pydantic_chunk_to_sqlalchemy(chunk, doc_id) 
            for chunk in doc.chunks
        ]
    
    return sqlalchemy_doc

def sqlalchemy_document_to_pydantic(doc: db_models.Document) -> pydantic_models.Document:
    """Convert SQLAlchemy Document to Pydantic Document"""
    pydantic_doc = pydantic_models.Document(
        id=to_str_id(doc.id),
        filename=doc.filename,
        content=doc.content,
        metadata=doc.doc_metadata,  # Note the attribute name change
        folder=doc.folder,
        uploaded=doc.uploaded
    )
    
    # Convert chunks if available
    if hasattr(doc, 'chunks') and doc.chunks:
        pydantic_doc.chunks = [
            sqlalchemy_chunk_to_pydantic(chunk) 
            for chunk in doc.chunks
        ]
    
    # Convert tags if available
    if hasattr(doc, 'tags') and doc.tags:
        pydantic_doc.tags = [tag.name for tag in doc.tags]
    
    return pydantic_doc

def pydantic_chunk_to_sqlalchemy(chunk: pydantic_models.Chunk, document_id: Union[str, UUID]) -> db_models.Chunk:
    """Convert Pydantic Chunk to SQLAlchemy Chunk"""
    # Handle UUID conversion based on database type
    chunk_id = to_uuid_or_str(chunk.id)
    doc_id = to_uuid_or_str(document_id)
    
    # Extract index from metadata or default to 0
    index = chunk.metadata.get('index', 0) if chunk.metadata else 0
    
    sqlalchemy_chunk = db_models.Chunk(
        id=chunk_id,
        document_id=doc_id,
        content=chunk.content,
        chunk_metadata=chunk.metadata,  # Note the attribute name change
        index=index,
        embedding_quality=chunk.metadata.get('embedding_quality', None) if chunk.metadata else None
    )
    return sqlalchemy_chunk

def sqlalchemy_chunk_to_pydantic(chunk: db_models.Chunk) -> pydantic_models.Chunk:
    """Convert SQLAlchemy Chunk to Pydantic Chunk"""
    pydantic_chunk = pydantic_models.Chunk(
        id=to_str_id(chunk.id),
        content=chunk.content,
        metadata=chunk.chunk_metadata  # Note the attribute name change
    )
    
    # Add embedding if available
    if hasattr(chunk, 'embedding') and chunk.embedding:
        pydantic_chunk.embedding = chunk.embedding
    
    return pydantic_chunk

def is_sqlalchemy_model(obj: Any) -> bool:
    """Check if an object is a SQLAlchemy model"""
    return hasattr(obj, '_sa_instance_state')

def is_pydantic_model(obj: Any) -> bool:
    """Check if an object is a Pydantic model"""
    return hasattr(obj, '__fields__')
```

### Phase 2: Update Database Models to Use JSONB

Update `app/db/models.py` to use JSONB for metadata fields:

```python
from sqlalchemy import (
    Column, Integer, String, Text, Float, Boolean, 
    DateTime, ForeignKey, Table, Index, UniqueConstraint
)
from sqlalchemy.dialects.postgresql import UUID as PostgresUUID, JSONB
from sqlalchemy.orm import relationship, backref

from app.db.session import Base
from app.core.config import DATABASE_TYPE

# Use appropriate types based on database type
if DATABASE_TYPE == 'postgresql':
    UUID = PostgresUUID
    JSONType = JSONB  # Use JSONB for PostgreSQL
else:
    # For SQLite and other databases
    UUID = String
    JSONType = JSON

# Update Document model
class Document(Base):
    """Document model for database"""
    __tablename__ = "documents"

    id = Column(UUID(as_uuid=True) if DATABASE_TYPE == 'postgresql' else UUID, primary_key=True, default=uuid.uuid4)
    filename = Column(String, nullable=False)
    content = Column(Text, nullable=True)
    doc_metadata = Column('metadata', JSONType, default={})  # Use JSONType
    folder = Column(String, ForeignKey('folders.path'), default="/")
    uploaded = Column(DateTime, default=datetime.utcnow)
    processing_status = Column(String, default="pending")
    processing_strategy = Column(String, nullable=True)
    file_size = Column(Integer, nullable=True)
    file_type = Column(String, nullable=True)
    last_accessed = Column(DateTime, default=datetime.utcnow)

    # Relationships
    chunks = relationship("Chunk", back_populates="document", cascade="all, delete-orphan")
    tags = relationship("Tag", secondary=document_tags, back_populates="documents")
    folder_rel = relationship("Folder", back_populates="documents")
    citations = relationship("Citation", back_populates="document")

# Update Chunk model
class Chunk(Base):
    """Chunk model for database"""
    __tablename__ = "chunks"

    id = Column(UUID(as_uuid=True) if DATABASE_TYPE == 'postgresql' else UUID, primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True) if DATABASE_TYPE == 'postgresql' else UUID, ForeignKey('documents.id'), nullable=False)
    content = Column(Text, nullable=False)
    chunk_metadata = Column('metadata', JSONType, default={})  # Use JSONType
    index = Column(Integer, nullable=False)
    embedding_quality = Column(Float, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    document = relationship("Document", back_populates="chunks")
    citations = relationship("Citation", back_populates="chunk")
```

### Phase 3: Create Alembic Migration for JSONB

Create a new migration to update the metadata columns to use JSONB:

```python
"""Update metadata columns to use JSONB

Revision ID: xxxx
Revises: yyyy
Create Date: 2025-03-20

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB

# revision identifiers, used by Alembic.
revision = 'xxxx'
down_revision = 'yyyy'
branch_labels = None
depends_on = None

def upgrade():
    # For PostgreSQL, convert JSON to JSONB
    if op.get_bind().dialect.name == 'postgresql':
        # Update documents table
        op.alter_column('documents', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        # Update chunks table
        op.alter_column('chunks', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        # Update other tables with metadata columns
        op.alter_column('conversations', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        op.alter_column('processing_jobs', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')
        
        op.alter_column('background_tasks', 'metadata', 
                        type_=JSONB, 
                        postgresql_using='metadata::jsonb')

def downgrade():
    # For PostgreSQL, convert JSONB back to JSON
    if op.get_bind().dialect.name == 'postgresql':
        # Update documents table
        op.alter_column('documents', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        # Update chunks table
        op.alter_column('chunks', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        # Update other tables with metadata columns
        op.alter_column('conversations', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        op.alter_column('processing_jobs', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
        
        op.alter_column('background_tasks', 'metadata', 
                        type_=sa.JSON, 
                        postgresql_using='metadata::json')
```

### Phase 4: Update Document Repository

Enhance `app/db/repositories/document_repository.py` to handle both model types:

```python
from typing import List, Optional, Dict, Any, Union
from uuid import UUID
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import func, or_, and_
import logging

from app.db.models import Document as DBDocument, Chunk as DBChunk, Tag, Folder, document_tags
from app.models.document import Document as PydanticDocument, Chunk as PydanticChunk
from app.db.repositories.base import BaseRepository
from app.db.adapters import (
    pydantic_document_to_sqlalchemy,
    sqlalchemy_document_to_pydantic,
    pydantic_chunk_to_sqlalchemy,
    to_uuid_or_str,
    is_sqlalchemy_model
)
from app.core.config import DATABASE_TYPE

logger = logging.getLogger("app.db.repositories.document_repository")

class DocumentRepository(BaseRepository[DBDocument]):
    """
    Repository for Document model
    """
    
    def __init__(self, session: Session):
        super().__init__(session, DBDocument)
    
    def create_document(self, 
                       filename: str, 
                       content: Optional[str] = None, 
                       metadata: Optional[Dict[str, Any]] = None, 
                       tags: Optional[List[str]] = None, 
                       folder: str = "/") -> PydanticDocument:
        """
        Create a new document
        
        Args:
            filename: Document filename
            content: Document content
            metadata: Document metadata
            tags: List of tag names
            folder: Document folder path
            
        Returns:
            Created document (Pydantic model)
        """
        try:
            # Ensure folder exists
            self._ensure_folder_exists(folder)
            
            # Create document
            document = DBDocument(
                filename=filename,
                content=content,
                doc_metadata=metadata or {},  # Changed to doc_metadata
                folder=folder,
                uploaded=datetime.utcnow(),
                processing_status="pending"
            )
            
            self.session.add(document)
            self.session.flush()  # Flush to get the document ID
            
            # Add tags if provided
            if tags:
                self._add_tags_to_document(document, tags)
            
            # Update folder document count
            folder_obj = self.session.query(Folder).filter(Folder.path == folder).first()
            if folder_obj:
                folder_obj.document_count += 1
            
            self.session.commit()
            self.session.refresh(document)
            
            # Convert to Pydantic model
            return sqlalchemy_document_to_pydantic(document)
        except Exception as e:
            self.session.rollback()
            logger.error(f"Error creating document: {str(e)}")
            raise
    
    def save_document_with_chunks(self, document: Union[PydanticDocument, DBDocument]) -> PydanticDocument:
        """
        Save a document with its chunks
        
        Args:
            document: Document to save (Pydantic or SQLAlchemy)
            
        Returns:
            Saved document (Pydantic)
        """
        try:
            # Convert to SQLAlchemy model if needed
            if not is_sqlalchemy_model(document):
                db_document = pydantic_document_to_sqlalchemy(document)
            else:
                db_document = document
            
            # Check if document exists
            existing = self.session.query(DBDocument).filter(
                DBDocument.id == db_document.id
            ).first()
            
            if existing:
                # Update existing document
                existing.filename = db_document.filename
                existing.content = db_document.content
                existing.doc_metadata = db_document.doc_metadata
                existing.folder = db_document.folder
                existing.last_accessed = datetime.utcnow()
                existing.processing_status = db_document.processing_status
                existing.processing_strategy = db_document.processing_strategy
                
                # Delete existing chunks
                self.session.query(DBChunk).filter(
                    DBChunk.document_id == existing.id
                ).delete()
                
                # Add new chunks
                if hasattr(db_document, 'chunks') and db_document.chunks:
                    for chunk in db_document.chunks:
                        chunk.document_id = existing.id
                        self.session.add(chunk)
                        
                target_document = existing
            else:
                # Add new document
                self.session.add(db_document)
                
                # Add chunks
                if hasattr(db_document, 'chunks') and db_document.chunks:
                    for chunk in db_document.chunks:
                        chunk.document_id = db_document.id
                        self.session.add(chunk)
                        
                target_document = db_document
            
            # Commit changes
            self.session.commit()
            
            # Refresh and convert back to Pydantic
            self.session.refresh(target_document)
            return sqlalchemy_document_to_pydantic(target_document)
        except Exception as e:
            self.session.rollback()
            logger.error(f"Error saving document: {str(e)}")
            raise
    
    def get_document_for_processing(self, document_id: Union[UUID, str]) -> Optional[PydanticDocument]:
        """
        Get a document for processing
        
        Args:
            document_id: Document ID
            
        Returns:
            Document if found, None otherwise
        """
        try:
            # Convert UUID to string for SQLite
            doc_id = to_uuid_or_str(document_id)
            
            document = self.session.query(DBDocument).filter(
                DBDocument.id == doc_id
            ).first()
            
            if not document:
                return None
            
            # Update processing status
            document.processing_status = "processing"
            document.last_accessed = datetime.utcnow()
            self.session.commit()
            
            # Convert to Pydantic model
            return sqlalchemy_document_to_pydantic(document)
        except Exception as e:
            self.session.rollback()
            logger.error(f"Error getting document for processing: {str(e)}")
            raise
    
    def update_document_processing_status(
        self, 
        document_id: Union[UUID, str], 
        status: str, 
        chunks: Optional[List[PydanticChunk]] = None
    ) -> Optional[PydanticDocument]:
        """
        Update document processing status and add chunks
        
        Args:
            document_id: Document ID
            status: New processing status
            chunks: Document chunks
            
        Returns:
            Updated document if found, None otherwise
        """
        try:
            # Convert UUID to string for SQLite
            doc_id = to_uuid_or_str(document_id)
            
            document = self.session.query(DBDocument).filter(
                DBDocument.id == doc_id
            ).first()
            
            if not document:
                return None
            
            # Update status
            document.processing_status = status
            document.last_accessed = datetime.utcnow()
            
            # Add chunks if provided
            if chunks:
                # Delete existing chunks
                self.session.query(DBChunk).filter(
                    DBChunk.document_id == doc_id
                ).delete()
                
                # Add new chunks
                for i, chunk in enumerate(chunks):
                    db_chunk = pydantic_chunk_to_sqlalchemy(chunk, doc_id)
                    db_chunk.index = i
                    self.session.add(db_chunk)
            
            self.session.commit()
            self.session.refresh(document)
            
            # Convert to Pydantic model
            return sqlalchemy_document_to_pydantic(document)
        except Exception as e:
            self.session.rollback()
            logger.error(f"Error updating document processing status: {str(e)}")
            raise
```

### Phase 5: Create Performance Tests

Create a new file `tests/performance/test_document_processing.py` to verify performance requirements:

```python
import pytest
import os
import time
import asyncio
import tempfile
import shutil
import uuid
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document
from app.core.config import UPLOAD_DIR
from app.db.repositories.document_repository import DocumentRepository
from app.db.session import SessionLocal

# Helper function to create a test file of a given size
def create_test_file(file_path, size_mb, content_type="text"):
    """Create a test file of the specified size and type"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    with open(file_path, "wb") as f:
        if content_type == "text":
            # Generate text content
            paragraph = "This is a test paragraph for document processing performance testing. " * 100
            paragraphs_needed = (size_mb * 1024 * 1024) // len(paragraph.encode('utf-8'))
            content = "\n\n".join([paragraph for _ in range(paragraphs_needed)])
            f.write(content.encode('utf-8'))
        elif content_type == "pdf":
            # For a real test, you'd generate an actual PDF
            # This is a simplified version that creates a binary file
            f.write(b"%PDF-1.7\n")  # PDF header
            # Generate random content to fill the file
            remaining_size = size_mb * 1024 * 1024 - 8  # Subtract header size
            f.write(os.urandom(remaining_size))

@pytest.fixture
def temp_upload_dir():
    """Create a temporary directory for uploads"""
    temp_dir = tempfile.mkdtemp()
    original_upload_dir = UPLOAD_DIR
    
    # Temporarily override the upload directory
    import app.core.config
    app.core.config.UPLOAD_DIR = temp_dir
    
    yield temp_dir
    
    # Restore the original upload directory and clean up
    app.core.config.UPLOAD_DIR = original_upload_dir
    shutil.rmtree(temp_dir)

@pytest.fixture
def document_repository():
    """Create a document repository for testing"""
    session = SessionLocal()
    try:
        yield DocumentRepository(session)
    finally:
        session.close()

@pytest.mark.asyncio
async def test_text_file_processing_performance(temp_upload_dir, document_repository):
    """Test processing performance for a 1MB text file"""
    # Create a 1MB text file
    document_id = str(uuid.uuid4())
    document_dir = os.path.join(temp_upload_dir, document_id)
    os.makedirs(document_dir, exist_ok=True)
    
    file_name = "test_1mb.txt"
    file_path = os.path.join(document_dir, file_name)
    create_test_file(file_path, 1, "text")
    
    # Create a Document object
    document = Document(
        id=document_id,
        filename=file_name,
        content="",  # Content will be read during processing
        metadata={"file_type": "txt"},
        folder="/test"
    )
    
    # Create a DocumentProcessor
    processor = DocumentProcessor()
    
    # Measure processing time
    start_time = time.time()
    processed_doc = await processor.process_document(document)
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"Text file (1MB) processing time: {processing_time:.2f} seconds")
    print(f"Number of chunks: {len(processed_doc.chunks)}")
    
    # Assert performance
    assert processing_time < 5, f"Text file processing took too long: {processing_time:.2f}s > 5s"
    assert len(processed_doc.chunks) > 0, "No chunks were created"

@pytest.mark.asyncio
async def test_pdf_file_processing_performance(temp_upload_dir, document_repository):
    """Test processing performance for a 10MB PDF file"""
    # Create a 10MB PDF file
    document_id = str(uuid.uuid4())
    document_dir = os.path.join(temp_upload_dir, document_id)
    os.makedirs(document_dir, exist_ok=True)
    
    file_name = "test_10mb.pdf"
    file_path = os.path.join(document_dir, file_name)
    create_test_file(file_path, 10, "pdf")
    
    # Create a Document object
    document = Document(
        id=document_id,
        filename=file_name,
        content="",  # Content will be read during processing
        metadata={"file_type": "pdf"},
        folder="/test"
    )
    
    # Create a DocumentProcessor
    processor = DocumentProcessor()
    
    # Measure processing time
    start_time = time.time()
    processed_doc = await processor.process_document(document)
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"PDF file (10MB) processing time: {processing_time:.2f} seconds")
    print(f"Number of chunks: {len(processed_doc.chunks)}")
    
    # Assert performance
    assert processing_time < 30, f"PDF processing took too long: {processing_time:.2f}s > 30s"
    assert len(processed_doc.chunks) > 0, "No chunks were created"

@pytest.mark.asyncio
async def test_concurrent_processing_performance(temp_upload_dir, document_repository):
    """Test concurrent processing of multiple documents"""
    # Create 5 documents of different sizes
    document_ids = []
    documents = []
    
    for i, size in enumerate([0.5, 1, 2, 3, 4]):  # Sizes in MB
        document_id = str(uuid.uuid4())
        document_ids.append(document_id)
        
        document_dir = os.path.join(temp_upload_dir, document_id)
        os.makedirs(document_dir, exist_ok=True)
        
        file_name = f"test_concurrent_{i}.txt"
        file_path = os.path.join(document_dir, file_name)
        create_test_file(file_path, size, "text")
        
        document = Document(
            id=document_id,
            filename=file_name,
            content="",
            metadata={"file_type": "txt"},
            folder="/test"
        )
        documents.append(document)
    
    # Create a DocumentProcessor
    processor = DocumentProcessor()
    
    # Process documents concurrently
    start_time = time.time()
    tasks = [processor.process_document(doc) for doc in documents]
    processed_docs = await asyncio.gather(*tasks)
    end_time = time.time()
    total_processing_time = end_time - start_time
    
    print(f"Concurrent processing time for 5 documents: {total_processing_time:.2f} seconds")
    print(f"Average time per document: {total_processing_time / 5:.2f} seconds")
    
    # Assert performance
    assert total_processing_time < 15, f"Concurrent processing took too long: {total_processing_time:.2f}s > 15s"
    assert all(len(doc.chunks) > 0 for doc in processed_docs), "Some documents have no chunks"
```

### Phase 6: Update Document Processor

Modify `app/rag/document_processor.py` to handle metadata field differences:

```python
async def process_document(self, document: Document) -> Document:
    """
    Process a document by splitting it into chunks
    """
    try:
        logger.info(f"Processing document: {document.filename}")
        
        # Convert document ID to string if it's a UUID
        document_id_str = to_str_id(document.id)
        
        # Get the document path
        file_path = os.path.join(UPLOAD_DIR, document_id_str, document.filename)
        
        # Get file extension for specialized handling
        _, ext = os.path.splitext(file_path.lower())
        
        # Use Chunking Judge if enabled
        if USE_CHUNKING_JUDGE:
            logger.info(f"Using Chunking Judge to analyze document: {document.filename}")
            chunking_judge = ChunkingJudge()
            try:
                analysis_result = await chunking_judge.analyze_document(document)
                
                # Update chunking strategy and parameters
                self.chunking_strategy = analysis_result["strategy"]
                if "parameters" in analysis_result and "chunk_size" in analysis_result["parameters"]:
                    self.chunk_size = analysis_result["parameters"]["chunk_size"]
                if "parameters" in analysis_result and "chunk_overlap" in analysis_result["parameters"]:
                    self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
                
                # Store the chunking analysis in document metadata
                # Access metadata attribute directly - works for both Pydantic and SQLAlchemy models
                document.metadata["chunking_analysis"] = analysis_result
                
                logger.info(f"Chunking Judge recommendation: strategy={self.chunking_strategy}, " +
                           f"chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
            except Exception as e:
                logger.error(f"Error using Chunking Judge: {str(e)}")
                # Fall back to DocumentAnalysisService
                logger.info(f"Falling back to DocumentAnalysisService for document: {document.filename}")
                analysis_result = await self.document_analysis_service.analyze_document(document)
                
                # Update chunking strategy and parameters
                self.chunking_strategy = analysis_result["strategy"]
                if "parameters" in analysis_result and "chunk_size" in analysis_result["parameters"]:
                    self.chunk_size = analysis_result["parameters"]["chunk_size"]
                if "parameters" in analysis_result and "chunk_overlap" in analysis_result["parameters"]:
                    self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
                
                # Store the document analysis in document metadata
                document.metadata["document_analysis"] = analysis_result
        else:
            # Use DocumentAnalysisService if Chunking Judge is disabled
            logger.info(f"Chunking Judge disabled, using DocumentAnalysisService for document: {document.filename}")
            analysis_result = await self.document_analysis_service.analyze_document(document)
            
            # Update chunking strategy and parameters
            self.chunking_strategy = analysis_result["strategy"]
            if "parameters" in analysis_result and "chunk_size" in analysis_result["parameters"]:
                self.chunk_size = analysis_result["parameters"]["chunk_size"]
            if "parameters" in analysis_result and "chunk_overlap" in analysis_result["parameters"]:
                self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
            
            # Store the document analysis in document metadata
            document.metadata["document_analysis"] = analysis_result
            
            logger.info(f"Document analysis recommendation: strategy={self.chunking_strategy}, " +
                       f"chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
        
        # Get appropriate text splitter for this file type
        self.text_splitter = self._get_text_splitter(ext)
        
        # Extract text from the document based on file type
        docs = await self._load_document(file_path)
        
        # Split the document into chunks
        chunks = self._split_document(docs)
        
        # Update the document with chunks
        document.chunks = []
        for i, chunk in enumerate(chunks):
            # Start with the chunk's existing metadata
            metadata = dict(chunk.metadata) if chunk.metadata else {}
            
            # Add document metadata
            metadata.update({
                "document_id": document_id_str,
                "index": i,
                "folder": document.folder
            })
            
            # Handle tags specially - store as string for ChromaDB compatibility
            if hasattr(document, 'tags') and document.tags:
                metadata["tags_list"] = document.tags
                metadata["tags"] = ",".join(document.tags)
            else:
                metadata["tags"] = ""
                metadata["tags_list"] = []
            
            # Create the chunk with processed metadata
            document.chunks.append(
                Chunk(
                    id=str(uuid.uuid4()),  # Generate a new UUID for each chunk
                    content=chunk.page_content,
                    metadata=metadata
                )
            )
        
        logger.info(f"Document processed into {len(document.chunks)} chunks")
        
        return document
    except FileNotFoundError as e:
        logger.error(f"File not found: {str(e)}")
        raise
    except IOError as e:
        logger.error(f"IO error processing document {document.filename}: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"Error processing document {document.filename}: {str(e)}")
        raise
```

## Implementation Timeline and Milestones

### Week 1: Core Architecture Updates
- Create adapter functions in `app/db/adapters.py`
- Update database models to use JSONB for PostgreSQL
- Create Alembic migration for JSONB conversion
- Update document repository to handle both model types
- Add comprehensive error handling and logging

### Week 2: Document Processing Updates
- Update document processor to handle metadata field differences
- Update document analysis service to handle metadata field differences
- Implement proper error handling and logging
- Add performance optimizations

### Week 3: Testing and Performance Optimization
- Create performance tests for 1MB text and 10MB PDF files
- Create tests for concurrent document processing
- Optimize document processing for performance
- Add comprehensive documentation

### Week 4: Integration and Deployment
- Integrate all components and ensure they work together
- Deploy to PostgreSQL database
- Verify performance in production environment
- Add monitoring and telemetry

## Performance Requirements

- Process a 1MB text file in under 5 seconds
- Process a 10MB PDF in under 30 seconds
- Handle concurrent processing of at least 10 documents without significant performance degradation

## Conclusion

This implementation plan addresses the model compatibility issues between Pydantic and SQLAlchemy models, focusing on fixing the architecture first to ensure a robust foundation for the system. The use of standalone adapter functions, JSONB for metadata, and comprehensive performance testing will ensure that the system meets the requirements and is maintainable in the long term.

================
File: docs/implementation/metis_rag_implementation_checklist.md
================
# Metis_RAG Implementation Checklist

## Phase 1: Database Integration (Weeks 1-2)

### Week 1: Database Setup and Schema Design
- [x] Choose PostgreSQL for both development and production
- [x] Create database connection module with connection pooling
- [x] Implement SQLAlchemy models:
  - [x] Document model
  - [x] Chunk model
  - [x] Tag model
  - [x] Folder model
  - [x] Conversation model
  - [x] Message model
  - [x] Citation model
  - [x] ProcessingJob model
  - [x] AnalyticsQuery model
- [x] Create Alembic migration scripts for schema versioning
- [x] Add database initialization to application startup

### Week 2: Repository Implementation and API Updates
- [x] Implement DocumentRepository with CRUD operations
- [x] Implement ConversationRepository with message management
- [x] Implement AnalyticsRepository with query logging
- [x] Add mem0 integration to repositories
- [ ] Update document API endpoints to use database
- [ ] Update chat API endpoints to use database
- [ ] Update analytics API endpoints to use database
- [ ] Add pagination, filtering, and sorting to list endpoints

## Phase 2: Intelligent Document Processing (Weeks 3-4)

### Week 3: Document Analysis Service
- [x] Implement DocumentAnalysisService class
- [x] Create detailed prompts for document analysis
- [x] Implement document structure analysis
- [x] Implement content type identification
- [x] Implement chunking strategy recommendation
- [x] Implement parameter optimization
- [x] Update DocumentProcessor to use DocumentAnalysisService
- [x] Add detailed logging and telemetry

### Week 4: Batch Processing System
- [x] Implement ProcessingJob model
- [x] Create WorkerPool for parallel processing
- [x] Implement job queue management
- [x] Add job status tracking with progress information
- [x] Create API endpoint for submitting processing jobs
- [x] Implement job cancellation capability
- [x] Create job history endpoint with filtering
- [x] Add error handling and recovery mechanisms

## Phase 3: Agentic Capabilities Foundation (Weeks 5-6)

### Week 5: Tool Interface and Registry
- [x] Define Tool abstract base class
- [x] Implement ToolRegistry for managing tools
- [x] Implement RAGTool for retrieving information
- [x] Implement CalculatorTool for calculations
- [x] Implement DatabaseTool for structured data queries
- [x] Add comprehensive logging for tool usage
- [x] Create tool documentation with examples
- [x] Add unit tests for all tools

### Week 6: Query Analysis and Logging
- [x] Implement QueryAnalyzer class
- [x] Create prompts for query complexity analysis
- [x] Implement query classification (simple vs. complex)
- [x] Implement tool requirement identification
- [x] Create ProcessLogger for comprehensive logging
- [x] Implement process step tracking
- [x] Add audit trail capabilities
- [x] Create API endpoints for query analysis

## Phase 4: Planning and Execution (Weeks 7-8)

### Week 7: Query Planner and Plan Executor
- [x] Implement QueryPlanner class
- [x] Create prompts for plan generation
- [x] Implement step sequencing logic
- [x] Create PlanExecutor for executing multi-step plans
- [x] Implement input/output handling between steps
- [x] Add error handling and recovery
- [x] Create API endpoints for plan management
- [x] Add unit tests for planning and execution

### Week 8: LangGraph Integration
- [x] Define LangGraph state models:
  - [x] QueryAnalysisState
  - [x] RetrievalState
  - [x] GenerationState
  - [x] RAGState
- [x] Implement state graph construction
- [x] Create node functions for each stage
- [x] Implement conditional edges for adaptive workflows
- [x] Add state transition logging
- [x] Create API endpoints for LangGraph RAG
- [x] Add integration tests for state machine
- [x] Implement streaming response support

## Phase 5: Response Quality (Weeks 9-10)

### Week 9: Response Synthesizer and Evaluator
- [x] Implement ResponseSynthesizer class
- [x] Create prompts for response synthesis
- [x] Implement context assembly optimization
- [x] Create ResponseEvaluator for quality assessment
- [x] Implement factual accuracy checking
- [x] Implement completeness evaluation
- [x] Implement relevance scoring
- [x] Add metrics for response quality
- [x] Add source attribution and citation tracking
- [x] Implement used sources extraction

### Week 10: Response Refiner and Audit Report Generator
- [x] Implement ResponseRefiner class
- [x] Create prompts for response refinement
- [x] Implement hallucination detection
- [x] Create AuditReportGenerator class
- [x] Implement information source tracking
- [x] Implement reasoning trace extraction
- [x] Create verification status determination
- [x] Add API endpoints for audit reports
- [x] Implement visualization for audit trails
- [x] Create ResponseQualityPipeline for end-to-end quality processing
- [x] Add configurable quality thresholds and refinement iterations
- [x] Implement LLM-based process analysis for audit reports
- [x] Add execution timeline tracking

### Additional LangGraph Integration for Response Quality
- [x] Define additional LangGraph state models:
  - [x] ResponseEvaluationState
  - [x] ResponseRefinementState
  - [x] AuditReportState
- [x] Update RAGStage enum with new stages
- [x] Create comprehensive documentation for response quality components
- [x] Add unit tests for response quality components
- [x] Implement integration tests for response quality pipeline

## Phase 6: Performance Optimization (Weeks 11-12)

### Week 11: Caching Implementation
- [x] Implement Cache interface
- [x] Create VectorSearchCache for search results
- [x] Create DocumentCache for document content
- [x] Create LLMResponseCache for LLM responses
- [x] Add disk-based persistence for caches
- [x] Implement cache invalidation strategies
- [x] Add cache statistics and monitoring
- [x] Optimize cache key generation

### Week 12: Background Task System
- [x] Implement TaskManager for background processing
- [x] Add resource monitoring
- [x] Implement adaptive scheduling based on load
- [x] Create task prioritization system
- [x] Implement task dependencies
- [x] Add comprehensive performance testing
- [x] Create performance dashboards
- [x] Implement system health monitoring

## Testing Infrastructure
### Unit Tests
- [x] Create test fixtures for database testing
- [x] Implement repository unit tests
- [x] Create service unit tests
- [x] Implement tool unit tests
- [x] Add state machine unit tests
- [x] Create cache unit tests
- [x] Implement task manager unit tests
- [x] Create response quality unit tests
- [x] Implement audit report unit tests
- [x] Implement audit report unit tests

### Integration Tests
- [x] Create API integration tests
- [x] Implement end-to-end workflow tests
- [x] Add LangGraph integration tests
- [x] Create document processing integration tests
- [x] Implement agentic workflow tests
- [x] Create response quality integration tests
- [x] Implement RAG pipeline integration tests

### Performance Tests
- [ ] Create query performance tests
- [ ] Implement document processing performance tests
- [x] Add cache performance tests
- [x] Create concurrency tests
- [x] Implement load testing

## Deployment

### Docker Configuration
- [ ] Create Dockerfile for application
- [ ] Create docker-compose.yml for local deployment
- [ ] Add PostgreSQL service configuration
- [ ] Add Ollama service configuration
- [x] Add Mem0 service configuration
- [ ] Implement health checks
- [ ] Create volume mounts for persistence
- [ ] Add environment variable configuration
- [ ] Create deployment documentation

================
File: docs/implementation/Metis_RAG_Implementation_Plan_Part1.md
================
# Metis_RAG Implementation Plan

## Overview

This document outlines a comprehensive implementation plan for enhancing the Metis_RAG system with database integration, intelligent document processing, agentic capabilities, and performance optimizations.

```mermaid
graph TD
    A[Current System] --> B[Phase 1: Database Integration]
    B --> C[Phase 2: Intelligent Document Processing]
    C --> D[Phase 3: Agentic Capabilities Foundation]
    D --> E[Phase 4: Planning and Execution]
    E --> F[Phase 5: Response Quality]
    F --> G[Phase 6: Performance Optimization]
```

## Key Decisions and Priorities

Based on project requirements, the following key decisions have been made:

1. **Database**: PostgreSQL for both development and production environments to ensure consistency
2. **Mem0 Integration**: Local Mem0 instance implementation
3. **Testing**: Comprehensive testing throughout development with pytest ecosystem
4. **Deployment**: Primary focus on containerized deployment with Docker, with support for bare metal installations
5. **Performance Targets**: 
   - Simple queries: 6 seconds maximum response time
   - Complex agentic tasks: Few minutes maximum completion time
   - Support for large document collections

## Phase 1: Database Integration (Weeks 1-2)

### Week 1: Database Setup and Schema Design

#### Database Configuration
- **Primary Database**: PostgreSQL for both development and production
- **Configuration**: Environment variable-based configuration for connection parameters
- **Migration Tool**: Alembic for schema migrations and version control

#### Schema Design

```mermaid
erDiagram
    Documents ||--o{ Chunks : contains
    Documents ||--o{ DocumentTags : has
    Tags ||--o{ DocumentTags : used_in
    Documents }|--|| Folders : stored_in
    Conversations ||--o{ Messages : contains
    Messages ||--o{ Citations : references
    Citations }o--|| Documents : cites
    Citations }o--|| Chunks : cites_specific
    ProcessingJobs ||--o{ Documents : processes
    AnalyticsQueries }o--|| Documents : uses
    
    Documents {
        uuid id PK
        string filename
        string content
        jsonb metadata
        string folder
        timestamp uploaded
        string processing_status
        string processing_strategy
        int file_size
        string file_type
        timestamp last_accessed
    }
    
    Chunks {
        uuid id PK
        uuid document_id FK
        string content
        jsonb metadata
        int index
        float embedding_quality
        timestamp created_at
    }
    
    Tags {
        serial id PK
        string name UK
        timestamp created_at
        int usage_count
    }
    
    DocumentTags {
        uuid document_id FK
        int tag_id FK
        timestamp added_at
    }
    
    Folders {
        string path PK
        string name
        string parent_path
        int document_count
        timestamp created_at
    }
    
    Conversations {
        uuid id PK
        timestamp created_at
        timestamp updated_at
        jsonb metadata
        int message_count
    }
    
    Messages {
        serial id PK
        uuid conversation_id FK
        string content
        string role
        timestamp timestamp
        int token_count
    }
    
    Citations {
        serial id PK
        int message_id FK
        uuid document_id FK
        uuid chunk_id FK
        float relevance_score
        string excerpt
        int character_range_start
        int character_range_end
    }
    
    ProcessingJobs {
        uuid id PK
        string status
        timestamp created_at
        timestamp completed_at
        int document_count
        int processed_count
        string strategy
        jsonb metadata
        float progress_percentage
        string error_message
    }
    
    AnalyticsQueries {
        serial id PK
        string query
        string model
        boolean use_rag
        timestamp timestamp
        float response_time_ms
        int token_count
        jsonb document_ids
        string query_type
        boolean successful
    }
```

#### Implementation Tasks
1. Create database connection module with connection pooling and transaction management
2. Implement SQLAlchemy models with proper relationships and indexes
3. Create Alembic migration scripts for schema versioning
4. Add database initialization and connection management to application startup

### Week 2: Repository Implementation and API Updates

#### Repository Classes
1. Implement DocumentRepository with CRUD operations and efficient querying
2. Implement ConversationRepository with message management and citation tracking
3. Implement AnalyticsRepository with query logging and performance metrics
4. Add mem0 integration to repositories for memory-enhanced operations

#### API Updates
1. Update document API endpoints to use database repositories
2. Update chat API endpoints to store conversations in database
3. Update analytics API endpoints to store and retrieve analytics data
4. Add pagination, filtering, and sorting to all list endpoints

================
File: docs/implementation/Metis_RAG_Implementation_Plan_Part2.md
================
## Phase 2: Intelligent Document Processing (Weeks 3-4)

### Week 3: Document Analysis Service

#### Document Analysis Service Implementation
```python
class DocumentAnalysisService:
    """
    Service for analyzing documents and determining optimal processing strategies
    """
    def __init__(self, llm_provider, sample_size=3):
        self.llm_provider = llm_provider
        self.sample_size = sample_size
        self.logger = logging.getLogger("app.services.document_analysis")
        
    async def analyze_document_batch(self, document_ids, file_paths):
        """
        Analyze a batch of documents and recommend a processing strategy
        
        Returns:
            Dict with processing strategy and parameters
        """
        start_time = time.time()
        self.logger.info(f"Starting analysis of {len(document_ids)} documents")
        
        # Sample documents from the batch
        samples = self._sample_documents(document_ids, file_paths)
        
        # Extract representative content from samples
        sample_content = await self._extract_sample_content(samples)
        
        # Use LLM to analyze samples and recommend strategy
        strategy = await self._recommend_strategy(sample_content)
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Analysis completed in {elapsed_time:.2f}s. Strategy: {strategy['strategy']}")
        
        return strategy
```

#### LLM Prompt Design
Create detailed prompts for document analysis with clear instructions for:
- Document structure analysis
- Content type identification
- Chunking strategy recommendation
- Parameter optimization
- Handling of different file types (PDF, TXT, CSV, MD)

#### Integration with DocumentProcessor
1. Update DocumentProcessor to use DocumentAnalysisService
2. Implement strategy selection based on analysis
3. Add detailed logging and telemetry for performance monitoring

### Week 4: Batch Processing System

#### Processing Job Model
```python
class ProcessingJob:
    """
    Model for document processing jobs
    """
    def __init__(self, id, document_ids, strategy=None, status="pending"):
        self.id = id
        self.document_ids = document_ids
        self.strategy = strategy
        self.status = status
        self.created_at = datetime.now()
        self.completed_at = None
        self.document_count = len(document_ids)
        self.processed_count = 0
        self.metadata = {}
        self.progress_percentage = 0
        self.error_message = None
        
    def update_progress(self, processed_count):
        """Update job progress"""
        self.processed_count = processed_count
        self.progress_percentage = (processed_count / self.document_count) * 100 if self.document_count > 0 else 0
        
    def complete(self):
        """Mark job as completed"""
        self.status = "completed"
        self.completed_at = datetime.now()
        self.processed_count = self.document_count
        self.progress_percentage = 100
        
    def fail(self, error_message):
        """Mark job as failed"""
        self.status = "failed"
        self.completed_at = datetime.now()
        self.error_message = error_message
```

#### Worker Pool Implementation
```python
class WorkerPool:
    """
    Pool of workers for processing documents in parallel
    """
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.active_workers = 0
        self.queue = asyncio.Queue()
        self.running = False
        self.logger = logging.getLogger("app.services.worker_pool")
        
    async def start(self):
        """Start the worker pool"""
        self.running = True
        self.logger.info(f"Starting worker pool with {self.max_workers} workers")
        for i in range(self.max_workers):
            asyncio.create_task(self._worker(i))
        
    async def stop(self):
        """Stop the worker pool"""
        self.logger.info("Stopping worker pool")
        self.running = False
        # Wait for queue to empty
        if not self.queue.empty():
            self.logger.info(f"Waiting for {self.queue.qsize()} remaining tasks")
            await self.queue.join()
        
    async def add_job(self, job_func, *args, **kwargs):
        """Add a job to the queue"""
        await self.queue.put((job_func, args, kwargs))
        self.logger.info(f"Added job to queue. Queue size: {self.queue.qsize()}")
        
    async def _worker(self, worker_id):
        """Worker process that executes jobs from the queue"""
        self.logger.info(f"Worker {worker_id} started")
        while self.running:
            try:
                # Get job from queue with timeout
                job_func, args, kwargs = await asyncio.wait_for(
                    self.queue.get(), timeout=1.0
                )
                
                # Execute job
                self.active_workers += 1
                self.logger.info(f"Worker {worker_id} executing job. Active workers: {self.active_workers}")
                try:
                    await job_func(*args, **kwargs)
                except Exception as e:
                    self.logger.error(f"Worker {worker_id} error processing job: {str(e)}")
                finally:
                    self.active_workers -= 1
                    self.queue.task_done()
                    self.logger.info(f"Worker {worker_id} completed job. Active workers: {self.active_workers}")
            except asyncio.TimeoutError:
                # No job available, continue waiting
                pass
            except Exception as e:
                self.logger.error(f"Worker {worker_id} unexpected error: {str(e)}")
```

#### API Endpoints
1. Create endpoint for submitting processing jobs
2. Implement job status tracking with detailed progress information
3. Add job cancellation capability
4. Create job history endpoint with filtering and sorting

## Phase 3: Agentic Capabilities Foundation (Weeks 5-6)

### Week 5: Tool Interface and Registry

#### Tool Interface
```python
class Tool(ABC):
    """
    Abstract base class for tools
    """
    @abstractmethod
    async def execute(self, input_data: Any) -> Any:
        """
        Execute the tool with the given input
        
        Args:
            input_data: Tool-specific input
            
        Returns:
            Tool-specific output
        """
        pass
        
    @abstractmethod
    def get_description(self) -> str:
        """
        Get a description of the tool
        
        Returns:
            Tool description
        """
        pass
        
    @abstractmethod
    def get_input_schema(self) -> Dict[str, Any]:
        """
        Get the input schema for the tool
        
        Returns:
            JSON Schema for tool input
        """
        pass
        
    @abstractmethod
    def get_output_schema(self) -> Dict[str, Any]:
        """
        Get the output schema for the tool
        
        Returns:
            JSON Schema for tool output
        """
        pass
        
    @abstractmethod
    def get_examples(self) -> List[Dict[str, Any]]:
        """
        Get examples of tool usage
        
        Returns:
            List of example input/output pairs
        """
        pass
```

#### Tool Implementations
1. Implement RAGTool for retrieving information using RAG
2. Implement CalculatorTool for performing mathematical calculations
3. Implement DatabaseTool for querying structured data
4. Add comprehensive logging and error handling to all tools

### Week 6: Query Analysis and Logging

#### Query Analyzer
```python
class QueryAnalyzer:
    """
    Analyzes queries to determine their complexity and requirements
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.services.query_analyzer")
        
    async def analyze(self, query: str) -> Dict[str, Any]:
        """
        Analyze a query to determine its complexity and requirements
        
        Returns:
            Dict with keys:
            - complexity: simple or complex
            - requires_tools: list of required tools
            - sub_queries: list of potential sub-queries
            - reasoning: explanation of the analysis
        """
        start_time = time.time()
        self.logger.info(f"Analyzing query: {query}")
        
        prompt = self._create_analysis_prompt(query)
        response = await self.llm_provider.generate(prompt=prompt)
        analysis = self._parse_analysis(response.get("response", ""))
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Query analysis completed in {elapsed_time:.2f}s. Complexity: {analysis.get('complexity')}")
        
        return analysis
```

#### Process Logger
```python
class ProcessLogger:
    """
    Logs the entire query processing workflow
    """
    def __init__(self, db_connection=None):
        self.db_connection = db_connection
        self.process_log = {}
        self.logger = logging.getLogger("app.services.process_logger")
        
    def start_process(self, query_id: str, query: str) -> None:
        """
        Start logging a new process
        
        Args:
            query_id: Unique query ID
            query: User query
        """
        self.logger.info(f"Starting process logging for query {query_id}")
        self.process_log[query_id] = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "steps": [],
            "final_response": None,
            "audit_report": None
        }
        
    def log_step(self, query_id: str, step_name: str, step_data: Dict[str, Any]) -> None:
        """
        Log a step in the process
        
        Args:
            query_id: Query ID
            step_name: Name of the step
            step_data: Data from the step
        """
        if query_id not in self.process_log:
            self.logger.warning(f"Unknown query ID: {query_id}")
            raise ValueError(f"Unknown query ID: {query_id}")
            
        self.logger.info(f"Logging step '{step_name}' for query {query_id}")
        self.process_log[query_id]["steps"].append({
            "step_name": step_name,
            "timestamp": datetime.now().isoformat(),
            "data": step_data
        })
        
        # Save to database if available
        if self.db_connection:
            self._save_to_db(query_id)

================
File: docs/implementation/Metis_RAG_Implementation_Plan_Part3.md
================
## Phase 4: Planning and Execution (Weeks 7-8)

### Week 7: Query Planner and Plan Executor

#### Query Planner
```python
class QueryPlanner:
    """
    Creates execution plans for complex queries
    """
    def __init__(self, llm_provider, tool_registry):
        self.llm_provider = llm_provider
        self.tool_registry = tool_registry
        self.logger = logging.getLogger("app.services.query_planner")
        
    async def create_plan(self, query: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Create an execution plan for a complex query
        
        Returns:
            Dict with keys:
            - steps: list of execution steps
            - tools: list of required tools
            - reasoning: explanation of the plan
        """
        start_time = time.time()
        self.logger.info(f"Creating plan for query: {query}")
        
        available_tools = self.tool_registry.list_tools()
        prompt = self._create_planning_prompt(query, analysis, available_tools)
        response = await self.llm_provider.generate(prompt=prompt)
        plan = self._parse_plan(response.get("response", ""))
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Plan creation completed in {elapsed_time:.2f}s. Steps: {len(plan.get('steps', []))}")
        
        return plan
```

#### Plan Executor
```python
class PlanExecutor:
    """
    Executes query plans
    """
    def __init__(self, tool_registry: ToolRegistry):
        self.tool_registry = tool_registry
        self.logger = logging.getLogger("app.services.plan_executor")
        
    async def execute_plan(self, plan: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        Execute a query plan
        
        Args:
            plan: Query plan
            query: Original query
            
        Returns:
            Dict with execution results
        """
        start_time = time.time()
        self.logger.info(f"Executing plan with {len(plan['steps'])} steps")
        
        results = {}
        
        for step in plan["steps"]:
            step_id = step["step_id"]
            tool_name = step.get("tool")
            step_start_time = time.time()
            
            self.logger.info(f"Executing step {step_id}: {step.get('action')} with tool {tool_name}")
            
            if tool_name:
                # Execute tool
                tool = self.tool_registry.get_tool(tool_name)
                if not tool:
                    error_msg = f"Tool not found: {tool_name}"
                    self.logger.error(error_msg)
                    results[step_id] = {"error": error_msg}
                    continue
                    
                try:
                    tool_input = self._prepare_input(step["input"], results)
                    tool_output = await tool.execute(tool_input)
                    results[step_id] = {"output": tool_output}
                    self.logger.info(f"Step {step_id} completed successfully")
                except Exception as e:
                    error_msg = f"Error executing tool {tool_name}: {str(e)}"
                    self.logger.error(error_msg)
                    results[step_id] = {"error": error_msg}
            else:
                # No tool, just record the action
                results[step_id] = {"action": step["action"]}
                self.logger.info(f"Recorded action for step {step_id}")
                
            step_elapsed_time = time.time() - step_start_time
            self.logger.info(f"Step {step_id} took {step_elapsed_time:.2f}s")
                
        total_elapsed_time = time.time() - start_time
        self.logger.info(f"Plan execution completed in {total_elapsed_time:.2f}s")
        
        return {
            "query": query,
            "plan": plan,
            "results": results,
            "execution_time": total_elapsed_time
        }
```

### Week 8: LangGraph Integration

#### LangGraph State Definitions
```python
class QueryAnalysisState(BaseModel):
    """State for query analysis"""
    query: str
    analysis: Optional[Dict[str, Any]] = None
    
class RetrievalState(BaseModel):
    """State for retrieval operations"""
    query: str
    refined_query: Optional[str] = None
    chunks: List[Dict[str, Any]] = []
    needs_refinement: bool = False
    
class GenerationState(BaseModel):
    """State for response generation"""
    query: str
    context: str
    response: Optional[str] = None
    
class RAGState(BaseModel):
    """Combined state for the RAG process"""
    query: str
    analysis: Optional[Dict[str, Any]] = None
    refined_query: Optional[str] = None
    chunks: List[Dict[str, Any]] = []
    context: Optional[str] = None
    response: Optional[str] = None
    needs_refinement: bool = False
    needs_reranking: bool = False
    execution_trace: List[Dict[str, Any]] = []
```

#### LangGraph Integration
```python
def _build_graph(self) -> StateGraph:
    """
    Build the state graph for the Agentic RAG process
    """
    # Create the state graph
    graph = StateGraph(AgenticRAGState)
    
    # Add nodes for each stage
    graph.add_node(RAGStage.QUERY_ANALYSIS, self._analyze_query)
    graph.add_node(RAGStage.QUERY_PLANNING, self._plan_query)
    graph.add_node(RAGStage.PLAN_EXECUTION, self._execute_plan)
    graph.add_node(RAGStage.RETRIEVAL, self._retrieve_chunks)
    graph.add_node(RAGStage.QUERY_REFINEMENT, self._refine_query)
    graph.add_node(RAGStage.CONTEXT_OPTIMIZATION, self._optimize_context)
    graph.add_node(RAGStage.GENERATION, self._generate_response)
    graph.add_node(RAGStage.RESPONSE_EVALUATION, self._evaluate_response)
    graph.add_node(RAGStage.RESPONSE_REFINEMENT, self._refine_response)
    graph.add_node(RAGStage.COMPLETE, self._finalize_response)
    
    # Define the edges between nodes with conditional routing
    # Start with query analysis
    graph.add_conditional_edges(
        RAGStage.QUERY_ANALYSIS,
        self._needs_planning,
        {
            True: RAGStage.QUERY_PLANNING,
            False: RAGStage.RETRIEVAL
        }
    )
    
    # After planning, execute the plan
    graph.add_edge(RAGStage.QUERY_PLANNING, RAGStage.PLAN_EXECUTION)
    
    # After plan execution, proceed to retrieval
    graph.add_edge(RAGStage.PLAN_EXECUTION, RAGStage.RETRIEVAL)
    
    # After retrieval, decide whether to refine the query or optimize the context
    graph.add_conditional_edges(
        RAGStage.RETRIEVAL,
        self._needs_refinement,
        {
            True: RAGStage.QUERY_REFINEMENT,
            False: RAGStage.CONTEXT_OPTIMIZATION
        }
    )
    
    # After query refinement, go back to retrieval with the refined query
    graph.add_edge(RAGStage.QUERY_REFINEMENT, RAGStage.RETRIEVAL)
    
    # After context optimization, proceed to generation
    graph.add_edge(RAGStage.CONTEXT_OPTIMIZATION, RAGStage.GENERATION)
    
    # After generation, evaluate the response
    graph.add_edge(RAGStage.GENERATION, RAGStage.RESPONSE_EVALUATION)
    
    # After evaluation, decide whether to refine the response or complete
    graph.add_conditional_edges(
        RAGStage.RESPONSE_EVALUATION,
        self._needs_response_refinement,
        {
            True: RAGStage.RESPONSE_REFINEMENT,
            False: RAGStage.COMPLETE
        }
    )
    
    # After response refinement, re-evaluate
    graph.add_edge(RAGStage.RESPONSE_REFINEMENT, RAGStage.RESPONSE_EVALUATION)
    
    # After completion, end the process
    graph.add_edge(RAGStage.COMPLETE, END)
    
    # Set the entry point
    graph.set_entry_point(RAGStage.QUERY_ANALYSIS)
    
    return graph
```

## Phase 5: Response Quality (Weeks 9-10)

### Week 9: Response Synthesizer and Evaluator

#### Response Synthesizer
```python
class ResponseSynthesizer:
    """
    Synthesizes a response from execution results
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.services.response_synthesizer")
        
    async def synthesize(self, execution_result: Dict[str, Any]) -> str:
        """
        Synthesize a response from execution results
        
        Args:
            execution_result: Result of plan execution
            
        Returns:
            Synthesized response
        """
        start_time = time.time()
        self.logger.info("Synthesizing response from execution results")
        
        prompt = self._create_synthesis_prompt(execution_result)
        response = await self.llm_provider.generate(prompt=prompt)
        synthesized_response = response.get("response", "")
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Response synthesis completed in {elapsed_time:.2f}s")
        
        return synthesized_response
```

#### Response Evaluator
```python
class ResponseEvaluator:
    """
    Evaluates response quality
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.services.response_evaluator")
        
    async def evaluate(self, query: str, response: str, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate response quality
        
        Args:
            query: Original query
            response: Synthesized response
            execution_result: Result of plan execution
            
        Returns:
            Dict with evaluation results
        """
        start_time = time.time()
        self.logger.info(f"Evaluating response for query: {query}")
        
        prompt = self._create_evaluation_prompt(query, response, execution_result)
        eval_response = await self.llm_provider.generate(prompt=prompt)
        evaluation = self._parse_evaluation(eval_response.get("response", ""))
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Response evaluation completed in {elapsed_time:.2f}s. Overall score: {evaluation.get('overall_score')}")
        
        return evaluation
```

### Week 10: Response Refiner and Audit Report Generator

#### Response Refiner
```python
class ResponseRefiner:
    """
    Refines responses based on evaluation
    """
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
        self.logger = logging.getLogger("app.services.response_refiner")
        
    async def refine(self, query: str, response: str, evaluation: Dict[str, Any], execution_result: Dict[str, Any]) -> str:
        """
        Refine a response based on evaluation
        
        Args:
            query: Original query
            response: Original response
            evaluation: Evaluation results
            execution_result: Result of plan execution
            
        Returns:
            Refined response
        """
        start_time = time.time()
        self.logger.info(f"Refining response for query: {query}")
        
        prompt = self._create_refinement_prompt(query, response, evaluation, execution_result)
        refined_response_result = await self.llm_provider.generate(prompt=prompt)
        refined_response = refined_response_result.get("response", "")
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Response refinement completed in {elapsed_time:.2f}s")
        
        return refined_response
```

#### Audit Report Generator
```python
class AuditReportGenerator:
    """
    Generates audit reports for query processing
    """
    def __init__(self, process_logger):
        self.process_logger = process_logger
        self.logger = logging.getLogger("app.services.audit_report_generator")
        
    async def generate_report(self, query_id: str) -> Dict[str, Any]:
        """
        Generate an audit report for a query
        
        Args:
            query_id: Query ID
            
        Returns:
            Audit report
        """
        start_time = time.time()
        self.logger.info(f"Generating audit report for query {query_id}")
        
        process_log = self.process_logger.get_process_log(query_id)
        if not process_log:
            error_msg = f"No process log found for query {query_id}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
            
        # Extract information from process log
        query = process_log["query"]
        steps = process_log["steps"]
        final_response = process_log["final_response"]
        
        # Generate report
        report = {
            "query_id": query_id,
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "process_summary": self._generate_process_summary(steps),
            "information_sources": self._extract_information_sources(steps),
            "reasoning_trace": self._extract_reasoning_trace(steps),
            "hallucination_assessment": self._assess_hallucination(steps),
            "verification_status": self._determine_verification_status(steps)
        }
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"Audit report generation completed in {elapsed_time:.2f}s")
        
        return report

================
File: docs/implementation/Metis_RAG_Implementation_Plan_Part4.md
================
## Phase 6: Performance Optimization (Weeks 11-12)

### Week 11: Caching Implementation

#### Cache Interface
```python
class Cache(Generic[T]):
    """
    Generic cache implementation with disk persistence
    """
    def __init__(
        self,
        name: str,
        ttl: int = 3600,
        max_size: int = 1000,
        persist: bool = True,
        persist_dir: str = "data/cache"
    ):
        self.name = name
        self.ttl = ttl
        self.max_size = max_size
        self.persist = persist
        self.persist_dir = persist_dir
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.hits = 0
        self.misses = 0
        self.logger = logging.getLogger(f"app.cache.{name}")
        
        # Create persist directory if needed
        if self.persist:
            os.makedirs(self.persist_dir, exist_ok=True)
            self._load_from_disk()
            self.logger.info(f"Loaded {len(self.cache)} items from disk cache")
    
    def get(self, key: str) -> Optional[T]:
        """Get a value from the cache"""
        if key in self.cache:
            entry = self.cache[key]
            if time.time() - entry["timestamp"] < self.ttl:
                self.hits += 1
                self.logger.debug(f"Cache hit for key: {key}")
                return entry["value"]
            else:
                # Expired, remove from cache
                del self.cache[key]
                self.logger.debug(f"Cache entry expired for key: {key}")
        
        self.misses += 1
        self.logger.debug(f"Cache miss for key: {key}")
        return None
    
    def set(self, key: str, value: T) -> None:
        """Set a value in the cache"""
        self.cache[key] = {
            "value": value,
            "timestamp": time.time()
        }
        self.logger.debug(f"Cache set for key: {key}")
        
        # Prune cache if it gets too large
        if len(self.cache) > self.max_size:
            self._prune()
            
        # Persist to disk if enabled
        if self.persist:
            self._save_to_disk()
```

#### Cache Implementations
1. Implement VectorSearchCache for caching vector search results
2. Implement DocumentCache for caching document content and metadata
3. Implement LLMResponseCache for caching LLM responses
4. Add cache monitoring and statistics collection

### Week 12: Background Task System

#### Background Task System Architecture

The Background Task System consists of several key components:

1. **TaskManager**: Central manager for background tasks
2. **ResourceMonitor**: Monitors system resources and provides adaptive throttling
3. **Scheduler**: Handles task scheduling, prioritization, and dependencies
4. **Task Models**: Represents tasks and their properties
5. **Task Repository**: Handles database operations for tasks

#### Task Models

```python
class TaskStatus(Enum):
    """Task status enum"""
    PENDING = "pending"
    SCHEDULED = "scheduled"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    WAITING = "waiting"

class TaskPriority(Enum):
    """Task priority enum"""
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    CRITICAL = "critical"

class TaskDependency:
    """Task dependency model"""
    def __init__(self, task_id: str, required_status: TaskStatus = TaskStatus.COMPLETED):
        self.task_id = task_id
        self.required_status = required_status

class Task:
    """Task model"""
    def __init__(
        self,
        id: str,
        name: str,
        task_type: str,
        params: Dict[str, Any] = None,
        priority: TaskPriority = TaskPriority.NORMAL,
        dependencies: List[TaskDependency] = None,
        schedule_time: Optional[datetime] = None,
        timeout_seconds: Optional[int] = None,
        max_retries: int = 0,
        metadata: Dict[str, Any] = None
    ):
        self.id = id
        self.name = name
        self.task_type = task_type
        self.params = params or {}
        self.priority = priority
        self.dependencies = dependencies or []
        self.schedule_time = schedule_time
        self.timeout_seconds = timeout_seconds
        self.max_retries = max_retries
        self.metadata = metadata or {}
        self.status = TaskStatus.PENDING
        self.created_at = datetime.now()
        self.scheduled_at = None
        self.started_at = None
        self.completed_at = None
        self.retry_count = 0
        self.result = None
        self.error = None
        self.progress = 0.0
        self.resource_usage = {}
        self.execution_time_ms = None
```

#### Task Manager

```python
class TaskManager:
    """
    Manager for background tasks
    """
    def __init__(
        self,
        max_concurrent_tasks: int = 10,
        resource_monitor: Optional["ResourceMonitor"] = None,
        scheduler: Optional["Scheduler"] = None
    ):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.resource_monitor = resource_monitor or ResourceMonitor()
        self.scheduler = scheduler or Scheduler(self.resource_monitor)
        self.tasks: Dict[str, Task] = {}
        self.running_tasks: Dict[str, asyncio.Task] = {}
        self.task_handlers: Dict[str, Callable[[Task], Awaitable[Any]]] = {}
        self.lock = asyncio.Lock()
        self.running = False
        self.logger = logging.getLogger("app.tasks.task_manager")
        
    async def start(self) -> None:
        """Start the task manager"""
        async with self.lock:
            if self.running:
                return
            
            self.running = True
            self.logger.info("Starting task manager")
            
            # Start resource monitor
            await self.resource_monitor.start()
            
            # Start scheduler
            await self.scheduler.start()
            
            # Start task processing loop
            asyncio.create_task(self._process_tasks())
    
    async def stop(self) -> None:
        """Stop the task manager"""
        async with self.lock:
            if not self.running:
                return
            
            self.running = False
            self.logger.info("Stopping task manager")
            
            # Stop scheduler
            await self.scheduler.stop()
            
            # Stop resource monitor
            await self.resource_monitor.stop()
            
            # Cancel all running tasks
            for task_id, task in self.running_tasks.items():
                task.cancel()
            
            # Wait for tasks to complete
            if self.running_tasks:
                await asyncio.gather(*self.running_tasks.values(), return_exceptions=True)
    
    async def submit(
        self,
        name: str,
        task_type: str,
        params: Dict[str, Any] = None,
        priority: TaskPriority = TaskPriority.NORMAL,
        dependencies: List[TaskDependency] = None,
        schedule_time: Optional[datetime] = None,
        timeout_seconds: Optional[int] = None,
        max_retries: int = 0,
        metadata: Dict[str, Any] = None
    ) -> str:
        """
        Submit a task for execution
        
        Args:
            name: Task name
            task_type: Task type
            params: Task parameters
            priority: Task priority
            dependencies: Task dependencies
            schedule_time: Schedule time
            timeout_seconds: Timeout in seconds
            max_retries: Maximum number of retries
            metadata: Additional metadata
            
        Returns:
            Task ID
        """
        # Validate task type
        if task_type not in self.task_handlers:
            raise ValueError(f"Unknown task type: {task_type}")
        
        # Create task
        task_id = str(uuid.uuid4())
        task = Task(
            id=task_id,
            name=name,
            task_type=task_type,
            params=params,
            priority=priority,
            dependencies=dependencies,
            schedule_time=schedule_time,
            timeout_seconds=timeout_seconds,
            max_retries=max_retries,
            metadata=metadata
        )
        
        # Add task to scheduler
        await self.scheduler.schedule_task(task)
        
        # Add task to tasks dict
        self.tasks[task_id] = task
        
        self.logger.info(f"Submitted task {task_id} of type {task_type}")
        
        return task_id
    
    def register_task_handler(
        self,
        task_type: str,
        handler: Callable[[Task], Awaitable[Any]]
    ) -> None:
        """
        Register a task handler
        
        Args:
            task_type: Task type
            handler: Task handler function
        """
        self.task_handlers[task_type] = handler
        self.logger.info(f"Registered handler for task type: {task_type}")
    
    async def cancel(self, task_id: str) -> bool:
        """
        Cancel a task
        
        Args:
            task_id: Task ID
            
        Returns:
            True if task was cancelled, False otherwise
        """
        # Get task
        task = self.tasks.get(task_id)
        if not task:
            self.logger.warning(f"Cannot cancel task {task_id}: Task not found")
            return False
        
        # Check if task can be cancelled
        if task.status not in (TaskStatus.PENDING, TaskStatus.SCHEDULED, TaskStatus.RUNNING):
            self.logger.warning(f"Cannot cancel task {task_id}: Task is in {task.status} state")
            return False
        
        # Cancel running task
        if task_id in self.running_tasks:
            self.running_tasks[task_id].cancel()
            self.logger.info(f"Cancelled running task {task_id}")
        
        # Update task status
        task.status = TaskStatus.CANCELLED
        task.completed_at = datetime.now()
        
        self.logger.info(f"Cancelled task {task_id}")
        
        return True
    
    def get_task(self, task_id: str) -> Optional[Task]:
        """
        Get a task by ID
        
        Args:
            task_id: Task ID
            
        Returns:
            Task or None if not found
        """
        return self.tasks.get(task_id)
    
    def get_tasks(
        self,
        status: Optional[TaskStatus] = None,
        task_type: Optional[str] = None,
        limit: int = 100,
        offset: int = 0
    ) -> List[Task]:
        """
        Get tasks with optional filtering
        
        Args:
            status: Filter by status
            task_type: Filter by task type
            limit: Maximum number of tasks to return
            offset: Offset for pagination
            
        Returns:
            List of tasks
        """
        # Filter tasks
        filtered_tasks = self.tasks.values()
        
        if status:
            filtered_tasks = [t for t in filtered_tasks if t.status == status]
        
        if task_type:
            filtered_tasks = [t for t in filtered_tasks if t.task_type == task_type]
        
        # Sort tasks by creation time (newest first)
        sorted_tasks = sorted(filtered_tasks, key=lambda t: t.created_at, reverse=True)
        
        # Apply pagination
        paginated_tasks = sorted_tasks[offset:offset + limit]
        
        return paginated_tasks
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get task manager statistics
        
        Returns:
            Statistics dictionary
        """
        # Count tasks by status
        status_counts = {}
        for status in TaskStatus:
            status_counts[status.value] = len([t for t in self.tasks.values() if t.status == status])
        
        # Get resource usage
        resources = {
            "cpu_percent": self.resource_monitor.get_cpu_percent(),
            "memory_percent": self.resource_monitor.get_memory_percent(),
            "disk_percent": self.resource_monitor.get_disk_percent(),
            "system_load": self.resource_monitor.get_system_load()
        }
        
        return {
            "tasks": status_counts,
            "resources": resources,
            "registered_handlers": list(self.task_handlers.keys()),
            "max_concurrent_tasks": self.max_concurrent_tasks,
            "current_concurrent_tasks": len(self.running_tasks)
        }
    
    def get_resource_alerts(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get recent resource alerts
        
        Args:
            limit: Maximum number of alerts to return
            
        Returns:
            List of resource alerts
        """
        return self.resource_monitor.get_recent_alerts(limit)
    
    async def _process_tasks(self) -> None:
        """Process scheduled tasks"""
        while self.running:
            try:
                # Get recommended concurrency based on system load
                recommended_concurrency = self.resource_monitor.get_recommended_concurrency(
                    self.max_concurrent_tasks
                )
                
                # Get next batch of tasks to execute
                available_slots = recommended_concurrency - len(self.running_tasks)
                if available_slots > 0:
                    # Get tasks from scheduler
                    tasks_to_run = await self.scheduler.get_next_tasks(available_slots)
                    
                    # Execute tasks
                    for task in tasks_to_run:
                        asyncio.create_task(self._execute_task(task))
                
                # Wait before checking again
                await asyncio.sleep(0.1)
            except Exception as e:
                self.logger.error(f"Error in task processing loop: {str(e)}")
                await asyncio.sleep(1.0)
    
    async def _execute_task(self, task: Task) -> None:
        """
        Execute a task
        
        Args:
            task: Task to execute
        """
        # Update task status
        task.status = TaskStatus.RUNNING
        task.started_at = datetime.now()
        
        # Get task handler
        handler = self.task_handlers.get(task.task_type)
        if not handler:
            task.status = TaskStatus.FAILED
            task.error = f"No handler registered for task type: {task.task_type}"
            task.completed_at = datetime.now()
            self.logger.error(f"Task {task.id} failed: {task.error}")
            return
        
        # Create task future
        task_future = asyncio.create_task(self._run_task_with_timeout(task, handler))
        self.running_tasks[task.id] = task_future
        
        try:
            # Wait for task to complete
            await task_future
        except asyncio.CancelledError:
            # Task was cancelled
            task.status = TaskStatus.CANCELLED
            task.completed_at = datetime.now()
            self.logger.info(f"Task {task.id} was cancelled")
        except Exception as e:
            # Task failed with exception
            task.status = TaskStatus.FAILED
            task.error = str(e)
            task.completed_at = datetime.now()
            self.logger.error(f"Task {task.id} failed: {str(e)}")
        finally:
            # Remove task from running tasks
            if task.id in self.running_tasks:
                del self.running_tasks[task.id]
    
    async def _run_task_with_timeout(
        self,
        task: Task,
        handler: Callable[[Task], Awaitable[Any]]
    ) -> None:
        """
        Run a task with timeout
        
        Args:
            task: Task to run
            handler: Task handler function
        """
        start_time = time.time()
        
        try:
            # Run task with timeout if specified
            if task.timeout_seconds:
                task.result = await asyncio.wait_for(
                    handler(task),
                    timeout=task.timeout_seconds
                )
            else:
                task.result = await handler(task)
            
            # Update task status
            task.status = TaskStatus.COMPLETED
        except asyncio.TimeoutError:
            # Task timed out
            task.status = TaskStatus.FAILED
            task.error = f"Task timed out after {task.timeout_seconds} seconds"
            self.logger.error(f"Task {task.id} timed out")
        except Exception as e:
            # Task failed
            task.status = TaskStatus.FAILED
            task.error = str(e)
            self.logger.error(f"Task {task.id} failed: {str(e)}")
            
            # Retry if retries are available
            if task.retry_count < task.max_retries:
                task.retry_count += 1
                task.status = TaskStatus.PENDING
                task.error = None
                task.progress = 0.0
                
                # Add exponential backoff
                backoff_seconds = 2 ** task.retry_count
                retry_time = datetime.now() + timedelta(seconds=backoff_seconds)
                
                # Reschedule task
                task.schedule_time = retry_time
                await self.scheduler.schedule_task(task)
                
                self.logger.info(f"Retrying task {task.id} in {backoff_seconds} seconds (retry {task.retry_count}/{task.max_retries})")
                return
        finally:
            # Update task completion time and execution time
            if task.status in (TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED):
                task.completed_at = datetime.now()
                task.execution_time_ms = (time.time() - start_time) * 1000
```

#### Resource Monitor

```python
class ResourceMonitor:
    """
    Monitor system resources
    """
    def __init__(
        self,
        thresholds: Optional["ResourceThreshold"] = None,
        check_interval: float = 5.0
    ):
        self.thresholds = thresholds or ResourceThreshold()
        self.check_interval = check_interval
        self.alerts: List[Dict[str, Any]] = []
        self.running = False
        self.task = None
        self.logger = logging.getLogger("app.tasks.resource_monitor")
    
    async def start(self) -> None:
        """Start resource monitoring"""
        if self.running:
            return
        
        self.running = True
        self.logger.info("Starting resource monitor")
        self.task = asyncio.create_task(self._monitor_resources())
    
    async def stop(self) -> None:
        """Stop resource monitoring"""
        if not self.running:
            return
        
        self.running = False
        self.logger.info("Stopping resource monitor")
        
        if self.task:
            self.task.cancel()
            try:
                await self.task
            except asyncio.CancelledError:
                pass
    
    def get_resource_usage(self) -> Dict[str, float]:
        """
        Get current resource usage
        
        Returns:
            Resource usage dictionary
        """
        return {
            "cpu_percent": self.get_cpu_percent(),
            "memory_percent": self.get_memory_percent(),
            "disk_percent": self.get_disk_percent(),
            "io_wait_percent": self.get_io_wait_percent()
        }
    
    def get_cpu_percent(self) -> float:
        """
        Get CPU usage percentage
        
        Returns:
            CPU usage percentage
        """
        return psutil.cpu_percent()
    
    def get_memory_percent(self) -> float:
        """
        Get memory usage percentage
        
        Returns:
            Memory usage percentage
        """
        return psutil.virtual_memory().percent
    
    def get_disk_percent(self) -> float:
        """
        Get disk usage percentage
        
        Returns:
            Disk usage percentage
        """
        return psutil.disk_usage("/").percent
    
    def get_io_wait_percent(self) -> float:
        """
        Get I/O wait percentage
        
        Returns:
            I/O wait percentage
        """
        cpu_times = psutil.cpu_times_percent()
        return getattr(cpu_times, "iowait", 0.0)
    
    def get_system_load(self) -> float:
        """
        Get system load factor (0.0-1.0)
        
        Returns:
            System load factor
        """
        # Calculate load factor based on CPU, memory, and disk usage
        cpu_load = self.get_cpu_percent() / 100.0
        memory_load = self.get_memory_percent() / 100.0
        disk_load = self.get_disk_percent() / 100.0
        io_wait_load = self.get_io_wait_percent() / 100.0
        
        # Weight the different factors
        weighted_load = (
            cpu_load * 0.4 +
            memory_load * 0.3 +
            disk_load * 0.1 +
            io_wait_load * 0.2
        )
        
        return min(1.0, max(0.0, weighted_load))
    
    def get_recommended_concurrency(self, max_concurrency: int) -> int:
        """
        Get recommended concurrency based on system load
        
        Args:
            max_concurrency: Maximum concurrency
            
        Returns:
            Recommended concurrency
        """
        load = self.get_system_load()
        
        # Calculate recommended concurrency
        if load < 0.5:
            # Low load, use full concurrency
            return max_concurrency
        elif load < 0.7:
            # Medium load, reduce concurrency by 25%
            return max(1, int(max_concurrency * 0.75))
        elif load < 0.9:
            # High load, reduce concurrency by 50%
            return max(1, int(max_concurrency * 0.5))
        else:
            # Very high load, reduce concurrency by 75%
            return max(1, int(max_concurrency * 0.25))
    
    def get_recent_alerts(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get recent resource alerts
        
        Args:
            limit: Maximum number of alerts to return
            
        Returns:
            List of resource alerts
        """
        return sorted(self.alerts, key=lambda a: a["timestamp"], reverse=True)[:limit]
    
    async def _monitor_resources(self) -> None:
        """Monitor system resources"""
        while self.running:
            try:
                # Get resource usage
                usage = self.get_resource_usage()
                
                # Check thresholds
                self._check_thresholds(usage)
                
                # Wait for next check
                await asyncio.sleep(self.check_interval)
            except Exception as e:
                self.logger.error(f"Error monitoring resources: {str(e)}")
                await asyncio.sleep(self.check_interval)
    
    def _check_thresholds(self, usage: Dict[str, float]) -> None:
        """
        Check resource thresholds and generate alerts
        
        Args:
            usage: Resource usage dictionary
        """
        timestamp = time.time()
        
        # Check CPU usage
        if usage["cpu_percent"] > self.thresholds.cpu_percent:
            self._add_alert(
                timestamp=timestamp,
                resource_type="cpu",
                current_value=usage["cpu_percent"],
                threshold=self.thresholds.cpu_percent,
                message=f"CPU usage is high: {usage['cpu_percent']:.1f}% (threshold: {self.thresholds.cpu_percent:.1f}%)"
            )
        
        # Check memory usage
        if usage["memory_percent"] > self.thresholds.memory_percent:
            self._add_alert(
                timestamp=timestamp,
                resource_type="memory",
                current_value=usage["memory_percent"],
                threshold=self.thresholds.memory_percent,
                message=f"Memory usage is high: {usage['memory_percent']:.1f}% (threshold: {self.thresholds.memory_percent:.1f}%)"
            )
        
        # Check disk usage
        if usage["disk_percent"] > self.thresholds.disk_percent:
            self._add_alert(
                timestamp=timestamp,
                resource_type="disk",
                current_value=usage["disk_percent"],
                threshold=self.thresholds.disk_percent,
                message=f"Disk usage is high: {usage['disk_percent']:.1f}% (threshold: {self.thresholds.disk_percent:.1f}%)"
            )
        
        # Check I/O wait
        if usage["io_wait_percent"] > self.thresholds.io_wait_percent:
            self._add_alert(
                timestamp=timestamp,
                resource_type="io_wait",
                current_value=usage["io_wait_percent"],
                threshold=self.thresholds.io_wait_percent,
                message=f"I/O wait is high: {usage['io_wait_percent']:.1f}% (threshold: {self.thresholds.io_wait_percent:.1f}%)"
            )
    
    def _add_alert(
        self,
        timestamp: float,
        resource_type: str,
        current_value: float,
        threshold: float,
        message: str
    ) -> None:
        """
        Add a resource alert
        
        Args:
            timestamp: Alert timestamp
            resource_type: Resource type
            current_value: Current value
            threshold: Threshold value
            message: Alert message
        """
        alert = {
            "timestamp": timestamp,
            "resource_type": resource_type,
            "current_value": current_value,
            "threshold": threshold,
            "message": message
        }
        
        self.alerts.append(alert)
        
        # Limit number of alerts
        if len(self.alerts) > 100:
            self.alerts = self.alerts[-100:]
        
        self.logger.warning(message)
```

#### Scheduler

```python
class Scheduler:
    """
    Task scheduler
    """
    def __init__(self, resource_monitor: Optional["ResourceMonitor"] = None):
        self.resource_monitor = resource_monitor
        self.pending_tasks: Dict[str, Task] = {}
        self.scheduled_tasks: Dict[str, Task] = {}
        self.waiting_tasks: Dict[str, Task] = {}
        self.lock = asyncio.Lock()
        self.running = False
        self.task = None
        self.logger = logging.getLogger("app.tasks.scheduler")
    
    async def start(self) -> None:
        """Start the scheduler"""
        if self.running:
            return
        
        self.running = True
        self.logger.info("Starting scheduler")
        self.task = asyncio.create_task(self._schedule_tasks())
    
    async def stop(self) -> None:
        """Stop the scheduler"""
        if not self.running:
            return
        
        self.running = False
        self.logger.info("Stopping scheduler")
        
        if self.task:
            self.task.cancel()
            try:
                await self.task
            except asyncio.CancelledError:
                pass
    
    async def schedule_task(self, task: Task) -> None:
        """
        Schedule a task
        
        Args:
            task: Task to schedule
        """
        async with self.lock:
            # Check if task has dependencies
            if task.dependencies:
                # Add to waiting tasks
                task.status = TaskStatus.WAITING
                self.waiting_tasks[task.id] = task
                self.logger.info(f"Task {task.id} is waiting for dependencies")
            elif task.schedule_time and task.schedule_time > datetime.now():
                # Add to scheduled tasks
                task.status = TaskStatus.SCHEDULED
                task.scheduled_at = task.schedule_time
                self.scheduled_tasks[task.id] = task
                self.logger.info(f"Task {task.id} scheduled for {task.schedule_time}")
            else:
                # Add to pending tasks
                task.status = TaskStatus.PENDING
                self.pending_tasks[task.id] = task
                self.logger.info(f"Task {task.id} added to pending tasks")
    
    async def get_next_tasks(self, max_tasks: int) -> List[Task]:
        """
        Get next tasks to execute
        
        Args:
            max_tasks: Maximum number of tasks to return
            
        Returns:
            List of tasks to execute
        """
        async with self.lock:
            # Check scheduled tasks
            now = datetime.now()
            for task_id, task in list(self.scheduled_tasks.items()):
                if task.schedule_time <= now:
                    # Move to pending tasks
                    self.pending_tasks[task_id] = task
                    del self.scheduled_tasks[task_id]
                    self.logger.info(f"Scheduled task {task_id} is now pending")
            
            # Check waiting tasks
            for task_id, task in list(self.waiting_tasks.items()):
                # Check if dependencies are satisfied
                dependencies_satisfied = True
                for dependency in task.dependencies:
                    # Get dependency task
                    dependency_task = self._get_task_by_id(dependency.task_id)
                    if not dependency_task:
                        self.logger.warning(f"Dependency task {dependency.task_id} not found for task {task_id}")
                        dependencies_satisfied = False
                        break
                    
                    # Check dependency status
                    if dependency_task.status != dependency.required_status:
                        dependencies_satisfied = False
                        break
                
                if dependencies_satisfied:
                    # Move to pending tasks
                    self.pending_tasks[task_id] = task
                    del self.waiting_tasks[task_id]
                    task.status = TaskStatus.PENDING
                    self.logger.info(f"Waiting task {task_id} dependencies satisfied, now pending")
            
            # Get pending tasks sorted by priority and creation time
            sorted_tasks = sorted(
                self.pending_tasks.values(),
                key=lambda t: (
                    # Sort by priority (higher first)
                    -self._get_priority_value(t.priority),
                    # Then by creation time (older first)
                    t.created_at
                )
            )
            
            # Get tasks to execute
            tasks_to_execute = sorted_tasks[:max_tasks]
            
            # Remove tasks from pending tasks
            for task in tasks_to_execute:
                del self.pending_tasks[task.id]
            
            return tasks_to_execute
    
    def _get_task_by_id(self, task_id: str) -> Optional[Task]:
        """
        Get a task by ID from any task list
        
        Args:
            task_id: Task ID
            
        Returns:
            Task or None if not found
        """
        if task_id in self.pending_tasks:
            return self.pending_tasks[task_id]
        elif task_id in self.scheduled_tasks:
            return self.scheduled_tasks[task_id]
        elif task_id in self.waiting_tasks:
            return self.waiting_tasks[task_id]
        return None
    
    def _get_priority_value(self, priority: TaskPriority) -> int:
        """
        Get numeric priority value
        
        Args:
            priority: Task priority
            
        Returns:
            Numeric priority value
        """
        priority_values = {
            TaskPriority.LOW: 0,
            TaskPriority.NORMAL: 1,
            TaskPriority.HIGH: 2,
            TaskPriority.CRITICAL: 3
        }
        return priority_values.get(priority, 1)
    
    async def _schedule_tasks(self) -> None:
        """Schedule tasks periodically"""
        while self.running:
            try:
                # Check scheduled tasks
                now = datetime.now()
                async with self.lock:
                    for task_id, task in list(self.scheduled_tasks.items()):
                        if task.schedule_time <= now:
                            # Move to pending tasks
                            self.pending_tasks[task_id] = task
                            del self.scheduled_tasks[task_id]
                            self.logger.info(f"Scheduled task {task_id} is now pending")
                
                # Wait before checking again
                await asyncio.sleep(1.0)
            except Exception as e:
                self.logger.error(f"Error in scheduling loop: {str(e)}")
                await asyncio.sleep(1.0)
```

#### Performance Monitoring and Dashboards

The Background Task System includes comprehensive performance monitoring and dashboards:

1. **Resource Monitoring**:
   - CPU, memory, disk, and I/O usage tracking
   - Adaptive throttling based on system load
   - Resource alerts for high usage

2. **Task Monitoring**:
   - Task status tracking
   - Progress reporting
   - Execution time measurement
   - Error tracking and retry management

3. **Performance Dashboards**:
   - Real-time task statistics
   - System resource usage visualization
   - Task execution history
   - Resource alerts display

4. **Adaptive Resource Management**:
   - Dynamic concurrency adjustment based on system load
   - Task prioritization for optimal resource allocation
   - Backoff strategies for retries

## Testing Infrastructure

### Unit Tests
```python
class TestDocumentRepository(unittest.TestCase):
    """Test the DocumentRepository class"""
    
    def setUp(self):
        """Set up test database"""
        self.engine = create_engine("postgresql://postgres:postgres@localhost:5432/metis_test")
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)
        self.session = self.Session()
        
    def tearDown(self):
        """Clean up test database"""
        self.session.close()
        
    def test_create_document(self):
        """Test creating a document"""
        repo = DocumentRepository(self.session)
        document = repo.create_document(
            filename="test.txt",
            tags=["test", "document"],
            folder="/test"
        )
        
        self.assertIsNotNone(document.id)
        self.assertEqual(document.filename, "test.txt")
        self.assertEqual(document.folder, "/test")
        self.assertEqual(len(document.tags), 2)
```

### Integration Tests
```python
class TestDocumentAPI(TestCase):
    """Test the document API endpoints"""
    
    def setUp(self):
        """Set up test client"""
        self.app = TestClient(app)
        
    def test_upload_document(self):
        """Test uploading a document"""
        # Create test file
        with open("test_file.txt", "w") as f:
            f.write("Test content")
        
        # Upload document
        with open("test_file.txt", "rb") as f:
            response = self.app.post(
                "/api/documents/upload",
                files={"file": ("test_file.txt", f, "text/plain")},
                data={"tags": "test,document", "folder": "/test"}
            )
        
        # Clean up
        os.remove("test_file.txt")
        
        # Check response
        self.assertEqual(response.status_code, 200)
        self.assertTrue(response.json()["success"])
        self.assertIn("document_id", response.json())
```

### Performance Tests
```python
class TestRAGPerformance(TestCase):
    """Test RAG performance"""
    
    def setUp(self):
        """Set up test environment"""
        self.app = TestClient(app)
        
        # Create test documents
        self._create_test_documents(100)  # Create 100 test documents
        
    def tearDown(self):
        """Clean up test environment"""
        self._cleanup_test_documents()
        
    def test_simple_query_performance(self):
        """Test performance of simple queries"""
        # Send query
        start_time = time.time()
        response = self.app.post(
            "/api/chat/query",
            json={
                "message": "What is artificial intelligence?",
                "use_rag": True
            }
        )
        elapsed_time = time.time() - start_time
        
        # Check response time
        self.assertLess(elapsed_time, 6.0)  # Should be under 6 seconds
        self.assertEqual(response.status_code, 200)
```

## Dependencies

1. **Database**:
   - SQLAlchemy (ORM for database access)
   - Alembic (database migrations)
   - psycopg2-binary (PostgreSQL driver)

2. **LLM Integration**:
   - httpx (async HTTP client for API calls)
   - tiktoken (token counting for LLMs)
   - langgraph (for state machine implementation)

3. **Document Processing**:
   - PyPDF2 (PDF processing)
   - python-docx (DOCX processing)
   - unstructured (general document processing)
   - langchain (document chunking strategies)

4. **Vector Storage**:
   - chromadb (vector database)
   - sentence-transformers (embedding models)

5. **Web Framework**:
   - FastAPI (API framework)
   - Pydantic (data validation)
   - Jinja2 (templating)
   - uvicorn (ASGI server)

6. **Testing**:
   - pytest (testing framework)
   - pytest-asyncio (async testing)
   - pytest-cov (coverage reporting)
   - pytest-benchmark (performance testing)

7. **Monitoring and Logging**:
   - prometheus-client (metrics)
   - structlog (structured logging)

8. **Memory Management**:
   - mem0 (memory layer for AI applications)
     - Conversation history storage and retrieval
     - User preferences management
     - Document interaction tracking
     - Enhanced context for RAG queries

9. **Utilities**:
   - python-multipart (file uploads)
   - python-dotenv (environment variables)
   - cachetools (in-memory caching)

## Deployment

### Docker Configuration
```dockerfile
# Dockerfile
FROM python:3.10-slim as base

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p data/uploads data/chroma_db data/cache

# Expose port
EXPOSE 8000

# Set health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

# Set entrypoint
ENTRYPOINT ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  metis-rag:
    build:
      context: .
      target: ${METIS_BUILD_TARGET:-production}
    image: metis-rag:${METIS_VERSION:-latest}
    container_name: metis-rag
    restart: unless-stopped
    ports:
      - "${METIS_PORT:-8000}:8000"
    volumes:
      - ./data:/app/data
      - ./config:/app/config
    environment:
      - METIS_CONFIG_FILE=/app/config/settings.json
      - METIS_DB_TYPE=postgresql
      - METIS_POSTGRES_DSN=postgresql://postgres:postgres@postgres:5432/metis
      - METIS_LLM_PROVIDER_TYPE=ollama
    networks:
      - metis-network
    depends_on:
      - postgres
      - ollama

  postgres:
    image: postgres:15-alpine
    container_name: metis-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=metis
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - metis-network

  ollama:
    image: ollama/ollama:latest
    container_name: metis-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    networks:
      - metis-network

networks:
  metis-network:
    driver: bridge

volumes:
  postgres-data:
```

## Revised Timeline (More Granular)

### Phase 1: Database Integration (Weeks 1-2)

#### Week 1:
- Choose database backend (PostgreSQL for both development and production)
- Create database schema with all required tables
- Implement DocumentRepository and migrate document storage
- Update document API endpoints to use the database

#### Week 2:
- Implement AnalyticsRepository and migrate analytics storage
- Update analytics API endpoints to use the database
- Implement ConversationRepository and migrate conversation storage
- Update chat API endpoints to use the database

### Phase 2: Intelligent Document Processing (Weeks 3-4)

#### Week 3:
- Implement DocumentAnalysisService with LLM prompt and response parsing
- Create detailed prompts for document analysis
- Integrate with DocumentProcessor
- Add unit and integration tests

#### Week 4:
- Implement ProcessingJob model and DocumentProcessingService
- Create WorkerPool for parallel processing
- Add API endpoints for job management
- Implement progress tracking and cancellation

### Phase 3: Agentic Capabilities Foundation (Weeks 5-6)

#### Week 5:
- Define Tool interface and implement ToolRegistry
- Implement RAGTool, CalculatorTool, and DatabaseTool
- Add unit tests for tools
- Create tool documentation

#### Week 6:
- Implement QueryAnalyzer with LLM-based query analysis
- Create ProcessLogger for comprehensive logging
- Add audit trail capabilities
- Implement API endpoints for query analysis

### Phase 4: Planning and Execution (Weeks 7-8)

#### Week 7:
- Implement QueryPlanner with LLM-based plan generation
- Create PlanExecutor for executing multi-step plans
- Add unit tests for planning and execution
- Create API endpoints for plan management

#### Week 8:
- Define LangGraph state models
  - Implemented QueryAnalysisState, PlanningState, ExecutionState, RetrievalState, GenerationState, and RAGState
  - Created state transitions and conditional edges for adaptive workflows
  - Added state validation and serialization
- Implement LangGraph integration with state machine
  - Created EnhancedLangGraphRAGAgent with state graph construction
  - Integrated QueryPlanner and PlanExecutor for complex queries
  - Added streaming support and execution trace tracking
  - Implemented error handling and recovery mechanisms
- Create conditional edges for adaptive workflows
  - Added complexity-based routing for simple vs. complex queries
  - Implemented tool selection based on query requirements
  - Created feedback loops for query refinement
  - Added context optimization for better responses
- Add API endpoints for LangGraph RAG
  - Created /enhanced_langgraph_query endpoint with streaming support
  - Added configuration flags for enabling/disabling LangGraph features
  - Implemented comprehensive error handling
  - Added detailed API documentation
- Comprehensive testing
  - Created integration tests for all LangGraph components
  - Implemented mock components for testing
  - Added tests for initialization, simple queries, complex queries, streaming, and error handling
  - Ensured all tests pass successfully

### Phase 5: Response Quality (Weeks 9-10)

#### Week 9:
- Implement ResponseSynthesizer for combining results
- Create ResponseEvaluator for quality assessment
- Add unit tests for response quality
- Implement metrics for response quality

#### Week 10:
- Implement ResponseRefiner for improving responses
- Create AuditReportGenerator for verification
- Add API endpoints for audit reports
- Implement visualization for audit trails

### Phase 6: Performance Optimization (Weeks 11-12)

#### Week 11:
- Implement Cache interface and cache implementations
- Add disk-based persistence for caches
- Create cache invalidation strategies
- Add cache statistics and monitoring

#### Week 12:
- Implement TaskManager for background processing
- Add resource monitoring and adaptive scheduling
- Create task prioritization and dependencies
- Implement comprehensive performance testing

================
File: docs/implementation/Metis_RAG_Implementation_Progress_Update.md
================
# Metis RAG Implementation Progress Update

## Implementation Checklist

- [x] Database Compatibility
  - [x] Update models to handle UUID correctly in both PostgreSQL and SQLite
  - [x] Update models to use JSONB for PostgreSQL and JSON for SQLite
  - [x] Rename metadata columns to avoid conflicts with SQLAlchemy
  - [x] Create Alembic migration for metadata column types

- [x] Model Compatibility
  - [x] Create adapter functions for Pydantic ↔ SQLAlchemy conversion
  - [x] Test adapter functions with different model types
  - [x] Handle UUID conversion between string and UUID objects
  - [x] Ensure metadata fields are correctly mapped

- [x] Testing Infrastructure
  - [x] Create test script for adapter functions
  - [x] Create test script for document processing
  - [x] Verify adapter functions work correctly
  - [x] Verify document processing works with adapter functions

- [x] Document Processor Updates
  - [x] Test document processor with adapter functions
  - [x] Update document processor to use adapter functions consistently
  - [x] Handle both Pydantic and SQLAlchemy models in processing pipeline
  - [ ] Optimize chunking for different database backends

- [x] Repository Layer Updates
  - [x] Update document repository to use adapter functions
  - [x] Update chunk repository to use adapter functions (handled in document repository)
  - [x] Update other repositories to use adapter functions (no changes needed)
  - [x] Add error handling for database-specific operations

- [x] API Layer Updates
  - [x] Update API endpoints to use adapter functions
  - [x] Ensure consistent model handling in request/response cycle
  - [x] Add validation for database-specific fields
  - [ ] Update API documentation

- [ ] Performance Testing
  - [ ] Test document processing performance with PostgreSQL
  - [ ] Test document processing performance with SQLite
  - [ ] Compare performance between database backends
  - [ ] Optimize performance bottlenecks

- [ ] Deployment
  - [ ] Update Docker configuration for PostgreSQL
  - [ ] Create database initialization scripts
  - [ ] Test containerized deployment
  - [ ] Document deployment process

## Issues Addressed

### 1. Database Compatibility Issues

We've addressed the database compatibility issues between SQLite and PostgreSQL:

- **UUID Handling**: We've updated the models to use the appropriate UUID type based on the database type.
- **JSONB vs JSON**: We've updated the models to use JSONB for PostgreSQL and JSON for SQLite.
- **Metadata Column Names**: We've renamed metadata columns to avoid conflicts with SQLAlchemy's reserved keywords (changed to doc_metadata, chunk_metadata, etc.).

### 2. Model Compatibility Issues

We've created adapter functions to convert between Pydantic models and SQLAlchemy models:

- **pydantic_document_to_sqlalchemy**: Converts a Pydantic Document to a SQLAlchemy Document.
- **sqlalchemy_document_to_pydantic**: Converts a SQLAlchemy Document to a Pydantic Document.
- **pydantic_chunk_to_sqlalchemy**: Converts a Pydantic Chunk to a SQLAlchemy Chunk.
- **sqlalchemy_chunk_to_pydantic**: Converts a SQLAlchemy Chunk to a Pydantic Chunk.

### 3. Database Migration

We've created an Alembic migration to update the database schema:

- **update_metadata_to_jsonb.py**: Updates the metadata columns to use JSONB for PostgreSQL.

### 4. Testing

We've created test scripts to verify that our changes work correctly:

- **test_adapter_functions.py**: Tests the adapter functions.
- **test_document_processing.py**: Tests the document processor with the adapter functions.

## Next Steps

### 1. Performance Testing

- Test document processing performance with PostgreSQL
- Test document processing performance with SQLite
- Compare performance between database backends
- Optimize performance bottlenecks

### 2. Chunking Optimization

- Optimize chunking for different database backends
- Implement database-specific chunking strategies
- Test chunking performance with different strategies

### 3. Documentation

- Update the API documentation to reflect the changes made
- Add examples of how to use the adapter functions
- Document the database compatibility features
- Create a guide for switching between database backends

### 4. Deployment

- Update Docker configuration for PostgreSQL
- Create database initialization scripts
- Test containerized deployment
- Document deployment process

## Conclusion

We've successfully implemented the database compatibility solution for the Metis RAG system. The key accomplishments include:

1. **Database Compatibility**: We've updated the models to handle UUID correctly in both PostgreSQL and SQLite, use JSONB for PostgreSQL and JSON for SQLite, and renamed metadata columns to avoid conflicts with SQLAlchemy.

2. **Model Compatibility**: We've created adapter functions for bidirectional conversion between Pydantic and SQLAlchemy models, handling UUID conversion and ensuring metadata fields are correctly mapped.

3. **Document Processor Updates**: We've updated the document processor to use adapter functions consistently and handle both Pydantic and SQLAlchemy models in the processing pipeline.

4. **Repository Layer Updates**: We've updated the document repository to use adapter functions and added error handling for database-specific operations.

5. **API Layer Updates**: We've updated the API endpoints to use adapter functions and ensure consistent model handling in the request/response cycle.

The implementation now correctly handles the differences between database systems, allowing the Metis RAG system to work seamlessly with both SQLite for development/testing and PostgreSQL for production. The adapter functions provide a clean separation between the domain models (Pydantic) and the database models (SQLAlchemy).

The next steps focus on performance testing, chunking optimization, documentation, and deployment to ensure the system works efficiently in all environments.

================
File: docs/implementation/Metis_RAG_Implementation_Steps.md
================
# Metis RAG Implementation Steps

This document outlines the implementation progress for the Metis RAG project.

## Completed Features

### UI Integration
- ✅ Updated base template with Metis_Chat UI
- ✅ Implemented light/dark mode support
- ✅ Enhanced chat interface with model controls
- ✅ Added conversation management features
- ✅ Implemented token tracking

### Document Management
- ✅ Integrated document management sidebar
- ✅ Improved document upload and processing UI
- ✅ Added document tagging and organization
- ✅ Implemented folder hierarchy for documents
- ✅ Added filtering by tags and folders

### RAG Engine
- ✅ Enhanced Ollama client with retry mechanisms
- ✅ Implemented conversation context in RAG
- ✅ Added metadata filtering for document retrieval
- ✅ Improved source citation display
- ✅ Enhanced logging throughout the retrieval process
- ✅ Improved prompt engineering for better context utilization
- ✅ Fixed metadata handling for tags and other list attributes

### Advanced Features
- ✅ Implemented multiple chunking strategies
  - Recursive chunking (default)
  - Token-based chunking
  - Markdown header chunking
- ✅ Added analytics and monitoring dashboard
- ✅ Optimized performance for large document collections
- ✅ Enhanced mobile experience

## Advanced Chunking Strategies

The document processor now supports multiple chunking strategies:

1. **Recursive Chunking**
   - Default strategy that recursively splits text by characters
   - Configurable chunk size and overlap

2. **Token-based Chunking**
   - Splits text based on token count rather than characters
   - Better for maintaining semantic meaning

3. **Markdown Header Chunking**
   - Splits markdown documents based on headers
   - Preserves document structure

## Analytics Dashboard

The analytics system tracks system usage and performance:

1. **Query Analytics**
   - Records query details (text, model, RAG usage)
   - Tracks response times and token counts
   - Monitors document usage

2. **System Statistics**
   - Tracks vector store performance
   - Monitors cache hit ratios
   - Provides overall system health metrics

## Performance Optimizations

Several optimizations improve performance with large document collections:

1. **Vector Store Caching**
   - In-memory cache for search results
   - Configurable TTL and size limits
   - Cache statistics tracking

2. **Metadata Filtering**
   - Pre-filtering before vector similarity calculation
   - Support for complex filter criteria
   - Optimized tag and folder filtering

## Mobile Experience

The mobile interface includes touch-friendly features:

1. **Responsive Design**
   - Optimized controls for touch
   - Improved layout for small screens
   - Better readability on mobile devices

2. **Touch Gestures**
   - Pull-to-refresh for document lists
   - Swipe actions for document management
   - Optimized for touch interactions

## Testing Framework

A comprehensive testing framework has been implemented to verify RAG functionality:

1. **RAG Retrieval Testing**
   - Automated test script for document processing
   - Test cases for different query types
   - Verification of source citations and relevance

2. **Document Processing Testing**
   - Tests for various document formats (PDF, TXT, CSV, MD)
   - Verification of chunking strategies
   - Metadata extraction validation

3. **Performance Metrics**
   - Response time tracking
   - Relevance scoring
   - Success rate calculation

================
File: docs/implementation/metis_rag_infrastructure_improvement_plan.md
================
# Metis_RAG Improvement Plan

## 1. Overview of Current System

Metis_RAG is a sophisticated Retrieval Augmented Generation (RAG) system with several advanced features:

- **Document Processing**: Supports multiple file types and chunking strategies
- **LLM-Enhanced RAG**: Uses LLM-based agents for chunking and retrieval optimization
- **Vector Storage**: Uses ChromaDB for embedding storage and retrieval
- **Web Interface**: Provides a user-friendly interface for document management and chat

The system architecture follows good practices with clear separation of concerns:
- API layer for handling HTTP requests
- Core RAG engine for document processing and query handling
- LLM agents for enhancing the RAG pipeline
- Vector store for efficient retrieval

```mermaid
graph TD
    User[User] --> WebUI[Web Interface]
    WebUI --> API[FastAPI Endpoints]
    API --> RAGEngine[RAG Engine]
    API --> DocProcessor[Document Processor]
    API --> Analytics[Analytics]
    
    RAGEngine --> VectorStore[Vector Store]
    RAGEngine --> OllamaClient[LLM Client]
    RAGEngine --> RetrievalJudge[Retrieval Judge]
    
    DocProcessor --> ChunkingJudge[Chunking Judge]
    DocProcessor --> SemanticChunker[Semantic Chunker]
    
    VectorStore --> ChromaDB[(ChromaDB)]
    
    LangGraphRAG[LangGraph RAG Agent] --> VectorStore
    LangGraphRAG --> OllamaClient
    LangGraphRAG --> RetrievalJudge
    LangGraphRAG --> ChunkingJudge
    LangGraphRAG --> SemanticChunker
```

## 2. Critical Issues to Address

### 2.1 In-Memory Data Storage

**Problem**: The system currently uses in-memory storage for:
- Document metadata and content (`documents` dictionary in `documents.py`)
- Tags and folders (`all_tags` and `all_folders` sets)
- Query analytics (`query_analytics` list and `document_usage` dictionary)
- Conversation history (`conversations` dictionary)

**Impact**: 
- Data is lost on application restart
- Not scalable to multiple workers/processes
- Not thread-safe (potential data corruption)
- Limited by available memory

### 2.2 Database Integration

**Solution**: Implement a persistent database solution to replace in-memory storage.

```mermaid
graph TD
    API[API Layer] --> DB[(Database)]
    DB --> Documents[Documents Table]
    DB --> Chunks[Chunks Table]
    DB --> Tags[Tags Table]
    DB --> Analytics[Analytics Table]
    DB --> Conversations[Conversations Table]
    
    VectorStore[Vector Store] --> ChromaDB[(ChromaDB)]
```

**Implementation Plan**:

1. **Database Selection**:
   - SQLite for simplicity and easy deployment in isolated environments
   - PostgreSQL as an optional backend for higher scalability needs

2. **Schema Design**:
   ```sql
   -- Documents table
   CREATE TABLE documents (
       id TEXT PRIMARY KEY,
       filename TEXT NOT NULL,
       folder TEXT NOT NULL DEFAULT '/',
       uploaded TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
       metadata JSONB,
       processing_status TEXT NOT NULL DEFAULT 'pending',
       processing_strategy TEXT
   );

   -- Chunks table
   CREATE TABLE chunks (
       id TEXT PRIMARY KEY,
       document_id TEXT REFERENCES documents(id) ON DELETE CASCADE,
       content TEXT NOT NULL,
       metadata JSONB,
       index INTEGER NOT NULL
   );

   -- Tags table
   CREATE TABLE tags (
       id SERIAL PRIMARY KEY,
       name TEXT UNIQUE NOT NULL
   );

   -- Document-Tag relationship
   CREATE TABLE document_tags (
       document_id TEXT REFERENCES documents(id) ON DELETE CASCADE,
       tag_id INTEGER REFERENCES tags(id) ON DELETE CASCADE,
       PRIMARY KEY (document_id, tag_id)
   );

   -- Analytics tables
   CREATE TABLE query_analytics (
       id SERIAL PRIMARY KEY,
       query TEXT NOT NULL,
       model TEXT NOT NULL,
       use_rag BOOLEAN NOT NULL,
       timestamp TIMESTAMP NOT NULL,
       response_time_ms FLOAT NOT NULL,
       token_count INTEGER NOT NULL
   );

   CREATE TABLE document_usage (
       document_id TEXT REFERENCES documents(id) ON DELETE CASCADE,
       usage_count INTEGER NOT NULL DEFAULT 0,
       last_used TIMESTAMP
   );

   -- Conversations table
   CREATE TABLE conversations (
       id TEXT PRIMARY KEY,
       created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
   );

   -- Messages table
   CREATE TABLE messages (
       id SERIAL PRIMARY KEY,
       conversation_id TEXT REFERENCES conversations(id) ON DELETE CASCADE,
       content TEXT NOT NULL,
       role TEXT NOT NULL,
       timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
       citations JSONB
   );

   -- Processing jobs table for batch document processing
   CREATE TABLE processing_jobs (
       id TEXT PRIMARY KEY,
       status TEXT NOT NULL DEFAULT 'pending',
       created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
       completed_at TIMESTAMP,
       document_count INTEGER NOT NULL DEFAULT 0,
       processed_count INTEGER NOT NULL DEFAULT 0,
       strategy TEXT,
       metadata JSONB
   );
   ```

3. **Database Access Layer**:
   - Create a `database.py` module with connection management
   - Implement repository classes for each entity (DocumentRepository, ChunkRepository, etc.)
   - Use SQLAlchemy ORM for database operations

4. **Migration Strategy**:
   - Create migration scripts to initialize the database
   - Implement data migration for existing in-memory data
   - Add database connection setup to application startup

## 3. Intelligent Document Processing

### 3.1 Strategic LLM Usage for Document Processing

**Problem**: The current approach uses LLM for all chunking decisions, which won't scale efficiently to thousands of documents.

**Solution**: Implement a tiered document processing system that uses LLMs strategically.

```mermaid
graph TD
    Upload[Document Upload] --> Batch[Batch Processing]
    Batch --> Sampling[Document Sampling]
    Sampling --> LLMAnalysis[LLM Analysis]
    LLMAnalysis --> Strategy[Processing Strategy Selection]
    Strategy --> Programmatic[Programmatic Processing]
    Strategy --> Hybrid[Hybrid Processing]
    Strategy --> LLMEnhanced[LLM-Enhanced Processing]
    
    Programmatic --> Indexing[Vector Indexing]
    Hybrid --> Indexing
    LLMEnhanced --> Indexing
```

**Implementation Plan**:

1. **Document Analysis Service**:
   ```python
   class DocumentAnalysisService:
       """
       Service for analyzing documents and determining optimal processing strategies
       """
       def __init__(self, llm_client, sample_size=3):
           self.llm_client = llm_client
           self.sample_size = sample_size
           
       async def analyze_document_batch(self, document_ids, file_paths):
           """
           Analyze a batch of documents and recommend a processing strategy
           
           Returns:
               Dict with processing strategy and parameters
           """
           # Sample documents from the batch
           samples = self._sample_documents(document_ids, file_paths)
           
           # Extract representative content from samples
           sample_content = await self._extract_sample_content(samples)
           
           # Use LLM to analyze samples and recommend strategy
           strategy = await self._recommend_strategy(sample_content)
           
           return strategy
           
       def _sample_documents(self, document_ids, file_paths):
           """Sample a subset of documents for analysis"""
           # Logic to select representative documents from the batch
           # Consider file types, sizes, and other metadata
           
       async def _extract_sample_content(self, samples):
           """Extract representative content from sample documents"""
           # Logic to extract content from various file types
           # For PDFs, extract from different sections
           # For text files, extract from beginning, middle, and end
           
       async def _recommend_strategy(self, sample_content):
           """Use LLM to recommend processing strategy"""
           # Create prompt for LLM
           prompt = self._create_strategy_prompt(sample_content)
           
           # Get recommendation from LLM
           response = await self.llm_client.generate(prompt=prompt)
           
           # Parse recommendation
           return self._parse_strategy_recommendation(response)
   ```

2. **Batch Processing System**:
   ```python
   class DocumentProcessingService:
       """
       Service for processing documents in batches
       """
       def __init__(self, db, vector_store, analysis_service, max_batch_size=50):
           self.db = db
           self.vector_store = vector_store
           self.analysis_service = analysis_service
           self.max_batch_size = max_batch_size
           
       async def create_processing_job(self, document_ids):
           """Create a new processing job for a batch of documents"""
           # Create job record in database
           job_id = self._generate_job_id()
           await self.db.create_processing_job(job_id, document_ids)
           
           # Start processing in background
           asyncio.create_task(self.process_job(job_id))
           
           return job_id
           
       async def process_job(self, job_id):
           """Process a batch of documents"""
           # Get job details
           job = await self.db.get_processing_job(job_id)
           document_ids = job["document_ids"]
           
           # Get file paths
           file_paths = await self.db.get_document_file_paths(document_ids)
           
           # Analyze documents and determine strategy
           strategy = await self.analysis_service.analyze_document_batch(
               document_ids, file_paths
           )
           
           # Update job with strategy
           await self.db.update_processing_job(job_id, {"strategy": strategy})
           
           # Process documents in smaller batches
           for i in range(0, len(document_ids), self.max_batch_size):
               batch = document_ids[i:i+self.max_batch_size]
               await self._process_batch(batch, strategy)
               
               # Update job progress
               progress = (i + len(batch)) / len(document_ids)
               await self.db.update_processing_job(
                   job_id, 
                   {"processed_count": i + len(batch)}
               )
           
           # Mark job as completed
           await self.db.complete_processing_job(job_id)
           
       async def _process_batch(self, document_ids, strategy):
           """Process a batch of documents using the determined strategy"""
           # Select processor based on strategy
           processor = self._get_processor(strategy)
           
           # Process documents
           for doc_id in document_ids:
               document = await self.db.get_document(doc_id)
               processed_doc = await processor.process_document(document)
               
               # Update document in database
               await self.db.update_document(doc_id, {
                   "chunks": processed_doc.chunks,
                   "processing_status": "completed",
                   "processing_strategy": strategy["strategy"]
               })
               
               # Add to vector store
               await self.vector_store.add_document(processed_doc)
               
       def _get_processor(self, strategy):
           """Get the appropriate document processor based on strategy"""
           strategy_type = strategy["strategy"]
           
           if strategy_type == "recursive":
               return RecursiveProcessor(
                   chunk_size=strategy["parameters"]["chunk_size"],
                   chunk_overlap=strategy["parameters"]["chunk_overlap"]
               )
           elif strategy_type == "token":
               return TokenProcessor(
                   chunk_size=strategy["parameters"]["chunk_size"],
                   chunk_overlap=strategy["parameters"]["chunk_overlap"]
               )
           elif strategy_type == "markdown":
               return MarkdownProcessor(
                   chunk_size=strategy["parameters"]["chunk_size"],
                   chunk_overlap=strategy["parameters"]["chunk_overlap"]
               )
           elif strategy_type == "semantic":
               return SemanticProcessor(
                   chunk_size=strategy["parameters"]["chunk_size"],
                   chunk_overlap=strategy["parameters"]["chunk_overlap"],
                   llm_client=self.llm_client
               )
           else:
               # Default to recursive
               return RecursiveProcessor()
   ```

3. **Processing Strategy Prompt**:
   ```
   You are a document processing expert. Your task is to analyze the following document samples and recommend the best processing strategy for a batch of similar documents.

   Available Strategies:
   - recursive: Splits text recursively by characters. Good for general text with natural separators. Fast and efficient.
   - token: Splits text by tokens. Good for preserving semantic units in technical content. Moderately fast.
   - markdown: Splits markdown documents by headers. Good for structured documents with clear sections. Fast for markdown files.
   - semantic: Uses LLM to identify natural semantic boundaries in text. Best for preserving meaning and context in complex documents. Slow but high quality.

   Document Samples:
   [Sample 1]
   {sample_1_content}

   [Sample 2]
   {sample_2_content}

   [Sample 3]
   {sample_3_content}

   Consider:
   1. Document structure and formatting
   2. Content complexity and semantic density
   3. Processing efficiency (thousands of documents need to be processed)
   4. Quality requirements (how important is semantic coherence)

   Recommend a strategy that balances quality and efficiency, using semantic chunking ONLY when absolutely necessary for complex documents where other methods would significantly degrade quality.

   Output your recommendation in JSON format:
   {
       "strategy": "...",  // One of: recursive, token, markdown, semantic
       "parameters": {
           "chunk_size": ...,  // Recommended chunk size
           "chunk_overlap": ...  // Recommended overlap size
       },
       "justification": "..." // Explanation of your reasoning
   }
   ```

### 3.2 Scalable Processing Architecture

**Problem**: The current implementation processes documents one at a time, which won't scale to thousands of documents.

**Solution**: Implement a scalable processing architecture with parallel processing and progress tracking.

```mermaid
graph TD
    API[API Endpoint] --> JobCreation[Create Processing Job]
    JobCreation --> Queue[Job Queue]
    Queue --> Workers[Worker Pool]
    Workers --> Processor1[Processor 1]
    Workers --> Processor2[Processor 2]
    Workers --> ProcessorN[Processor N]
    Processor1 --> VectorStore[Vector Store]
    Processor2 --> VectorStore
    ProcessorN --> VectorStore
    API --> JobStatus[Job Status Endpoint]
    JobStatus --> DB[(Database)]
```

**Implementation Plan**:

1. **Worker Pool**:
   ```python
   class WorkerPool:
       """
       Pool of workers for processing documents in parallel
       """
       def __init__(self, max_workers=4):
           self.max_workers = max_workers
           self.active_workers = 0
           self.queue = asyncio.Queue()
           self.running = False
           
       async def start(self):
           """Start the worker pool"""
           self.running = True
           for _ in range(self.max_workers):
               asyncio.create_task(self._worker())
           
       async def stop(self):
           """Stop the worker pool"""
           self.running = False
           # Wait for queue to empty
           await self.queue.join()
           
       async def add_job(self, job_func, *args, **kwargs):
           """Add a job to the queue"""
           await self.queue.put((job_func, args, kwargs))
           
       async def _worker(self):
           """Worker process that executes jobs from the queue"""
           while self.running:
               try:
                   # Get job from queue with timeout
                   job_func, args, kwargs = await asyncio.wait_for(
                       self.queue.get(), timeout=1.0
                   )
                   
                   # Execute job
                   self.active_workers += 1
                   try:
                       await job_func(*args, **kwargs)
                   except Exception as e:
                       logger.error(f"Error processing job: {str(e)}")
                   finally:
                       self.active_workers -= 1
                       self.queue.task_done()
               except asyncio.TimeoutError:
                   # No job available, continue waiting
                   pass
   ```

2. **Progress Tracking**:
   ```python
   class ProgressTracker:
       """
       Track progress of document processing jobs
       """
       def __init__(self, db):
           self.db = db
           
       async def create_job(self, job_type, document_ids, metadata=None):
           """Create a new job and return its ID"""
           job_id = str(uuid.uuid4())
           await self.db.execute(
               """
               INSERT INTO processing_jobs 
               (id, status, document_count, metadata, job_type) 
               VALUES (?, ?, ?, ?, ?)
               """,
               (job_id, "pending", len(document_ids), json.dumps(metadata or {}), job_type)
           )
           return job_id
           
       async def update_progress(self, job_id, processed_count, status=None):
           """Update job progress"""
           updates = {"processed_count": processed_count}
           if status:
               updates["status"] = status
               
           await self.db.update_job(job_id, updates)
           
       async def complete_job(self, job_id, status="completed"):
           """Mark a job as completed"""
           await self.db.update_job(
               job_id, 
               {
                   "status": status,
                   "completed_at": datetime.now().isoformat()
               }
           )
           
       async def get_job_status(self, job_id):
           """Get the current status of a job"""
           job = await self.db.get_job(job_id)
           
           if not job:
               return None
               
           progress = job["processed_count"] / job["document_count"] if job["document_count"] > 0 else 0
           
           return {
               "id": job["id"],
               "status": job["status"],
               "progress": progress,
               "document_count": job["document_count"],
               "processed_count": job["processed_count"],
               "created_at": job["created_at"],
               "completed_at": job["completed_at"],
               "metadata": json.loads(job["metadata"])
           }
   ```

3. **API Endpoints**:
   ```python
   @router.post("/process-batch")
   async def process_document_batch(
       request: BatchProcessRequest,
       background_tasks: BackgroundTasks,
       progress_tracker: ProgressTracker = Depends(get_progress_tracker),
       worker_pool: WorkerPool = Depends(get_worker_pool)
   ):
       """
       Process a batch of documents
       """
       # Validate document IDs
       invalid_ids = [doc_id for doc_id in request.document_ids if not await document_exists(doc_id)]
       if invalid_ids:
           raise HTTPException(
               status_code=404,
               detail=f"Documents not found: {', '.join(invalid_ids)}"
           )
       
       # Create processing job
       job_id = await progress_tracker.create_job(
           "batch_processing",
           request.document_ids,
           {
               "force_reprocess": request.force_reprocess,
               "chunking_strategy": request.chunking_strategy,
               "chunk_size": request.chunk_size,
               "chunk_overlap": request.chunk_overlap
           }
       )
       
       # Add job to worker pool
       await worker_pool.add_job(
           process_document_batch_job,
           job_id,
           request.document_ids,
           request.force_reprocess,
           request.chunking_strategy,
           request.chunk_size,
           request.chunk_overlap
       )
       
       return {
           "success": True,
           "message": f"Processing started for {len(request.document_ids)} documents",
           "job_id": job_id
       }
       
   @router.get("/job-status/{job_id}")
   async def get_job_status(
       job_id: str,
       progress_tracker: ProgressTracker = Depends(get_progress_tracker)
   ):
       """
       Get the status of a processing job
       """
       status = await progress_tracker.get_job_status(job_id)
       
       if not status:
           raise HTTPException(
               status_code=404,
               detail=f"Job {job_id} not found"
           )
           
       return status
   ```

## 4. Self-Hosted Model Interface

### 4.1 Model-Agnostic Interface

**Problem**: The system is currently tied to Ollama models.

**Solution**: Implement a provider-agnostic model interface for self-hosted models.

```mermaid
graph TD
    LLMInterface[LLM Interface] --> OllamaProvider[Ollama Provider]
    LLMInterface --> HuggingFaceProvider[HuggingFace Provider]
    LLMInterface --> LocalAIProvider[LocalAI Provider]
    LLMInterface --> CustomProvider[Custom Provider]
```

**Implementation Plan**:

1. **Abstract LLM Interface**:
   ```python
   from abc import ABC, abstractmethod
   from typing import Dict, Any, Optional, AsyncGenerator

   class LLMProvider(ABC):
       """
       Abstract interface for LLM providers
       """
       @abstractmethod
       async def generate(
           self,
           prompt: str,
           system_prompt: Optional[str] = None,
           stream: bool = False,
           parameters: Dict[str, Any] = None
       ) -> Dict[str, Any]:
           """
           Generate text from the LLM
           
           Args:
               prompt: The prompt to send to the LLM
               system_prompt: Optional system prompt
               stream: Whether to stream the response
               parameters: Model-specific parameters
               
           Returns:
               Dict with response text or stream
           """
           pass
           
       @abstractmethod
       async def create_embedding(
           self,
           text: str,
           model: Optional[str] = None
       ) -> List[float]:
           """
           Create an embedding for the given text
           
           Args:
               text: The text to embed
               model: Optional embedding model name
               
           Returns:
               List of embedding values
           """
           pass
           
       @abstractmethod
       async def get_available_models(self) -> List[Dict[str, Any]]:
           """
           Get a list of available models
           
           Returns:
               List of model information dictionaries
           """
           pass
   ```

2. **Ollama Provider Implementation**:
   ```python
   class OllamaProvider(LLMProvider):
       """
       LLM provider for Ollama
       """
       def __init__(self, base_url: str = "http://localhost:11434"):
           self.base_url = base_url
           self.client = httpx.AsyncClient(base_url=base_url, timeout=60.0)
           
       async def generate(
           self,
           prompt: str,
           system_prompt: Optional[str] = None,
           stream: bool = False,
           parameters: Dict[str, Any] = None
       ) -> Dict[str, Any]:
           # Implementation for Ollama
           # ...
           
       async def create_embedding(
           self,
           text: str,
           model: Optional[str] = None
       ) -> List[float]:
           # Implementation for Ollama
           # ...
           
       async def get_available_models(self) -> List[Dict[str, Any]]:
           # Implementation for Ollama
           # ...
   ```

3. **HuggingFace Provider Implementation**:
   ```python
   class HuggingFaceProvider(LLMProvider):
       """
       LLM provider for local HuggingFace models
       """
       def __init__(self, model_path: str = None):
           self.model_path = model_path
           self.models = {}
           self.embedding_models = {}
           
       async def generate(
           self,
           prompt: str,
           system_prompt: Optional[str] = None,
           stream: bool = False,
           parameters: Dict[str, Any] = None
       ) -> Dict[str, Any]:
           # Implementation for HuggingFace
           # ...
           
       async def create_embedding(
           self,
           text: str,
           model: Optional[str] = None
       ) -> List[float]:
           # Implementation for HuggingFace
           # ...
           
       async def get_available_models(self) -> List[Dict[str, Any]]:
           # Implementation for HuggingFace
           # ...
   ```

4. **Provider Factory**:
   ```python
   class LLMProviderFactory:
       """
       Factory for creating LLM providers
       """
       @staticmethod
       def create_provider(provider_type: str, **kwargs) -> LLMProvider:
           """
           Create an LLM provider of the specified type
           
           Args:
               provider_type: Type of provider (ollama, huggingface, etc.)
               **kwargs: Provider-specific arguments
               
           Returns:
               LLM provider instance
           """
           if provider_type == "ollama":
               return OllamaProvider(**kwargs)
           elif provider_type == "huggingface":
               return HuggingFaceProvider(**kwargs)
           elif provider_type == "localai":
               return LocalAIProvider(**kwargs)
           else:
               raise ValueError(f"Unknown provider type: {provider_type}")
   ```

## 5. Architecture Improvements

### 5.1 Dependency Injection

**Problem**: Some components create their dependencies internally, making testing and configuration difficult.

**Solution**: Implement a proper dependency injection system.

```python
# app/dependencies.py
from fastapi import Depends
from app.rag.vector_store import VectorStore
from app.rag.llm_interface import LLMProvider, LLMProviderFactory
from app.rag.rag_engine import RAGEngine
from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.database import get_db_session
from app.core.config import LLM_PROVIDER_TYPE, LLM_PROVIDER_CONFIG

def get_db():
    db = get_db_session()
    try:
        yield db
    finally:
        db.close()

def get_llm_provider():
    return LLMProviderFactory.create_provider(
        LLM_PROVIDER_TYPE,
        **LLM_PROVIDER_CONFIG
    )

def get_vector_store(llm_provider: LLMProvider = Depends(get_llm_provider)):
    return VectorStore(llm_provider=llm_provider)

def get_chunking_judge(llm_provider: LLMProvider = Depends(get_llm_provider)):
    return ChunkingJudge(llm_provider=llm_provider)

def get_retrieval_judge(llm_provider: LLMProvider = Depends(get_llm_provider)):
    return RetrievalJudge(llm_provider=llm_provider)

def get_rag_engine(
    vector_store: VectorStore = Depends(get_vector_store),
    llm_provider: LLMProvider = Depends(get_llm_provider),
    retrieval_judge: RetrievalJudge = Depends(get_retrieval_judge)
):
    return RAGEngine(
        vector_store=vector_store,
        llm_provider=llm_provider,
        retrieval_judge=retrieval_judge
    )

def get_langgraph_rag_agent(
    vector_store: VectorStore = Depends(get_vector_store),
    llm_provider: LLMProvider = Depends(get_llm_provider),
    chunking_judge: ChunkingJudge = Depends(get_chunking_judge),
    retrieval_judge: RetrievalJudge = Depends(get_retrieval_judge)
):
    return LangGraphRAGAgent(
        vector_store=vector_store,
        llm_provider=llm_provider,
        chunking_judge=chunking_judge,
        retrieval_judge=retrieval_judge
    )

def get_worker_pool():
    return WorkerPool()

def get_progress_tracker(db = Depends(get_db)):
    return ProgressTracker(db)

def get_document_analysis_service(llm_provider: LLMProvider = Depends(get_llm_provider)):
    return DocumentAnalysisService(llm_provider)

def get_document_processing_service(
    db = Depends(get_db),
    vector_store: VectorStore = Depends(get_vector_store),
    analysis_service = Depends(get_document_analysis_service)
):
    return DocumentProcessingService(db, vector_store, analysis_service)
```

### 5.2 Configuration Management

**Problem**: Configuration is scattered across different files and environment variables.

**Solution**: Implement a centralized configuration system with environment variable overrides.

```python
# app/core/config.py
import os
import json
from pathlib import Path
from typing import Dict, Any, Optional
from pydantic import BaseSettings, Field

class DatabaseSettings(BaseSettings):
    """Database configuration"""
    type: str = Field("sqlite", env="DB_TYPE")
    sqlite_path: str = Field("data/metis_rag.db", env="SQLITE_PATH")
    postgres_dsn: Optional[str] = Field(None, env="POSTGRES_DSN")
    
    class Config:
        env_prefix = "METIS_"

class LLMSettings(BaseSettings):
    """LLM configuration"""
    provider_type: str = Field("ollama", env="LLM_PROVIDER_TYPE")
    provider_config: Dict[str, Any] = Field(
        default_factory=lambda: {"base_url": "http://localhost:11434"}
    )
    default_model: str = Field("gemma3:4b", env="DEFAULT_MODEL")
    default_embedding_model: str = Field("nomic-embed-text", env="DEFAULT_EMBEDDING_MODEL")
    chunking_judge_model: str = Field("gemma3:4b", env="CHUNKING_JUDGE_MODEL")
    retrieval_judge_model: str = Field("gemma3:4b", env="RETRIEVAL_JUDGE_MODEL")
    
    class Config:
        env_prefix = "METIS_"

class VectorStoreSettings(BaseSettings):
    """Vector store configuration"""
    type: str = Field("chroma", env="VECTOR_STORE_TYPE")
    chroma_path: str = Field("data/chroma_db", env="CHROMA_DB_DIR")
    
    class Config:
        env_prefix = "METIS_"

class ProcessingSettings(BaseSettings):
    """Document processing configuration"""
    upload_dir: str = Field("data/uploads", env="UPLOAD_DIR")
    chunk_size: int = Field(1500, env="CHUNK_SIZE")
    chunk_overlap: int = Field(150, env="CHUNK_OVERLAP")
    use_chunking_judge: bool = Field(True, env="USE_CHUNKING_JUDGE")
    use_retrieval_judge: bool = Field(True, env="USE_RETRIEVAL_JUDGE")
    max_workers: int = Field(4, env="MAX_PROCESSING_WORKERS")
    
    class Config:
        env_prefix = "METIS_"

class APISettings(BaseSettings):
    """API configuration"""
    host: str = Field("0.0.0.0", env="HOST")
    port: int = Field(8000, env="PORT")
    cors_origins: list = Field(["*"], env="CORS_ORIGINS")
    
    class Config:
        env_prefix = "METIS_"

class Settings(BaseSettings):
    """Application settings"""
    project_name: str = Field("Metis RAG", env="PROJECT_NAME")
    api_v1_str: str = Field("/api", env="API_V1_STR")
    base_dir: Path = Path(__file__).resolve().parent.parent.parent
    
    database: DatabaseSettings = DatabaseSettings()
    llm: LLMSettings = LLMSettings()
    vector_store: VectorStoreSettings = VectorStoreSettings()
    processing: ProcessingSettings = ProcessingSettings()
    api: APISettings = APISettings()
    
    use_langgraph_rag: bool = Field(True, env="USE_LANGGRAPH_RAG")
    
    class Config:
        env_prefix = "METIS_"
        
    def load_from_file(self, file_path: str):
        """Load settings from a JSON file"""
        if os.path.exists(file_path):
            with open(file_path, "r") as f:
                config_data = json.load(f)
                
            # Update settings from file
            for section, values in config_data.items():
                if hasattr(self, section) and isinstance(getattr(self, section), BaseSettings):
                    section_obj = getattr(self, section)
                    for key, value in values.items():
                        if hasattr(section_obj, key):
                            setattr(section_obj, key, value)
                elif hasattr(self, section):
                    setattr(self, section, values)

# Create settings instance
settings = Settings()

# Load from config file if exists
config_file = os.environ.get("METIS_CONFIG_FILE", "config/settings.json")
if os.path.exists(config_file):
    settings.load_from_file(config_file)

# Create directories
os.makedirs(settings.processing.upload_dir, exist_ok=True)
os.makedirs(settings.vector_store.chroma_path, exist_ok=True)
os.makedirs(os.path.dirname(settings.database.sqlite_path), exist_ok=True)

# Export settings for convenience
DATABASE_SETTINGS = settings.database
LLM_SETTINGS = settings.llm
VECTOR_STORE_SETTINGS = settings.vector_store
PROCESSING_SETTINGS = settings.processing
API_SETTINGS = settings.api

# Export individual settings for backward compatibility
API_V1_STR = settings.api_v1_str
PROJECT_NAME = settings.project_name
BASE_DIR = settings.base_dir

UPLOAD_DIR = settings.processing.upload_dir
CHUNK_SIZE = settings.processing.chunk_size
CHUNK_OVERLAP = settings.processing.chunk_overlap
USE_CHUNKING_JUDGE = settings.processing.use_chunking_judge
USE_RETRIEVAL_JUDGE = settings.processing.use_retrieval_judge

DEFAULT_MODEL = settings.llm.default_model
DEFAULT_EMBEDDING_MODEL = settings.llm.default_embedding_model
CHUNKING_JUDGE_MODEL = settings.llm.chunking_judge_model
RETRIEVAL_JUDGE_MODEL = settings.llm.retrieval_judge_model

CHROMA_DB_DIR = settings.vector_store.chroma_path

USE_LANGGRAPH_RAG = settings.use_langgraph_rag

LLM_PROVIDER_TYPE = settings.llm.provider_type
LLM_PROVIDER_CONFIG = settings.llm.provider_config
```

## 6. Performance Optimizations

### 6.1 Caching Improvements

**Problem**: The current caching is limited to in-memory and only for vector search results.

**Solution**: Implement a more comprehensive caching strategy with disk-based persistence.

```python
# app/core/cache.py
import os
import json
import time
import hashlib
import pickle
from typing import Any, Dict, Optional, TypeVar, Generic, Callable
from pathlib import Path

T = TypeVar('T')

class Cache(Generic[T]):
    """
    Generic cache implementation with disk persistence
    """
    def __init__(
        self,
        name: str,
        ttl: int = 3600,
        max_size: int = 1000,
        persist: bool = True,
        persist_dir: str = "data/cache"
    ):
        self.name = name
        self.ttl = ttl
        self.max_size = max_size
        self.persist = persist
        self.persist_dir = persist_dir
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.hits = 0
        self.misses = 0
        
        # Create persist directory if needed
        if self.persist:
            os.makedirs(self.persist_dir, exist_ok=True)
            self._load_from_disk()
    
    def get(self, key: str) -> Optional[T]:
        """Get a value from the cache"""
        if key in self.cache:
            entry = self.cache[key]
            if time.time() - entry["timestamp"] < self.ttl:
                self.hits += 1
                return entry["value"]
            else:
                # Expired, remove from cache
                del self.cache[key]
        
        self.misses += 1
        return None
    
    def set(self, key: str, value: T) -> None:
        """Set a value in the cache"""
        self.cache[key] = {
            "value": value,
            "timestamp": time.time()
        }
        
        # Prune cache if it gets too large
        if len(self.cache) > self.max_size:
            self._prune()
            
        # Persist to disk if enabled
        if self.persist:
            self._save_to_disk()
    
    def invalidate(self, key: str) -> None:
        """Invalidate a specific cache entry"""
        if key in self.cache:
            del self.cache[key]
            
            # Persist changes if enabled
            if self.persist:
                self._save_to_disk()
    
    def clear(self) -> None:
        """Clear the entire cache"""
        self.cache = {}
        
        # Remove persisted cache if enabled
        if self.persist:
            cache_file = self._get_cache_file()
            if os.path.exists(cache_file):
                os.remove(cache_file)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_requests = self.hits + self.misses
        hit_ratio = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            "name": self.name,
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_ratio": hit_ratio,
            "ttl_seconds": self.ttl,
            "persist": self.persist
        }
    
    def _prune(self) -> None:
        """Remove oldest entries from cache"""
        # Sort by timestamp and keep the newest entries
        sorted_cache = sorted(
            self.cache.items(),
            key=lambda x: x[1]["timestamp"],
            reverse=True
        )
        
        # Keep only half of the max cache size
        keep_count = self.max_size // 2
        self.cache = dict(sorted_cache[:keep_count])
    
    def _get_cache_file(self) -> str:
        """Get the cache file path"""
        return os.path.join(self.persist_dir, f"{self.name}_cache.pkl")
    
    def _save_to_disk(self) -> None:
        """Save cache to disk"""
        try:
            with open(self._get_cache_file(), "wb") as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            print(f"Error saving cache to disk: {str(e)}")
    
    def _load_from_disk(self) -> None:
        """Load cache from disk"""
        cache_file = self._get_cache_file()
        if os.path.exists(cache_file):
            try:
                with open(cache_file, "rb") as f:
                    self.cache = pickle.load(f)
                    
                # Remove expired entries
                current_time = time.time()
                expired_keys = [
                    key for key, entry in self.cache.items()
                    if current_time - entry["timestamp"] >= self.ttl
                ]
                
                for key in expired_keys:
                    del self.cache[key]
            except Exception as e:
                print(f"Error loading cache from disk: {str(e)}")
                self.cache = {}

# Create cache instances
vector_search_cache = Cache[List[Dict[str, Any]]](
    name="vector_search",
    ttl=3600,  # 1 hour
    max_size=1000,
    persist=True
)

document_cache = Cache[Document](
    name="document",
    ttl=86400,  # 24 hours
    max_size=500,
    persist=True
)

analytics_cache = Cache[Dict[str, Any]](
    name="analytics",
    ttl=300,  # 5 minutes
    max_size=100,
    persist=True
)

llm_response_cache = Cache[Dict[str, Any]](
    name="llm_response",
    ttl=86400,  # 24 hours
    max_size=1000,
    persist=True
)
```

### 6.2 Asynchronous Processing

**Problem**: Some operations block the main thread, affecting responsiveness.

**Solution**: Expand asynchronous processing beyond document processing.

```python
# app/core/tasks.py
import asyncio
import logging
from typing import Dict, Any, Callable, Awaitable, List, Optional

logger = logging.getLogger("app.core.tasks")

class TaskManager:
    """
    Manager for asynchronous tasks
    """
    def __init__(self, max_concurrent_tasks: int = 10):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.tasks: Dict[str, asyncio.Task] = {}
        
    async def submit(
        self,
        task_id: str,
        func: Callable[..., Awaitable[Any]],
        *args,
        **kwargs
    ) -> str:
        """
        Submit a task for execution
        
        Args:
            task_id: Unique identifier for the task
            func: Async function to execute
            *args, **kwargs: Arguments to pass to the function
            
        Returns:
            Task ID
        """
        if task_id in self.tasks and not self.tasks[task_id].done():
            raise ValueError(f"Task {task_id} is already running")
            
        # Create and start the task
        task = asyncio.create_task(self._run_task(task_id, func, *args, **kwargs))
        self.tasks[task_id] = task
        
        return task_id
        
    async def _run_task(
        self,
        task_id: str,
        func: Callable[..., Awaitable[Any]],
        *args,
        **kwargs
    ) -> Any:
        """
        Run a task with semaphore control
        """
        async with self.semaphore:
            try:
                logger.info(f"Starting task {task_id}")
                result = await func(*args, **kwargs)
                logger.info(f"Task {task_id} completed successfully")
                return result
            except Exception as e:
                logger.error(f"Task {task_id} failed: {str(e)}")
                raise
            finally:
                # Clean up completed task
                if task_id in self.tasks and self.tasks[task_id].done():
                    del self.tasks[task_id]
    
    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the status of a task
        
        Args:
            task_id: Task ID
            
        Returns:
            Task status or None if task not found
        """
        if task_id not in self.tasks:
            return None
            
        task = self.tasks[task_id]
        
        if task.done():
            if task.exception():
                status = "failed"
                result = str(task.exception())
            else:
                status = "completed"
                result = task.result()
        else:
            status = "running"
            result = None
            
        return {
            "id": task_id,
            "status": status,
            "result": result
        }
        
    def cancel_task(self, task_id: str) -> bool:
        """
        Cancel a running task
        
        Args:
            task_id: Task ID
            
        Returns:
            True if task was cancelled, False otherwise
        """
        if task_id in self.tasks and not self.tasks[task_id].done():
            self.tasks[task_id].cancel()
            return True
        return False
        
    def get_all_tasks(self) -> List[Dict[str, Any]]:
        """
        Get status of all tasks
        
        Returns:
            List of task status dictionaries
        """
        return [
            {
                "id": task_id,
                "status": "completed" if task.done() and not task.exception() else
                         "failed" if task.done() and task.exception() else
                         "running"
            }
            for task_id, task in self.tasks.items()
        ]

# Create task manager instance
task_manager = TaskManager()
```

## 7. Deployment and DevOps

### 7.1 Containerization

**Problem**: The application lacks proper containerization for production deployment.

**Solution**: Enhance Docker configuration for production readiness.

```dockerfile
# Dockerfile
FROM python:3.10-slim as base

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p data/uploads data/chroma_db data/cache

# Expose port
EXPOSE 8000

# Set health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

# Set entrypoint
ENTRYPOINT ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

# Production stage
FROM base as production

# Set production environment variables
ENV METIS_ENV=production

# Development stage
FROM base as development

# Install development dependencies
RUN pip install --no-cache-dir pytest pytest-asyncio pytest-cov black isort mypy

# Set development environment variables
ENV METIS_ENV=development

# Set entrypoint for development
ENTRYPOINT ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  metis-rag:
    build:
      context: .
      target: ${METIS_BUILD_TARGET:-production}
    image: metis-rag:${METIS_VERSION:-latest}
    container_name: metis-rag
    restart: unless-stopped
    ports:
      - "${METIS_PORT:-8000}:8000"
    volumes:
      - ./data:/app/data
      - ./config:/app/config
    environment:
      - METIS_CONFIG_FILE=/app/config/settings.json
      - METIS_DB_TYPE=${METIS_DB_TYPE:-sqlite}
      - METIS_SQLITE_PATH=/app/data/metis_rag.db
      - METIS_LLM_PROVIDER_TYPE=${METIS_LLM_PROVIDER_TYPE:-ollama}
    networks:
      - metis-network
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    container_name: metis-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    networks:
      - metis-network

networks:
  metis-network:
    driver: bridge
```

### 7.2 Monitoring and Logging

**Problem**: The system lacks comprehensive monitoring and structured logging.

**Solution**: Implement a robust monitoring and logging system.

```python
# app/core/logging.py
import logging
import json
import sys
import time
import uuid
from typing import Dict, Any, Optional
from contextvars import ContextVar

# Create context variable for request ID
request_id_var: ContextVar[str] = ContextVar('request_id', default='')

class JSONFormatter(logging.Formatter):
    """
    Formatter that outputs JSON strings after parsing the log record
    """
    def __init__(self, **kwargs):
        self.json_default = kwargs.pop("json_default", str)
        self.json_encoder = kwargs.pop("json_encoder", json.JSONEncoder)
        self.json_indent = kwargs.pop("json_indent", None)
        self.json_separators = kwargs.pop("json_separators", None)
        self.prefix = kwargs.pop("prefix", "")
        super().__init__(**kwargs)

    def format(self, record):
        message = record.getMessage()
        extra = self.get_extra_fields(record)
        json_record = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "name": record.name,
            "message": message,
            **extra
        }
        
        # Add request ID if available
        request_id = request_id_var.get()
        if request_id:
            json_record["request_id"] = request_id
            
        # Add exception info if available
        if record.exc_info:
            json_record["exception"] = self.formatException(record.exc_info)
            
        return self.prefix + json.dumps(
            json_record,
            default=self.json_default,
            cls=self.json_encoder,
            indent=self.json_indent,
            separators=self.json_separators
        )
        
    def get_extra_fields(self, record):
        """
        Get extra fields from the record
        """
        extra = {}
        for key, value in record.__dict__.items():
            if key not in {
                "args", "asctime", "created", "exc_info", "exc_text", "filename",
                "funcName", "id", "levelname", "levelno", "lineno", "module",
                "msecs", "message", "msg", "name", "pathname", "process",
                "processName", "relativeCreated", "stack_info", "thread", "threadName"
            }:
                extra[key] = value
        return extra

class RequestIDMiddleware:
    """
    Middleware to add request ID to each request
    """
    def __init__(self, app):
        self.app = app
        
    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)
            
        # Generate request ID
        request_id = str(uuid.uuid4())
        request_id_var.set(request_id)
        
        # Add request ID to response headers
        original_send = send
        
        async def send_with_request_id(message):
            if message["type"] == "http.response.start":
                headers = message.get("headers", [])
                headers.append((b"X-Request-ID", request_id.encode()))
                message["headers"] = headers
            await original_send(message)
            
        return await self.app(scope, receive, send_with_request_id)

def setup_logging(level=logging.INFO):
    """
    Set up logging with JSON formatter
    """
    # Create JSON formatter
    json_formatter = JSONFormatter()
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    
    # Remove existing handlers
    for handler in root_logger.handlers:
        root_logger.removeHandler(handler)
    
    # Add console handler with JSON formatter
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(json_formatter)
    root_logger.addHandler(console_handler)
    
    # Configure app logger
    app_logger = logging.getLogger("app")
    app_logger.setLevel(level)
    
    # Add file handler for app logger
    file_handler = logging.FileHandler("data/logs/app.log")
    file_handler.setFormatter(json_formatter)
    app_logger.addHandler(file_handler)
    
    # Log startup message
    app_logger.info("Logging initialized", extra={"service": "metis-rag"})
    
    return root_logger
```

```python
# app/core/metrics.py
import time
from typing import Dict, Any, List, Optional, Callable
from prometheus_client import Counter, Histogram, Gauge, Summary, CollectorRegistry, generate_latest

# Create registry
registry = CollectorRegistry()

# Define metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total number of HTTP requests',
    ['method', 'endpoint', 'status'],
    registry=registry
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint'],
    registry=registry
)

document_count = Gauge(
    'document_count',
    'Number of documents in the system',
    registry=registry
)

chunk_count = Gauge(
    'chunk_count',
    'Number of chunks in the system',
    registry=registry
)

query_count = Counter(
    'query_count',
    'Total number of queries',
    ['use_rag', 'model'],
    registry=registry
)

query_duration_seconds = Histogram(
    'query_duration_seconds',
    'Query duration in seconds',
    ['use_rag', 'model'],
    registry=registry
)

llm_tokens_total = Counter(
    'llm_tokens_total',
    'Total number of tokens processed by LLM',
    ['model', 'operation'],
    registry=registry
)

cache_hits_total = Counter(
    'cache_hits_total',
    'Total number of cache hits',
    ['cache_name'],
    registry=registry
)

cache_misses_total = Counter(
    'cache_misses_total',
    'Total number of cache misses',
    ['cache_name'],
    registry=registry
)

processing_jobs_total = Counter(
    'processing_jobs_total',
    'Total number of processing jobs',
    ['status'],
    registry=registry
)

processing_documents_total = Counter(
    'processing_documents_total',
    'Total number of processed documents',
    ['strategy'],
    registry=registry
)

class MetricsMiddleware:
    """
    Middleware to collect HTTP metrics
    """
    def __init__(self, app):
        self.app = app
        
    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)
            
        # Start timer
        start_time = time.time()
        
        # Get request method and path
        method = scope.get("method", "UNKNOWN")
        path = scope["path"]
        
        # Process request
        response_status = "500"  # Default to error
        
        async def send_with_metrics(message):
            nonlocal response_status
            if message["type"] == "http.response.start":
                response_status = str(message["status"])
            await send(message)
            
        try:
            await self.app(scope, receive, send_with_metrics)
        finally:
            # Record metrics
            duration = time.time() - start_time
            http_requests_total.labels(method, path, response_status).inc()
            http_request_duration_seconds.labels(method, path).observe(duration)
            
def get_metrics():
    """
    Get metrics in Prometheus format
    """
    return generate_latest(registry)

def update_document_metrics(doc_count: int, chunks: int):
    """
    Update document metrics
    """
    document_count.set(doc_count)
    chunk_count.set(chunks)

def record_query_metrics(
    use_rag: bool,
    model: str,
    duration: float,
    token_count: int
):
    """
    Record query metrics
    """
    query_count.labels(str(use_rag), model).inc()
    query_duration_seconds.labels(str(use_rag), model).observe(duration)
    llm_tokens_total.labels(model, "query").inc(token_count)

def record_cache_metrics(cache_name: str, hits: int, misses: int):
    """
    Record cache metrics
    """
    cache_hits_total.labels(cache_name).inc(hits)
    cache_misses_total.labels(cache_name).inc(misses)

def record_processing_job_metrics(status: str, count: int = 1):
    """
    Record processing job metrics
    """
    processing_jobs_total.labels(status).inc(count)

def record_document_processing_metrics(strategy: str, count: int = 1):
    """
    Record document processing metrics
    """
    processing_documents_total.labels(strategy).inc(count)
```

## 8. Implementation Roadmap

### Phase 1: Database Migration (Weeks 1-2)
- Design database schema
- Implement database access layer
- Migrate in-memory storage to database
- Add database connection management

### Phase 2: Intelligent Document Processing (Weeks 3-4)
- Implement document analysis service
- Create batch processing system
- Add worker pool for parallel processing
- Implement progress tracking

### Phase 3: Self-Hosted Model Interface (Weeks 5-6)
- Create abstract LLM interface
- Implement provider implementations (Ollama, HuggingFace)
- Add provider factory
- Update existing code to use the new interface

### Phase 4: Architecture Improvements (Weeks 7-8)
- Implement dependency injection
- Enhance configuration management
- Improve error handling
- Add request ID tracking

### Phase 5: Performance Optimizations (Weeks 9-10)
- Implement disk-based caching
- Add asynchronous task management
- Optimize vector search
- Implement query optimization

### Phase 6: Deployment and DevOps (Weeks 11-12)
- Enhance Docker configuration
- Implement monitoring and logging
- Add health checks
- Create deployment documentation

## 9. Conclusion

The Metis_RAG system is already a sophisticated RAG implementation with advanced features like LLM-based chunking and retrieval optimization. The proposed improvements focus on making the system production-ready by addressing critical issues like in-memory storage, adding intelligent document processing for scalability, and enhancing performance.

Key improvements include:

1. **Database Integration**: Replace in-memory storage with a persistent database to support larger datasets and ensure data durability.

2. **Intelligent Document Processing**: Implement a tiered approach where LLMs direct the processing strategy but programmatic methods handle the bulk of the work, enabling efficient processing of thousands of documents.

3. **Self-Hosted Model Interface**: Create a provider-agnostic interface that supports multiple self-hosted models while maintaining isolation.

4. **Architecture Improvements**: Implement dependency injection and centralized configuration to improve maintainability and testability.

5. **Performance Optimizations**: Add disk-based caching and asynchronous processing to improve responsiveness and throughput.

6. **Deployment and DevOps**: Enhance containerization and add monitoring for production readiness.

By following this improvement plan, Metis_RAG will evolve into a robust, scalable, and feature-rich RAG system suitable for production use in isolated environments.

================
File: docs/security/credentials_in_url_vulnerability.md
================
# Credentials in URL Query Parameters Security Vulnerability

## Overview

A security vulnerability was identified where user credentials (username and password) were being passed in URL query parameters (e.g., `/login?username=user&password=pass`). This is a significant security risk for the following reasons:

1. **Log Exposure**: Credentials in URLs are logged by web servers, proxies, and potentially other systems
2. **Browser History**: They remain in browser history and can be seen by anyone with access to the device
3. **Referrer Headers**: They may be visible in referrer headers when navigating to other sites
4. **Caching**: They can be cached in various places (browser cache, proxy cache, etc.)

## Root Cause Analysis

This was not an intentional feature of the application. The authentication code uses secure methods:
- The login form in login.html submits credentials via POST request
- The auth.py API endpoints use OAuth2PasswordRequestForm for secure credential handling
- The middleware/auth.py only redirects to the login page with a redirect parameter

The issue was likely coming from:
- A bookmark with credentials in the URL
- A script or tool constructing these insecure URLs
- A browser extension or other tool modifying URLs
- Users manually typing credentials in the URL

## Implemented Solution

### 1. Server-side Detection and Redirection

The login route now detects credentials in URL parameters and:
- Logs the security event (without logging the actual credentials)
- Redirects to a clean login URL
- Preserves any legitimate redirect parameters
- Adds a security warning flag to the URL

```python
# Check for credentials in URL params (security vulnerability)
params = request.query_params
has_credentials = "username" in params or "password" in params

if has_credentials:
    # Log security event (without logging the actual credentials)
    client_host = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("user-agent", "unknown")
    logger.warning(
        f"Security alert: Credentials detected in URL parameters. "
        f"IP: {client_host}, "
        f"User-Agent: {user_agent}"
    )
    
    # Get redirect param if it exists
    redirect_param = params.get("redirect", "")
    # Create clean URL (without credentials)
    clean_url = "/login" + (f"?redirect={redirect_param}" if redirect_param else "")
    
    # Redirect to clean URL with warning flag
    return RedirectResponse(
        url=clean_url + ("&" if redirect_param else "?") + "security_warning=credentials_in_url",
        status_code=status.HTTP_303_SEE_OTHER
    )
```

### 2. Enhanced Security Headers

Added additional security headers to prevent leaking sensitive information:

```python
# Add Referrer-Policy to prevent leaking URL parameters to external sites
response.headers["Referrer-Policy"] = "same-origin"

# Add HSTS header for HTTPS enforcement
response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"

# Add Cache-Control for sensitive pages
if path in ["/login", "/register", "/forgot-password", "/reset-password"]:
    response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"

# Enhanced Content Security Policy
response.headers["Content-Security-Policy"] = (
    "default-src 'self'; "
    "script-src 'self'; "
    "style-src 'self'; "
    "img-src 'self' data:; "
    "connect-src 'self';"
    "form-action 'self';"
)
```

### 3. Client-side Detection and Warning

Added client-side JavaScript to detect and clean credentials from URLs:

```javascript
// Check for credentials in URL and clean them up
function checkAndCleanCredentialsInUrl() {
    const urlParams = new URLSearchParams(window.location.search);
    const hasUsername = urlParams.has('username');
    const hasPassword = urlParams.has('password');
    
    if (hasUsername || hasPassword) {
        // Show warning
        const errorMessage = document.getElementById('error-message');
        errorMessage.textContent = 'WARNING: Credentials should never be included in URLs as this is a security risk. The URL has been cleaned.';
        errorMessage.style.display = 'block';
        
        // Save redirect parameter if present
        const redirect = urlParams.get('redirect');
        
        // Clean the URL (keep only redirect parameter if it exists)
        const cleanUrl = redirect 
            ? `/login?redirect=${encodeURIComponent(redirect)}` 
            : '/login';
        
        // Replace current URL without reloading
        window.history.replaceState({}, document.title, cleanUrl);
    }
}
```

### 4. User Education

Added a security notice on the login page:

```html
<div class="security-notice" style="margin-bottom: 15px; padding: 8px; background-color: #fff8e6; border-left: 4px solid #ffe066; font-size: 0.9em;">
    <strong>Security Notice:</strong> Never include your credentials in URLs or bookmarks. Always use this secure login form.
</div>
```

### 5. Comprehensive Logging

Added a middleware to log all suspicious requests with sensitive parameters in URLs:

```python
async def log_suspicious_requests(request: Request, call_next):
    """
    Middleware to log suspicious requests that might pose security risks
    """
    path = request.url.path
    query_params = request.query_params
    
    # Check for sensitive parameters in URLs
    sensitive_params = ['username', 'password', 'token', 'key', 'secret', 'api_key', 'auth']
    found_sensitive = [param for param in sensitive_params if param in query_params]
    
    if found_sensitive:
        # Collect request metadata for security analysis
        client_ip = request.client.host if request.client else "unknown"
        user_agent = request.headers.get("user-agent", "unknown")
        referrer = request.headers.get("referer", "none")
        
        # Log security event (without logging the actual sensitive values)
        logger.warning(
            f"Security alert: Sensitive parameters ({', '.join(found_sensitive)}) "
            f"detected in URL for path: {path}. "
            f"IP: {client_ip}, "
            f"User-Agent: {user_agent}, "
            f"Referrer: {referrer}"
        )
    
    # Continue with the request
    response = await call_next(request)
    return response
```

## Additional Recommendations

1. **Rate Limiting**: Implement rate limiting for login attempts to prevent brute force attacks
2. **Automated Alerts**: Set up an automated alert system for suspicious login patterns
3. **URL Parameter Review**: Review existing URL parameter handling across the application
4. **Log Auditing**: Regularly audit logs for security anomalies
5. **Security Training**: Provide security training for developers on secure authentication practices

## Testing

To verify the fix:
1. Try accessing `/login?username=test&password=test123`
2. You should be redirected to `/login?security_warning=credentials_in_url`
3. A warning message should appear on the login page
4. Check the logs for a security alert entry

## References

- [OWASP - Password in URL](https://owasp.org/www-community/vulnerabilities/Information_exposure_through_query_strings_in_url)
- [CWE-598: Information Exposure Through Query Strings in URL](https://cwe.mitre.org/data/definitions/598.html)
- [NIST Guidelines for Password-Based Authentication](https://pages.nist.gov/800-63-3/sp800-63b.html)

================
File: docs/security/login_csp_fix.md
================
# Login Form Content Security Policy (CSP) Fix

## Issue Description

The Metis RAG application was experiencing an issue with the login functionality due to Content Security Policy (CSP) restrictions. The CSP was blocking inline JavaScript in the login.html file, which prevented the JavaScript code from properly handling the form submission, storing the authentication token, and redirecting after login.

Specifically, the browser console showed errors like:

```
Refused to execute inline script because it violates the following Content Security Policy directive: "script-src 'self'".
```

This resulted in the login form submitting directly to the current URL (/login) instead of the API endpoint (/api/auth/token), causing a 405 Method Not Allowed error since the /login route only accepts GET requests.

## Root Cause Analysis

The root cause of the issue was:

1. The login form had inline JavaScript in the login.html file
2. The Content Security Policy (CSP) was configured to only allow scripts from the same origin ('self')
3. The CSP explicitly disallowed inline scripts for security reasons (preventing cross-site scripting - XSS)
4. When the JavaScript was blocked, the browser fell back to the default HTML form submission behavior

## Solution Implemented

The solution was to move the inline JavaScript to an external file to comply with the CSP:

1. Created a new external JavaScript file: `app/static/js/login_handler.js`
2. Moved all the login form handling code from the inline script to this external file
3. Updated the login.html template to include the external script instead of the inline one
4. Ensured the form had the correct action attribute pointing to the API endpoint

### Code Changes

1. Created `app/static/js/login_handler.js` with all the login form handling logic
2. Modified `app/templates/login.html` to:
   - Remove the inline script
   - Add a reference to the external script
   - Set the correct form action attribute

## Testing and Verification

The fix was tested by:

1. Opening the login page in a browser
2. Entering valid credentials
3. Submitting the form
4. Verifying successful login and redirection to the main page
5. Confirming the username was displayed in the UI

## Security Considerations

This fix improves security by:

1. Complying with Content Security Policy best practices
2. Reducing the risk of Cross-Site Scripting (XSS) attacks
3. Ensuring credentials are properly submitted via POST requests rather than potentially exposing them in URLs

## Lessons Learned

1. Always follow CSP best practices by avoiding inline scripts and styles
2. Use external JavaScript files for event handling and form submission logic
3. Ensure HTML forms have proper action attributes as a fallback if JavaScript fails
4. Check browser console for CSP violations when debugging authentication issues

## Future Improvements

1. Consider implementing nonce-based CSP for cases where inline scripts are necessary
2. Review other parts of the application for similar CSP violations
3. Add comprehensive CSP testing to the CI/CD pipeline

================
File: docs/security/row_level_security_fix.md
================
# Row Level Security (RLS) Fix

## Issue

The Metis RAG application was experiencing errors related to PostgreSQL Row Level Security (RLS) policies. The application was trying to use `current_setting('app.current_user_id')` in PostgreSQL RLS policies, but this variable wasn't properly configured in the database, causing syntax errors like:

```
syntax error at or near "NULL"
[SQL: SET app.current_user_id = NULL]
```

## Solution

A new Alembic migration (`fix_rls_current_setting.py`) was created to address this issue. The migration:

1. Creates a custom PostgreSQL function `safe_get_current_user_id()` that safely handles the case when the `app.current_user_id` variable doesn't exist:

```sql
CREATE OR REPLACE FUNCTION safe_get_current_user_id() RETURNS uuid AS $$
DECLARE
    user_id uuid;
BEGIN
    BEGIN
        -- Try to get the current user ID
        user_id := current_setting('app.current_user_id')::uuid;
    EXCEPTION WHEN OTHERS THEN
        -- If it fails, return NULL
        user_id := NULL;
    END;
    RETURN user_id;
END;
$$ LANGUAGE plpgsql;
```

2. Updates all RLS policies to use this safe function instead of directly using `current_setting()`:

```sql
CREATE POLICY "Users can view their own documents" 
ON documents FOR SELECT 
USING (user_id = safe_get_current_user_id() OR is_public = true);
```

## Implementation Details

The migration modifies the following RLS policies:

- "Users can view their own documents"
- "Users can update their own documents"
- "Users can delete their own documents"
- "Users can view documents shared with them"
- "Users can update documents shared with write permission"
- "Users can view their own document sections"
- "Users can view document sections shared with them"
- "Users can update their own document sections"
- "Users can update document sections shared with write permission"

## How to Apply

The migration can be applied using the following command:

```bash
alembic upgrade heads
```

## Rollback

If needed, the migration can be rolled back using:

```bash
alembic downgrade fix_rls_current_setting-1
```

## Future Considerations

For a more permanent solution, consider:

1. Properly configuring the PostgreSQL database to support custom variables in the `app` namespace
2. Adding the following to your `postgresql.conf` file:
   ```
   custom_variable_classes = 'app'
   ```
3. Or using a different approach for RLS that doesn't rely on session variables

================
File: docs/setup/Metis_RAG_Improvement_Plan.md
================
# Metis RAG Improvement Plan

## Overview

This document outlines the plan to integrate the best features of Metis_Chat and Metis_RAG into a unified application, combining Metis_Chat's superior UI with Metis_RAG's document retrieval capabilities.

## Current State Analysis

### Metis_Chat Strengths
- Modern, responsive UI with light/dark mode
- Robust conversation management
- Streaming responses with token tracking

### Metis_RAG Strengths
- Document upload and processing
- Vector storage for semantic search
- Retrieval-augmented generation

## Implementation Roadmap

### Phase 1: Foundation Integration ✅
1. ✅ Merge UI frameworks
2. ✅ Integrate RAG engine
3. ✅ Implement document management
4. ✅ Create unified configuration

### Phase 2: Feature Enhancement ✅
1. ✅ Implement RAG toggle
2. ✅ Add document upload capabilities
3. ✅ Enhance conversation management
4. ✅ Implement source citation

### Phase 3: Advanced Features ✅
1. ✅ Add document organization and tagging
2. ✅ Implement advanced chunking strategies
3. ✅ Add analytics dashboard
4. ✅ Implement user feedback collection

### Phase 4: Optimization and Polish ✅
1. ✅ Optimize performance for large collections
2. ✅ Enhance mobile experience
3. ✅ Add keyboard shortcuts
4. ✅ Implement advanced visualization

## Recent Enhancements

1. **Enhanced RAG Engine**
   - Improved logging throughout the retrieval process
   - Enhanced prompt engineering for better context utilization
   - Fixed metadata handling for tags and other list attributes
   - Added comprehensive testing framework

2. **Document Processing Improvements**
   - Fixed issues with processing Markdown and PDF files
   - Improved error handling with fallback mechanisms
   - Enhanced chunking strategies for different document types

3. **Testing Framework**
   - Created test script for RAG retrieval verification
   - Implemented test cases for different query types
   - Added analysis of retrieval success rate

## Current Issues and Improvements (March 2025)

Based on recent testing, we've identified several issues that need to be addressed:

### 1. Streaming Text Formatting Issues
- [ ] Fix token-by-token streaming that causes incorrect word breaks (e.g., "St abil ization")
- [ ] Modify token processing to accumulate tokens until complete words are formed
- [ ] Update frontend JavaScript to properly handle streamed tokens
- [ ] Implement buffer that only renders complete words
- [ ] Add specific tests for streaming functionality

### 2. Multi-Document Synthesis
- [ ] Improve synthesis of information across multiple documents
- [ ] Update prompt templates in `app/rag/rag_engine.py` with better synthesis instructions
- [ ] Implement cross-document reference analysis
- [ ] Create metadata that links related concepts across documents
- [ ] Refine relevance scoring algorithm

### 3. File Handling Problems
- [ ] Fix directory permissions in `app/utils/file_utils.py`
- [ ] Implement robust path handling with absolute paths
- [ ] Add path validation and normalization
- [ ] Improve error handling for file operations
- [ ] Update test fixtures to properly set up and tear down test directories

### 4. Edge Case Handling
- [ ] Implement comprehensive input sanitization
- [ ] Add global error handlers for different exception types
- [ ] Create standardized error responses
- [ ] Expand test suite with more edge cases

### 5. Response Time Optimization
- [ ] Reduce average response time (currently ~9.8s)
- [ ] Implement caching layer for frequently accessed documents and queries
- [ ] Optimize vector store implementation
- [ ] Process multiple chunks in parallel where possible
- [ ] Review prompt design for efficiency

## Implementation Timeline

### Phase 1 (Immediate - 1 week)
- [ ] Fix streaming text formatting issues
- [ ] Address file handling problems
- [ ] Implement basic input sanitization

### Phase 2 (2-3 weeks)
- [ ] Enhance prompt engineering for better multi-document synthesis
- [ ] Implement caching for performance improvement
- [ ] Expand edge case testing

### Phase 3 (4-6 weeks)
- [ ] Implement cross-document reference analysis
- [ ] Optimize vector search algorithms
- [ ] Add parallel processing capabilities

## Future Enhancements

1. **Multi-modal RAG Support**
   - Support for image and audio content
   - Cross-modal retrieval

2. **Advanced LLM Integration**
   - Support for more LLM providers
   - Model fine-tuning capabilities

3. **Enterprise Features**
   - User authentication and access control
   - Document sharing and collaboration

4. **Integration Capabilities**
   - API for external application integration
   - Plugin system for extending functionality

5. **UI Enhancements**
   - Visual indicators for source documents in responses
   - Debug mode to show retrieval details
   - Improved relevance threshold controls

## Technical Considerations

### Performance Optimization
- Caching for frequently accessed documents
- Optimized vector search with detailed logging
- Background processing for document ingestion
- Improved metadata handling for better filtering

### Security Considerations
- Input validation
- Document access controls
- Secure API endpoints
- Proper error handling and logging

### Testing and Quality Assurance
- Comprehensive test suite for RAG functionality
- Automated testing for document processing
- Performance benchmarking
- Continuous integration

### Monitoring and Evaluation
- [ ] Implement comprehensive logging throughout the application
- [ ] Track performance metrics and error rates
- [ ] Develop monitoring dashboard for system performance
- [ ] Include metrics for response time, accuracy, and error rates
- [ ] Schedule automated test runs
- [ ] Compare results over time to track improvements

## Success Metrics

### Current Metrics
- Response time under 2 seconds for RAG queries (currently ~9.8s)
- Support for up to 10,000 documents
- Intuitive document management
- Seamless switching between modes
- 100% success rate in RAG retrieval tests
- Proper source citations in responses
- Support for all document types (PDF, TXT, CSV, MD)

### New Metrics to Track
- [ ] Zero instances of word-breaking in streaming responses
- [ ] 90%+ factual accuracy in multi-document synthesis
- [ ] Zero file handling errors in test suite
- [ ] 100% success rate in edge case tests
- [ ] Reduced average response time to under 3 seconds
- [ ] Comprehensive logging coverage across all components

================
File: docs/setup/Metis_RAG_Setup_Plan.md
================
# Metis RAG Setup Plan

This document outlines a detailed plan to get the Metis RAG application running with Python 3.10, which should resolve the pydantic-core wheel building issue you're encountering with Python 3.13.

## Overview

```mermaid
graph TD
    A[Install Python 3.10] --> B[Create New Virtual Environment]
    B --> C[Install Requirements]
    C --> D[Configure Environment Variables]
    D --> E[Ensure Ollama is Running]
    E --> F[Run the Application]
    F --> G[Test the Application]
```

## Detailed Steps

### 1. Install Python 3.10

First, we need to install Python 3.10 on your system:

#### For macOS (using Homebrew):
```bash
brew install python@3.10
```

#### For Linux (Ubuntu/Debian):
```bash
sudo apt update
sudo apt install python3.10 python3.10-venv python3.10-dev
```

#### For Windows:
Download the Python 3.10 installer from the official Python website and run it.

### 2. Create a New Virtual Environment with Python 3.10

Once Python 3.10 is installed, exit your current virtual environment and create a new one:

```bash
# Exit current virtual environment
deactivate

# Create a new virtual environment with Python 3.10
python3.10 -m venv venv_py310

# Activate the new virtual environment
source venv_py310/bin/activate  # On macOS/Linux
# or
# venv_py310\Scripts\activate  # On Windows
```

### 3. Install Requirements

With the Python 3.10 virtual environment activated, install the requirements:

```bash
# Install wheel and setuptools first
pip install --upgrade pip wheel setuptools

# Install the requirements
pip install -r requirements.txt
```

### 4. Configure Environment Variables

Create a `.env` file based on the `.env.example`:

```bash
cp .env.example .env
```

Edit the `.env` file if needed, particularly the Ollama settings:

```
# API Settings
API_HOST=0.0.0.0
API_PORT=8000

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=llama3
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# Document Settings
UPLOAD_DIR=./uploads
CHROMA_DB_DIR=./chroma_db
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Security Settings
CORS_ORIGINS=*
```

### 5. Ensure Ollama is Running

Make sure Ollama is running and has the required models:

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If needed, pull the required models
ollama pull llama3
ollama pull nomic-embed-text
```

### 6. Run the Application

With everything set up, run the application:

```bash
# Create necessary directories if they don't exist
mkdir -p uploads chroma_db

# Run the application
uvicorn app.main:app --reload
```

### 7. Test the Application

Once the application is running, you can access it at:
- Main interface: http://localhost:8000
- API documentation: http://localhost:8000/docs

You can also run the RAG retrieval test script to verify that the document processing and retrieval are working correctly:

```bash
# Run the RAG retrieval test script
python test_rag_retrieval.py
```

This script creates test documents, processes them, and tests the RAG retrieval with specific queries to verify that the system correctly retrieves and uses information from the documents.

## Troubleshooting Common Issues

1. **If you still encounter issues with pydantic-core**:
   Try installing it separately with:
   ```bash
   pip install pydantic-core
   ```

2. **If you encounter issues with langchain or langchain-community**:
   Try installing specific versions:
   ```bash
   pip install langchain==0.0.335 langchain-community==0.0.13
   ```

3. **If Ollama connection fails**:
   Ensure Ollama is running and accessible at http://localhost:11434

4. **If directories are not accessible**:
   Check permissions for the uploads and chroma_db directories

5. **If you encounter issues with specific dependencies**:
   You might need to install system-level dependencies. For example, on Ubuntu:
   ```bash
   sudo apt-get install build-essential python3-dev
   ```

6. **If you encounter issues with document processing**:
   Ensure you have the necessary dependencies for document processing:
   ```bash
   pip install pypdf>=3.15.1 unstructured>=0.10.16
   ```
   
7. **If you encounter issues with RAG retrieval**:
   Run the test script to diagnose issues:
   ```bash
   python test_rag_retrieval.py
   ```
   Check the logs for detailed information about the retrieval process.

## Alternative Approach: Using Docker

If you continue to face issues with the Python setup, you can alternatively use Docker:

```bash
# Create a .env file
cp .env.example .env

# Build and start the containers
docker-compose up -d

# Access the application at http://localhost:8000
```

This approach isolates all dependencies within containers and should work regardless of your local Python setup.

================
File: docs/technical/AUTHENTICATION.md
================
# Metis RAG Authentication System

This document provides an overview of the authentication system implemented in the Metis RAG application.

## Overview

The authentication system provides user management and access control for the Metis RAG application. It ensures that users can only access their own documents and conversations, and that sensitive operations are protected.

## Features

- User registration and login
- JWT-based authentication
- Role-based access control (user/admin)
- Resource ownership validation
- Protected API endpoints
- Middleware for route protection

## Components

### Models

- `User` model in `app/models/user.py`
- Database model in `app/db/models.py`

### Repositories

- `UserRepository` in `app/db/repositories/user_repository.py`

### API Endpoints

- `/api/auth/register` - Register a new user
- `/api/auth/token` - Login and get access token
- `/api/auth/me` - Get current user information
- `/api/auth/users` - List all users (admin only)
- `/api/auth/users/{user_id}` - Get user by ID (admin or self)

### Security

- Password hashing with bcrypt
- JWT token generation and validation
- Token expiration and refresh

### Middleware

- `AuthMiddleware` in `app/middleware/auth.py` - Protects routes and API endpoints

### Frontend

- Login and registration pages
- Authentication UI elements in the base template
- JavaScript for token management and authentication

## Database Schema

The authentication system adds the following to the database schema:

- `users` table with fields:
  - `id` (UUID, primary key)
  - `username` (string, unique)
  - `email` (string, unique)
  - `password_hash` (string)
  - `full_name` (string, optional)
  - `is_active` (boolean)
  - `is_admin` (boolean)
  - `created_at` (timestamp)
  - `last_login` (timestamp)
  - `metadata` (JSONB)

- Foreign key relationships:
  - `documents.user_id` references `users.id`
  - `conversations.user_id` references `users.id`

## Usage

### Registration

```python
# Register a new user
response = requests.post(
    "http://localhost:8000/api/auth/register",
    json={
        "username": "testuser",
        "email": "test@example.com",
        "password": "securepassword",
        "full_name": "Test User"
    }
)
```

### Login

```python
# Login and get access token
response = requests.post(
    "http://localhost:8000/api/auth/token",
    data={
        "username": "testuser",
        "password": "securepassword"
    }
)
token = response.json()["access_token"]
```

### Authenticated Requests

```python
# Make authenticated request
response = requests.get(
    "http://localhost:8000/api/auth/me",
    headers={"Authorization": f"Bearer {token}"}
)
```

## Testing

A test script is provided in `scripts/test_authentication.py` that demonstrates the authentication flow:

1. Register a new user
2. Login with user credentials
3. Get current user information
4. Upload a document
5. List user's documents
6. Create a conversation
7. List conversations

Run the test script with:

```bash
python scripts/test_authentication.py
```

## Implementation Details

### Password Hashing

Passwords are hashed using bcrypt with automatic salt generation. The hashing is handled by the `passlib` library.

### Token Generation

JWT tokens are generated using the `python-jose` library with the HS256 algorithm. Tokens include:
- Subject (`sub`): Username
- User ID (`user_id`): UUID of the user
- Expiration (`exp`): Token expiration timestamp

### Middleware Protection

The `AuthMiddleware` protects routes based on configuration:
- Protected routes: Redirects to login if not authenticated
- API routes: Returns 401 Unauthorized if not authenticated
- Excluded routes: Allows access without authentication

## Configuration

Authentication settings are configured in `app/core/config.py`:

- `SECRET_KEY`: Secret key for JWT token generation
- `ALGORITHM`: Algorithm for JWT token generation (default: HS256)
- `ACCESS_TOKEN_EXPIRE_MINUTES`: Token expiration time in minutes

## Security Considerations

- HTTPS should be used in production to protect tokens in transit
- Passwords are never stored in plain text
- Tokens have a limited lifetime
- Resource ownership is validated for all operations
- CORS is configured to restrict cross-origin requests

================
File: docs/technical/Fix_SQLAlchemy_Metadata_Conflict.md
================
# Fix SQLAlchemy Metadata Naming Conflicts

## 1. Problem Summary

SQLAlchemy's Declarative API reserves the attribute name `metadata` for internal use. However, several of your model classes in `app/db/models.py` use `metadata` as a column name:

```python
metadata = Column(JSONB, default={})  # This conflicts with SQLAlchemy's reserved attribute
```

The error message from the application:

```
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved when using the Declarative API.
```

Interestingly, your adapter functions in `app/db/adapters.py` are already set up to use different attribute names, but the models themselves need to be updated.

## 2. Required Code Changes

### 2.1. Update app/db/models.py

```mermaid
graph TD
    A[Problem: 'metadata' column name conflicts with SQLAlchemy] --> B[Change column names in SQLAlchemy models]
    B --> C1[Document: metadata → doc_metadata]
    B --> C2[Chunk: metadata → chunk_metadata] 
    B --> C3[Conversation: metadata → conv_metadata]
    B --> C4[ProcessingJob: metadata → job_metadata]
    B --> C5[AnalyticsQuery: document_ids → document_id_list]
```

Specifically:

1. In the `Document` class:
   ```python
   # Change this:
   metadata = Column(JSONB, default={})
   
   # To this:
   doc_metadata = Column(JSONB, default={})
   ```

2. In the `Chunk` class:
   ```python
   # Change this:
   metadata = Column(JSONB, default={})
   
   # To this:
   chunk_metadata = Column(JSONB, default={})
   ```

3. In the `Conversation` class:
   ```python
   # Change this:
   metadata = Column(JSONB, default={})
   
   # To this:
   conv_metadata = Column(JSONB, default={})
   ```

4. In the `ProcessingJob` class:
   ```python
   # Change this:
   metadata = Column(JSONB, default={})
   
   # To this:
   job_metadata = Column(JSONB, default={})
   ```

5. In the `AnalyticsQuery` class:
   ```python
   # Change this:
   document_ids = Column(JSONB, default=[])
   
   # To this:
   document_id_list = Column(JSONB, default=[])
   ```

### 2.2. Create Database Migration

Create a new migration script using Alembic to rename these columns in the database:

```python
"""rename_metadata_columns

Revision ID: [alembic will generate]
Revises: [current head]
Create Date: [current date]

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB

# revision identifiers
revision = '[alembic will generate]'
down_revision = '[current head]'
branch_labels = None
depends_on = None


def upgrade():
    # Rename metadata column in documents table
    op.alter_column('documents', 'metadata', new_column_name='doc_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename metadata column in chunks table
    op.alter_column('chunks', 'metadata', new_column_name='chunk_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename metadata column in conversations table
    op.alter_column('conversations', 'metadata', new_column_name='conv_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename metadata column in processing_jobs table
    op.alter_column('processing_jobs', 'metadata', new_column_name='job_metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    # Rename document_ids column in analytics_queries table
    op.alter_column('analytics_queries', 'document_ids', new_column_name='document_id_list', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'[]'::jsonb"))


def downgrade():
    # Revert column name changes
    op.alter_column('documents', 'doc_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('chunks', 'chunk_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('conversations', 'conv_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('processing_jobs', 'job_metadata', new_column_name='metadata', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'{}'::jsonb"))
    
    op.alter_column('analytics_queries', 'document_id_list', new_column_name='document_ids', 
                   existing_type=JSONB(), nullable=True, server_default=sa.text("'[]'::jsonb"))
```

## 3. Implementation Process

```mermaid
graph TD
    A[Start] --> B[Update app/db/models.py]
    B --> C[Create new Alembic migration]
    C --> D[Run database migration]
    D --> E[Test application]
    E --> F[End]
```

1. **Update Model Definitions**: Modify the SQLAlchemy model classes in `app/db/models.py` as outlined above.

2. **Create Database Migration**: 
   ```bash
   # Generate a new migration
   alembic revision --autogenerate -m "rename_metadata_columns"
   
   # Edit the generated migration file to ensure it correctly renames columns
   ```

3. **Run the Migration**:
   ```bash
   alembic upgrade head
   ```

4. **Test the Application**:
   ```bash
   python -m scripts.run_app
   ```

## 4. Verification

After implementing these changes, verify that:

1. The application starts without errors
2. Document processing works correctly
3. Queries return expected results
4. Data persistence functions properly

## 5. Root Cause Analysis

This issue likely arose when upgrading SQLAlchemy versions or implementing more complex ORM features. The `metadata` attribute is reserved in SQLAlchemy's Declarative API because it holds table definitions and other metadata about the model class. When a column is also named `metadata`, it creates an attribute conflict.

The adapter functions in `app/db/adapters.py` suggest that this naming issue was known and worked around in that part of the code, but the model definitions weren't updated to match.

## 6. Next Steps

After fixing this specific issue, consider a code review focused on:

1. Other potential attribute naming conflicts with SQLAlchemy
2. Consistency in naming conventions throughout the application
3. Automated tests to catch similar issues in the future

================
File: docs/technical/metis_rag_visualization.md
================
# Metis_RAG System Visualization

## System Architecture

```mermaid
graph TD
    subgraph "Frontend"
        UI[Web UI]
        API_Client[API Client]
    end

    subgraph "API Layer"
        API[FastAPI Endpoints]
        Auth[Authentication]
        API --> Auth
    end

    subgraph "Core RAG Engine"
        QueryAnalyzer[Query Analyzer]
        QueryPlanner[Query Planner]
        PlanExecutor[Plan Executor]
        ResponseSynthesizer[Response Synthesizer]
        ResponseEvaluator[Response Evaluator]
        ProcessLogger[Process Logger]
        
        QueryAnalyzer --> QueryPlanner
        QueryPlanner --> PlanExecutor
        PlanExecutor --> ResponseSynthesizer
        ResponseSynthesizer --> ResponseEvaluator
        ProcessLogger -.-> QueryAnalyzer
        ProcessLogger -.-> QueryPlanner
        ProcessLogger -.-> PlanExecutor
        ProcessLogger -.-> ResponseSynthesizer
        ProcessLogger -.-> ResponseEvaluator
    end

    subgraph "Tool System"
        ToolRegistry[Tool Registry]
        RAGTool[RAG Tool]
        CalculatorTool[Calculator Tool]
        DatabaseTool[Database Tool]
        
        ToolRegistry --> RAGTool
        ToolRegistry --> CalculatorTool
        ToolRegistry --> DatabaseTool
        PlanExecutor --> ToolRegistry
    end

    subgraph "Document Processing"
        DocumentAnalysisService[Document Analysis Service]
        WorkerPool[Worker Pool]
        ProcessingJob[Processing Job]
        
        DocumentAnalysisService --> WorkerPool
        WorkerPool --> ProcessingJob
    end

    subgraph "Database Layer"
        DB[(PostgreSQL)]
        DocumentRepository[Document Repository]
        ConversationRepository[Conversation Repository]
        AnalyticsRepository[Analytics Repository]
        
        DocumentRepository --> DB
        ConversationRepository --> DB
        AnalyticsRepository --> DB
    end

    subgraph "Vector Store"
        ChromaDB[(ChromaDB)]
        RAGTool --> ChromaDB
    end

    UI --> API_Client
    API_Client --> API
    API --> QueryAnalyzer
    API --> DocumentAnalysisService
    API --> DocumentRepository
    API --> ConversationRepository
    API --> AnalyticsRepository
    RAGTool --> DocumentRepository
    ProcessingJob --> DocumentRepository
```

## Database Schema

```mermaid
erDiagram
    Documents ||--o{ Chunks : contains
    Documents ||--o{ DocumentTags : has
    Tags ||--o{ DocumentTags : used_in
    Documents }|--|| Folders : stored_in
    Conversations ||--o{ Messages : contains
    Messages ||--o{ Citations : references
    Citations }o--|| Documents : cites
    Citations }o--|| Chunks : cites_specific
    ProcessingJobs ||--o{ Documents : processes
    AnalyticsQueries }o--|| Documents : uses
    
    Documents {
        uuid id PK
        string filename
        string content
        jsonb metadata
        string folder
        timestamp uploaded
        string processing_status
        string processing_strategy
        int file_size
        string file_type
        timestamp last_accessed
    }
    
    Chunks {
        uuid id PK
        uuid document_id FK
        string content
        jsonb metadata
        int index
        float embedding_quality
        timestamp created_at
    }
    
    Tags {
        serial id PK
        string name UK
        timestamp created_at
        int usage_count
    }
    
    DocumentTags {
        uuid document_id FK
        int tag_id FK
        timestamp added_at
    }
    
    Folders {
        string path PK
        string name
        string parent_path
        int document_count
        timestamp created_at
    }
    
    Conversations {
        uuid id PK
        timestamp created_at
        timestamp updated_at
        jsonb metadata
        int message_count
    }
    
    Messages {
        serial id PK
        uuid conversation_id FK
        string content
        string role
        timestamp timestamp
        int token_count
    }
    
    Citations {
        serial id PK
        int message_id FK
        uuid document_id FK
        uuid chunk_id FK
        float relevance_score
        string excerpt
        int character_range_start
        int character_range_end
    }
    
    ProcessingJobs {
        uuid id PK
        string status
        timestamp created_at
        timestamp completed_at
        int document_count
        int processed_count
        string strategy
        jsonb metadata
        float progress_percentage
        string error_message
    }
    
    AnalyticsQueries {
        serial id PK
        string query
        string model
        boolean use_rag
        timestamp timestamp
        float response_time_ms
        int token_count
        jsonb document_ids
        string query_type
        boolean successful
    }
```

## Query Processing Flow

```mermaid
sequenceDiagram
    participant User
    participant API as API Layer
    participant QA as Query Analyzer
    participant QP as Query Planner
    participant PE as Plan Executor
    participant TR as Tool Registry
    participant RT as RAG Tool
    participant VS as Vector Store
    participant RS as Response Synthesizer
    participant RE as Response Evaluator
    participant PL as Process Logger

    User->>API: Submit Query
    API->>QA: Analyze Query
    QA->>PL: Log Analysis
    QA->>QP: Create Plan
    QP->>PL: Log Plan
    QP->>PE: Execute Plan
    
    loop For Each Step
        PE->>TR: Get Tool
        TR->>RT: Execute RAG Tool
        RT->>VS: Retrieve Chunks
        VS-->>RT: Return Chunks
        RT-->>PE: Return Results
        PE->>PL: Log Step Results
    end
    
    PE->>RS: Synthesize Response
    RS->>PL: Log Synthesis
    RS->>RE: Evaluate Response
    RE->>PL: Log Evaluation
    
    alt Response Needs Refinement
        RE->>RS: Refine Response
        RS->>PL: Log Refinement
    end
    
    RE-->>API: Return Final Response
    API-->>User: Display Response
```

## Document Processing Flow

```mermaid
sequenceDiagram
    participant User
    participant API as API Layer
    participant DAS as Document Analysis Service
    participant WP as Worker Pool
    participant PJ as Processing Job
    participant DR as Document Repository
    participant VS as Vector Store

    User->>API: Upload Document
    API->>DR: Store Document
    API->>DAS: Analyze Document
    DAS->>WP: Create Processing Job
    WP->>PJ: Execute Job
    
    PJ->>DR: Retrieve Document
    DR-->>PJ: Return Document
    PJ->>PJ: Chunk Document
    PJ->>VS: Store Chunks & Embeddings
    PJ->>DR: Update Document Status
    
    DR-->>API: Return Success
    API-->>User: Confirm Processing
```

## Tool System

```mermaid
classDiagram
    class Tool {
        <<abstract>>
        +String name
        +String description
        +execute(input_data) Any
        +get_description() String
        +get_input_schema() Dict
        +get_output_schema() Dict
        +get_examples() List
    }
    
    class ToolRegistry {
        -Dict tools
        +register_tool(tool) void
        +get_tool(name) Tool
        +list_tools() List
        +get_tool_examples(name) List
        +get_tool_count() int
    }
    
    class RAGTool {
        -RAGEngine rag_engine
        +execute(input_data) Dict
        +get_input_schema() Dict
        +get_output_schema() Dict
        +get_examples() List
    }
    
    class DatabaseTool {
        -DatabaseConnection db_connection
        +execute(input_data) Dict
        +get_input_schema() Dict
        +get_output_schema() Dict
        +get_examples() List
    }
    
    class CalculatorTool {
        +execute(input_data) Dict
        +get_input_schema() Dict
        +get_output_schema() Dict
        +get_examples() List
    }
    
    Tool <|-- RAGTool
    Tool <|-- DatabaseTool
    Tool <|-- CalculatorTool
    ToolRegistry o-- Tool
```

## LangGraph Integration

```mermaid
graph TD
    subgraph "LangGraph State Machine"
        QA[Query Analysis]
        QP[Query Planning]
        PE[Plan Execution]
        RT[Retrieval]
        QR[Query Refinement]
        CO[Context Optimization]
        GN[Generation]
        RE[Response Evaluation]
        RR[Response Refinement]
        CP[Complete]
        
        QA -->|Simple Query| RT
        QA -->|Complex Query| QP
        QP --> PE
        PE --> RT
        RT -->|Needs Refinement| QR
        RT -->|Good Results| CO
        QR --> RT
        CO --> GN
        GN --> RE
        RE -->|Needs Refinement| RR
        RE -->|Good Response| CP
        RR --> RE
    end
```

## Key Components

### Core RAG Engine
- **Query Analyzer**: Determines query complexity and required tools
- **Query Planner**: Creates execution plans for complex queries
- **Plan Executor**: Executes query plans using appropriate tools
- **Response Synthesizer**: Generates coherent responses from execution results
- **Response Evaluator**: Assesses response quality and determines if refinement is needed
- **Process Logger**: Records the entire query processing workflow for auditing

### Tool System
- **Tool Registry**: Manages available tools
- **RAG Tool**: Retrieves information from documents using vector search
- **Calculator Tool**: Performs mathematical calculations
- **Database Tool**: Queries structured data

### Document Processing
- **Document Analysis Service**: Analyzes documents to determine optimal processing strategies
- **Worker Pool**: Manages parallel document processing
- **Processing Job**: Represents a document processing task

### Database Layer
- **Document Repository**: Manages document storage and retrieval
- **Conversation Repository**: Stores conversation history
- **Analytics Repository**: Records query analytics

### Vector Store
- **ChromaDB**: Stores document chunks and embeddings for semantic search

================
File: docs/technical/query_refinement_fix.md
================
# Query Refinement Fix Documentation

## Issue Summary

The Metis RAG system was experiencing poor response quality for certain queries, particularly those involving entity names like "Stabilium". Analysis of the logs revealed several issues:

1. **Entity Name Corruption**: The Retrieval Judge was refining queries but introducing typos in entity names (e.g., "Stabilium" → "Stabilim").

2. **Excessive Context Filtering**: The system was filtering out too many chunks, resulting in insufficient context for the LLM (only 525 characters in one case).

3. **Citation Errors**: The system was attempting to add citations with non-existent chunk IDs, resulting in broken references.

4. **Authentication Issues**: Analytics requests were failing with 401 errors due to missing authentication tokens.

## Implemented Fixes

### 1. Entity Preservation in Query Refinement

Enhanced the `_parse_refined_query` method in `RetrievalJudge` to:
- Identify key entities (capitalized words) in the original query
- Check for potential typos in the refined query using string similarity
- Replace any similar but incorrect entity names with the original versions
- Add missing entities back to the query when necessary

This ensures that important entity names like "Stabilium" are preserved correctly during query refinement.

```python
# Key implementation in RetrievalJudge._parse_refined_query
key_entities = []
for word in original_query.split():
    clean_word = word.strip(",.;:!?()[]{}\"'")
    if clean_word and clean_word[0].isupper() and len(clean_word) >= 3:
        key_entities.append(clean_word)

# Check if entities are preserved in the refined query
for entity in key_entities:
    # Check for similarity with words in the refined query
    for i, word in enumerate(refined_words):
        similarity = self._string_similarity(clean_word, entity)
        if similarity >= similarity_threshold and similarity < 1.0:
            # Replace the typo with the correct entity
            refined_words[i] = refined_words[i].replace(clean_word, entity)
```

### 2. Minimum Context Requirements

Modified the context filtering in `_enhanced_retrieval` to ensure sufficient context:
- Set minimum thresholds for both chunk count (at least 3) and context length (1000+ characters)
- Implemented progressive threshold reduction when relevance filtering returns insufficient context
- Added fallback to include the best available chunks regardless of relevance when necessary

This prevents the system from providing too little context to the LLM, which was causing poor responses.

```python
# Key implementation in RAGEngine._enhanced_retrieval
MIN_CHUNKS = 3  # Minimum number of chunks
MIN_CONTEXT_LENGTH = 1000  # Minimum context length in characters

# If we don't have enough chunks after filtering
if len(relevant_results) < MIN_CHUNKS:
    # Progressively lower threshold until we get enough chunks
    adjusted_threshold = relevance_threshold
    while len(relevant_results) < MIN_CHUNKS and adjusted_threshold > 0.2:
        adjusted_threshold -= 0.1
        # Reapply filtering with lower threshold
        # ...
```

### 3. Robust Chunk ID Management

Enhanced the chunk ID management in `_parse_chunks_evaluation` to:
- Create a mapping of valid chunk IDs at the start of evaluation
- Ensure all relevance scores are mapped to valid, existing chunk IDs
- Add robust error handling for missing or invalid IDs
- Track validation results for better debugging

This prevents the system from referencing non-existent chunks in citations.

```python
# Key implementation in RetrievalJudge._parse_chunks_evaluation
# Create a mapping of valid chunk IDs for validation
valid_chunk_ids = {}
chunk_id_to_index = {}  # For reverse lookup

# Build mappings between indices and chunk IDs
for i, chunk in enumerate(chunks):
    if "chunk_id" in chunk and chunk["chunk_id"]:
        # Map from index to chunk_id
        valid_chunk_ids[str(i+1)] = chunk["chunk_id"]
        # Map from chunk_id to index
        chunk_id_to_index[chunk["chunk_id"]] = i+1
```

### 4. Enhanced Citation Handling

Improved the `add_citation` method in `ConversationRepository` to:
- Validate message existence before attempting to add citations
- Try to find matching chunks when only document ID is available
- Provide better fallback mechanisms when chunk IDs don't exist
- Add detailed logging for debugging citation issues

This ensures that citations are handled gracefully even when chunk IDs are missing or invalid.

```python
# Key implementation in ConversationRepository.add_citation
# If we have a valid document but no chunk_id, try to find a suitable chunk
if not chunk_id and excerpt:
    # Try to find a chunk that contains the excerpt
    stmt = select(Chunk).filter(
        Chunk.document_id == document_id,
        Chunk.content.contains(excerpt[:100])  # Use first 100 chars of excerpt
    ).limit(1)
    result = await self.session.execute(stmt)
    matching_chunk = result.scalars().first()
    
    if matching_chunk:
        chunk_id = matching_chunk.id
```

### 5. Authentication Fix for Analytics

Modified the analytics endpoint to handle internal requests without authentication:
- Detect internal requests from localhost/127.0.0.1
- Skip authentication for internal requests
- Add fallback authentication methods (headers, query params)
- Return 200 status for internal requests even on error to prevent RAG engine failures

This ensures that analytics data is still recorded even when authentication is not available.

```python
# Key implementation in analytics.py
# Check if this is an internal request (from localhost/127.0.0.1)
is_internal = client_host in ["127.0.0.1", "localhost", "::1"]

# For internal requests, we don't require authentication
if not is_internal and "auth_token" not in cookies:
    # Check for token in headers as fallback
    auth_header = request.headers.get("Authorization")
    if not auth_header or not auth_header.startswith("Bearer "):
        logger.warning("Unauthenticated external request to analytics endpoint")
        # For analytics, we'll just log a warning but still process the request
        query_data["user_id"] = None  # Mark as anonymous
```

## Testing

A test script (`tests/test_query_refinement_fix.py`) has been created to verify the fixes:
- Tests entity preservation in query refinement
- Tests minimum context requirements
- Tests citation handling with non-existent chunk IDs

Run the test script with:

```bash
python tests/test_query_refinement_fix.py
```

## Future Improvements

1. **Enhanced Entity Recognition**: Implement more sophisticated entity recognition using NLP techniques to better identify and preserve important entities in queries.

2. **Adaptive Context Optimization**: Develop a more adaptive approach to context optimization that considers query complexity, document relevance, and available context window.

3. **Chunk Tracking Improvements**: Implement a more robust chunk tracking system that maintains chunk identity throughout the entire RAG pipeline.

4. **Comprehensive Testing**: Add more comprehensive testing for edge cases and regression testing for the query refinement process.

================
File: docs/technical/reorganization_summary.md
================
# Metis RAG Project Reorganization Summary

## Overview

This document summarizes the reorganization of the Metis RAG project file structure that was completed on March 27, 2025. The goal was to improve the organization of files in the root directory by moving them to appropriate subdirectories based on their purpose.

## Files Moved

### Documentation Files

**Moved to docs/implementation/**
- llm_enhanced_rag_implementation_plan_updated.md
- Mem0_Docker_Integration_Plan.md
- Metis_RAG_Access_Control_Implementation_Plan.md
- Metis_RAG_Authentication_Implementation_Plan.md
- Metis_RAG_Database_Integration_Plan.md
- Metis_RAG_Implementation_Checklist.md
- Metis_RAG_Implementation_Plan_Part1.md
- Metis_RAG_Implementation_Plan_Part2.md
- Metis_RAG_Implementation_Plan_Part3.md
- Metis_RAG_Implementation_Plan_Part4.md
- Metis_RAG_Implementation_Progress_Update.md
- Metis_RAG_Implementation_Steps.md

**Moved to docs/setup/**
- Metis_RAG_Setup_Plan.md

**Moved to docs/technical/**
- Fix_SQLAlchemy_Metadata_Conflict.md
- technical_documentation.md
- metis_rag_testing_prompt.md
- metis_rag_visualization.md

### Test Files

**Moved to tests/**
- run_api_test.py
- run_authentication_test.py
- run_metis_rag_e2e_demo.py
- run_metis_rag_e2e_test.py
- test_auth_simple.py
- test_chat_api.py
- test_db_connection_simple.py
- test_db_connection.py
- test_db_simple.py
- test_document_upload.py
- test_rag_retrieval.py
- try_rag_query.py
- test_document_upload_enhanced.html

**Moved to tests/data/**
- test_document.txt
- test_upload_document.txt

**Moved to tests/results/**
- chunking_judge_real_results.json

### Utility Scripts

**Moved to scripts/utils/**
- create_test_user_simple.py
- simple_app.py
- view_visualization.py

### Configuration Files

**Moved to config/**
- .env.example
- .env.test

### Data Files

**Moved to data/**
- nonexistent.db
- test.db
- cookies.txt
- metis_rag_visualization.html

## Files Remaining in Root Directory

The following files remain in the root directory as they are essential project files:
- .clinerules-code
- .dockerignore
- .env
- .gitignore
- README.md
- alembic.ini
- pyproject.toml
- repomix-output.txt
- requirements.txt

## Benefits of Reorganization

1. **Improved Navigation**: Files are now organized by purpose, making it easier to find what you're looking for.
2. **Better Maintainability**: Related files are grouped together, making maintenance and updates more straightforward.
3. **Cleaner Root Directory**: The root directory now contains only the most essential files, reducing clutter.
4. **Logical Grouping**: Files are grouped by their function and relationship to each other.
5. **Better Onboarding**: New developers can more quickly understand the project structure and find relevant files.

## Next Steps

1. Update any import paths or file references in code that may have been affected by the reorganization.
2. Update documentation to reference the new file locations.
3. Consider implementing similar organization for any new files added to the project in the future.

================
File: docs/technical/technical_documentation.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies:
  - Recursive: Splits text recursively by characters, good for general text
  - Token: Splits text by token count rather than character count, useful for LLMs with token limits
  - Markdown: Splits markdown documents by headers, good for structured documents
  - Semantic: Uses LLM to identify natural semantic boundaries, best for preserving meaning
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

#### Chunking Judge

The Chunking Judge is an LLM-based agent that enhances the document processing pipeline by:
- Analyzing document structure, content type, and formatting
- Selecting the most appropriate chunking strategy (recursive, markdown, or semantic)
- Recommending optimal chunk size and overlap parameters
- Providing detailed justification for its recommendations

Our testing shows the Chunking Judge effectively:
- Recognizes document structures, even identifying markdown-like elements in plain text
- Selects appropriate strategies that align with document structure
- Optimizes parameters based on document characteristics
- Adapts to different document types without manual configuration

#### Semantic Chunker

The Semantic Chunker is an advanced chunking strategy that:
- Uses LLM to identify natural semantic boundaries in text
- Preserves semantic meaning and context in chunks
- Creates more coherent, self-contained chunks than traditional methods
- Respects the logical flow of information in documents

Key features include:
- Intelligent boundary detection based on topic transitions and subject matter shifts
- Handling of long documents by processing in sections
- Caching for performance optimization
- Fallback mechanisms for error handling

#### Retrieval Judge

The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by:
- Analyzing queries to determine optimal retrieval parameters
- Evaluating retrieved chunks for relevance
- Refining queries when needed to improve retrieval precision
- Optimizing context assembly for better response generation

Our testing shows the Retrieval Judge significantly improves retrieval quality:
- 89.26% faster retrieval times through effective caching
- Transforms ambiguous queries into specific, detailed requests
- Reduces context size by 76.4% on average while maintaining relevance
- Performs best with domain-specific and complex queries

## Deployment Options

Metis RAG can be deployed in several ways:

1. **Local Development**
   - Run directly with Python and uvicorn
   - Suitable for development and testing

2. **Docker Deployment**
   - Use the provided Dockerfile and docker-compose.yml
   - Containerized deployment for easier management

3. **Production Deployment**
   - Use a reverse proxy like Nginx
   - Configure proper authentication
   - Set up monitoring and logging

## Configuration

Metis RAG is configured through environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| OLLAMA_BASE_URL | URL for Ollama API | http://localhost:11434 |
| DEFAULT_MODEL | Default LLM model | llama3 |
| DEFAULT_EMBEDDING_MODEL | Model for embeddings | nomic-embed-text |
| UPLOAD_DIR | Directory for uploaded files | ./uploads |
| CHROMA_DB_DIR | Directory for vector DB | ./chroma_db |
| CHUNK_SIZE | Default chunk size | 500 |
| CHUNK_OVERLAP | Default chunk overlap | 50 |

## API Reference

### Document API

- `POST /api/documents/upload` - Upload a document
- `GET /api/documents/list` - List all documents
- `GET /api/documents/{document_id}` - Get document details
- `DELETE /api/documents/{document_id}` - Delete a document
- `POST /api/documents/process` - Process documents

### Chat API

- `POST /api/chat/message` - Send a chat message
- `GET /api/chat/history` - Get chat history
- `DELETE /api/chat/history` - Clear chat history

## Performance Considerations

For optimal performance:
- Use appropriate chunking strategies for different document types:
  - Semantic chunking for complex documents where preserving meaning is critical
  - Markdown chunking for structured documents with clear headers
  - Recursive chunking for general text with natural separators
- Configure chunk size based on your specific use case
- Consider hardware requirements for embedding generation
- Monitor vector store size and performance
- Enable the Chunking Judge to automatically select optimal chunking strategies
- Enable the Retrieval Judge for complex queries requiring precision
- Leverage caching for improved performance (33.33% vector store cache hit rate)
- Consider using a smaller model for the Judges in latency-sensitive applications

### Retrieval Judge Performance

Our testing shows that the Retrieval Judge significantly improves retrieval performance:
- 89.26% faster retrieval times compared to standard retrieval (18.41s vs 171.47s on average)
- Effective caching of both vector store queries and LLM responses
- Query analysis takes 9.03s on average
- Query refinement is very fast (2.08s on average)
- Context optimization reduces context size by 76.4% on average

### Semantic Chunking Performance

The Semantic Chunker provides several advantages over traditional chunking methods:
- Creates more coherent, self-contained chunks that maintain semantic integrity
- Reduces the need for large chunk overlaps since boundaries are semantically meaningful
- Improves retrieval precision by ensuring chunks contain complete concepts
- Works particularly well with complex, technical, or narrative content
- Caching mechanism minimizes the performance impact of LLM-based chunking

================
File: docs/technical/TESTING.md
================
# Metis RAG Testing Strategy

This document outlines the comprehensive testing strategy for the Metis RAG application, covering automated unit tests, integration tests, performance benchmarks, UI testing, and more.

## Table of Contents

1. [Testing Objectives](#testing-objectives)
2. [Testing Levels](#testing-levels)
3. [Test Implementation](#test-implementation)
4. [Running Tests](#running-tests)
5. [Test Reports](#test-reports)
6. [CI/CD Integration](#cicd-integration)

## Testing Objectives

The primary objectives of this testing strategy are to:

1. Ensure the factual accuracy and quality of RAG responses
2. Verify proper handling of various file types and multiple file uploads
3. Test core RAG components (retrieval, augmentation, generation)
4. Validate end-to-end functionality across system boundaries
5. Measure and optimize performance
6. Identify and address edge cases and potential failure points
7. Establish regression testing to prevent feature regressions
8. Generate detailed test reports for demonstration purposes

## Testing Levels

### Unit Testing

Unit tests focus on testing individual components in isolation:

- **RAG Engine Testing**: Tests query function, context retrieval, response generation, conversation history handling, and citation generation.
- **Vector Store Testing**: Tests document addition and retrieval, embedding creation and storage, search functionality, metadata filtering, and caching mechanisms.
- **Document Processor Testing**: Tests different chunking strategies, handling of various file types, metadata extraction, and error handling.
- **Ollama Client Testing**: Tests model generation, embedding creation, streaming vs. non-streaming responses, and error handling.

### Integration Testing

Integration tests focus on testing component interactions and API endpoints:

- **API Endpoint Testing**: Tests all API endpoints for correct responses, error handling, request validation, and authentication.
- **Component Interaction Testing**: Tests RAG Engine with Vector Store, Document Processor with Vector Store, RAG Engine with Ollama Client, and end-to-end document processing and retrieval flow.

### System Testing

System tests focus on testing end-to-end workflows and performance:

- **End-to-End Workflow Testing**: Tests complete document upload, processing, and retrieval workflow, chat conversation with context from multiple documents, and analytics functionality.
- **Performance Testing**: Measures response time, throughput, and resource utilization under various loads.
- **Stress Testing**: Tests with high concurrency, large document sets, and complex queries.

### Edge Case Testing

Edge case tests focus on testing unusual inputs, error handling, and system resilience:

- **Input Validation**: Tests with malformed queries, invalid documents, and extremely long or short inputs.
- **Error Handling**: Tests network failures, service unavailability, and database connection issues.
- **Resource Constraints**: Tests under memory limitations, limited disk space, and CPU constraints.

## Test Implementation

The test implementation is organized into several test suites:

### RAG Quality Tests (`tests/test_rag_quality.py`)

Tests for factual accuracy, relevance, and citation quality:

- **Factual Accuracy Testing**: Tests if responses contain expected facts from the source documents.
- **Multi-Document Retrieval**: Tests retrieval across multiple documents.
- **Citation Quality**: Tests if responses properly cite sources.
- **API Integration**: Tests the complete flow through the API endpoints.

### File Handling Tests (`tests/test_file_handling.py`)

Tests for different file types, multiple file uploads, and large files:

- **File Type Support**: Tests processing of different file types (PDF, TXT, CSV, MD).
- **Chunking Strategies**: Tests different chunking strategies (recursive, token, markdown).
- **Large File Handling**: Tests processing of very large files.
- **Multiple File Upload**: Tests concurrent processing of multiple documents.

### Performance Tests (`tests/test_performance.py`)

Tests for response time, throughput, and resource utilization:

- **Query Response Time**: Measures response time for different query types.
- **Throughput**: Measures throughput under various concurrency levels.
- **Resource Utilization**: Monitors CPU and memory usage during sustained load.
- **API Performance**: Measures performance of API endpoints.

### Edge Case Tests (`tests/test_edge_cases.py`)

Tests for unusual inputs, error handling, and system resilience:

- **Special Character Queries**: Tests queries with special characters, SQL injection, XSS, etc.
- **Malformed Documents**: Tests handling of malformed or corrupted documents.
- **Network Failures**: Tests handling of network failures when communicating with Ollama.
- **Invalid Model Names**: Tests handling of invalid model names.
- **Concurrent Processing**: Tests handling of concurrent document processing requests.
- **Vector Store Resilience**: Tests vector store resilience to invalid operations.

## Running Tests

### Prerequisites

- Python 3.10 or higher
- All dependencies installed (`pip install -r requirements.txt`)
- Ollama running locally (for tests that interact with the LLM)

### Running All Tests

To run all tests, use the `run_tests.py` script:

```bash
python run_tests.py
```

This will run all test suites and generate a comprehensive report.

### Running Specific Test Suites

To run a specific test suite, use the `--suite` option:

```bash
python run_tests.py --suite rag_quality
python run_tests.py --suite file_handling
python run_tests.py --suite performance
python run_tests.py --suite edge_cases
```

### Additional Options

- `--failfast`: Stop on first failure
- `--open-report`: Open the HTML report in a browser
- `--save-reports`: Save reports to a timestamped directory

### Running Individual Tests

You can also run individual test files directly using pytest:

```bash
pytest -xvs tests/test_rag_quality.py
pytest -xvs tests/test_file_handling.py
pytest -xvs tests/test_performance.py
pytest -xvs tests/test_edge_cases.py
```

## Test Reports

The test runner generates several reports:

- **Master Report**: `metis_rag_test_report.json` and `metis_rag_test_report.html`
- **RAG Quality Reports**: `test_quality_results.json`, `test_multi_doc_results.json`, etc.
- **File Handling Reports**: `test_file_type_results.json`, `test_chunking_strategy_results.json`, etc.
- **Performance Reports**: `performance_benchmark_report.json`, `performance_benchmark_report.html`, etc.
- **Edge Case Reports**: `edge_case_test_report.json`, `edge_case_test_report.html`, etc.

These reports provide detailed information about test results, including:

- Test success/failure status
- Performance metrics
- Factual accuracy metrics
- Edge case handling metrics
- Detailed logs and error messages

## CI/CD Integration

The testing framework is designed to be integrated into a CI/CD pipeline. Here's a recommended approach:

1. **Continuous Integration**: Run unit and integration tests on every commit.
2. **Nightly Builds**: Run performance and edge case tests nightly.
3. **Release Validation**: Run all tests before releasing a new version.

### GitHub Actions Example

```yaml
name: Metis RAG Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run tests
      run: |
        python run_tests.py
    - name: Upload test reports
      uses: actions/upload-artifact@v2
      with:
        name: test-reports
        path: |
          metis_rag_test_report.html
          performance_benchmark_report.html
          edge_case_test_report.html
```

## Extending the Test Suite

To add new tests:

1. Create a new test file in the `tests` directory.
2. Add the test file to the appropriate test suite in `run_tests.py`.
3. Implement test functions using the pytest framework.
4. Generate test reports in JSON format for integration with the reporting system.

## Best Practices

- **Isolation**: Ensure tests are isolated and don't depend on each other.
- **Cleanup**: Clean up any resources created during tests.
- **Mocking**: Use mocks for external dependencies when appropriate.
- **Assertions**: Use clear assertions that provide helpful error messages.
- **Documentation**: Document test functions with clear docstrings.
- **Reporting**: Generate detailed reports for analysis and visualization.

================
File: docs/async_database_implementation.md
================
# Async Database Implementation

This document provides an overview of the new asynchronous database implementation in Metis RAG, which enhances the system with true async database operations for improved performance and scalability.

## Overview

The original `DatabaseTool` implementation used synchronous SQLite operations wrapped in async methods, which could block the event loop and cause performance issues in an otherwise asynchronous application. The new implementation provides true asynchronous database operations using:

- `aiosqlite` for SQLite databases
- `asyncpg` for PostgreSQL databases
- Asynchronous file operations for CSV and JSON data sources

## Benefits

- **Improved Performance**: Non-blocking database operations allow the application to handle more concurrent requests
- **Reduced Latency**: Async operations prevent the event loop from being blocked during database queries
- **PostgreSQL Support**: Native support for PostgreSQL databases with connection pooling
- **Better Resource Management**: Proper connection lifecycle management with connection pooling
- **Enhanced Security**: Secure connection handling with connection IDs to avoid exposing credentials

## Key Components

### 1. Database Connection Manager

The `DatabaseConnectionManager` class in `app/db/connection_manager.py` provides:

- Connection pooling for both SQLite and PostgreSQL
- Secure connection ID generation
- Connection lifecycle management
- Support for multiple database types

### 2. Async DatabaseTool

The new `DatabaseTool` implementation in `app/rag/tools/database_tool.py` provides:

- True async database operations
- Support for SQLite, PostgreSQL, CSV, and JSON data sources
- Proper error handling and performance logging
- Backward compatibility with existing tool usage patterns

## Migration Guide

### Automatic Migration

The easiest way to migrate is to use the provided migration script:

```bash
python scripts/migrate_to_async_database_tool.py
```

This script will:
1. Rename the old `DatabaseTool` to `DatabaseToolLegacy`
2. Install the new async implementation as `DatabaseTool`
3. Update imports in the codebase
4. Create a backup of the original implementation

### Manual Migration

If you prefer to migrate manually, follow these steps:

1. Rename the existing `app/rag/tools/database_tool.py` to `app/rag/tools/database_tool_legacy.py`
2. Rename the class inside from `DatabaseTool` to `DatabaseToolLegacy`
3. Move `app/rag/tools/database_tool_async.py` to `app/rag/tools/database_tool.py`
4. Update any imports in your code that reference the `DatabaseTool`

### Testing the Migration

After migration, run the tests to ensure everything works correctly:

```bash
pytest tests/unit/test_database_tool_async.py
```

## Using PostgreSQL

The new implementation adds support for PostgreSQL databases. To use PostgreSQL:

1. Ensure you have the required dependencies:
   ```bash
   pip install asyncpg
   ```

2. Use a PostgreSQL connection string as the source:
   ```python
   result = await database_tool.execute({
       'query': 'SELECT * FROM users',
       'source': 'postgresql://username:password@localhost:5432/mydb'
   })
   ```

## Configuration

The database connection manager can be configured through environment variables:

- `DATABASE_POOL_SIZE`: Maximum number of connections in the pool (default: 10)
- `DATABASE_MAX_OVERFLOW`: Maximum number of connections that can be created beyond the pool size (default: 20)
- `DATABASE_POOL_RECYCLE`: Number of seconds after which a connection is recycled (default: 3600)

## Troubleshooting

### Connection Issues

If you experience connection issues:

1. Check that the database server is running and accessible
2. Verify that the connection string is correct
3. Ensure you have the required permissions to access the database
4. Check for firewall or network issues

### Performance Issues

If you experience performance issues:

1. Adjust the connection pool size based on your application's needs
2. Use query parameters to avoid SQL injection and improve performance
3. Add appropriate indexes to your database tables
4. Use the `limit` parameter to restrict the number of results returned

### Compatibility Issues

If you experience compatibility issues:

1. The legacy implementation is still available as `DatabaseToolLegacy`
2. Check for any code that might be directly importing from `database_tool.py`
3. Ensure all async methods are properly awaited

## Advanced Usage

### Custom Connection Management

You can access the connection manager directly for advanced use cases:

```python
from app.db.connection_manager import connection_manager

# Register a connection
conn_id = connection_manager.register_connection('postgresql://username:password@localhost:5432/mydb')

# Get a connection
conn = await connection_manager.get_connection(conn_id)

# Use the connection
# ...

# Close the connection
await connection_manager.close(conn_id)
```

### Transaction Management

For operations that require transactions:

```python
async with connection_manager.get_connection(conn_id) as conn:
    await conn.execute("BEGIN")
    try:
        # Perform multiple operations
        await conn.execute("INSERT INTO users (name) VALUES (?)", ("John",))
        await conn.execute("UPDATE counters SET value = value + 1 WHERE name = ?", ("user_count",))
        await conn.execute("COMMIT")
    except Exception as e:
        await conn.execute("ROLLBACK")
        raise
```

## Future Enhancements

Planned enhancements for future releases:

1. Schema introspection capabilities
2. Query explanation and optimization
3. Support for PostgreSQL extensions like pgvector
4. MCP server interface for AI agent integration

================
File: docs/authentication_demo.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG Authentication Demo</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f7fa;
        }
        
        h1, h2, h3 {
            color: #2c3e50;
        }
        
        .container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }
        
        .panel {
            flex: 1;
            min-width: 300px;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            padding: 20px;
            margin-bottom: 20px;
        }
        
        .form-group {
            margin-bottom: 15px;
        }
        
        label {
            display: block;
            margin-bottom: 5px;
            font-weight: 600;
        }
        
        input[type="text"],
        input[type="email"],
        input[type="password"],
        textarea {
            width: 100%;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 16px;
        }
        
        button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
            transition: background-color 0.3s;
        }
        
        button:hover {
            background-color: #2980b9;
        }
        
        button:disabled {
            background-color: #95a5a6;
            cursor: not-allowed;
        }
        
        .result {
            background-color: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            margin-top: 15px;
            overflow-x: auto;
            white-space: pre-wrap;
            font-family: monospace;
            max-height: 300px;
            overflow-y: auto;
        }
        
        .success {
            color: #27ae60;
        }
        
        .error {
            color: #e74c3c;
        }
        
        .token-info {
            margin-top: 20px;
            padding: 15px;
            background-color: #ecf0f1;
            border-radius: 4px;
        }
        
        .token-info p {
            margin: 5px 0;
        }
        
        .tabs {
            display: flex;
            margin-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        
        .tab {
            padding: 10px 20px;
            cursor: pointer;
            border-bottom: 3px solid transparent;
        }
        
        .tab.active {
            border-bottom-color: #3498db;
            font-weight: bold;
        }
        
        .tab-content {
            display: none;
        }
        
        .tab-content.active {
            display: block;
        }
        
        .status-indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 5px;
        }
        
        .status-indicator.active {
            background-color: #27ae60;
        }
        
        .status-indicator.inactive {
            background-color: #e74c3c;
        }
        
        .timer {
            font-size: 14px;
            color: #7f8c8d;
            margin-top: 5px;
        }
        
        .flow-diagram {
            width: 100%;
            max-width: 800px;
            margin: 20px auto;
            display: block;
        }
        
        .code-block {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
            font-family: monospace;
        }
        
        .header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }
        
        .user-info {
            display: flex;
            align-items: center;
        }
        
        .avatar {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background-color: #3498db;
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            margin-right: 10px;
        }
    </style>
</head>
<body>
    <h1>Metis RAG Authentication Demo</h1>
    
    <div class="header">
        <p>This demo demonstrates the JWT authentication flow for the Metis RAG application.</p>
        <div class="user-info" id="userInfo" style="display: none;">
            <div class="avatar" id="userAvatar">U</div>
            <div>
                <strong id="userDisplayName">User</strong>
                <button id="logoutBtn" style="margin-left: 10px;">Logout</button>
            </div>
        </div>
    </div>
    
    <div class="tabs">
        <div class="tab active" data-tab="auth">Authentication</div>
        <div class="tab" data-tab="api">API Access</div>
        <div class="tab" data-tab="tokens">Token Management</div>
        <div class="tab" data-tab="docs">Documentation</div>
    </div>
    
    <div class="tab-content active" id="auth">
        <div class="container">
            <div class="panel">
                <h2>Register</h2>
                <form id="registerForm">
                    <div class="form-group">
                        <label for="regUsername">Username</label>
                        <input type="text" id="regUsername" required>
                    </div>
                    <div class="form-group">
                        <label for="regEmail">Email</label>
                        <input type="email" id="regEmail" required>
                    </div>
                    <div class="form-group">
                        <label for="regPassword">Password</label>
                        <input type="password" id="regPassword" required>
                    </div>
                    <div class="form-group">
                        <label for="regFullName">Full Name</label>
                        <input type="text" id="regFullName">
                    </div>
                    <button type="submit">Register</button>
                </form>
                <div id="registerResult" class="result" style="display: none;"></div>
            </div>
            
            <div class="panel">
                <h2>Login</h2>
                <form id="loginForm">
                    <div class="form-group">
                        <label for="loginUsername">Username</label>
                        <input type="text" id="loginUsername" required>
                    </div>
                    <div class="form-group">
                        <label for="loginPassword">Password</label>
                        <input type="password" id="loginPassword" required>
                    </div>
                    <button type="submit">Login</button>
                </form>
                <div id="loginResult" class="result" style="display: none;"></div>
            </div>
        </div>
        
        <div class="panel">
            <h2>Authentication Status</h2>
            <div>
                <p>
                    <span class="status-indicator inactive" id="authStatus"></span>
                    <span id="authStatusText">Not authenticated</span>
                </p>
                <div id="tokenInfo" class="token-info" style="display: none;">
                    <p><strong>Access Token:</strong> <span id="accessTokenDisplay"></span></p>
                    <p><strong>Refresh Token:</strong> <span id="refreshTokenDisplay"></span></p>
                    <p><strong>Expires:</strong> <span id="tokenExpiry"></span></p>
                    <div class="timer" id="tokenTimer"></div>
                </div>
            </div>
        </div>
    </div>
    
    <div class="tab-content" id="api">
        <div class="container">
            <div class="panel">
                <h2>Protected API Endpoints</h2>
                <div class="form-group">
                    <label for="apiEndpoint">Endpoint</label>
                    <select id="apiEndpoint">
                        <option value="/api/auth/me">Get Current User (/api/auth/me)</option>
                        <option value="/api/documents">List Documents (/api/documents)</option>
                        <option value="/api/system/info">System Info (/api/system/info)</option>
                    </select>
                </div>
                <button id="callApiBtn">Call API</button>
                <div id="apiResult" class="result" style="display: none;"></div>
            </div>
            
            <div class="panel">
                <h2>API Request Details</h2>
                <div class="code-block" id="requestDetails">
                    // API request details will appear here
                </div>
            </div>
        </div>
    </div>
    
    <div class="tab-content" id="tokens">
        <div class="container">
            <div class="panel">
                <h2>Token Management</h2>
                <button id="refreshTokenBtn">Refresh Token</button>
                <p>Use this to manually refresh your access token using the refresh token.</p>
                <div id="refreshResult" class="result" style="display: none;"></div>
            </div>
            
            <div class="panel">
                <h2>Token Decoder</h2>
                <div class="form-group">
                    <label for="tokenToDecode">JWT Token</label>
                    <textarea id="tokenToDecode" rows="5" placeholder="Paste a JWT token here"></textarea>
                </div>
                <button id="decodeTokenBtn">Decode Token</button>
                <div id="decodedToken" class="result" style="display: none;"></div>
            </div>
        </div>
        
        <div class="panel">
            <h2>Token Lifecycle</h2>
            <p>JWT tokens have a limited lifetime. The access token expires after a short period (default: 30 minutes), while the refresh token lasts longer (default: 7 days).</p>
            <p>When the access token expires, you can use the refresh token to get a new access token without requiring the user to log in again.</p>
            <p>This demo will automatically refresh the access token when it expires.</p>
        </div>
    </div>
    
    <div class="tab-content" id="docs">
        <div class="panel">
            <h2>Authentication Flow</h2>
            <p>The authentication flow consists of the following steps:</p>
            <ol>
                <li>User registers or logs in with credentials</li>
                <li>Server validates credentials and issues access and refresh tokens</li>
                <li>Client includes access token in API requests</li>
                <li>When access token expires, client uses refresh token to get a new access token</li>
                <li>User identity remains consistent regardless of token refreshes or password changes</li>
            </ol>
            
            <h3>Flow Diagram</h3>
            <pre class="code-block">
Client                                 Server
  |                                      |
  |  Register/Login with credentials     |
  | ------------------------------------> |
  |                                      |
  |  Access token + Refresh token        |
  | <------------------------------------ |
  |                                      |
  |  API request with Access token       |
  | ------------------------------------> |
  |                                      |
  |  Protected resource                  |
  | <------------------------------------ |
  |                                      |
  |  API request (token expired)         |
  | ------------------------------------> |
  |                                      |
  |  401 Unauthorized                    |
  | <------------------------------------ |
  |                                      |
  |  Refresh token request               |
  | ------------------------------------> |
  |                                      |
  |  New access token                    |
  | <------------------------------------ |
  |                                      |
  |  API request with new Access token   |
  | ------------------------------------> |
  |                                      |
  |  Protected resource                  |
  | <------------------------------------ |
  |                                      |
            </pre>
        </div>
        
        <div class="panel">
            <h2>API Endpoints</h2>
            <h3>Registration</h3>
            <pre class="code-block">
POST /api/auth/register
Content-Type: application/json

{
  "username": "testuser",
  "email": "test@example.com",
  "password": "securepassword",
  "full_name": "Test User"
}
            </pre>
            
            <h3>Login</h3>
            <pre class="code-block">
POST /api/auth/token
Content-Type: application/x-www-form-urlencoded

username=testuser&password=securepassword
            </pre>
            
            <h3>Token Refresh</h3>
            <pre class="code-block">
POST /api/auth/refresh
Content-Type: application/json

{
  "refresh_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
            </pre>
            
            <h3>Get Current User</h3>
            <pre class="code-block">
GET /api/auth/me
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
            </pre>
        </div>
    </div>
    
    <script>
        // Base URL for API
        const BASE_URL = 'http://localhost:8000';
        
        // DOM Elements
        const registerForm = document.getElementById('registerForm');
        const loginForm = document.getElementById('loginForm');
        const registerResult = document.getElementById('registerResult');
        const loginResult = document.getElementById('loginResult');
        const authStatus = document.getElementById('authStatus');
        const authStatusText = document.getElementById('authStatusText');
        const tokenInfo = document.getElementById('tokenInfo');
        const accessTokenDisplay = document.getElementById('accessTokenDisplay');
        const refreshTokenDisplay = document.getElementById('refreshTokenDisplay');
        const tokenExpiry = document.getElementById('tokenExpiry');
        const tokenTimer = document.getElementById('tokenTimer');
        const callApiBtn = document.getElementById('callApiBtn');
        const apiEndpoint = document.getElementById('apiEndpoint');
        const apiResult = document.getElementById('apiResult');
        const requestDetails = document.getElementById('requestDetails');
        const refreshTokenBtn = document.getElementById('refreshTokenBtn');
        const refreshResult = document.getElementById('refreshResult');
        const tokenToDecode = document.getElementById('tokenToDecode');
        const decodeTokenBtn = document.getElementById('decodeTokenBtn');
        const decodedToken = document.getElementById('decodedToken');
        const userInfo = document.getElementById('userInfo');
        const userAvatar = document.getElementById('userAvatar');
        const userDisplayName = document.getElementById('userDisplayName');
        const logoutBtn = document.getElementById('logoutBtn');
        const tabs = document.querySelectorAll('.tab');
        const tabContents = document.querySelectorAll('.tab-content');
        
        // Token storage
        let accessToken = localStorage.getItem('access_token');
        let refreshToken = localStorage.getItem('refresh_token');
        let tokenExpiryTime = localStorage.getItem('token_expiry');
        let tokenRefreshInterval;
        let currentUser = null;
        
        // Initialize
        function init() {
            // Check if user is logged in
            if (accessToken) {
                updateAuthStatus(true);
                startTokenTimer();
                fetchCurrentUser();
            } else {
                updateAuthStatus(false);
            }
            
            // Set up event listeners
            registerForm.addEventListener('submit', handleRegister);
            loginForm.addEventListener('submit', handleLogin);
            callApiBtn.addEventListener('click', callApi);
            refreshTokenBtn.addEventListener('click', handleRefreshToken);
            decodeTokenBtn.addEventListener('click', decodeToken);
            logoutBtn.addEventListener('click', logout);
            
            // Tab switching
            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    tabs.forEach(t => t.classList.remove('active'));
                    tabContents.forEach(c => c.classList.remove('active'));
                    
                    tab.classList.add('active');
                    document.getElementById(tab.dataset.tab).classList.add('active');
                });
            });
            
            // Populate token decoder if token exists
            if (accessToken) {
                tokenToDecode.value = accessToken;
            }
        }
        
        // Update authentication status display
        function updateAuthStatus(isAuthenticated) {
            if (isAuthenticated) {
                authStatus.classList.remove('inactive');
                authStatus.classList.add('active');
                authStatusText.textContent = 'Authenticated';
                tokenInfo.style.display = 'block';
                
                // Display token info
                accessTokenDisplay.textContent = truncateToken(accessToken);
                refreshTokenDisplay.textContent = truncateToken(refreshToken);
                
                const expiryDate = new Date(parseInt(tokenExpiryTime));
                tokenExpiry.textContent = expiryDate.toLocaleString();
                
                // Show user info if available
                if (currentUser) {
                    userInfo.style.display = 'flex';
                    userAvatar.textContent = currentUser.username.charAt(0).toUpperCase();
                    userDisplayName.textContent = currentUser.full_name || currentUser.username;
                }
            } else {
                authStatus.classList.remove('active');
                authStatus.classList.add('inactive');
                authStatusText.textContent = 'Not authenticated';
                tokenInfo.style.display = 'none';
                userInfo.style.display = 'none';
                
                // Clear token timer
                if (tokenRefreshInterval) {
                    clearInterval(tokenRefreshInterval);
                }
            }
        }
        
        // Handle register form submission
        async function handleRegister(e) {
            e.preventDefault();
            
            const userData = {
                username: document.getElementById('regUsername').value,
                email: document.getElementById('regEmail').value,
                password: document.getElementById('regPassword').value,
                full_name: document.getElementById('regFullName').value
            };
            
            try {
                const response = await fetch(`${BASE_URL}/api/auth/register`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(userData)
                });
                
                const data = await response.json();
                
                registerResult.style.display = 'block';
                
                if (response.ok) {
                    registerResult.innerHTML = `<span class="success">Registration successful!</span><pre>${JSON.stringify(data, null, 2)}</pre>`;
                    // Pre-fill login form
                    document.getElementById('loginUsername').value = userData.username;
                    document.getElementById('loginPassword').value = userData.password;
                } else {
                    registerResult.innerHTML = `<span class="error">Registration failed:</span><pre>${JSON.stringify(data, null, 2)}</pre>`;
                }
            } catch (error) {
                registerResult.style.display = 'block';
                registerResult.innerHTML = `<span class="error">Error:</span><pre>${error.message}</pre>`;
            }
        }
        
        // Handle login form submission
        async function handleLogin(e) {
            e.preventDefault();
            
            const formData = new URLSearchParams();
            formData.append('username', document.getElementById('loginUsername').value);
            formData.append('password', document.getElementById('loginPassword').value);
            
            try {
                const response = await fetch(`${BASE_URL}/api/auth/token`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/x-www-form-urlencoded'
                    },
                    body: formData
                });
                
                const data = await response.json();
                
                loginResult.style.display = 'block';
                
                if (response.ok) {
                    loginResult.innerHTML = `<span class="success">Login successful!</span><pre>${JSON.stringify(data, null, 2)}</pre>`;
                    
                    // Store tokens
                    accessToken = data.access_token;
                    refreshToken = data.refresh_token;
                    tokenExpiryTime = Date.now() + (data.expires_in * 1000);
                    
                    localStorage.setItem('access_token', accessToken);
                    localStorage.setItem('refresh_token', refreshToken);
                    localStorage.setItem('token_expiry', tokenExpiryTime);
                    
                    updateAuthStatus(true);
                    startTokenTimer();
                    fetchCurrentUser();
                    
                    // Populate token decoder
                    tokenToDecode.value = accessToken;
                } else {
                    loginResult.innerHTML = `<span class="error">Login failed:</span><pre>${JSON.stringify(data, null, 2)}</pre>`;
                }
            } catch (error) {
                loginResult.style.display = 'block';
                loginResult.innerHTML = `<span class="error">Error:</span><pre>${error.message}</pre>`;
            }
        }
        
        // Fetch current user
        async function fetchCurrentUser() {
            if (!accessToken) return;
            
            try {
                const response = await fetch(`${BASE_URL}/api/auth/me`, {
                    headers: {
                        'Authorization': `Bearer ${accessToken}`
                    }
                });
                
                if (response.ok) {
                    currentUser = await response.json();
                    
                    // Update user info display
                    userInfo.style.display = 'flex';
                    userAvatar.textContent = currentUser.username.charAt(0).toUpperCase();
                    userDisplayName.textContent = currentUser.full_name || currentUser.username;
                } else {
                    // Token might be invalid
                    if (response.status === 401) {
                        await refreshAccessToken();
                        fetchCurrentUser();
                    }
                }
            } catch (error) {
                console.error('Error fetching user:', error);
            }
        }
        
        // Call API endpoint
        async function callApi() {
            const endpoint = apiEndpoint.value;
            
            // Check if token needs refresh
            if (tokenExpiryTime && Date.now() > tokenExpiryTime) {
                await refreshAccessToken();
            }
            
            try {
                // Update request details
                requestDetails.textContent = `// Request
fetch('${BASE_URL}${endpoint}', {
    headers: {
        'Authorization': 'Bearer ${truncateToken(accessToken)}'
    }
})`;
                
                const response = await fetch(`${BASE_URL}${endpoint}`, {
                    headers: {
                        'Authorization': `Bearer ${accessToken}`
                    }
                });
                
                const data = await response.json();
                
                apiResult.style.display = 'block';
                
                if (response.ok) {
                    apiResult.innerHTML = `<span class="success">API call successful!</span><pre>${JSON.stringify(data, null, 2)}</pre>`;
                } else {
                    apiResult.innerHTML = `<span class="error">API call failed:</span><pre>${JSON.stringify(data, null, 2)}</pre>`;
                    
                    // Token might be invalid
                    if (response.status === 401) {
                        await refreshAccessToken();
                    }
                }
            } catch (error) {
                apiResult.style.display = 'block';
                apiResult.innerHTML = `<span class="error">Error:</span><pre>${error.message}</pre>`;
            }
        }
        
        // Handle manual token refresh
        async function handleRefreshToken() {
            const result = await refreshAccessToken();
            
            refreshResult.style.display = 'block';
            
            if (result.success) {
                refreshResult.innerHTML = `<span class="success">Token refreshed successfully!</span><pre>${JSON.stringify(result.data, null, 2)}</pre>`;
            } else {
                refreshResult.innerHTML = `<span class="error">Token refresh failed:</span><pre>${result.error}</pre>`;
            }
        }
        
        // Refresh access token
        async function refreshAccessToken() {
            if (!refreshToken) {
                return { success: false, error: 'No refresh token available' };
            }
            
            try {
                const response = await fetch(`${BASE_URL}/api/auth/refresh`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ refresh_token: refreshToken })
                });
                
                const data = await response.json();
                
                if (response.ok) {
                    // Update tokens
                    accessToken = data.access_token;
                    tokenExpiryTime = Date.now() + (data.expires_in * 1000);
                    
                    localStorage.setItem('access_token', accessToken);
                    localStorage.setItem('token_expiry', tokenExpiryTime);
                    
                    // Update display
                    accessTokenDisplay.textContent = truncateToken(accessToken);
                    const expiryDate = new Date(parseInt(tokenExpiryTime));
                    tokenExpiry.textContent = expiryDate.toLocaleString();
                    
                    // Populate token decoder
                    tokenToDecode.value = accessToken;
                    
                    return { success: true, data };
                } else {
                    // If refresh fails, user needs to log in again
                    if (response.status === 401) {
                        logout();
                    }
                    
                    return { success: false, error: JSON.stringify(data, null, 2) };
                }
            } catch (error) {
                return { success: false, error: error.message };
            }
        }
        
        // Decode JWT token
        function decodeToken() {
            const token = tokenToDecode.value.trim();
            
            if (!token) {
                decodedToken.style.display = 'block';
                decodedToken.innerHTML = '<span class="error">Please enter a token</span>';
                return;
            }
            
            try {
                // Split the token
                const parts = token.split('.');
                
                if (parts.length !== 3) {
                    throw new Error('Invalid JWT token format');
                }
                
                // Decode the payload (second part)
                const payload = JSON.parse(atob(parts[1]));
                
                // Format dates
                if (payload.exp) {
                    payload.exp_date = new Date(payload.exp * 1000).toLocaleString();
                }
                
                if (payload.iat) {
                    payload.iat_date = new Date(payload.iat * 1000).toLocaleString();
                }
                
                decodedToken.style.display = 'block';
                decodedToken.innerHTML = `<span class="success">Token decoded:</span><pre>${JSON.stringify(payload, null, 2)}</pre>`;
            } catch (error) {
                decodedToken.style.display = 'block';
                decodedToken.innerHTML = `<span class="error">Error decoding token:</span><pre>${error.message}</pre>`;
            }
        }
        
        // Logout
        function logout() {
            // Clear tokens
            accessToken = null;
            refreshToken = null;
            tokenExpiryTime = null;
            currentUser = null;
            
            localStorage.removeItem('access_token');
            localStorage.removeItem('refresh_token');
            localStorage.removeItem('token_expiry');
            
            updateAuthStatus(false);
            
            // Clear results
            loginResult.style.display = 'none';
            registerResult.style.display = 'none';
            apiResult.style.display = 'none';
            refreshResult.style.display = 'none';
            
            // Switch to auth tab
            tabs.forEach(t => t.classList.remove('active'));
            tabContents.forEach(c => c.classList.remove('active'));
            
            document.querySelector('.tab[data-tab="auth"]').classList.add('active');
            document.getElementById('auth').classList.add('active');
        }
        
        // Start token timer
        function startTokenTimer() {
            if (tokenRefreshInterval) {
                clearInterval(tokenRefreshInterval);
            }
            
            tokenRefreshInterval = setInterval(() => {
                if (!tokenExpiryTime) return;
                
                const now = Date.now();
                const timeLeft = tokenExpiryTime - now;
                
                if (timeLeft <= 0) {
                    tokenTimer.textContent = 'Token expired. Refreshing...';
                    refreshAccessToken();
                } else {
                    const minutes = Math.floor(timeLeft / 60000);
                    const seconds = Math.floor((timeLeft % 60000) / 1000);
                    tokenTimer.textContent = `Token expires in: ${minutes}m ${seconds}s`;
                    
                    // Auto refresh when less than 1 minute left
                    if (timeLeft < 60000) {
                        tokenTimer.style.color = '#e74c3c';
                    } else {
                        tokenTimer.style.color = '#7f8c8d';
                    }
                }
            }, 1000);
        }
        
        // Helper function to truncate token for display
        function truncateToken(token) {
            if (!token) return '';
            return token.substring(0, 10) + '...' + token.substring(token.length - 5);
        }
        
        // Initialize the app
        init();
    </script>
</body>
</html>

================
File: docs/authentication_system_improvements.md
================
# Authentication System Improvements

## Overview

This document outlines the improvements made to the Metis RAG authentication system to address issues identified during testing. The authentication system provides user registration, login, password reset, and admin user management functionality.

## Issues Identified

During testing with the `scripts/test_authentication.py` script, several issues were identified:

1. SQLAlchemy AsyncSession mismatch causing `'AsyncSession' object has no attribute 'query'` errors
2. Password reset implementation using raw SQL without proper text() function
3. Database schema mismatch with missing `doc_metadata` column
4. UUID handling issues in user retrieval by ID
5. Transaction management issues in user deletion
6. Document reassignment issues when deleting users

## Improvements Checklist

### Phase 1: Database and ORM Alignment

- [x] Fixed SQLAlchemy AsyncSession mismatch in BaseRepository
  - Updated BaseRepository class to use async methods instead of synchronous ones
  - Converted all query methods to use SQLAlchemy 2.0 style async queries
  - Ensured proper await statements for all database operations

- [x] Fixed password reset implementation
  - Added proper use of text() function for raw SQL queries
  - Ensured consistent use of async/await patterns
  - Fixed token generation and validation

- [x] Created migration for database schema alignment
  - Created migration file to add missing doc_metadata column
  - Updated Alembic environment to use async database connection
  - Prepared migration to safely handle existing data

### Phase 2: Remaining Issues (Completed)

- [x] Successfully applied database migration
  - Resolved issues with running async migrations
  - Verified column addition in database schema
  - Migrated data from old metadata column to new doc_metadata column

- [x] Fixed user retrieval by ID
  - Improved UUID handling with proper type conversion
  - Enhanced error handling for invalid UUID formats

- [x] Fixed user deletion functionality
  - Implemented document reassignment to system user
  - Added transaction management with proper error handling
  - Stored original owner information in document metadata

## Testing Results

The authentication test script (`scripts/test_authentication.py`) now shows all features working correctly:

### Working Features:
- ✅ User Registration
- ✅ User Login
- ✅ Current User Info Retrieval
- ✅ Password Reset Request
- ✅ Admin Login
- ✅ Listing All Users
- ✅ Getting User by ID
- ✅ Creating New Users
- ✅ Updating Users
- ✅ Deleting Users

## Technical Details

### BaseRepository Changes

The BaseRepository class was updated to use async methods:

```python
# Before
def get_by_id(self, id: Union[int, str, UUID]) -> Optional[ModelType]:
    return self.session.query(self.model_class).filter(self.model_class.id == id).first()

# After
async def get_by_id(self, id: Union[int, str, UUID]) -> Optional[ModelType]:
    stmt = select(self.model_class).where(self.model_class.id == id)
    result = await self.session.execute(stmt)
    return result.scalars().first()
```

### UUID Handling Improvements

Enhanced UUID handling in the get_by_id method:

```python
async def get_by_id(self, id: Union[int, str, UUID]) -> Optional[PydanticUser]:
    # Convert string ID to UUID if needed
    if isinstance(id, str):
        try:
            id = UUID(id)
        except ValueError:
            return None
    
    # Use SQLAlchemy's native UUID handling
    stmt = select(DBUser).where(DBUser.id == id)
    result = await self.session.execute(stmt)
    user = result.scalars().first()
    
    if not user:
        return None
    
    return self._db_user_to_pydantic(user)
```

### User Deletion with Document Reassignment

Implemented document reassignment when deleting users:

```python
async def delete_user(self, user_id: Union[UUID, str]) -> bool:
    # Convert string ID to UUID if needed
    if isinstance(user_id, str):
        try:
            user_id = UUID(user_id)
        except ValueError:
            return False
    
    # Get the system user
    system_user_stmt = select(DBUser).where(DBUser.username == 'system')
    system_user_result = await self.session.execute(system_user_stmt)
    system_user = system_user_result.scalars().first()
    
    if not system_user:
        raise ValueError("System user not found. Please run scripts/create_system_user.py first.")
    
    # First get all documents owned by this user
    docs_stmt = select(Document).where(Document.user_id == user_id)
    docs_result = await self.session.execute(docs_stmt)
    documents = docs_result.scalars().all()
    
    # Update each document individually
    for doc in documents:
        # Create updated metadata with previous owner info
        if not doc.doc_metadata:
            doc.doc_metadata = {}
        
        # Convert to dict if it's not already
        if not isinstance(doc.doc_metadata, dict):
            doc.doc_metadata = {}
            
        # Add previous owner info
        doc.doc_metadata['previous_owner'] = str(user_id)
        
        # Reassign to system user
        doc.user_id = system_user.id
        self.session.add(doc)
    
    # Get the DB user
    user_stmt = select(DBUser).where(DBUser.id == user_id)
    user_result = await self.session.execute(user_stmt)
    db_user = user_result.scalars().first()
    
    if not db_user:
        return False
    
    # Delete the user
    await self.session.delete(db_user)
    await self.session.commit()
    
    return True
```

### Password Reset Implementation

The password reset implementation was updated to use the text() function:

```python
# Before
query = """
INSERT INTO password_reset_tokens (id, user_id, token, created_at, expires_at, is_used)
VALUES (:id, :user_id, :token, :created_at, :expires_at, :is_used)
"""
await db.execute(query, values)

# After
query = text("""
INSERT INTO password_reset_tokens (id, user_id, token, created_at, expires_at, is_used)
VALUES (:id, :user_id, :token, :created_at, :expires_at, :is_used)
""")
await db.execute(query, values)
```

### Database Migration

A migration was created to add the missing column:

```python
def upgrade():
    # Check if doc_metadata column exists in documents table
    conn = op.get_bind()
    inspector = sa.inspect(conn)
    columns = [col['name'] for col in inspector.get_columns('documents')]
    
    # Add doc_metadata column if it doesn't exist
    if 'doc_metadata' not in columns:
        op.add_column('documents',
                     sa.Column('doc_metadata', JSONB(),
                              nullable=True,
                              server_default=text("'{}'::jsonb")))
```

## System User Creation

A script was created to ensure a system user exists for document reassignment:

```python
async def create_system_user():
    """Create a system user if it doesn't exist."""
    engine = create_async_engine('postgresql+asyncpg://postgres:postgres@localhost:5432/metis_rag')
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
    
    async with async_session() as session:
        # Check if system user already exists
        stmt = select(User).where(User.username == 'system')
        result = await session.execute(stmt)
        system_user = result.scalars().first()
        
        if not system_user:
            # Create system user
            system_user_id = uuid.uuid4()
            system_user = User(
                id=system_user_id,
                username='system',
                email='system@metisrag.internal',
                password_hash=get_password_hash('not_accessible'),
                full_name='System User',
                is_active=True,
                is_admin=True
            )
            session.add(system_user)
            await session.commit()
```

## Conclusion

The authentication system has been fully improved, with all 10 key features now working correctly. The improvements include:

1. Proper async/await patterns throughout the codebase
2. Enhanced UUID handling for robust ID management
3. Database schema alignment with ORM models
4. Document reassignment to system user when deleting users
5. Improved transaction management for data integrity

These changes ensure a more robust and reliable authentication system for the Metis RAG application.

================
File: docs/authentication_testing_guide.md
================
# Metis RAG JWT Authentication System

This document provides an overview of the JWT authentication system implemented for the Metis RAG application, including how to test it and integrate it with client applications.

## Overview

The Metis RAG authentication system uses JSON Web Tokens (JWT) to secure API endpoints and maintain user sessions. The system includes:

1. **Access Tokens**: Short-lived tokens used for API requests
2. **Refresh Tokens**: Longer-lived tokens used to obtain new access tokens
3. **Persistent User Identity**: User identity is preserved across authentication events

## Authentication Flow

The authentication flow works as follows:

1. User registers or logs in with credentials
2. Server validates credentials and issues access and refresh tokens
3. Client includes access token in API requests
4. When access token expires, client uses refresh token to get a new access token
5. User identity remains consistent regardless of token refreshes or password changes

```mermaid
sequenceDiagram
    participant Client
    participant Server
    participant Database
    
    Client->>Server: Register/Login with credentials
    Server->>Database: Validate credentials
    Database-->>Server: User information
    Server->>Server: Generate JWT tokens
    Server-->>Client: Return access & refresh tokens
    
    Note over Client,Server: Using Access Token
    
    Client->>Server: API request with access token
    Server->>Server: Validate token
    Server-->>Client: Protected resource
    
    Note over Client,Server: Token Refresh
    
    Client->>Server: Request with expired token
    Server-->>Client: 401 Unauthorized
    Client->>Server: Refresh token request
    Server->>Server: Validate refresh token
    Server-->>Client: New access token
```

## Token Structure

### Access Token

Access tokens contain the following claims:

- `sub`: Username (subject)
- `user_id`: User's unique identifier
- `exp`: Expiration time
- `iat`: Issued at time
- `token_type`: "access"
- `aud`: Audience (API identifier)
- `iss`: Issuer (Metis RAG)
- `jti`: Unique token identifier

### Refresh Token

Refresh tokens contain similar claims but with a longer expiration time and a different token type:

- `token_type`: "refresh"

## Testing the Authentication System

The repository includes a test script (`run_authentication_test.py`) that demonstrates the complete authentication flow:

1. User registration
2. Login to obtain tokens
3. Accessing protected endpoints
4. Token refresh
5. Using the new access token

### Running the Test

To test the authentication system:

1. Start the Metis RAG application:
   ```bash
   python -m scripts.run_app
   ```

2. In a separate terminal, run the authentication test script:
   ```bash
   python run_authentication_test.py
   ```

The test script will:
- Register a test user (if not already registered)
- Log in to get access and refresh tokens
- Access a protected endpoint
- Refresh the access token
- Access the protected endpoint with the new token

## API Endpoints

### Registration

```
POST /api/auth/register
```

Request body:
```json
{
  "username": "testuser",
  "email": "test@example.com",
  "password": "securepassword",
  "full_name": "Test User"
}
```

Response:
```json
{
  "id": "user-uuid",
  "username": "testuser",
  "email": "test@example.com",
  "full_name": "Test User",
  "is_active": true,
  "is_admin": false,
  "created_at": "2025-03-25T12:00:00",
  "last_login": null,
  "metadata": {}
}
```

### Login

```
POST /api/auth/token
```

Form data:
```
username=testuser&password=securepassword
```

Response:
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer",
  "expires_in": 1800,
  "refresh_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
```

### Token Refresh

```
POST /api/auth/refresh
```

Request body:
```json
{
  "refresh_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
```

Response:
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer",
  "expires_in": 1800,
  "refresh_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
```

### Get Current User

```
GET /api/auth/me
```

Headers:
```
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

Response:
```json
{
  "id": "user-uuid",
  "username": "testuser",
  "email": "test@example.com",
  "full_name": "Test User",
  "is_active": true,
  "is_admin": false,
  "created_at": "2025-03-25T12:00:00",
  "last_login": "2025-03-25T12:30:00",
  "metadata": {}
}
```

## Client Integration

### JavaScript Example

```javascript
// Login and get tokens
async function login(username, password) {
  const formData = new URLSearchParams();
  formData.append('username', username);
  formData.append('password', password);
  
  const response = await fetch('http://localhost:8000/api/auth/token', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/x-www-form-urlencoded',
    },
    body: formData
  });
  
  if (!response.ok) {
    throw new Error('Login failed');
  }
  
  const tokenData = await response.json();
  
  // Store tokens securely
  localStorage.setItem('access_token', tokenData.access_token);
  localStorage.setItem('refresh_token', tokenData.refresh_token);
  localStorage.setItem('token_expiry', Date.now() + (tokenData.expires_in * 1000));
  
  return tokenData;
}

// Access protected endpoint
async function fetchProtectedResource(url) {
  // Check if token needs refresh
  const tokenExpiry = localStorage.getItem('token_expiry');
  if (tokenExpiry && Date.now() > tokenExpiry) {
    await refreshToken();
  }
  
  const accessToken = localStorage.getItem('access_token');
  
  const response = await fetch(url, {
    headers: {
      'Authorization': `Bearer ${accessToken}`
    }
  });
  
  if (response.status === 401) {
    // Token might be invalid, try refreshing
    await refreshToken();
    // Retry with new token
    return fetchProtectedResource(url);
  }
  
  return response.json();
}

// Refresh token
async function refreshToken() {
  const refreshToken = localStorage.getItem('refresh_token');
  
  const response = await fetch('http://localhost:8000/api/auth/refresh', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ refresh_token: refreshToken })
  });
  
  if (!response.ok) {
    // Refresh failed, redirect to login
    window.location.href = '/login';
    throw new Error('Token refresh failed');
  }
  
  const tokenData = await response.json();
  
  // Update stored tokens
  localStorage.setItem('access_token', tokenData.access_token);
  localStorage.setItem('token_expiry', Date.now() + (tokenData.expires_in * 1000));
  
  return tokenData;
}
```

### Python Example

```python
import requests
import time

class MetisRAGClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.access_token = None
        self.refresh_token = None
        self.token_expiry = 0
    
    def login(self, username, password):
        """Login and get tokens"""
        login_data = {
            "username": username,
            "password": password
        }
        
        response = requests.post(
            f"{self.base_url}/api/auth/token",
            data=login_data,
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        response.raise_for_status()
        token_data = response.json()
        
        self.access_token = token_data["access_token"]
        self.refresh_token = token_data["refresh_token"]
        self.token_expiry = time.time() + token_data["expires_in"]
        
        return token_data
    
    def refresh_token(self):
        """Refresh the access token"""
        if not self.refresh_token:
            raise ValueError("No refresh token available")
        
        refresh_data = {
            "refresh_token": self.refresh_token
        }
        
        response = requests.post(
            f"{self.base_url}/api/auth/refresh",
            json=refresh_data
        )
        
        response.raise_for_status()
        token_data = response.json()
        
        self.access_token = token_data["access_token"]
        self.token_expiry = time.time() + token_data["expires_in"]
        
        return token_data
    
    def get_resource(self, endpoint):
        """Get a protected resource"""
        # Check if token needs refresh
        if time.time() > self.token_expiry:
            self.refresh_token()
        
        headers = {"Authorization": f"Bearer {self.access_token}"}
        response = requests.get(f"{self.base_url}{endpoint}", headers=headers)
        
        # If unauthorized, try refreshing token
        if response.status_code == 401:
            self.refresh_token()
            headers = {"Authorization": f"Bearer {self.access_token}"}
            response = requests.get(f"{self.base_url}{endpoint}", headers=headers)
        
        response.raise_for_status()
        return response.json()
```

## Security Considerations

1. **Token Storage**: 
   - Store tokens securely
   - For web applications, use HttpOnly cookies for access tokens
   - For mobile applications, use secure storage mechanisms

2. **Token Expiration**:
   - Access tokens have a short lifetime (default: 30 minutes)
   - Refresh tokens have a longer lifetime (default: 7 days)

3. **HTTPS**:
   - Always use HTTPS in production to protect tokens in transit

4. **Token Validation**:
   - All tokens are validated for:
     - Signature
     - Expiration time
     - Required claims
     - Token type

## Persistent User-Document Relationships

The authentication system maintains persistent relationships between users and their documents:

1. User identity (user_id) is preserved across all authentication events
2. Document ownership is tied to user_id, not to authentication tokens
3. Password resets and token refreshes do not affect document access rights
4. Account deactivation temporarily prevents login but maintains document relationships

This ensures that users always have access to their documents, regardless of authentication status or history.

## Customization

The authentication system can be customized through environment variables:

- `SECRET_KEY`: JWT signing key
- `ALGORITHM`: JWT algorithm (default: HS256)
- `ACCESS_TOKEN_EXPIRE_MINUTES`: Access token lifetime in minutes (default: 30)
- `REFRESH_TOKEN_EXPIRE_DAYS`: Refresh token lifetime in days (default: 7)
- `JWT_AUDIENCE`: Token audience claim (default: metis-rag-api)
- `JWT_ISSUER`: Token issuer claim (default: metis-rag)

## Troubleshooting

### Invalid Token

If you receive a 401 Unauthorized error with "Invalid token or token expired":

1. Check that the token is correctly included in the Authorization header
2. Verify that the token has not expired
3. Try refreshing the token

### Refresh Token Failed

If token refresh fails:

1. The refresh token may have expired
2. The refresh token may be invalid
3. The user account may have been deactivated

In these cases, the user needs to log in again with their credentials.

================
File: docs/document_upload_improvements_phase1.md
================
# Metis RAG Document Upload Improvements - Phase 1

This document summarizes the improvements made to the document upload functionality in Phase 1.

## Issues Addressed

1. **Multiple File Selection Issue**
   - Fixed the issue where users could only select one file at a time despite the UI supporting multiple file selection
   - Added debugging and monitoring to ensure multiple file selection works correctly

2. **Limited File Type Support**
   - Expanded supported file types from 4 to 10 formats
   - Added file size limits specific to each file type
   - Updated UI to inform users about supported formats

3. **Basic Validation**
   - Enhanced file validation to include size checks
   - Added basic content validation for PDF files
   - Improved error messages with specific details

4. **Error Feedback**
   - Implemented more detailed error notifications
   - Added support for displaying multiple errors at once
   - Enhanced styling for error messages

## Implementation Details

### 1. Multiple File Selection Fix

Added a dedicated JavaScript file (`document-upload-fix.js`) that:
- Ensures the `multiple` attribute is properly set on the file input
- Adds debugging to log file selection events
- Monitors browser compatibility
- Enhances event listeners to properly handle multiple files

Fixed JavaScript timing issues:
- Modified scripts to wait for DocumentManager to be fully initialized
- Used setTimeout to ensure proper script loading order
- Added dynamic script loading in the HTML template
- Added error handling and logging for better debugging

### 2. Enhanced File Type Support

Updated the file validation in `file_utils.py`:
```python
# Set of allowed file extensions with max size in MB
ALLOWED_EXTENSIONS = {
    ".pdf": 20,    # 20MB max for PDFs
    ".txt": 10,    # 10MB max for text files
    ".csv": 15,    # 15MB max for CSV files
    ".md": 10,     # 10MB max for markdown files
    ".docx": 20,   # 20MB max for Word documents
    ".doc": 20,    # 20MB max for older Word documents
    ".rtf": 15,    # 15MB max for rich text files
    ".html": 10,   # 10MB max for HTML files
    ".json": 10,   # 10MB max for JSON files
    ".xml": 10     # 10MB max for XML files
}
```

Updated the file input in the HTML:
```html
<input type="file" id="document-file" accept=".pdf,.txt,.csv,.md,.docx,.doc,.rtf,.html,.json,.xml" multiple required>
```

Added information about supported formats:
```html
<p class="supported-formats">Supported formats: PDF, Word, Text, CSV, Markdown, HTML, JSON, XML</p>
```

### 3. Improved Validation

Enhanced the `validate_file` function to:
- Return both a boolean and a detailed error message
- Check file size against type-specific limits
- Validate file content (basic PDF validation implemented)
- Reset file position after validation

Updated API endpoints to use the enhanced validation:
```python
# Validate file
is_valid, error_message = await validate_file(file)
if not is_valid:
    errors.append({
        "filename": file.filename,
        "error": error_message
    })
    continue
```

### 4. Enhanced Error Feedback

Added a new JavaScript file (`error-feedback-enhancement.js`) that:
- Implements a more detailed notification system
- Supports displaying multiple errors at once
- Adds styling for error messages
- Enhances the DocumentManager to use better error handling

Improved error handling:
- Added detailed error messages for file validation failures
- Created a structured notification system for displaying errors
- Added CSS styling for better error presentation
- Implemented proper error handling for asynchronous operations

## Next Steps

### Phase 2 - UX Improvements
- Redesign file selection interface
- Implement batch actions
- Improve progress indicators

### Phase 3 - Backend Optimization
- Implement parallel processing
- Add chunked uploads
- Optimize database operations

### Phase 4 - Advanced Features
- Add content analysis
- Implement smart organization
- Develop enhanced processing options

## Testing

To test the Phase 1 improvements:

1. Navigate to the Documents page
2. Try selecting multiple files of different types
3. Verify that all selected files appear in the file list
4. Try uploading files that exceed the size limits
5. Try uploading unsupported file types
6. Verify that error messages are clear and helpful

## Troubleshooting

If you encounter issues with the multiple file selection:

1. Check the browser console for error messages
2. Verify that all JavaScript files are loading correctly
3. Try clearing the browser cache and reloading the page
4. Ensure the DocumentManager class is fully initialized before our enhancement scripts run
5. Check that the file input element has the `multiple` attribute correctly set

Common issues and solutions:

- **JavaScript timing issues**: The enhancement scripts might run before the DocumentManager class is fully initialized. Solution: Use the dynamic script loading approach with setTimeout.
- **Browser compatibility**: Some older browsers might not fully support multiple file selection. Solution: Check the browser detection logs in the console.
- **File validation errors**: If files fail validation, check the detailed error messages in the notifications.

================
File: docs/document_upload_improvements_phase2_plan.md
================
# Metis RAG Document Upload Improvements - Phase 2 Implementation Plan

## Overview

This document outlines the implementation plan for Phase 2 of the document upload improvements, focusing on enhancing the user experience through a redesigned file selection interface, batch actions, and improved progress indicators.

## Current Implementation Analysis

The current document upload implementation has the following characteristics:

### File Selection Interface
- Basic file input with multiple selection enabled
- Simple file list display showing filenames and sizes
- Basic drag-and-drop zone with minimal styling
- No file previews or detailed metadata display
- Limited visual feedback during selection

### Batch Actions
- Basic selection of documents via checkboxes
- Limited to processing and deleting selected documents
- No batch tagging or folder assignment
- No confirmation dialogs for destructive actions

### Progress Indicators
- Simple progress bars for individual files and overall progress
- No detailed status information or time estimates
- Limited visual feedback during upload process
- No summary view for completed uploads

## UI Organization Strategy

To address concerns about interface clutter while maintaining workflow efficiency, we'll implement a hybrid approach:

### 1. Progressive Disclosure Pattern
- Use collapsible sections for different functional areas
- Show only essential UI elements by default
- Expand sections only when needed
- Provide clear visual cues for available actions

### 2. Modal Dialogs for Complex Operations
- Use modal dialogs for focused tasks like batch tagging
- Keep the main interface clean and uncluttered
- Provide detailed options and previews within modals

### 3. Workflow-Based Organization
- Organize the interface based on the document lifecycle
- Group related actions together
- Use visual hierarchy to guide users through the workflow

## Implementation Plan

### 1. File Selection Interface Redesign

#### 1.1 Enhanced Drag-and-Drop Zone

The drag-and-drop zone will be redesigned to be more intuitive and visually appealing:

```html
<div id="upload-section" class="collapsible-section">
  <div class="section-header">
    <h3>Upload Documents</h3>
    <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
  </div>
  
  <div class="section-content">
    <div id="drop-zone" class="drop-zone">
      <div class="drop-zone-icon">
        <i class="fas fa-cloud-upload-alt"></i>
      </div>
      <p>Drag and drop files here or click to select</p>
      <p class="supported-formats">Supported formats: PDF, Word, Text, CSV, Markdown, HTML, JSON, XML</p>
      <form id="upload-form">
        <input type="file" id="document-file" accept=".pdf,.txt,.csv,.md,.docx,.doc,.rtf,.html,.json,.xml" multiple required>
        <button type="button" class="select-files-btn">Select Files</button>
      </form>
    </div>
  </div>
</div>
```

CSS for the enhanced drop zone:
```css
.drop-zone {
  border: 2px dashed var(--border-color);
  border-radius: 8px;
  padding: 30px;
  text-align: center;
  transition: all 0.3s;
  background-color: var(--input-bg);
}

.drop-zone.drag-over {
  border-color: var(--accent-color);
  background-color: rgba(var(--accent-color-rgb), 0.1);
  transform: scale(1.02);
}

.drop-zone-icon {
  font-size: 48px;
  color: var(--muted-color);
  margin-bottom: 15px;
}

.select-files-btn {
  background-color: var(--primary-color);
  color: white;
  border: none;
  padding: 10px 20px;
  border-radius: 4px;
  cursor: pointer;
  margin-top: 15px;
}

/* Hide the actual file input */
#document-file {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
```

#### 1.2 File Preview System

The file preview system will be implemented as a collapsible section that appears after files are selected:

```html
<div id="selected-files-section" class="collapsible-section">
  <div class="section-header">
    <h3>Selected Files (<span id="file-count">0</span>)</h3>
    <div class="section-actions">
      <button id="view-toggle-grid" class="view-toggle-btn active" title="Grid View">
        <i class="fas fa-th-large"></i>
      </button>
      <button id="view-toggle-list" class="view-toggle-btn" title="List View">
        <i class="fas fa-list"></i>
      </button>
      <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
    </div>
  </div>
  
  <div class="section-content">
    <div id="file-list" class="file-list grid-view">
      <!-- File previews will be added here -->
    </div>
    
    <div class="file-actions">
      <button id="clear-files-btn" class="btn">Clear All</button>
      <button id="upload-files-btn" class="btn primary">Upload Files</button>
    </div>
  </div>
</div>
```

JavaScript for creating file previews:
```javascript
createFilePreview(file) {
  const preview = document.createElement('div');
  preview.className = 'file-preview';
  preview.dataset.filename = file.name;
  
  // Create thumbnail based on file type
  const thumbnail = document.createElement('div');
  thumbnail.className = 'file-thumbnail';
  
  if (file.type.startsWith('image/')) {
    this.createImageThumbnail(file, thumbnail);
  } else if (file.type === 'application/pdf') {
    this.createPDFThumbnail(file, thumbnail);
  } else {
    this.createGenericThumbnail(file, thumbnail);
  }
  
  // Add file info
  const fileInfo = document.createElement('div');
  fileInfo.className = 'file-info';
  fileInfo.innerHTML = `
    <div class="file-name">${file.name}</div>
    <div class="file-meta">
      <span class="file-size">${this.formatFileSize(file.size)}</span>
      <span class="file-type">${this.getFileExtension(file.name)}</span>
    </div>
    <div class="file-date">Modified: ${new Date(file.lastModified).toLocaleDateString()}</div>
  `;
  
  // Add remove button
  const removeBtn = document.createElement('button');
  removeBtn.className = 'file-remove';
  removeBtn.innerHTML = '&times;';
  removeBtn.addEventListener('click', () => this.removeFileFromQueue(file));
  
  preview.appendChild(thumbnail);
  preview.appendChild(fileInfo);
  preview.appendChild(removeBtn);
  
  return preview;
}
```

#### 1.3 File Reordering and Organization

File reordering will be implemented using HTML5 Drag and Drop API:

```javascript
setupFileReordering() {
  const fileList = document.getElementById('file-list');
  
  // Make file items draggable
  fileList.querySelectorAll('.file-preview').forEach(item => {
    item.setAttribute('draggable', 'true');
    
    item.addEventListener('dragstart', e => {
      e.dataTransfer.setData('text/plain', item.dataset.filename);
      item.classList.add('dragging');
    });
    
    item.addEventListener('dragend', () => {
      item.classList.remove('dragging');
    });
    
    item.addEventListener('dragover', e => {
      e.preventDefault();
      const dragging = fileList.querySelector('.dragging');
      if (dragging && dragging !== item) {
        const rect = item.getBoundingClientRect();
        const y = e.clientY - rect.top;
        if (y < rect.height / 2) {
          fileList.insertBefore(dragging, item);
        } else {
          fileList.insertBefore(dragging, item.nextSibling);
        }
        // Update file queue order
        this.updateFileQueueOrder();
      }
    });
  });
}
```

### 2. Batch Actions Implementation

#### 2.1 Multi-select Mechanism

The multi-select mechanism will be implemented with a batch actions toolbar that appears only when files are selected:

```html
<div id="batch-actions-toolbar" class="batch-actions-toolbar">
  <div class="selection-count">
    <span id="selected-count">0</span> files selected
  </div>
  <div class="batch-actions-buttons">
    <button id="batch-tag-btn" class="batch-btn">
      <i class="fas fa-tags"></i> Tag
    </button>
    <button id="batch-folder-btn" class="batch-btn">
      <i class="fas fa-folder"></i> Move
    </button>
    <button id="batch-delete-btn" class="batch-btn danger">
      <i class="fas fa-trash"></i> Delete
    </button>
    <button id="batch-process-btn" class="batch-btn primary">
      <i class="fas fa-cogs"></i> Process
    </button>
  </div>
</div>
```

CSS for the batch actions toolbar:
```css
.batch-actions-toolbar {
  position: fixed;
  bottom: -60px;
  left: 0;
  right: 0;
  background-color: var(--card-bg);
  border-top: 1px solid var(--border-color);
  padding: 10px 20px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  transition: bottom 0.3s;
  z-index: 100;
  box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
}

.batch-actions-toolbar.visible {
  bottom: 0;
}

.batch-actions-buttons {
  display: flex;
  gap: 10px;
}

.batch-btn {
  padding: 8px 15px;
  border-radius: 4px;
  border: 1px solid var(--border-color);
  background-color: var(--card-bg);
  cursor: pointer;
  display: flex;
  align-items: center;
  gap: 5px;
}

.batch-btn.primary {
  background-color: var(--primary-color);
  color: white;
  border-color: var(--primary-color);
}

.batch-btn.danger {
  background-color: var(--error-color);
  color: white;
  border-color: var(--error-color);
}
```

#### 2.2 Batch Tagging Modal

The batch tagging functionality will be implemented as a modal dialog:

```html
<div id="batch-tag-modal" class="modal">
  <div class="modal-content">
    <div class="modal-header">
      <h3 class="modal-title">Add Tags to <span id="tag-file-count">0</span> Files</h3>
      <button class="modal-close">&times;</button>
    </div>
    <div class="modal-body">
      <div class="tag-input-container">
        <input type="text" id="batch-tag-input" class="tag-input" placeholder="Add a tag...">
        <div id="batch-tag-suggestions" class="tag-suggestions"></div>
      </div>
      <div id="batch-tag-list" class="tag-list"></div>
      
      <div class="batch-options">
        <label class="checkbox-label">
          <input type="checkbox" id="merge-tags" checked>
          Merge with existing tags
        </label>
      </div>
    </div>
    <div class="modal-footer">
      <button id="cancel-batch-tag" class="btn">Cancel</button>
      <button id="apply-batch-tag" class="btn primary">Apply Tags</button>
    </div>
  </div>
</div>
```

#### 2.3 Batch Folder Assignment Modal

The batch folder assignment will also be implemented as a modal dialog:

```html
<div id="batch-folder-modal" class="modal">
  <div class="modal-content">
    <div class="modal-header">
      <h3 class="modal-title">Move <span id="folder-file-count">0</span> Files</h3>
      <button class="modal-close">&times;</button>
    </div>
    <div class="modal-body">
      <div class="folder-select-container">
        <label for="batch-folder-select">Select Destination Folder:</label>
        <select id="batch-folder-select" class="folder-select">
          <option value="/">Root</option>
          <!-- Folders will be populated dynamically -->
        </select>
        
        <div class="new-folder-container">
          <label for="new-folder-input">Or Create New Folder:</label>
          <div class="new-folder-input-group">
            <input type="text" id="new-folder-input" placeholder="New folder name">
            <button id="create-folder-btn" class="btn">Create</button>
          </div>
        </div>
      </div>
    </div>
    <div class="modal-footer">
      <button id="cancel-batch-folder" class="btn">Cancel</button>
      <button id="apply-batch-folder" class="btn primary">Move Files</button>
    </div>
  </div>
</div>
```

#### 2.4 Batch Deletion Confirmation Modal

The batch deletion confirmation will be implemented as a modal dialog:

```html
<div id="batch-delete-modal" class="modal">
  <div class="modal-content">
    <div class="modal-header">
      <h3 class="modal-title">Delete <span id="delete-file-count">0</span> Files</h3>
      <button class="modal-close">&times;</button>
    </div>
    <div class="modal-body">
      <div class="warning-message">
        <i class="fas fa-exclamation-triangle"></i>
        <p>Are you sure you want to delete these files? This action cannot be undone.</p>
      </div>
      
      <div id="files-to-delete" class="files-to-delete">
        <!-- File list will be populated dynamically -->
      </div>
    </div>
    <div class="modal-footer">
      <button id="cancel-batch-delete" class="btn">Cancel</button>
      <button id="confirm-batch-delete" class="btn danger">Delete Files</button>
    </div>
  </div>
</div>
```

### 3. Progress Indicators Enhancement

#### 3.1 Collapsible Progress Panel

The progress indicators will be implemented as a collapsible panel that expands during upload:

```html
<div id="progress-section" class="collapsible-section">
  <div class="section-header">
    <h3>Upload Progress</h3>
    <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
  </div>
  
  <div class="section-content">
    <div class="progress-container">
      <div class="overall-progress">
        <div class="progress-header">
          <span class="progress-title">Overall Progress</span>
          <span class="progress-stats">
            <span id="completed-count">0</span>/<span id="total-count">0</span> files
          </span>
        </div>
        <div class="progress-bar">
          <div id="overall-progress-fill" class="progress-bar-fill" style="width: 0%"></div>
        </div>
        <div class="progress-details">
          <span id="overall-percent">0%</span>
          <span id="time-remaining">Estimating time...</span>
        </div>
      </div>
      
      <div id="file-progress-list" class="file-progress-list">
        <!-- Individual file progress items will be added here -->
      </div>
    </div>
  </div>
</div>
```

#### 3.2 Enhanced File Progress Items

Each file progress item will include status indicators and time estimates:

```javascript
createProgressElement(file) {
  const progressItem = document.createElement('div');
  progressItem.className = 'file-progress-item';
  progressItem.dataset.filename = file.name;
  
  progressItem.innerHTML = `
    <div class="file-progress-icon">
      <i class="fas fa-file"></i>
    </div>
    <div class="file-progress-content">
      <div class="file-progress-header">
        <span class="file-progress-name">${this.truncateFilename(file.name)}</span>
        <span class="file-progress-status" data-status="queued">Queued</span>
      </div>
      <div class="file-progress-bar">
        <div class="file-progress-fill" style="width: 0%"></div>
      </div>
      <div class="file-progress-details">
        <span class="file-progress-size">${this.formatFileSize(file.size)}</span>
        <span class="file-progress-percent">0%</span>
        <span class="file-progress-time"></span>
      </div>
    </div>
  `;
  
  return progressItem;
}
```

#### 3.3 Upload Summary Modal

The upload summary will be implemented as a modal dialog that appears after upload completion:

```html
<div id="upload-summary-modal" class="modal">
  <div class="modal-content">
    <div class="modal-header">
      <h3 class="modal-title">Upload Complete</h3>
      <button class="modal-close">&times;</button>
    </div>
    <div class="modal-body">
      <div class="summary-stats">
        <div class="summary-stat">
          <span class="stat-value" id="summary-total">0</span>
          <span class="stat-label">Total Files</span>
        </div>
        <div class="summary-stat">
          <span class="stat-value" id="summary-success">0</span>
          <span class="stat-label">Successful</span>
        </div>
        <div class="summary-stat">
          <span class="stat-value" id="summary-failed">0</span>
          <span class="stat-label">Failed</span>
        </div>
      </div>
      
      <div class="summary-details">
        <div id="summary-success-list" class="summary-list">
          <!-- Successful files will be listed here -->
        </div>
        <div id="summary-failed-list" class="summary-list">
          <!-- Failed files will be listed here -->
        </div>
      </div>
    </div>
    <div class="modal-footer">
      <button id="process-uploaded-btn" class="btn primary">Process Files</button>
      <button id="close-summary-btn" class="btn">Close</button>
    </div>
  </div>
</div>
```

## Implementation Phases

The implementation will be divided into three phases:

### Phase 2A: File Selection Interface (2 weeks)
1. Implement collapsible sections framework
2. Redesign drag-and-drop zone
3. Implement file preview system
4. Add file reordering functionality
5. Test and refine the file selection interface

### Phase 2B: Batch Actions (2 weeks)
1. Implement batch actions toolbar
2. Create batch tagging modal and functionality
3. Create batch folder assignment modal and functionality
4. Implement batch deletion with confirmation
5. Test and refine batch actions

### Phase 2C: Progress Indicators (2 weeks)
1. Implement collapsible progress panel
2. Enhance file progress items with status indicators
3. Add time estimation functionality
4. Create upload summary modal
5. Test and refine progress indicators

## Testing Strategy

### Unit Testing
- Test individual components in isolation
- Verify that each component behaves as expected
- Test edge cases and error handling

### Integration Testing
- Test interactions between components
- Verify that data flows correctly between components
- Test the complete upload workflow

### User Acceptance Testing
- Have real users test the new interface
- Gather feedback on usability and intuitiveness
- Identify any issues or areas for improvement

### Cross-Browser Testing
- Test on different browsers (Chrome, Firefox, Safari, Edge)
- Verify that the interface works correctly on all supported browsers
- Test on different screen sizes and devices

## Success Criteria

The implementation will be considered successful if:

1. Users can easily select and upload multiple files with clear visual feedback
2. Batch operations work correctly and save time compared to individual operations
3. Progress indicators provide accurate and helpful information during uploads
4. The interface is responsive and works well on different screen sizes
5. Error states are clearly communicated with actionable recovery options
6. The interface remains clean and uncluttered despite the added functionality

## Conclusion

This implementation plan addresses the goals of Phase 2 while maintaining a clean and efficient user interface. By using collapsible sections and modal dialogs, we can provide powerful functionality without overwhelming the user with too many options at once. The progressive disclosure pattern ensures that users see only what they need when they need it, while still having access to advanced features when required.

================
File: docs/document_upload_improvements_phase2_prompt.md
================
# Metis RAG Document Upload Improvements - Phase 2 Prompt

## Overview

Now that we've completed Phase 1 of the document upload improvements, which fixed the multiple file selection issue and enhanced file validation, we're ready to move on to Phase 2. This phase will focus on improving the user experience (UX) of the document upload process.

## Phase 2 Goals

1. **Redesign the File Selection Interface**
   - Create a more intuitive and visually appealing file selection area
   - Implement drag-and-drop functionality across the entire upload area
   - Add file preview capabilities for selected documents
   - Improve visual feedback during the selection process

2. **Implement Batch Actions**
   - Add ability to apply tags to multiple selected files at once
   - Enable folder assignment for multiple files simultaneously
   - Implement batch deletion with confirmation
   - Add batch processing options with configurable parameters

3. **Improve Progress Indicators**
   - Redesign progress bars with more detailed information
   - Add individual file progress tracking with status indicators
   - Implement overall batch progress visualization
   - Add estimated time remaining for uploads

## Implementation Approach

### 1. File Selection Interface Redesign

The current file selection interface is functional but could be more intuitive. We should:

- Create a modern drag-and-drop zone that highlights when files are dragged over it
- Add file preview thumbnails for common file types (PDF, DOCX, etc.)
- Display file metadata (size, type, last modified) in the selection list
- Implement a grid/list view toggle for selected files
- Add the ability to reorder files before upload

### 2. Batch Actions Implementation

Currently, users need to perform actions on files individually. We should:

- Add a multi-select mechanism with checkboxes for files
- Create a batch actions toolbar that appears when multiple files are selected
- Implement batch tagging with tag suggestions based on file content
- Add batch folder assignment with folder browser/creation
- Create confirmation dialogs for destructive batch actions

### 3. Progress Indicators Enhancement

The current progress indicators are basic. We should:

- Redesign progress bars with animated fills and percentage displays
- Add status icons for different stages (queued, uploading, processing, complete, error)
- Implement a detailed progress panel that can be expanded/collapsed
- Add upload speed and time remaining estimates
- Create a summary view for completed uploads with success/failure counts

## Technical Considerations

- Use modern CSS features (Grid, Flexbox, CSS Variables) for responsive layouts
- Implement client-side file handling with the File API
- Use requestAnimationFrame for smooth progress animations
- Consider using Web Workers for background processing of large files
- Implement proper error handling and recovery mechanisms

## Success Criteria

- Users can easily select and upload multiple files with clear visual feedback
- Batch operations work correctly and save time compared to individual operations
- Progress indicators provide accurate and helpful information during uploads
- The interface is responsive and works well on different screen sizes
- Error states are clearly communicated with actionable recovery options

## Getting Started

To begin Phase 2, we should:

1. Create wireframes or mockups of the new interface
2. Identify the key components that need to be built or modified
3. Develop a prototype of the new file selection interface
4. Implement the batch actions functionality
5. Enhance the progress indicators
6. Test the new features with real users
7. Refine based on feedback

Let's start by exploring modern file upload interfaces for inspiration and then create a design prototype for our improved document upload experience.

================
File: docs/document_upload_improvements_phase2_readme.md
================
# Metis RAG Document Upload Improvements - Phase 2

This document provides an overview of the Phase 2 improvements to the document upload interface in Metis RAG.

## Overview

Phase 2 of the document upload improvements focuses on enhancing the user experience (UX) of the document upload process. The improvements include:

1. Redesigned file selection interface
2. Batch actions for multiple files
3. Improved progress indicators

## How to Access the Enhanced Interface

The enhanced document upload interface is available at:

```
/documents-enhanced
```

This is a separate route from the original document interface (`/documents`), allowing for side-by-side comparison and gradual transition.

## New Features

### 1. Redesigned File Selection Interface

#### Collapsible Sections
- All major sections are now collapsible, reducing visual clutter
- Sections can be expanded/collapsed as needed
- Only relevant sections are visible at any given time

#### Enhanced Drag-and-Drop Zone
- More visually appealing drag-and-drop area
- Clear visual feedback when files are dragged over the zone
- Improved file selection button

#### File Preview System
- Grid/list view toggle for selected files
- File thumbnails with appropriate icons for different file types
- Detailed file metadata (size, type, last modified date)
- Easy file removal with a single click

#### File Reordering
- Drag-and-drop reordering of selected files
- Visual feedback during reordering
- Automatic queue update when files are reordered

### 2. Batch Actions

#### Batch Actions Toolbar
- Appears automatically when files are selected
- Shows the number of selected files
- Provides quick access to common batch operations

#### Batch Tagging
- Apply tags to multiple files at once
- Option to merge with existing tags or replace them
- Tag suggestions based on existing tags

#### Batch Folder Assignment
- Move multiple files to a folder at once
- Create new folders on the fly
- Visual folder browser

#### Batch Deletion
- Delete multiple files at once
- Confirmation dialog with file list
- Clear visual warning for destructive action

### 3. Improved Progress Indicators

#### Enhanced Progress Bars
- Overall progress with percentage and file count
- Individual file progress with status indicators
- Estimated time remaining for uploads

#### Status Indicators
- Clear visual status for each file (queued, uploading, processing, complete, error)
- Color-coded status indicators
- Detailed error messages when issues occur

#### Upload Summary
- Comprehensive summary after upload completion
- Statistics on successful and failed uploads
- Option to process uploaded files immediately

## Technical Implementation

The enhanced document upload interface is implemented using:

1. **CSS**: `document-upload-enhanced.css` - Contains all the styling for the enhanced interface
2. **JavaScript**: `document-upload-enhanced.js` - Implements the enhanced functionality
3. **HTML**: `documents_enhanced.html` - The template for the enhanced interface

The implementation follows a progressive enhancement approach, building on top of the existing document upload functionality while adding new features.

## Dark Mode Support

The enhanced interface fully supports dark mode, using the same color palette as the rest of the application. The interface automatically adapts to the user's theme preference.

## Browser Compatibility

The enhanced interface is compatible with all modern browsers:
- Chrome 80+
- Firefox 75+
- Safari 13.1+
- Edge 80+

## Usage Instructions

### Uploading Files

1. Navigate to `/documents-enhanced`
2. Click on the "Upload Documents" section if it's collapsed
3. Either drag files into the drop zone or click "Select Files" to browse
4. Selected files will appear in the "Selected Files" section
5. Click "Upload Files" to start the upload process

### Using Batch Actions

1. Select files by clicking on them in the file list (checkbox will be checked)
2. The batch actions toolbar will appear at the bottom of the screen
3. Click on the desired action (Tag, Move, Delete, Process)
4. Complete the action in the modal dialog that appears
5. Confirm the action to apply it to all selected files

### Monitoring Upload Progress

1. The "Upload Progress" section will automatically expand during upload
2. Overall progress is shown at the top
3. Individual file progress is shown below
4. After completion, a summary dialog will appear

## Future Improvements

Phase 3 will focus on backend optimizations:
- Parallel processing
- Chunked uploads
- Optimized database operations

Phase 4 will add advanced features:
- Content analysis
- Smart organization
- Enhanced processing options

================
File: docs/document_upload_ui_mockup.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG Document Upload UI Mockup</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <style>
        :root {
            --primary-color: #3498db;
            --secondary-color: #2ecc71;
            --accent-color: #f39c12;
            --error-color: #e74c3c;
            --text-color: #333;
            --muted-color: #7f8c8d;
            --border-color: #ddd;
            --card-bg: #fff;
            --input-bg: #f5f5f5;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: #f9f9f9;
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            padding: 20px;
        }

        header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        h1 {
            color: var(--primary-color);
            margin-bottom: 10px;
        }

        .description {
            color: var(--muted-color);
            max-width: 700px;
            margin: 0 auto;
        }

        /* Collapsible Section Styles */
        .collapsible-section {
            margin-bottom: 20px;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow: hidden;
        }

        .section-header {
            background-color: var(--card-bg);
            padding: 15px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            cursor: pointer;
        }

        .section-header h3 {
            margin: 0;
            font-size: 1.1rem;
            color: var(--primary-color);
        }

        .toggle-btn {
            background: none;
            border: none;
            color: var(--muted-color);
            cursor: pointer;
            font-size: 1rem;
        }

        .section-content {
            padding: 15px;
            background-color: var(--card-bg);
        }

        .collapsed .section-content {
            display: none;
        }

        /* Drop Zone Styles */
        .drop-zone {
            border: 2px dashed var(--border-color);
            border-radius: 8px;
            padding: 30px;
            text-align: center;
            background-color: var(--input-bg);
        }

        .drop-zone-icon {
            font-size: 48px;
            color: var(--muted-color);
            margin-bottom: 15px;
        }

        .select-files-btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            margin-top: 15px;
        }

        /* File Preview Styles */
        .file-list {
            margin-top: 15px;
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 15px;
        }

        .file-preview {
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow: hidden;
            position: relative;
            background-color: white;
        }

        .file-thumbnail {
            height: 120px;
            background-color: var(--input-bg);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 40px;
            color: var(--muted-color);
        }

        .file-info {
            padding: 10px;
        }

        .file-name {
            font-weight: 500;
            margin-bottom: 5px;
        }

        .file-meta {
            display: flex;
            justify-content: space-between;
            font-size: 0.8rem;
            color: var(--muted-color);
        }

        .file-remove {
            position: absolute;
            top: 5px;
            right: 5px;
            background-color: var(--error-color);
            color: white;
            border: none;
            width: 24px;
            height: 24px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
        }

        /* Progress Indicators */
        .progress-bar {
            height: 10px;
            background-color: var(--input-bg);
            border-radius: 5px;
            overflow: hidden;
            margin-bottom: 5px;
        }

        .progress-bar-fill {
            height: 100%;
            background-color: var(--primary-color);
            border-radius: 5px;
            width: 65%;
        }

        .file-progress-item {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 10px;
            border: 1px solid var(--border-color);
            border-radius: 4px;
            margin-bottom: 10px;
        }

        .file-progress-status {
            font-size: 0.8rem;
            padding: 2px 6px;
            border-radius: 10px;
            background-color: var(--primary-color);
            color: white;
        }

        /* Batch Actions Toolbar */
        .batch-actions-toolbar {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background-color: var(--card-bg);
            border-top: 1px solid var(--border-color);
            padding: 15px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            z-index: 100;
            box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
        }

        .batch-btn {
            padding: 8px 15px;
            border-radius: 4px;
            border: 1px solid var(--border-color);
            background-color: var(--card-bg);
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 5px;
        }

        .batch-btn.primary {
            background-color: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        .batch-btn.danger {
            background-color: var(--error-color);
            color: white;
            border-color: var(--error-color);
        }

        /* Modal Styles */
        .modal-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            display: flex;
            align-items: center;
            justify-content: center;
            z-index: 1000;
        }

        .modal {
            background-color: var(--card-bg);
            border-radius: 8px;
            width: 90%;
            max-width: 500px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
        }

        .modal-header {
            padding: 15px 20px;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .modal-body {
            padding: 20px;
        }

        .modal-footer {
            padding: 15px 20px;
            border-top: 1px solid var(--border-color);
            display: flex;
            justify-content: flex-end;
            gap: 10px;
        }

        .btn {
            padding: 8px 15px;
            border-radius: 4px;
            border: 1px solid var(--border-color);
            background-color: var(--card-bg);
            cursor: pointer;
        }

        .btn.primary {
            background-color: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Metis RAG Document Upload UI Mockup</h1>
            <p class="description">This mockup demonstrates the UI design concepts for the Phase 2 document upload improvements, focusing on a clean interface with progressive disclosure of functionality.</p>
        </header>

        <!-- Upload Section -->
        <div class="collapsible-section">
            <div class="section-header">
                <h3>Upload Documents</h3>
                <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
            </div>
            <div class="section-content">
                <div class="drop-zone">
                    <div class="drop-zone-icon">
                        <i class="fas fa-cloud-upload-alt"></i>
                    </div>
                    <p>Drag and drop files here or click to select</p>
                    <p>Supported formats: PDF, Word, Text, CSV, Markdown, HTML, JSON, XML</p>
                    <button class="select-files-btn">Select Files</button>
                </div>
            </div>
        </div>

        <!-- Selected Files Section -->
        <div class="collapsible-section">
            <div class="section-header">
                <h3>Selected Files (3)</h3>
                <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
            </div>
            <div class="section-content">
                <div class="file-list">
                    <!-- File Preview 1 -->
                    <div class="file-preview">
                        <div class="file-thumbnail">
                            <i class="fas fa-file-pdf"></i>
                        </div>
                        <div class="file-info">
                            <div class="file-name">quarterly_report.pdf</div>
                            <div class="file-meta">
                                <span>2.4 MB</span>
                                <span>PDF</span>
                            </div>
                        </div>
                        <button class="file-remove">&times;</button>
                    </div>

                    <!-- File Preview 2 -->
                    <div class="file-preview">
                        <div class="file-thumbnail">
                            <i class="fas fa-file-word"></i>
                        </div>
                        <div class="file-info">
                            <div class="file-name">project_proposal.docx</div>
                            <div class="file-meta">
                                <span>1.8 MB</span>
                                <span>DOCX</span>
                            </div>
                        </div>
                        <button class="file-remove">&times;</button>
                    </div>

                    <!-- File Preview 3 -->
                    <div class="file-preview">
                        <div class="file-thumbnail">
                            <i class="fas fa-file-csv"></i>
                        </div>
                        <div class="file-info">
                            <div class="file-name">sales_data.csv</div>
                            <div class="file-meta">
                                <span>856 KB</span>
                                <span>CSV</span>
                            </div>
                        </div>
                        <button class="file-remove">&times;</button>
                    </div>
                </div>
            </div>
        </div>

        <!-- Progress Section -->
        <div class="collapsible-section">
            <div class="section-header">
                <h3>Upload Progress</h3>
                <button class="toggle-btn"><i class="fas fa-chevron-down"></i></button>
            </div>
            <div class="section-content">
                <div>
                    <h4>Overall Progress (65%)</h4>
                    <div class="progress-bar">
                        <div class="progress-bar-fill"></div>
                    </div>
                    <p>2/3 files complete • Estimated time remaining: 45s</p>
                    
                    <div class="file-progress-list">
                        <!-- File Progress Item 1 -->
                        <div class="file-progress-item">
                            <i class="fas fa-file-pdf"></i>
                            <div style="flex-grow: 1;">
                                <div style="display: flex; justify-content: space-between;">
                                    <span>quarterly_report.pdf</span>
                                    <span class="file-progress-status">Complete</span>
                                </div>
                                <div class="progress-bar">
                                    <div class="progress-bar-fill" style="width: 100%"></div>
                                </div>
                            </div>
                        </div>

                        <!-- File Progress Item 2 -->
                        <div class="file-progress-item">
                            <i class="fas fa-file-word"></i>
                            <div style="flex-grow: 1;">
                                <div style="display: flex; justify-content: space-between;">
                                    <span>project_proposal.docx</span>
                                    <span class="file-progress-status">Complete</span>
                                </div>
                                <div class="progress-bar">
                                    <div class="progress-bar-fill" style="width: 100%"></div>
                                </div>
                            </div>
                        </div>

                        <!-- File Progress Item 3 -->
                        <div class="file-progress-item">
                            <i class="fas fa-file-csv"></i>
                            <div style="flex-grow: 1;">
                                <div style="display: flex; justify-content: space-between;">
                                    <span>sales_data.csv</span>
                                    <span class="file-progress-status" style="background-color: var(--accent-color);">Uploading</span>
                                </div>
                                <div class="progress-bar">
                                    <div class="progress-bar-fill" style="width: 45%"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Batch Actions Toolbar -->
        <div class="batch-actions-toolbar">
            <div>
                <span>3</span> files selected
            </div>
            <div style="display: flex; gap: 10px;">
                <button class="batch-btn">
                    <i class="fas fa-tags"></i> Tag
                </button>
                <button class="batch-btn">
                    <i class="fas fa-folder"></i> Move
                </button>
                <button class="batch-btn danger">
                    <i class="fas fa-trash"></i> Delete
                </button>
                <button class="batch-btn primary">
                    <i class="fas fa-cogs"></i> Process
                </button>
            </div>
        </div>

        <!-- Batch Tag Modal (shown for demonstration) -->
        <div class="modal-overlay">
            <div class="modal">
                <div class="modal-header">
                    <h3>Add Tags to 3 Files</h3>
                    <button>&times;</button>
                </div>
                <div class="modal-body">
                    <input type="text" placeholder="Add a tag..." style="width: 100%; padding: 8px; margin-bottom: 15px;">
                    <div style="display: flex; gap: 5px; flex-wrap: wrap; margin-bottom: 15px;">
                        <div style="background-color: var(--primary-color); color: white; padding: 5px 10px; border-radius: 15px;">
                            quarterly <span>&times;</span>
                        </div>
                        <div style="background-color: var(--primary-color); color: white; padding: 5px 10px; border-radius: 15px;">
                            finance <span>&times;</span>
                        </div>
                    </div>
                    
                    <label>
                        <input type="checkbox" checked>
                        Merge with existing tags
                    </label>
                </div>
                <div class="modal-footer">
                    <button class="btn">Cancel</button>
                    <button class="btn primary">Apply Tags</button>
                </div>
            </div>
        </div>
    </div>
</body>
</html>

================
File: docs/file_structure_proposal.md
================
# Metis RAG File Structure Reorganization Proposal

## Current Issues

The root directory of the Metis RAG project contains numerous files of different types and purposes, making it difficult to navigate and understand the project structure. This proposal aims to reorganize these files into a more logical and maintainable structure.

## Proposed Directory Structure

```
metis_rag/
├── app/                      # Main application code (existing structure)
├── config/                   # Configuration files
│   ├── .env.example
│   ├── docker-compose.yml
│   └── requirements.txt
├── docs/                     # Documentation
│   ├── implementation/       # Implementation plans and technical details
│   │   ├── llm_enhanced_rag_implementation_plan.md
│   │   ├── mem0_integration_plan.md
│   │   └── langgraph_integration.md
│   ├── setup/                # Setup and improvement plans
│   │   ├── Metis_RAG_Setup_Plan.md
│   │   └── Metis_RAG_Improvement_Plan.md
│   ├── technical/            # Technical documentation
│   │   ├── technical_documentation.md
│   │   └── TESTING.md
│   └── demos/                # Demo documentation
│       ├── Metis_RAG_Technical_Demo.md
│       └── Metis_RAG_Technical_Demo.html
├── scripts/                  # Utility scripts
│   ├── maintenance/          # Maintenance scripts
│   │   ├── clear_cache.py
│   │   ├── clear_database.py
│   │   └── reprocess_documents.py
│   ├── generation/           # Data generation scripts
│   │   ├── generate_pdf.py
│   │   └── generate_test_data.py
│   └── demo/                 # Demo scripts
│       ├── demo_presentation.py
│       └── demo_tests.py
├── tests/                    # Test code and data
│   ├── integration/          # Integration tests (existing)
│   ├── unit/                 # Unit tests (existing)
│   ├── retrieval_judge/      # Retrieval judge tests (existing)
│   ├── data/                 # Test data files
│   │   ├── test_data.csv
│   │   ├── test_document.txt
│   │   └── sample_report.pdf
│   └── results/              # Test results
│       ├── chunking_judge_results/
│       │   ├── chunking_judge_real_results.json
│       │   └── chunking_judge_test_results.json
│       ├── retrieval_results/
│       │   ├── test_citation_results.json
│       │   ├── test_multi_doc_results.json
│       │   └── test_response_time_results.json
│       └── quality_results/
│           ├── test_quality_results.json
│           └── metis_rag_test_report.json
├── data/                     # Application data
│   ├── chroma_db/            # Vector database
│   ├── uploads/              # User uploads
│   ├── test_docs/            # Test documents
│   ├── test_perf_chroma/     # Performance test database
│   └── test_quality_chroma/  # Quality test database
├── .env                      # Environment variables (not in version control)
├── .gitignore                # Git ignore file
├── Dockerfile                # Docker configuration
├── pyproject.toml            # Python project configuration
├── README.md                 # Project README
└── run_tests.py              # Main test runner script
```

## Migration Plan

1. **Create the new directory structure**:
   ```bash
   mkdir -p config docs/{implementation,setup,technical,demos} scripts/{maintenance,generation,demo} tests/{data,results/{chunking_judge_results,retrieval_results,quality_results}} data
   ```

2. **Move configuration files**:
   ```bash
   mv .env.example docker-compose.yml requirements.txt config/
   ```

3. **Move documentation files**:
   ```bash
   mv llm_enhanced_rag_implementation_plan*.md mem0_integration_plan.md docs/implementation/
   mv Metis_RAG_*Plan.md docs/setup/
   mv technical_documentation.md TESTING.md docs/technical/
   mv Metis_RAG_Technical_Demo.* docs/demos/
   ```

4. **Move utility scripts**:
   ```bash
   mv clear_*.py reprocess_documents.py scripts/maintenance/
   mv generate_*.py scripts/generation/
   mv demo_*.py scripts/demo/
   ```

5. **Move test data and results**:
   ```bash
   mv test_data.csv test_document.txt sample_report.pdf tests/data/
   mv chunking_judge*results.json tests/results/chunking_judge_results/
   mv test_citation_results.json test_multi_doc_results.json test_response_time_results.json tests/results/retrieval_results/
   mv test_quality_results.json metis_rag_test_report.json tests/results/quality_results/
   mv metis_rag_test_report.html tests/results/
   ```

6. **Move data directories**:
   ```bash
   mv chroma_db/ uploads/ test_docs/ test_perf_chroma/ test_quality_chroma/ data/
   ```

7. **Update import paths and file references** in Python code to reflect the new structure.

8. **Update documentation** to reference the new file locations.

## Benefits of the New Structure

1. **Improved Navigation**: Files are organized by purpose, making it easier to find what you're looking for.

2. **Better Maintainability**: Related files are grouped together, making maintenance and updates more straightforward.

3. **Cleaner Root Directory**: The root directory contains only the most essential files, reducing clutter.

4. **Logical Grouping**: Files are grouped by their function and relationship to each other.

5. **Scalability**: The structure can easily accommodate new files and components as the project grows.

6. **Better Onboarding**: New developers can more quickly understand the project structure and find relevant files.

## Implementation Considerations

1. **Path Updates**: Scripts and code that reference files by path will need to be updated to reflect the new structure.

2. **Documentation Updates**: Documentation should be updated to reference the new file locations.

3. **Gradual Migration**: The migration can be done gradually, starting with the most disorganized areas.

4. **Version Control**: The reorganization should be done in a separate branch and thoroughly tested before merging.

5. **Docker Configuration**: If using Docker, the Dockerfile and docker-compose.yml may need updates to reflect the new paths.

## Next Steps

1. Review this proposal and make any necessary adjustments.
2. Create a new git branch for the reorganization.
3. Implement the directory structure changes.
4. Update file paths in code and documentation.
5. Test the application to ensure everything works correctly.
6. Merge the changes into the main branch.

================
File: docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md
================
# Metis RAG Authentication Implementation Detailed Plan

## Overview

This document outlines a comprehensive plan for implementing a secure authentication system for the Metis RAG application. The authentication system will not only verify user identities but also enforce proper access controls specifically designed for RAG applications where document-level security is crucial.

## Current State Analysis

The application currently has:

- A database schema with a `users` table including essential fields (id, username, email, password_hash, etc.)
- Plans to add `user_id` foreign keys to `documents` and `conversations` tables
- A basic `AuthMiddleware` class in `app/middleware/auth.py` that handles authorization headers
- Protected routes defined for both web UI and API endpoints
- No proper permissions system for document access in vector databases

## Objectives

1. Implement JWT-based authentication with FastAPI
2. Establish document-level access control using PostgreSQL Row Level Security (RLS)
3. Implement vector database security using metadata filtering
4. Set up proper document permission models (ownership, sharing, role-based access)
5. Create secure endpoints for user management
6. Integrate authentication with the RAG query pipeline

## Implementation Approach

Our approach combines three security layers:

1. **JWT Authentication Layer**: User identity verification with JWT tokens
2. **Database Access Control Layer**: PostgreSQL RLS policies
3. **Vector Database Security Layer**: Metadata-based filtering for vector searches

### Authentication Architecture

```mermaid
flowchart TD
    subgraph Authentication
        A[JWT Token Auth] --> B[User Authentication]
        B --> C[Token Validation]
    end
    
    subgraph Database
        D[PostgreSQL Database] --> E[Row Level Security]
        E --> F[Document Access Control]
    end
    
    subgraph Vector_DB
        G[Vector Database] --> H[Metadata Filtering]
        H --> I[Post-Query Permission Check]
    end
    
    subgraph RAG_System
        J[RAG Query Pipeline] --> K[User Request]
        K --> L[Document Retrieval]
        L --> M[Permission Filtering]
        M --> N[LLM Response]
    end
    
    A --> J
    F --> L
    I --> M
```

## Persistent User-Document Relationships

A critical feature of our implementation is maintaining persistent relationships between users and their documents across all authentication events. This ensures that user access to their data remains consistent regardless of credential changes, password resets, or periods of inactivity.

### Authentication vs. Authorization Separation

Our system clearly separates:

1. **Authentication** (JWT token-based): Verifies a user's identity during a session
   - JWT tokens are temporary and expire after a set time period
   - Tokens can be refreshed/renewed without affecting data access rights
   - Password resets generate new credentials but preserve the same user identity

2. **Authorization** (Database-level): Determines what resources a user can access
   - Based on permanent database relationships, not session tokens
   - Implemented through foreign key relationships and Row Level Security
   - Persists regardless of authentication status or history

### Database-Level Persistence

The core of persistence is implemented at the database level through:

1. **Foreign Key Relationships**: The `user_id` columns in `documents` and `conversations` tables create permanent relationships that exist independent of authentication status.

2. **Document Permissions Table**: Stores access rights permanently so sharing arrangements are preserved even when users log out or refresh tokens.

3. **Row Level Security Policies**: Based on the user's identity (user_id), not on their current authentication token or session.

### Handling Edge Cases

Our implementation addresses common edge cases:

1. **Password Resets**: When a user resets their password, they receive new credentials but maintain the same user_id, preserving all document relationships.

2. **Account Deactivation/Reactivation**: Setting the `is_active` flag to false temporarily prevents login but maintains all document relationships for when the account is reactivated.

3. **Extended Periods of Inactivity**: Unlike session-based systems where data relationships might be lost, our database-level approach ensures relationships persist indefinitely.

4. **Token Expiration/Renewal**: When tokens expire and are renewed, the user continues to access the same documents without interruption.

## Detailed Implementation Plan

### Phase 1: JWT Authentication System (COMPLETED)

#### 1.1 JWT Authentication Core

- [x] Create JWT configuration settings in `app/core/config.py`
  - Secret key
  - Algorithm
  - Token expiration time
  - Refresh token settings
  - Added JWT audience and issuer settings

- [x] Implement JWT token handling in `app/core/security.py`
  - Token generation function
  - Token validation function
  - Token refresh function
  - Password hashing utilities
  - Added audience verification options

- [x] Create JWT bearer class in `app/middleware/jwt_bearer.py`
  - Dependency for FastAPI route protection
  - Token extraction from request headers
  - Token validation using security functions
  - Comprehensive error handling

#### 1.2 User Authentication Endpoints

- [x] Implement login endpoint in `app/api/auth.py`
  - User credential verification
  - JWT token generation
  - Login audit logging
  - Security event tracking

- [x] Implement token refresh endpoint
  - Validate refresh token
  - Generate new access token
  - Update last login timestamp
  - Maintain token claims

- [x] Create user registration endpoint
  - Input validation
  - Password hashing
  - User record creation
  - Initial user permission setup

#### 1.3 Authentication Middleware Integration

- [x] Update `app/middleware/auth.py` to use JWT validation
  - Extract JWT from Authorization header
  - Validate token and extract user identity
  - Handle token expiration and errors
  - Support for both API and web UI authentication

- [x] Apply authentication middleware to FastAPI app
  - Configure middleware in `app/main.py`
  - Set up protection for relevant endpoints
  - Added documentation endpoints

#### Completion Prompt for Phase 1:

```
Implement JWT authentication system for Metis RAG based on the plan in docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md. Focus on Phase 1: JWT Authentication System, which includes:

1. Setting up JWT configuration in app/core/config.py
2. Implementing token handling functions in app/core/security.py
3. Creating a JWT bearer class for FastAPI route protection
4. Building user authentication endpoints (login, token refresh, registration)
5. Updating the authentication middleware to use JWT validation

The existing user model is defined in app/models/user.py and the current authentication middleware is in app/middleware/auth.py.
```

### Phase 2: Database Level Security (COMPLETED)

#### 2.1 Database Schema Updates

- [x] Update `documents` table
  - ✅ User ID foreign key already exists
  - ✅ Added `is_public` boolean flag for public documents

- [x] Update `conversations` table
  - ✅ User ID foreign key already exists

- [x] Create `document_permissions` table for document sharing
  - ✅ Document ID reference with CASCADE delete
  - ✅ User ID reference with CASCADE delete
  - ✅ Permission level (read, write, admin)
  - ✅ Created timestamp
  - ✅ Unique constraint on document_id and user_id

#### 2.2 Row Level Security Implementation

- [x] Enable RLS on document tables
  ```sql
  ALTER TABLE documents ENABLE ROW LEVEL SECURITY;
  ALTER TABLE chunks ENABLE ROW LEVEL SECURITY;
  ```

- [x] Create ownership RLS policies
  ```sql
  CREATE POLICY "Users can view their own documents"
  ON documents FOR SELECT
  USING (user_id = current_setting('app.current_user_id')::uuid OR is_public = true);
  
  CREATE POLICY "Users can update their own documents"
  ON documents FOR UPDATE
  USING (user_id = current_setting('app.current_user_id')::uuid);
  
  CREATE POLICY "Users can delete their own documents"
  ON documents FOR DELETE
  USING (user_id = current_setting('app.current_user_id')::uuid);
  ```

- [x] Create document sharing RLS policies
  ```sql
  CREATE POLICY "Users can view documents shared with them"
  ON documents FOR SELECT
  USING (id IN (
    SELECT document_id FROM document_permissions WHERE user_id = current_setting('app.current_user_id')::uuid
  ));
  
  CREATE POLICY "Users can update documents shared with write permission"
  ON documents FOR UPDATE
  USING (id IN (
    SELECT document_id FROM document_permissions
    WHERE user_id = current_setting('app.current_user_id')::uuid
    AND permission_level IN ('write', 'admin')
  ));
  ```

- [x] Create policies for document sections (chunks)
  ```sql
  CREATE POLICY "Users can view their own document sections"
  ON chunks FOR SELECT
  USING (document_id IN (
    SELECT id FROM documents WHERE user_id = current_setting('app.current_user_id')::uuid OR is_public = true
  ));
  
  CREATE POLICY "Users can view document sections shared with them"
  ON chunks FOR SELECT
  USING (document_id IN (
    SELECT document_id FROM document_permissions WHERE user_id = current_setting('app.current_user_id')::uuid
  ));
  ```

#### 2.3 Database Context Middleware

- [x] Create database context middleware in `app/middleware/db_context.py`
  - ✅ Extract user ID from JWT token
  - ✅ Set database session parameter for RLS
  - ✅ Handle authentication errors gracefully

- [x] Register middleware in `app/main.py`
  - ✅ Add middleware after authentication middleware
  - ✅ Ensure proper ordering for security

#### 2.4 Repository Layer Updates

- [x] Update document repository in `app/db/repositories/document_repository.py`
  - ✅ Modify CRUD operations to include user context
  - ✅ Implement document sharing functionality
  - ✅ Add permission checking methods

- [x] Update conversation repository in `app/db/repositories/conversation_repository.py`
  - ✅ Associate conversations with users
  - ✅ Filter conversations by user ID

#### Completion Prompt for Phase 2:

```
Implement database-level security for Metis RAG based on the plan in docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md. Focus on Phase 2: Database Level Security, which includes:

1. Updating the database schema (documents and conversations tables)
2. Creating a document_permissions table for document sharing
3. Implementing Row Level Security (RLS) policies
4. Updating the document and conversation repositories to work with user context

The existing database models are defined in app/db/models.py and the repositories are in app/db/repositories/.
```

### Phase 3: Vector Database Security (COMPLETED)

#### 3.1 Vector Storage with User Context

- [x] Update document processing pipeline
  - ✅ Include `user_id` in document metadata during embedding
  - ✅ Add permission flags to vector metadata
  - ✅ Store document categorization information

- [x] Modify vector database schema
  - ✅ Add user ID field to vector metadata
  - ✅ Add permission level field to metadata
  - ✅ Include document categorization in metadata

#### 3.2 Secure Vector Search Implementation

- [x] Implement metadata filtering in vector search
  - ✅ Filter by user ID during vector search
  - ✅ Include shared documents in search results
  - ✅ Consider permission levels in search results

- [x] Create post-retrieval filtering function
  - ✅ Verify document permissions after similarity search
  - ✅ Check for document access rights
  - ✅ Log unauthorized access attempts

#### 3.3 RAG Pipeline Integration

- [x] Update RAG query processor
  - ✅ Include user context in query processing
  - ✅ Apply security filters to retrieved documents
  - ✅ Handle security-related errors gracefully

- [x] Implement secure document chunking
  - ✅ Preserve security metadata during chunking
  - ✅ Ensure chunk-level permissions match document permissions
  - ✅ Handle permission transitions within documents

#### Completion Prompt for Phase 3:

```
Implement vector database security for Metis RAG based on the plan in docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md. Focus on Phase 3: Vector Database Security, which includes:

1. Updating the document processing pipeline to include user context
2. Modifying the vector database schema to store permission information
3. Implementing secure vector search with metadata filtering
4. Updating the RAG query processor to respect document permissions

The existing RAG implementation is in app/rag/ directory.
```

### Phase 4: Advanced Permission Models

#### 4.1 Role-Based Access Control

- [ ] Create roles table
  ```sql
  CREATE TABLE roles (
    id UUID PRIMARY KEY,
    name VARCHAR NOT NULL UNIQUE,
    description VARCHAR,
    permissions JSONB
  );
  ```

- [ ] Create user-role associations
  ```sql
  CREATE TABLE user_roles (
    user_id UUID REFERENCES users(id),
    role_id UUID REFERENCES roles(id),
    PRIMARY KEY (user_id, role_id)
  );
  ```

- [ ] Implement role-based permission checking
  - Create permission utility functions
  - Add role checking to security middleware
  - Include role-based filters in queries

#### 4.2 Document Sharing and Collaboration

- [ ] Create document sharing API
  - Endpoint for granting user access
  - Endpoint for revoking user access
  - Endpoint for listing document collaborators

- [ ] Implement sharing notification system
  - Email notifications for shared documents
  - In-app notifications
  - Activity logging for shared documents

#### 4.3 Multi-tenant Isolation

- [ ] Implement organization/team model
  ```sql
  CREATE TABLE organizations (
    id UUID PRIMARY KEY,
    name VARCHAR NOT NULL,
    settings JSONB
  );
  
  CREATE TABLE organization_members (
    organization_id UUID REFERENCES organizations(id),
    user_id UUID REFERENCES users(id),
    role VARCHAR NOT NULL,
    PRIMARY KEY (organization_id, user_id)
  );
  ```

- [ ] Associate documents with organizations
  - Add organization_id to documents table
  - Create RLS policies for organization-based access
  - Implement cross-organization sharing

#### Completion Prompt for Phase 4:

```
Implement advanced permission models for Metis RAG based on the plan in docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md. Focus on Phase 4: Advanced Permission Models, which includes:

1. Setting up role-based access control
2. Implementing document sharing and collaboration features
3. Creating multi-tenant isolation with organizations/teams
4. Updating the security middleware to handle these advanced permissions

This phase builds on the previous authentication and security implementations.
```

### Phase 5: Testing and Security Hardening

#### 5.1 Authentication Testing

- [ ] Create unit tests for JWT functions
  - Token generation tests
  - Token validation tests
  - Token refresh tests

- [ ] Create integration tests for auth endpoints
  - Login flow tests
  - Registration flow tests
  - Password reset flow tests

#### 5.2 Permission Testing

- [ ] Test document-level permissions
  - Owner access tests
  - Shared document access tests
  - Public document access tests

- [ ] Test vector search security
  - Verify correct filtering by user
  - Test cross-user access prevention
  - Benchmark performance impact

#### 5.3 Security Hardening

- [ ] Implement rate limiting
  - Add rate limits to login endpoints
  - Set up IP-based rate limiting
  - Create lockout mechanism for repeated failures

- [ ] Set up security monitoring
  - Log authentication attempts
  - Alert on suspicious activities
  - Track permission violations

- [ ] Conduct security review
  - Dependency vulnerability scanning
  - JWT configuration review
  - Database security audit

#### Completion Prompt for Phase 5:

```
Implement testing and security hardening for Metis RAG based on the plan in docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md. Focus on Phase 5: Testing and Security Hardening, which includes:

1. Creating unit and integration tests for authentication
2. Testing document-level permissions and vector search security
3. Implementing rate limiting for API endpoints
4. Setting up security monitoring and conducting a security review

This is the final phase of the authentication implementation plan.
```

## Implementation Checklist

### Phase 1: JWT Authentication System (COMPLETED)
- [x] JWT configuration in `app/core/config.py`
- [x] JWT token handling in `app/core/security.py`
- [x] JWT bearer class in `app/middleware/jwt_bearer.py`
- [x] Login endpoint in `app/api/auth.py`
- [x] Token refresh endpoint
- [x] User registration endpoint
- [x] Update authentication middleware
- [x] Apply middleware to FastAPI app

### Phase 2: Database Level Security (COMPLETED)
- [x] Update `documents` table with `is_public` column (user_id already existed)
- [x] Update `conversations` table with `user_id` column (already existed)
- [x] Create `document_permissions` table
- [x] Enable RLS on document tables
- [x] Create ownership RLS policies
- [x] Create document sharing RLS policies
- [x] Create policies for document sections (chunks)
- [x] Create database context middleware
- [x] Update document repository
- [x] Update conversation repository

### Phase 3: Vector Database Security (PARTIALLY COMPLETED)
- [x] Update document processing pipeline with user context
- [x] Modify vector database schema
- [x] Implement metadata filtering in vector search
- [x] Create post-retrieval filtering function
- [x] Update RAG query processor
- [x] Implement secure document chunking

### Phase 4: Advanced Permission Models (COMPLETED)
- [x] Create roles table
- [x] Create user-role associations
- [x] Implement role-based permission checking
- [x] Create document sharing API
- [x] Implement sharing notification system
- [x] Implement organization/team model
- [x] Associate documents with organizations

### Phase 5: Testing and Security Hardening (PARTIALLY COMPLETED)
- [x] Create unit tests for JWT functions
- [x] Create integration tests for auth endpoints
- [x] Test document-level permissions
- [x] Test vector search security
- [ ] Implement rate limiting
- [ ] Set up security monitoring
- [ ] Conduct security review

### Persistent User-Document Relationship Tasks (PARTIALLY COMPLETED)
- [x] Implement password reset functionality that preserves user identity
- [x] Create account deactivation/reactivation features that maintain document relationships
- [x] Add user identity preservation functions to authentication system
- [ ] Develop user data migration utilities (if needed)
- [ ] Create admin tools for managing persistent relationships
- [x] Add unit tests for persistent relationships during authentication events

## Security Considerations

1. **JWT Security Best Practices**
   - Use strong secret keys for JWT signing
   - Set appropriate token expiration times
   - Implement token refresh mechanisms
   - Consider using asymmetric key cryptography for production

2. **Password Security**
   - Implement strong password hashing with Argon2 or bcrypt
   - Enforce password complexity requirements
   - Implement account lockout after failed attempts
   - Set up secure password reset flows

3. **Database Security**
   - Ensure proper indexing for RLS policies to maintain performance
   - Test RLS policies thoroughly to avoid security loopholes
   - Monitor query performance with RLS enabled
   - Consider using prepared statements for all database queries

4. **Vector Database Considerations**
   - Balance security filtering with query performance
   - Consider the trade-offs between pre-filtering and post-filtering
   - Monitor memory usage during vector operations
   - Implement proper error handling for permission-related failures

5. **API Security**
   - Implement CSRF protection for web interfaces
   - Set up proper CORS policies
   - Consider using API keys for machine-to-machine communication
   - Implement request validation for all endpoints

6. **Persistent Relationship Security**
   - Ensure user identity preservation during credential changes
   - Implement secure password reset flows that maintain document relationships
   - Create audit logs for all relationship changes
   - Develop administrative tools for relationship management
   - Implement safeguards against unauthorized relationship modifications

## Implementation Progress

### Phase 1 Completion (March 2025)

Phase 1 of the authentication system has been successfully implemented and tested. The implementation includes:

1. **JWT Configuration**: Added JWT settings in `app/core/config.py` including secret key, algorithm, token expiration times, and audience/issuer claims.

2. **Token Handling**: Enhanced `app/core/security.py` with comprehensive token generation, validation, and refresh functions.

3. **JWT Bearer Class**: Created `app/middleware/jwt_bearer.py` for FastAPI route protection with proper token extraction and validation.

4. **Authentication Endpoints**: Updated `app/api/auth.py` with login, token refresh, and registration endpoints.

5. **Middleware Integration**: Enhanced `app/middleware/auth.py` to use JWT validation and applied it in `app/main.py`.

6. **Testing**: Created and validated the authentication flow with `run_authentication_test.py` and an interactive demo in `docs/authentication_demo.html`.

### Phase 2 Completion (March 2025)

Phase 2 of the database-level security has been fully implemented. The implementation includes:

1. **Database Schema Updates**:
   - Added `is_public` boolean flag to the `documents` table
   - Created `document_permissions` table with proper foreign key constraints and indexes

2. **Row Level Security Implementation**:
   - Enabled Row Level Security on `documents` and `chunks` tables
   - Created RLS policies for document ownership
   - Created RLS policies for document sharing
   - Created RLS policies for document sections (chunks)

3. **Database Context Middleware**:
   - Created `app/middleware/db_context.py` to set database context for RLS
   - Implemented JWT token extraction for user identification
   - Added middleware to the FastAPI application

4. **Repository Layer Updates**:
   - Updated document repository to include user context and permission checking
   - Implemented document sharing functionality in the repository
   - Added permission checking methods for various operations
   - Updated conversation repository to properly use the user_id field

### Phase 3 Completion (March 2025)

Phase 3 of the vector database security has been fully implemented. The implementation includes:

1. **Vector Storage with User Context**:
   - Updated document processing pipeline to include user_id in document metadata
   - Added permission flags to vector metadata
   - Modified vector database schema to store permission information

2. **Secure Vector Search Implementation**:
   - Implemented metadata filtering in vector search based on user permissions
   - Created post-retrieval filtering function to verify document access rights
   - Added logging for unauthorized access attempts

3. **RAG Pipeline Integration**:
   - Refactored RAGEngine into multiple files for better maintainability:
     - rag_engine_base.py: Base class with core functionality
     - rag_retrieval.py: Retrieval-related functionality
     - rag_generation.py: Response generation functionality
     - rag_engine.py: Main class that combines all components
     - system_prompts.py: Separate file for system prompts
   - Updated RAG query processor to include user context and respect document permissions
   - Modified retrieve method to include user_id for permission filtering

4. **Secure Document Chunking**:
   - Enhanced document processor to preserve security metadata during chunking
   - Implemented inheritance of security properties from parent documents to chunks
   - Added support for section-specific permissions within documents
   - Created post-retrieval permission checking as a secondary security layer
   - Added detailed logging for unauthorized access attempts

### Phase 4 Completion (March 2025)

Phase 4 of the advanced permission models has been fully implemented. The implementation includes:

1. **Role-Based Access Control**:
  - Created roles and user_roles tables in the database
  - Implemented role-based permission checking
  - Created API endpoints for managing roles
  - Created and ran a script to set up default roles (admin, editor, viewer)
  - Assigned admin role to existing admin users

2. **Document Sharing and Collaboration**:
  - Created a notification system for document sharing
  - Enhanced the document sharing API to send notifications
  - Added API endpoints for managing notifications
  - Implemented permission checking for document sharing

3. **Multi-tenant Isolation**:
  - Created organizations and organization_members tables
  - Added organization_id to documents table
  - Updated document repository to support organization-based access control
  - Created API endpoints for managing organizations
  - Implemented permission checking for organization operations

4. **Migration Process**:
  - Created migration scripts for the new tables and relationships
  - Created a merge migration to resolve multiple head revisions
  - Ran the migrations to update the database schema
  - Created and ran a script to set up default roles

### Implementation Notes

During implementation, we encountered and resolved the following issues:

1. **Audience Verification**: Disabled audience verification in JWT validation to avoid issues with token validation. In a production environment, this should be properly configured.

2. **Password Hashing**: There was an issue with the bcrypt library, but we worked around it by directly updating the user's password hash in the database.

3. **Token Claims**: Added standard JWT claims (iss, aud, jti) for better security and compliance.

4. **Migration Issues**: Encountered issues with Alembic migrations due to multiple head revisions. Resolved by creating a merge migration and modifying the migration script to check if tables already exist before creating them.

5. **Database Context**: Implemented a database context middleware that sets the `app.current_user_id` parameter in the database session, which is used by the RLS policies to filter rows based on the current user.

6. **RAGEngine Refactoring**: Refactored the RAGEngine into multiple files to improve maintainability and make it easier to add security features. This approach allowed us to add user context and permission filtering without making the code too complex.

7. **Secure Document Chunking**: Implemented secure document chunking by modifying the DocumentProcessor class to preserve security metadata during chunking and ensure that chunk-level permissions match document permissions. We also added support for handling permission transitions within documents, allowing different sections of a document to have different permission levels.

8. **Post-Retrieval Security**: Added a post-retrieval permission checking mechanism to the VectorStore class as an additional security layer. This ensures that even if the initial query filtering misses something, we still verify permissions before returning results to the user.

9. **Phase 4 Migration Issues**: Encountered similar issues with multiple head revisions during Phase 4 migrations. Resolved by creating a merge migration (`merge_heads_for_phase4.py`) to combine the existing head revision with our new Phase 4 migrations.

10. **Session Management**: Fixed issues with database session management in the `create_default_roles.py` script by using the proper `AsyncSessionLocal` factory instead of directly creating an `AsyncSession`.

## Phase 5 Completion (March 2025)

Phase 5 of the testing and security hardening has been partially implemented. The implementation includes:

1. **Unit Tests**: Created comprehensive unit tests for JWT functions and password hashing utilities in `tests/unit/test_security_utils.py`.

2. **Integration Tests**: Implemented integration tests for:
   - Authentication endpoints in `tests/integration/test_auth_endpoints.py`
   - Database-level permissions (RLS) in `tests/integration/test_permissions_db.py`
   - Vector database security in `tests/integration/test_permissions_vector.py`

3. **End-to-End Tests**: Created end-to-end tests for:
   - Complete authentication flows in `tests/e2e/test_auth_flows.py`
   - Complex permission scenarios in `tests/e2e/test_permission_scenarios.py`

4. **Persistent Relationship Tests**: Implemented tests for persistent user-document relationships during:
   - Password reset
   - Account deactivation/reactivation
   - Token expiry and refresh

5. **Testing Documentation**: Created a comprehensive testing guide in `tests/authentication_testing_guide.md` that provides instructions for running the tests and understanding the test structure.

The remaining tasks in Phase 5 include:
- Implementing rate limiting for API endpoints
- Setting up security monitoring and alerting
- Conducting a comprehensive security review

## Next Steps

1. ✅ Complete Phase 1 (JWT Authentication)
2. ✅ Complete Phase 2 (Database Level Security)
   - ✅ Update database schema (added is_public flag, document_permissions table)
   - ✅ Implement Row Level Security policies
   - ✅ Create database context middleware
   - ✅ Update repository layer to include user context
3. ✅ Complete Phase 3 (Vector Database Security)
   - ✅ Update document processing pipeline with user context
   - ✅ Modify vector database schema
   - ✅ Implement metadata filtering in vector search
   - ✅ Update RAG query processor
   - ✅ Complete secure document chunking
4. ✅ Complete Phase 4 (Advanced Permission Models)
   - ✅ Create roles table and user-role associations
   - ✅ Implement role-based permission checking
   - ✅ Create document sharing API with notifications
   - ✅ Implement organization/team model
   - ✅ Associate documents with organizations
5. ✅ Partially Complete Phase 5 (Testing and Security Hardening)
   - ✅ Create unit tests for JWT functions
   - ✅ Create integration tests for auth endpoints
   - ✅ Test document-level permissions
   - ✅ Test vector search security
   - ✅ Test persistent user-document relationships
   - [ ] Implement rate limiting
   - [ ] Set up security monitoring
6. Complete remaining Phase 5 tasks and conduct comprehensive security review

================
File: docs/Metis_RAG_Authentication_Implementation_Prompt.md
================
# Metis RAG Authentication Implementation Prompt

## Task Overview

Implement the JWT authentication system for the Metis RAG application based on the detailed plan in `docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md`. This implementation will focus on Phase 1 of the plan, which establishes the core JWT authentication infrastructure.

## Background

The Metis RAG application requires a secure authentication system that not only verifies user identities but also maintains persistent relationships between users and their documents. The authentication system should be designed to ensure that user access to their data remains consistent regardless of credential changes, password resets, or periods of inactivity.

## Current State

- The application has a database schema with a `users` table including essential fields (id, username, email, password_hash, etc.)
- There's a basic `AuthMiddleware` class in `app/middleware/auth.py` that handles authorization headers
- Protected routes are defined for both web UI and API endpoints
- No proper JWT authentication is implemented yet

## Implementation Requirements

Please implement Phase 1 of the authentication system as outlined in the detailed plan:

1. **JWT Authentication Core**
   - Create JWT configuration settings in `app/core/config.py`
   - Implement JWT token handling in `app/core/security.py`
   - Create JWT bearer class in `app/middleware/jwt_bearer.py`

2. **User Authentication Endpoints**
   - Implement login endpoint in `app/api/endpoints/auth.py`
   - Implement token refresh endpoint
   - Create user registration endpoint

3. **Authentication Middleware Integration**
   - Update `app/middleware/auth.py` to use JWT validation
   - Apply authentication middleware to FastAPI app

## Key Considerations

- The implementation should maintain persistent user-document relationships
- JWT tokens should be temporary but user identity should be preserved across authentication events
- The system should clearly separate authentication (identity verification) from authorization (access control)
- Password reset functionality should preserve the same user identity
- The implementation should follow security best practices for JWT tokens

## Specific Files to Modify/Create

1. `app/core/config.py` - Add JWT configuration settings
2. `app/core/security.py` - Implement JWT token handling functions
3. `app/middleware/jwt_bearer.py` - Create JWT bearer class
4. `app/api/endpoints/auth.py` - Implement authentication endpoints
5. `app/middleware/auth.py` - Update to use JWT validation
6. `app/main.py` - Apply authentication middleware

## Testing Requirements

After implementation, the following should be testable:
- User registration with proper password hashing
- User login with JWT token generation
- Token refresh functionality
- Protected route access with valid JWT tokens
- Rejection of invalid or expired tokens

## Resources

- The existing user model is defined in `app/models/user.py`
- The current authentication middleware is in `app/middleware/auth.py`
- Detailed implementation plan: `docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md`

## Deliverables

1. Implemented JWT authentication system
2. Brief documentation of any design decisions or deviations from the plan
3. Example of how to test the authentication system

================
File: docs/Metis_RAG_Authentication_Testing_Plan.md
================
# Metis RAG Authentication Testing Plan

This plan details the testing strategy for the implemented authentication and authorization system for the Metis RAG application. It covers the components outlined in Phases 1-4 of the `Metis_RAG_Authentication_Implementation_Detailed_Plan.md` and focuses on the testing tasks listed in Phase 5 and the persistent user-document relationship requirements.

**Guiding Principles:**

*   **Coverage:** Ensure all critical authentication and authorization paths are tested.
*   **Isolation:** Use unit tests for isolated logic (e.g., JWT functions).
*   **Interaction:** Use integration tests for component interactions (e.g., API endpoints + DB).
*   **User Flows:** Use end-to-end (e2e) tests for complete user scenarios.
*   **Security Focus:** Include tests specifically designed to probe for security vulnerabilities (e.g., unauthorized access attempts).
*   **Persistence:** Verify that user-data relationships remain intact across various authentication events (e.g., password resets, token refreshes).

**Proposed Test Structure (within `tests/` directory):**

```
tests/
├── unit/
│   ├── test_security_utils.py  # For JWT and password hashing functions (Phase 5.1)
│   └── ... (other unit tests)
├── integration/
│   ├── test_auth_endpoints.py    # For /login, /register, /refresh, password reset (Phase 5.1)
│   ├── test_permissions_db.py    # For RLS, document_permissions table (Phase 5.2)
│   ├── test_permissions_vector.py # For vector metadata filtering (Phase 5.2)
│   ├── test_rbac.py              # For role creation, assignment, checking (Phase 4 / 5.2)
│   ├── test_sharing.py           # For document sharing API & logic (Phase 4 / 5.2)
│   ├── test_multi_tenancy.py     # For organization isolation (Phase 4 / 5.2)
│   └── ... (other integration tests)
├── e2e/
│   ├── test_auth_flows.py        # Full login, register, use protected resource flows
│   ├── test_permission_scenarios.py # Complex scenarios involving multiple users, roles, sharing
│   └── ... (other e2e tests)
└── conftest.py                 # Fixtures for setting up users, roles, docs, etc.
```

**Detailed Test Areas Overview:**

```mermaid
graph TD
    subgraph TestingPlan [Authentication Testing Plan]
        direction LR

        subgraph Phase5_1 [Phase 5.1: Authentication Core]
            direction TB
            T1_1[Unit: JWT Functions] --> T1_2(test_security_utils.py);
            T1_3[Integration: Auth Endpoints] --> T1_4(test_auth_endpoints.py);
            T1_5[E2E: User Auth Flows] --> T1_6(test_auth_flows.py);
        end

        subgraph Phase5_2 [Phase 5.2: Permission System]
            direction TB
            T2_1[Integration: DB Permissions (RLS)] --> T2_2(test_permissions_db.py);
            T2_3[Integration: Vector DB Permissions] --> T2_4(test_permissions_vector.py);
            T2_5[Integration: RBAC] --> T2_6(test_rbac.py);
            T2_7[Integration: Sharing] --> T2_8(test_sharing.py);
            T2_9[Integration: Multi-Tenancy] --> T2_10(test_multi_tenancy.py);
            T2_11[E2E: Complex Permission Scenarios] --> T2_12(test_permission_scenarios.py);
        end

        subgraph Persistence [Persistent Relationships]
            direction TB
            P1[Integration: Password Reset] --> T1_4;
            P2[Integration: Account Deactivation/Reactivation] --> T1_4;
            P3[Integration: Token Refresh & Expiry] --> T1_4;
            P4[Unit/Integration: Verify Data Links] --> T2_2 & T2_4;
        end

        subgraph Phase5_3 [Phase 5.3: Security Hardening (Verification)]
             direction TB
             SH1[Manual/Config Review: Rate Limiting] --> SH_Verify(Verification Steps);
             SH2[Manual/Config Review: Monitoring/Logging] --> SH_Verify;
             SH3[Manual/Automated: Security Review (Scans, Audits)] --> SH_Verify;
        end

    end
```

**I. Authentication Core Testing (Phase 5.1 & Persistence)**

*   **Location:** `tests/unit/test_security_utils.py`, `tests/integration/test_auth_endpoints.py`, `tests/e2e/test_auth_flows.py`
*   **Unit Tests (`test_security_utils.py`):**
    *   Verify `create_access_token`, `create_refresh_token` output structure and claims (iss, aud, exp, sub, jti).
    *   Verify `verify_password` against known hashes.
    *   Verify `get_password_hash` produces valid hashes.
    *   Test token validation logic (`decode_token`) for:
        *   Valid tokens.
        *   Expired tokens.
        *   Tokens with invalid signatures.
        *   Tokens with incorrect audience/issuer (if re-enabled).
        *   Malformed tokens.
*   **Integration Tests (`test_auth_endpoints.py`):**
    *   `/register`: Success, duplicate username/email, invalid input.
    *   `/login`: Success (correct credentials), failure (incorrect password, non-existent user), inactive user login attempt.
    *   `/refresh`: Success (valid refresh token), failure (invalid/expired refresh token).
    *   Password Reset Flow: Request reset, validate token, set new password, login with new password, ensure old password fails. **(Persistence)**
    *   Account Deactivation/Reactivation: Deactivate user, verify login fails, verify data relationships persist, reactivate user, verify login succeeds. **(Persistence)**
    *   Token Expiry: Simulate token expiry, verify protected endpoint access fails, refresh token, verify access succeeds. **(Persistence)**
*   **E2E Tests (`test_auth_flows.py`):**
    *   Simulate a full user journey: Register -> Login -> Access Protected Resource -> Refresh Token -> Access Protected Resource -> Logout.

**II. Permission System Testing (Phase 5.2, Phase 4 Features & Persistence)**

*   **Location:** `tests/integration/test_permissions_db.py`, `tests/integration/test_permissions_vector.py`, `tests/integration/test_rbac.py`, `tests/integration/test_sharing.py`, `tests/integration/test_multi_tenancy.py`, `tests/e2e/test_permission_scenarios.py`
*   **Database Permissions (`test_permissions_db.py`):** (Requires setting `app.current_user_id` in test session context)
    *   Test RLS policies for `documents` and `chunks`:
        *   Owner can SELECT, UPDATE, DELETE own documents.
        *   Non-owner cannot access private documents.
        *   Any authenticated user can SELECT public documents (`is_public=true`).
        *   User can SELECT documents shared via `document_permissions`.
        *   User can UPDATE documents shared with 'write'/'admin' permission.
        *   User cannot UPDATE documents shared with 'read' permission.
        *   Verify access to `chunks` mirrors access to parent `document`.
    *   Test `document_permissions` table logic: Granting, revoking, listing permissions.
*   **Vector DB Permissions (`test_permissions_vector.py`):**
    *   Verify vector search results only include chunks where the user has access (own, public, shared) based on `user_id` and permission metadata.
    *   Test post-retrieval filtering correctly removes any chunks that might have slipped through pre-filtering.
    *   Test scenarios preventing cross-user data leakage via vector search.
    *   Verify persistence: Ensure vector metadata links (`user_id`) remain correct after auth events (password reset etc.). **(Persistence)**
*   **RBAC (`test_rbac.py`):**
    *   Test role creation, assignment, removal.
    *   Test permission checking logic based on roles (e.g., admin actions vs. viewer actions).
    *   Verify interaction with RLS (if roles influence RLS policies).
*   **Sharing (`test_sharing.py`):**
    *   Test document sharing API endpoints (grant, revoke, list collaborators).
    *   Verify notifications are triggered (mock notification service).
    *   Test access control based on sharing permissions.
*   **Multi-Tenancy (`test_multi_tenancy.py`):**
    *   Test organization creation, member management.
    *   Verify document association with organizations.
    *   Test RLS policies enforcing organization boundaries (users in Org A cannot see Org B's private documents).
    *   Test cross-organization sharing rules (if applicable).
*   **E2E Tests (`test_permission_scenarios.py`):**
    *   Scenario 1: User A uploads doc, User B cannot see it. User A shares with User B (read). User B can query, User B cannot update. User A revokes access. User B cannot query.
    *   Scenario 2: Admin user creates roles, assigns roles. Verify users with different roles have appropriate access levels to documents and system features.
    *   Scenario 3: User in Org A uploads doc. User in Org B cannot access it. Admin shares across orgs (if feature exists). Verify access.

**III. Security Hardening Verification (Phase 5.3)**

*   **Location:** Primarily manual review, configuration checks, potentially automated scans.
*   **Rate Limiting:**
    *   Verify configuration of rate limiting middleware (e.g., limits on `/login`, `/register`).
    *   Manually test exceeding limits to confirm blocking/throttling.
    *   Check for account lockout mechanism after repeated failures.
*   **Monitoring & Logging:**
    *   Review logging configuration.
    *   Manually trigger auth events (login success/fail, token refresh, permission denied) and verify logs capture relevant information.
    *   Check if alerting mechanisms are configured for suspicious activities (requires setup).
*   **Security Review:**
    *   Run dependency vulnerability scans (e.g., `safety check`, `pip-audit`).
    *   Review JWT configuration (`app/core/config.py`) for strong secrets, appropriate expiry, algorithm choice. (Note: Plan mentions audience verification disabled - flag this as a production risk).
    *   Review database security settings and RLS policies for potential bypasses.
    *   Review password hashing implementation (`app/core/security.py`).

================
File: docs/Metis_RAG_Database_Security_Implementation_Plan.md
================
# Metis RAG Database Security Implementation Plan

## Overview

This document outlines the high-level implementation plan for Phase 2 (Database Level Security) of the Metis RAG Authentication Implementation. This phase focuses on implementing database-level security mechanisms to ensure proper access control for documents and conversations.

## Current State Analysis

Based on the code analysis:
- The `users` table is properly implemented with all necessary fields
- The `documents` and `conversations` tables already have `user_id` foreign key columns
- JWT authentication (Phase 1) has been successfully implemented

## Implementation Strategy

```mermaid
flowchart TD
    A[Database Schema Updates] --> B[Row Level Security]
    B --> C[Repository Layer Updates]
    C --> D[API Integration]
    D --> E[Testing]
```

### 1. Database Schema Updates

- Create a `document_permissions` table for document sharing
  - Fields: document_id, user_id, permission_level, created_at
  - Indexes for efficient querying
  - Unique constraint on document_id + user_id

- Add `is_public` flag to documents table
  - Boolean field to indicate publicly accessible documents

### 2. Row Level Security (RLS) Implementation

- Enable RLS on document tables
  - Apply to `documents` and `chunks` tables

- Create ownership RLS policies
  - SELECT policy: Users can view their own documents or public documents
  - UPDATE/DELETE policies: Users can only modify their own documents

- Create document sharing RLS policies
  - SELECT policy: Users can view documents shared with them
  - UPDATE policy: Users with write permission can update shared documents

### 3. Repository Layer Updates

- Update document repository
  - Add user context to CRUD operations
  - Implement permission checking methods
  - Add methods for document sharing

- Update conversation repository
  - Add user context to conversation operations
  - Implement access control for conversations

### 4. API Integration

- Create middleware for setting user context
  - Set current user ID for RLS policies

- Update document endpoints
  - Add user context to document creation
  - Implement document sharing endpoints
  - Filter document lists by user access

- Update conversation endpoints
  - Add user context to conversation creation
  - Filter conversation lists by user access

### 5. Testing Strategy

- Unit tests for permission models
  - Test document ownership verification
  - Test document sharing between users

- Integration tests for RLS
  - Test access control across different users
  - Test public document access

- Performance testing
  - Measure impact of RLS on query performance

## Security Considerations

- Ensure proper indexing for RLS policies to maintain performance
- Implement comprehensive error handling for permission-related failures
- Add audit logging for security-related operations
- Consider the trade-offs between pre-filtering and post-filtering for vector database operations

## Next Steps

After completing Phase 2, proceed to Phase 3 (Vector Database Security) to extend the security model to the vector database components of the RAG system.

================
File: docs/Metis_RAG_PG_MCP_Integration_Plan.md
================
# Metis RAG PostgreSQL MCP Integration Plan

This document outlines a detailed implementation plan for enhancing Metis RAG with concepts from the pg-mcp project, focusing on creating a fully asynchronous database layer that leverages PostgreSQL capabilities while maintaining compatibility with SQLite for development and testing.

## Background

Metis RAG currently uses a mixed database architecture:
- Core database operations use SQLAlchemy's async features
- The DatabaseTool uses synchronous SQLite connections wrapped in async methods

This inconsistency creates potential performance bottlenecks as synchronous operations can block the event loop in an otherwise asynchronous application.

The pg-mcp project provides a Model Context Protocol server for PostgreSQL with:
- Fully asynchronous database operations using asyncpg
- Secure connection management with connection IDs
- Rich schema introspection capabilities
- Support for PostgreSQL extensions like pgvector

## Implementation Phases

### Phase 1: True Async DatabaseTool Implementation

This phase focuses on eliminating the synchronous bottleneck in the DatabaseTool by implementing true async database access for both SQLite and PostgreSQL.

#### 1.1 Create Async Database Connection Manager

- [x] Create a new `app/db/connection_manager.py` module
- [x] Implement a `DatabaseConnectionManager` class with:
  - [x] Connection pooling for both SQLite and PostgreSQL
  - [x] Secure connection ID generation (similar to pg-mcp)
  - [x] Methods to register and retrieve connections
  - [x] Connection lifecycle management (acquire, release, close)
- [x] Add configuration options for connection pools
- [x] Implement proper error handling and logging

#### 1.2 Refactor DatabaseTool for True Async Operation

- [x] Create a new `app/rag/tools/database_tool_async.py` file
- [x] Replace synchronous sqlite3 with aiosqlite for SQLite operations
- [x] Add asyncpg support for PostgreSQL operations
- [x] Implement database type detection (SQLite vs PostgreSQL)
- [x] Update query execution methods to use appropriate async driver
- [x] Maintain backward compatibility with existing tool usage patterns
- [x] Add proper error handling and performance logging

#### 1.3 Update CSV and JSON Data Source Handling

- [x] Refactor CSV file handling to use aiosqlite for in-memory database
- [x] Refactor JSON file handling to use aiosqlite for in-memory database
- [x] Implement async file reading for CSV and JSON sources
- [x] Optimize data loading for large files

#### 1.4 Add Unit Tests for Async Database Operations

- [x] Create test fixtures for both SQLite and PostgreSQL connections
- [x] Test connection pooling and lifecycle management
- [x] Test query execution with various parameter types
- [x] Test error handling and recovery
- [x] Test compatibility with existing code
- [x] Test performance under concurrent load

#### 1.5 Update Dependencies and Documentation

- [x] Add aiosqlite and asyncpg to requirements.txt
- [x] Add aiofiles to requirements.txt
- [x] Create documentation with new async database capabilities
- [x] Document connection string formats for different database types
- [x] Create examples of using the enhanced DatabaseTool
- [x] Create migration script to help users transition to the new implementation

### Phase 1 Status

- [x] Core functionality implemented and tested
- [x] SQLite database operations working correctly
- [x] CSV and JSON handling fixed and working correctly with async operations
- [x] All changes committed to feature branch and backed up
- [x] Documentation and migration tools created

### Phase 2: PostgreSQL-specific Capabilities

This phase adds PostgreSQL-specific features to Metis RAG, leveraging the async foundation built in Phase 1.

#### 2.1 Implement Schema Introspection

- [x] Create a new `app/db/schema_inspector.py` module
- [x] Implement methods to retrieve:
  - [x] Database schemas
  - [x] Tables with descriptions and row counts
  - [x] Columns with data types and descriptions
  - [x] Indexes and constraints
- [x] Add caching for schema information to improve performance
- [x] Create API endpoints to expose schema information

#### 2.2 Add Query Explanation Capabilities

- [x] Implement EXPLAIN query execution
- [x] Create visualization helpers for execution plans
- [x] Add query optimization suggestions based on execution plans
- [x] Integrate with existing query analysis tools

#### 2.3 Support PostgreSQL Extensions

- [x] Add support for pgvector extension
  - [x] Implement vector similarity search methods
  - [x] Optimize for RAG vector embeddings
- [ ] Add support for PostGIS (if geospatial data is relevant)
- [x] Create extension detection and configuration helpers
- [x] Document extension usage and best practices
#### 2.4 Create a Dedicated PostgreSQLTool

- [x] Implement a new `PostgreSQLTool` class extending the base `Tool`
- [x] Expose PostgreSQL-specific capabilities
- [x] Add methods for advanced query operations
- [x] Implement proper error handling and logging
- [x] Create comprehensive documentation and examples
- [ ] Create comprehensive documentation and examples
#### 2.5 Update Testing Infrastructure

- [x] Create PostgreSQL-specific test fixtures
- [x] Test schema introspection with various database structures
- [x] Test query explanation with complex queries
- [x] Test extension functionality
- [x] Benchmark performance against baseline
- [ ] Benchmark performance against baseline

### Phase 3: MCP Server Interface (Optional)

This optional phase creates an MCP server that exposes Metis RAG's database through the Model Context Protocol.

#### 3.1 Create MCP Server Foundation

- [ ] Set up MCP server structure
- [ ] Implement server configuration
- [ ] Create connection management
- [ ] Implement security controls
- [ ] Set up logging and monitoring

#### 3.2 Implement MCP Tools

- [ ] Create connect/disconnect tools
- [ ] Implement query execution tool
- [ ] Add query explanation tool
- [ ] Create schema introspection tools
- [ ] Implement proper error handling

#### 3.3 Implement MCP Resources

- [ ] Create schema resources
- [ ] Implement table and column resources
- [ ] Add index and constraint resources
- [ ] Create data sample resources
- [ ] Implement extension resources

#### 3.4 Add AI Agent Integration

- [ ] Create example prompts for AI agents
- [ ] Implement context providers for database schema
- [ ] Add documentation for AI agent integration
- [ ] Create demo applications

#### 3.5 Testing and Documentation

- [ ] Create comprehensive tests for MCP server
- [ ] Document server setup and configuration
- [ ] Create usage examples
- [ ] Benchmark performance

## Implementation Considerations

### Database Abstraction

- Maintain a clear abstraction layer between database-specific code and generic code
- Use factory patterns to create appropriate database handlers based on connection type
- Keep SQLite support for development and testing environments
- Document differences between SQLite and PostgreSQL behavior

### Connection Management

- Implement secure connection ID generation to avoid exposing credentials
- Use connection pooling for both SQLite and PostgreSQL
- Implement proper connection lifecycle management
- Add monitoring for connection usage and performance

### Error Handling

- Create specific error types for different database issues
- Implement proper error logging and reporting
- Add retry mechanisms for transient errors
- Ensure errors are propagated appropriately to callers

### Performance Optimization

- Implement connection pooling with appropriate pool sizes
- Add query result caching where appropriate
- Use prepared statements for frequently executed queries
- Implement proper transaction management
- Add performance monitoring and logging

### Migration Path

- Provide tools to migrate from SQLite to PostgreSQL
- Document migration process and potential issues
- Ensure backward compatibility with existing data
- Create validation tools to verify data integrity after migration

## Success Metrics

- Elimination of synchronous database operations in async context
- Improved application responsiveness under load
- Reduced query execution time for complex operations
- Successful integration with PostgreSQL-specific features
- Comprehensive test coverage for all database operations
- Clear documentation for developers and users

## Timeline

- Phase 1: 2-3 weeks
- Phase 2: 3-4 weeks
- Phase 3: 2-3 weeks (if implemented)

Total estimated time: 7-10 weeks

================
File: docs/performance_optimization_guide.md
================
# Metis RAG Performance Testing and Optimization Guide

This guide explains how to use the performance testing and optimization scripts for the Metis RAG system.

## Overview

The Metis RAG system supports both SQLite and PostgreSQL databases. These scripts help you:

1. Benchmark database performance
2. Optimize chunking strategies
3. Implement database-specific enhancements
4. Update Docker configuration for PostgreSQL support
5. Migrate data between databases

## 1. Database Performance Benchmarking

The `benchmark_database_performance.py` script benchmarks SQLite vs PostgreSQL performance for various operations:

```bash
# Run benchmark for SQLite
python scripts/benchmark_database_performance.py --db-type sqlite

# Run benchmark for PostgreSQL
python scripts/benchmark_database_performance.py --db-type postgresql

# Generate HTML comparison report
python scripts/benchmark_database_performance.py --report-only \
  --sqlite-results tests/results/db_benchmark_sqlite_YYYYMMDD_HHMMSS.json \
  --postgres-results tests/results/db_benchmark_postgresql_YYYYMMDD_HHMMSS.json
```

This script tests:
- Document operations (create, retrieve, update, delete)
- Chunk operations (insert, retrieve, update)
- Query performance
- Batch processing

The HTML report provides a detailed comparison between SQLite and PostgreSQL, including:
- Performance metrics for each operation
- Charts comparing the two databases
- Recommendations based on the benchmark results

## 2. Chunking Strategy Optimization

The `optimize_chunking_strategy.py` script tests different chunking strategies and parameters to find the optimal configuration for each database backend:

```bash
# Optimize chunking strategies for SQLite
python scripts/optimize_chunking_strategy.py --db-type sqlite

# Optimize chunking strategies for PostgreSQL
python scripts/optimize_chunking_strategy.py --db-type postgresql
```

This script tests combinations of:
- Chunking strategies (recursive, token, markdown, semantic)
- Chunk sizes (500, 1000, 2000, 4000 characters)
- Chunk overlaps (50, 100, 200, 400 characters)
- File sizes (small, medium, large, xlarge)
- File types (txt, md)

The output includes recommendations for the optimal chunking strategy for each file type and size.

## 3. Database-Specific Enhancements

The `implement_database_enhancements.py` script implements database-specific enhancements for PostgreSQL and SQLite:

```bash
# Check enhancements for SQLite (dry run)
python scripts/implement_database_enhancements.py --db-type sqlite

# Apply enhancements for SQLite
python scripts/implement_database_enhancements.py --db-type sqlite --apply

# Check enhancements for PostgreSQL (dry run)
python scripts/implement_database_enhancements.py --db-type postgresql

# Apply enhancements for PostgreSQL
python scripts/implement_database_enhancements.py --db-type postgresql --apply
```

### PostgreSQL Enhancements

- GIN indexes on JSONB metadata fields
- Full-text search indexes on document and chunk content
- Semantic search function using vector embeddings
- Connection pooling optimization
- Materialized views for frequently accessed data

### SQLite Enhancements

- WAL mode for better concurrency
- Busy timeout for concurrent access
- Foreign key constraints
- Optimized synchronous mode
- Indexes for text search and JSON queries

## 4. Docker Configuration Update

The `update_docker_config.py` script updates the Docker configuration to support both SQLite and PostgreSQL:

```bash
# Check updates (dry run)
python scripts/update_docker_config.py

# Apply updates
python scripts/update_docker_config.py --apply
```

This script:
- Updates docker-compose.yml to include PostgreSQL service
- Updates Dockerfile to install PostgreSQL client libraries
- Creates database initialization scripts
- Creates environment-based configuration switching
- Creates deployment scripts for both SQLite and PostgreSQL
- Creates deployment documentation

After running this script, you can start the system with either SQLite or PostgreSQL:

```bash
# Start with SQLite
cd config
./start-sqlite.sh

# Start with PostgreSQL
cd config
./start-postgresql.sh
```

## 5. Database Migration

The `migrate_database.py` script migrates data between SQLite and PostgreSQL databases:

```bash
# Migrate from SQLite to PostgreSQL
python scripts/migrate_database.py --source sqlite --target postgresql

# Migrate from PostgreSQL to SQLite
python scripts/migrate_database.py --source postgresql --target sqlite

# Specify custom export file
python scripts/migrate_database.py --source sqlite --target postgresql --export-file data/my_export.json
```

This script:
1. Exports data from the source database to JSON
2. Imports data to the target database from JSON
3. Verifies data integrity after migration

## Performance Optimization Workflow

For optimal performance, follow this workflow:

1. **Benchmark both databases**:
   ```bash
   python scripts/benchmark_database_performance.py --db-type sqlite
   python scripts/benchmark_database_performance.py --db-type postgresql
   ```

2. **Generate comparison report**:
   ```bash
   python scripts/benchmark_database_performance.py --report-only \
     --sqlite-results tests/results/db_benchmark_sqlite_YYYYMMDD_HHMMSS.json \
     --postgres-results tests/results/db_benchmark_postgresql_YYYYMMDD_HHMMSS.json
   ```

3. **Optimize chunking strategies**:
   ```bash
   python scripts/optimize_chunking_strategy.py --db-type sqlite
   python scripts/optimize_chunking_strategy.py --db-type postgresql
   ```

4. **Implement database enhancements**:
   ```bash
   python scripts/implement_database_enhancements.py --db-type sqlite --apply
   python scripts/implement_database_enhancements.py --db-type postgresql --apply
   ```

5. **Update Docker configuration**:
   ```bash
   python scripts/update_docker_config.py --apply
   ```

6. **Migrate data if needed**:
   ```bash
   python scripts/migrate_database.py --source sqlite --target postgresql
   ```

## Performance Monitoring

After implementing optimizations, monitor system performance:

1. **Database Query Performance**:
   - Use the `EXPLAIN ANALYZE` SQL command to analyze query execution plans
   - Monitor slow queries in database logs

2. **API Response Times**:
   - Use the `/api/system/metrics` endpoint to view API performance metrics
   - Monitor response times for different endpoints

3. **Resource Usage**:
   - Monitor CPU, memory, and disk usage
   - Watch for database connection pool saturation

4. **Scaling Considerations**:
   - For larger deployments, consider using PostgreSQL with connection pooling
   - For smaller deployments, SQLite may be sufficient

## Conclusion

By using these performance testing and optimization scripts, you can:
- Make informed decisions about which database to use
- Optimize chunking strategies for your specific use case
- Implement database-specific enhancements for better performance
- Configure Docker for production deployment
- Migrate data between databases as needed

These optimizations will help ensure that your Metis RAG system performs efficiently at scale.

================
File: docs/performance_optimization.md
================
# Metis RAG Performance Testing and Optimization

This document provides an overview of the performance testing and optimization tools available for the Metis RAG system, focusing on database performance, chunking strategies, and deployment configurations.

## Table of Contents

1. [Database Performance Testing](#database-performance-testing)
2. [Chunking Strategy Optimization](#chunking-strategy-optimization)
3. [Database-Specific Enhancements](#database-specific-enhancements)
4. [Deployment Configuration](#deployment-configuration)
5. [Running the Scripts](#running-the-scripts)

## Database Performance Testing

The `benchmark_database_performance.py` script provides comprehensive benchmarking of SQLite vs PostgreSQL performance for various operations:

- Document CRUD operations
- Chunk storage and retrieval
- Query performance
- Batch processing

### Usage

```bash
# Benchmark SQLite performance
python scripts/benchmark_database_performance.py --db-type sqlite

# Benchmark PostgreSQL performance
python scripts/benchmark_database_performance.py --db-type postgresql

# Generate HTML comparison report (after running both benchmarks)
python scripts/benchmark_database_performance.py --db-type sqlite --html --postgres-results path/to/postgres/results.json
```

### Key Metrics

The benchmark measures the following metrics:

- **Document Operations**: Create, read, update, and delete performance for documents of different sizes
- **Chunk Operations**: Insertion, retrieval, and update performance for chunks with different chunking strategies
- **Query Performance**: Response time for different types of queries
- **Batch Processing**: Processing time for batches of documents of different sizes

### Output

The script generates a detailed JSON report with all benchmark results and can also generate an HTML report with visualizations comparing SQLite and PostgreSQL performance.

## Chunking Strategy Optimization

The `optimize_chunking_strategy.py` script tests different chunking strategies and parameters to find the optimal configuration for each database backend:

- Compares different chunking strategies (recursive, token, markdown, semantic)
- Optimizes chunk size parameters
- Optimizes chunk overlap parameters

### Usage

```bash
# Optimize chunking strategies for SQLite
python scripts/optimize_chunking_strategy.py --db-type sqlite

# Optimize chunking strategies for PostgreSQL
python scripts/optimize_chunking_strategy.py --db-type postgresql

# Generate HTML report
python scripts/optimize_chunking_strategy.py --db-type sqlite --html
```

### Key Metrics

The optimization process measures:

- Processing time for different chunking strategies
- Storage and retrieval performance for different chunk sizes
- Query performance with different chunk configurations

### Output

The script generates recommendations for optimal chunking strategies, chunk sizes, and chunk overlaps for different file types and sizes, along with an HTML report visualizing the results.

## Database-Specific Enhancements

### PostgreSQL Enhancements

The `implement_postgres_enhancements.py` script implements PostgreSQL-specific optimizations:

- JSONB operators for metadata queries
- Full-text search capabilities
- Connection pooling configuration
- Index optimization

```bash
# View planned enhancements
python scripts/implement_postgres_enhancements.py

# Apply enhancements
python scripts/implement_postgres_enhancements.py --apply
```

### SQLite Optimizations

The `optimize_sqlite_for_concurrency.py` script implements SQLite-specific optimizations:

- WAL (Write-Ahead Logging) mode for better concurrency
- PRAGMA settings for performance
- Index optimization
- Connection pooling configuration

```bash
# View planned optimizations
python scripts/optimize_sqlite_for_concurrency.py

# Apply optimizations
python scripts/optimize_sqlite_for_concurrency.py --apply
```

## Deployment Configuration

The `update_docker_for_postgres.py` script updates the Docker configuration for PostgreSQL support:

- Updates docker-compose.yml to include a PostgreSQL service
- Updates Dockerfile to install PostgreSQL client libraries
- Creates database initialization scripts
- Updates environment configuration

```bash
# View planned changes
python scripts/update_docker_for_postgres.py

# Apply changes
python scripts/update_docker_for_postgres.py --apply
```

## Running the Scripts

### Prerequisites

- Python 3.10 or higher
- SQLite 3.9 or higher (for JSON support)
- PostgreSQL 12 or higher (for JSONB and full-text search)
- Docker and Docker Compose (for deployment configuration)

### Environment Setup

1. Install the required dependencies:

```bash
pip install -r requirements.txt
```

2. Configure the database connection:

For SQLite:
```
DATABASE_TYPE=sqlite
DATABASE_URL=sqlite:///./test.db
```

For PostgreSQL:
```
DATABASE_TYPE=postgresql
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=metis_rag
```

### Performance Testing Workflow

For a comprehensive performance evaluation, follow these steps:

1. Benchmark both database backends:
```bash
python scripts/benchmark_database_performance.py --db-type sqlite
python scripts/benchmark_database_performance.py --db-type postgresql --html --sqlite-results path/to/sqlite/results.json
```

2. Optimize chunking strategies for your chosen database:
```bash
python scripts/optimize_chunking_strategy.py --db-type postgresql --html
```

3. Apply database-specific enhancements:
```bash
# For PostgreSQL
python scripts/implement_postgres_enhancements.py --apply

# For SQLite
python scripts/optimize_sqlite_for_concurrency.py --apply
```

4. Update deployment configuration:
```bash
python scripts/update_docker_for_postgres.py --apply
```

### Interpreting Results

The HTML reports provide detailed visualizations and comparisons of performance metrics. Key considerations when interpreting results:

- **Document Size Impact**: Larger documents may perform differently than smaller ones
- **Chunking Strategy Tradeoffs**: Different strategies balance processing speed vs. retrieval quality
- **Concurrency Requirements**: PostgreSQL generally handles concurrent access better than SQLite
- **Resource Utilization**: Monitor CPU, memory, and disk usage during benchmarks

## Best Practices

### SQLite Optimization

- Use WAL mode for better concurrency
- Increase cache size for better performance
- Use appropriate indexes for common queries
- Consider connection pooling for web applications

### PostgreSQL Optimization

- Use JSONB operators for efficient metadata queries
- Implement full-text search for content queries
- Configure connection pooling for high-traffic applications
- Use appropriate indexes for common query patterns

### Chunking Optimization

- Use different chunking strategies for different file types
- Adjust chunk size based on content complexity
- Balance chunk overlap for context preservation vs. storage efficiency
- Consider database performance when choosing chunking parameters

## Conclusion

Performance optimization is an ongoing process. Regularly benchmark your system as your data and usage patterns evolve, and adjust your configuration accordingly.

For production deployments, consider:

- Regular database maintenance (VACUUM, ANALYZE)
- Monitoring database performance
- Scaling resources based on usage patterns
- Implementing caching for frequently accessed data

================
File: docs/prompt_manager_refactor.md
================
# Prompt Manager Refactor

## Overview

This document describes the refactoring of the prompt architecture in Metis RAG to address issues with hallucination, citation misuse, and inconsistent handling of missing information.

## Problem Statement

The original prompt architecture had several issues:

1. **Fragmented Prompt Architecture**: The system used multiple separate prompts (system prompt, conversation templates) with overlapping and sometimes contradictory instructions.

2. **Direct Message Injection**: When no documents were found, the system injected "Note: No documents..." messages directly into the context, which could be confused with actual document content.

3. **Inconsistent Error Handling**: Different error conditions resulted in different messages being injected into the context.

4. **Redundant Instructions**: The same instructions for handling missing information appeared in multiple places.

## Solution: Single Source of Truth

The refactoring introduces a `PromptManager` class that serves as a single source of truth for all prompt-related operations. Key components:

1. **PromptManager**: A new class responsible for all prompt-related operations
   - Maintains templates and instructions
   - Creates appropriate prompts based on the current state
   - Handles all scenarios (with/without documents, with/without conversation history)

2. **State-Based Prompt Selection**: Instead of injecting messages into the context, we use explicit state to select the appropriate prompt
   - Track whether documents were found
   - Track whether documents met relevance thresholds
   - Select the appropriate template based on this state

3. **Clear Separation of Data and Instructions**:
   - Keep retrieved documents as pure data without injected messages
   - Keep conversation history as pure data
   - Apply instructions through templates, not by modifying the data

## Implementation Details

### 1. PromptManager Class

The `PromptManager` class in `app/rag/prompt_manager.py` provides:

- Template loading and management
- State-based prompt selection
- Consistent instructions across different scenarios

### 2. Modified RAG Engine

The RAG Engine has been updated to:

- Use the `PromptManager` for prompt creation
- Track retrieval state explicitly instead of injecting messages
- Pass the retrieval state to the prompt creation functions

### 3. Modified Retrieval Process

The retrieval process has been updated to:

- Return empty context instead of injecting messages
- Let the `PromptManager` handle the appropriate response based on retrieval state

## Benefits

1. **Consistency**: All instructions come from a single source of truth
2. **Maintainability**: Changes to prompting logic only need to be made in one place
3. **Clarity**: Clear separation between data and instructions
4. **Flexibility**: Easy to add new scenarios or modify existing ones
5. **Reliability**: Reduced chance of hallucination and citation misuse

## Testing

Unit tests have been added in `tests/unit/test_prompt_manager.py` to verify the functionality of the `PromptManager` class.

## Future Improvements

1. **Template Externalization**: Move templates to external files for easier editing
2. **Dynamic Template Loading**: Support loading templates at runtime
3. **A/B Testing**: Add support for testing different prompt variations

================
File: scripts/demo/demo_cache.py
================
#!/usr/bin/env python
"""
Demo script for the Metis_RAG caching system.

This script demonstrates the usage of the caching system by:
1. Creating cache instances
2. Setting and getting values
3. Measuring performance improvements
4. Showing cache statistics
"""

import os
import sys
import time
import json
import random
import shutil
from typing import List, Dict, Any

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from app.cache import (
    Cache,
    VectorSearchCache,
    DocumentCache,
    LLMResponseCache,
    CacheManager
)

def clear_test_cache_dir():
    """Clear the test cache directory"""
    test_cache_dir = "data/demo_cache"
    if os.path.exists(test_cache_dir):
        shutil.rmtree(test_cache_dir)
    os.makedirs(test_cache_dir, exist_ok=True)
    return test_cache_dir

def demo_basic_cache():
    """Demonstrate basic cache usage"""
    print("\n=== Basic Cache Demo ===")
    
    # Create a cache instance
    cache_dir = clear_test_cache_dir()
    cache = Cache[str](
        name="demo_cache",
        ttl=60,  # 60 seconds TTL
        max_size=100,
        persist=True,
        persist_dir=cache_dir
    )
    
    # Set some values
    print("Setting values...")
    cache.set("key1", "value1")
    cache.set("key2", "value2")
    cache.set("key3", "value3")
    
    # Get values
    print(f"key1: {cache.get('key1')}")
    print(f"key2: {cache.get('key2')}")
    print(f"key3: {cache.get('key3')}")
    print(f"non-existent key: {cache.get('non_existent')}")
    
    # Delete a value
    print("\nDeleting key2...")
    cache.delete("key2")
    print(f"key2 after deletion: {cache.get('key2')}")
    
    # Get cache statistics
    print("\nCache statistics:")
    stats = cache.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # Clear the cache
    print("\nClearing cache...")
    cache.clear()
    print(f"key1 after clearing: {cache.get('key1')}")
    print(f"key3 after clearing: {cache.get('key3')}")

def demo_vector_search_cache():
    """Demonstrate vector search cache usage"""
    print("\n=== Vector Search Cache Demo ===")
    
    # Create a vector search cache
    cache_dir = clear_test_cache_dir()
    vector_cache = VectorSearchCache(
        ttl=60,
        max_size=100,
        persist=True,
        persist_dir=cache_dir
    )
    
    # Create some test results
    results = [
        {
            "chunk_id": f"chunk{i}",
            "content": f"Test content {i}",
            "metadata": {"document_id": f"doc{i % 3}"},
            "distance": i / 10
        }
        for i in range(1, 6)
    ]
    
    # Set results
    print("Setting search results...")
    vector_cache.set_results("test query", 5, results)
    
    # Get results
    print("\nGetting search results...")
    cached_results = vector_cache.get_results("test query", 5)
    print(f"Found {len(cached_results)} results")
    for i, result in enumerate(cached_results):
        print(f"  Result {i+1}: {result['chunk_id']} (distance: {result['distance']})")
    
    # Invalidate by document ID
    print("\nInvalidating results for doc1...")
    count = vector_cache.invalidate_by_document_id("doc1")
    print(f"Invalidated {count} results")
    
    # Try to get results again
    print("\nGetting search results after invalidation...")
    cached_results = vector_cache.get_results("test query", 5)
    if cached_results is None:
        print("  No results found (cache invalidated)")
    else:
        print(f"  Found {len(cached_results)} results")

def demo_document_cache():
    """Demonstrate document cache usage"""
    print("\n=== Document Cache Demo ===")
    
    # Create a document cache
    cache_dir = clear_test_cache_dir()
    document_cache = DocumentCache(
        ttl=60,
        max_size=100,
        persist=True,
        persist_dir=cache_dir
    )
    
    # Create a test document
    document = {
        "id": "doc1",
        "content": "This is a test document content.",
        "metadata": {
            "filename": "test.txt",
            "tags": ["test", "document", "example"],
            "folder": "/test"
        }
    }
    
    # Set document
    print("Setting document...")
    document_cache.set_document("doc1", document)
    
    # Get document
    print("\nGetting document...")
    cached_document = document_cache.get_document("doc1")
    if cached_document:
        print(f"  Document ID: {cached_document['id']}")
        print(f"  Content: {cached_document['content']}")
        print(f"  Metadata: {cached_document['metadata']}")
    
    # Get document content
    print("\nGetting document content...")
    content = document_cache.get_document_content("doc1")
    print(f"  Content: {content}")
    
    # Get document metadata
    print("\nGetting document metadata...")
    metadata = document_cache.get_document_metadata("doc1")
    print(f"  Metadata: {metadata}")
    
    # Invalidate document
    print("\nInvalidating document...")
    document_cache.invalidate_document("doc1")
    
    # Try to get document again
    print("\nGetting document after invalidation...")
    cached_document = document_cache.get_document("doc1")
    if cached_document is None:
        print("  Document not found (cache invalidated)")
    else:
        print(f"  Document found: {cached_document}")

def demo_llm_response_cache():
    """Demonstrate LLM response cache usage"""
    print("\n=== LLM Response Cache Demo ===")
    
    # Create an LLM response cache
    cache_dir = clear_test_cache_dir()
    llm_cache = LLMResponseCache(
        ttl=60,
        max_size=100,
        persist=True,
        persist_dir=cache_dir
    )
    
    # Create a test response
    response = {
        "response": "This is a test response from the LLM.",
        "model": "test-model",
        "prompt": "What is RAG?",
        "tokens": 15,
        "finish_reason": "stop"
    }
    
    # Set response
    print("Setting LLM response...")
    llm_cache.set_response(
        prompt="What is RAG?",
        model="test-model",
        response=response,
        temperature=0.0
    )
    
    # Get response
    print("\nGetting LLM response...")
    cached_response = llm_cache.get_response(
        prompt="What is RAG?",
        model="test-model",
        temperature=0.0
    )
    if cached_response:
        print(f"  Response: {cached_response['response']}")
        print(f"  Model: {cached_response['model']}")
        print(f"  Tokens: {cached_response.get('tokens')}")
    
    # Try with different temperature
    print("\nGetting LLM response with different temperature...")
    cached_response = llm_cache.get_response(
        prompt="What is RAG?",
        model="test-model",
        temperature=0.5
    )
    if cached_response is None:
        print("  Response not found (different parameters)")
    else:
        print(f"  Response found: {cached_response}")
    
    # Check if response should be cached
    print("\nChecking if response should be cached...")
    should_cache = llm_cache.should_cache_response(
        prompt="What is RAG?",
        model="test-model",
        temperature=0.2,
        response=response
    )
    print(f"  Should cache: {should_cache}")
    
    should_cache = llm_cache.should_cache_response(
        prompt="What is RAG?",
        model="test-model",
        temperature=0.7,  # High temperature
        response=response
    )
    print(f"  Should cache (high temperature): {should_cache}")

def demo_cache_manager():
    """Demonstrate cache manager usage"""
    print("\n=== Cache Manager Demo ===")
    
    # Create a cache manager
    cache_dir = clear_test_cache_dir()
    cache_manager = CacheManager(
        cache_dir=cache_dir,
        enable_caching=True
    )
    
    # Set some values in each cache
    print("Setting values in caches...")
    cache_manager.vector_search_cache.set_results(
        "test query", 
        5, 
        [{"chunk_id": "chunk1", "metadata": {"document_id": "doc1"}}]
    )
    
    cache_manager.document_cache.set_document(
        "doc1", 
        {"id": "doc1", "content": "Test content"}
    )
    
    cache_manager.llm_response_cache.set_response(
        "What is RAG?",
        "test-model",
        {"response": "RAG stands for Retrieval-Augmented Generation"}
    )
    
    # Get cache statistics
    print("\nGetting cache statistics...")
    stats = cache_manager.get_all_cache_stats()
    print(json.dumps(stats, indent=2))
    
    # Invalidate a document
    print("\nInvalidating document doc1...")
    cache_manager.invalidate_document("doc1")
    
    # Check that the document is invalidated in all caches
    print("\nChecking caches after invalidation...")
    vector_results = cache_manager.vector_search_cache.get_results("test query", 5)
    document = cache_manager.document_cache.get_document("doc1")
    
    print(f"  Vector results: {vector_results}")
    print(f"  Document: {document}")
    
    # Clear all caches
    print("\nClearing all caches...")
    cache_manager.clear_all_caches()
    
    # Create a cache manager with caching disabled
    print("\nCreating cache manager with caching disabled...")
    disabled_manager = CacheManager(
        cache_dir=cache_dir,
        enable_caching=False
    )
    
    # Try to set and get values
    print("Setting and getting values with caching disabled...")
    disabled_manager.vector_search_cache.set_results(
        "test query", 
        5, 
        [{"chunk_id": "chunk1"}]
    )
    
    result = disabled_manager.vector_search_cache.get_results("test query", 5)
    print(f"  Result: {result}")
    
    # Get statistics
    print("\nGetting statistics with caching disabled...")
    stats = disabled_manager.get_all_cache_stats()
    print(json.dumps(stats, indent=2))

def demo_performance():
    """Demonstrate performance improvements with caching"""
    print("\n=== Performance Demo ===")
    
    # Create a cache
    cache_dir = clear_test_cache_dir()
    cache = Cache[str](
        name="perf_cache",
        ttl=60,
        max_size=1000,
        persist=True,
        persist_dir=cache_dir
    )
    
    # Function to simulate an expensive operation
    def expensive_operation(key):
        """Simulate an expensive operation"""
        time.sleep(0.1)  # Simulate 100ms of work
        return f"Result for {key}"
    
    # Function to get a value with caching
    def get_with_cache(key):
        """Get a value with caching"""
        # Check cache first
        result = cache.get(key)
        if result is not None:
            return result
        
        # Cache miss, perform expensive operation
        result = expensive_operation(key)
        cache.set(key, result)
        return result
    
    # Measure performance without caching
    print("Measuring performance without caching...")
    keys = [f"key{i}" for i in range(10)]
    
    start_time = time.time()
    for key in keys:
        expensive_operation(key)
    uncached_time = time.time() - start_time
    
    print(f"  Time without caching: {uncached_time:.3f} seconds")
    
    # Measure performance with caching (first run - cache misses)
    print("\nMeasuring performance with caching (first run)...")
    start_time = time.time()
    for key in keys:
        get_with_cache(key)
    first_cached_time = time.time() - start_time
    
    print(f"  Time with caching (first run): {first_cached_time:.3f} seconds")
    
    # Measure performance with caching (second run - cache hits)
    print("\nMeasuring performance with caching (second run)...")
    start_time = time.time()
    for key in keys:
        get_with_cache(key)
    second_cached_time = time.time() - start_time
    
    print(f"  Time with caching (second run): {second_cached_time:.3f} seconds")
    
    # Calculate speedup
    speedup = uncached_time / second_cached_time
    print(f"\nSpeedup with caching: {speedup:.1f}x")
    
    # Get cache statistics
    stats = cache.get_stats()
    print(f"\nCache statistics after performance test:")
    print(f"  Hits: {stats['hits']}")
    print(f"  Misses: {stats['misses']}")
    print(f"  Hit ratio: {stats['hit_ratio']:.2f}")

if __name__ == "__main__":
    print("=== Metis_RAG Caching System Demo ===")
    
    # Run demos
    demo_basic_cache()
    demo_vector_search_cache()
    demo_document_cache()
    demo_llm_response_cache()
    demo_cache_manager()
    demo_performance()
    
    print("\nDemo completed successfully!")

================
File: scripts/demo/demo_presentation.py
================
#!/usr/bin/env python3
"""
Metis RAG Demo Presentation Script

This script simulates a basic interaction with the Metis RAG system for demonstration purposes.
It shows the key steps in the RAG process, from query processing to response generation.
"""

import time
import json
import random
from datetime import datetime

# ANSI color codes for terminal output
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def print_header(text):
    """Print a formatted header."""
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'=' * 80}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{text.center(80)}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{'=' * 80}{Colors.ENDC}\n")

def print_step(step_num, step_name):
    """Print a formatted step header."""
    print(f"\n{Colors.BLUE}{Colors.BOLD}[Step {step_num}] {step_name}{Colors.ENDC}")
    print(f"{Colors.BLUE}{'-' * 50}{Colors.ENDC}\n")

def simulate_typing(text, delay=0.03):
    """Simulate typing by printing characters with a delay."""
    for char in text:
        print(char, end='', flush=True)
        time.sleep(delay)
    print()

def progress_bar(duration, description):
    """Display a progress bar for the given duration."""
    steps = 40
    for i in range(steps + 1):
        progress = i / steps
        bar = '█' * i + '░' * (steps - i)
        percentage = int(progress * 100)
        print(f"\r{Colors.CYAN}{description}: [{bar}] {percentage}%{Colors.ENDC}", end='')
        time.sleep(duration / steps)
    print()

def simulate_document_processing():
    """Simulate document processing."""
    print_step(1, "Document Processing")
    
    documents = [
        {"name": "quarterly_report.txt", "size": "245 KB", "pages": 12},
        {"name": "technical_documentation.md", "size": "189 KB", "pages": 8},
        {"name": "product_specifications.csv", "size": "78 KB", "rows": 156}
    ]
    
    print(f"{Colors.YELLOW}Available Documents:{Colors.ENDC}")
    for i, doc in enumerate(documents, 1):
        pages = doc.get('pages', doc.get('rows', 'N/A'))
        print(f"{i}. {doc['name']} ({doc['size']}, {pages} pages/rows)")
    
    print(f"\n{Colors.GREEN}Processing documents...{Colors.ENDC}")
    
    for doc in documents:
        print(f"\n{Colors.CYAN}Processing {doc['name']}...{Colors.ENDC}")
        
        # Determine chunking strategy based on file extension
        if doc['name'].endswith('.md'):
            strategy = "Markdown Header Chunking"
        elif doc['name'].endswith('.csv'):
            strategy = "Token-based Chunking"
        else:
            strategy = "Recursive Chunking"
        
        print(f"  Using {Colors.YELLOW}{strategy}{Colors.ENDC}")
        progress_bar(2, f"Extracting text from {doc['name']}")
        
        # Show chunking process
        chunk_count = random.randint(5, 20)
        print(f"  Created {Colors.GREEN}{chunk_count} chunks{Colors.ENDC}")
        progress_bar(1.5, "Generating embeddings")
        
        # Show vector storage
        print(f"  Stored in vector database with {Colors.GREEN}metadata{Colors.ENDC}")
    
    print(f"\n{Colors.GREEN}All documents processed successfully!{Colors.ENDC}")

def simulate_query_processing():
    """Simulate query processing."""
    print_step(2, "Query Processing")
    
    # User query
    query = "What are the key performance metrics for the new product line?"
    print(f"{Colors.YELLOW}User Query:{Colors.ENDC}")
    simulate_typing(f"{Colors.BOLD}{query}{Colors.ENDC}", 0.05)
    
    print(f"\n{Colors.CYAN}Processing query...{Colors.ENDC}")
    progress_bar(1, "Generating query embedding")
    
    # Vector search
    print(f"\n{Colors.CYAN}Searching vector database...{Colors.ENDC}")
    progress_bar(2, "Performing semantic search")
    
    # Retrieved chunks
    print(f"\n{Colors.GREEN}Retrieved relevant chunks:{Colors.ENDC}")
    chunks = [
        {"text": "The new product line demonstrated a 27% increase in efficiency metrics compared to previous generation...", "source": "quarterly_report.txt", "similarity": 0.89},
        {"text": "Key performance indicators include: processing speed (45 units/min), energy efficiency (0.8 kWh), and reliability score (98.7%)...", "source": "product_specifications.csv", "similarity": 0.87},
        {"text": "Performance testing revealed consistent results across all operational environments with metrics exceeding target thresholds...", "source": "technical_documentation.md", "similarity": 0.76}
    ]
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\n{Colors.YELLOW}Chunk {i} (Similarity: {chunk['similarity']:.2f}){Colors.ENDC}")
        print(f"Source: {chunk['source']}")
        print(f"Text: \"{chunk['text']}\"")
    
    # Context assembly
    print(f"\n{Colors.CYAN}Assembling context for LLM...{Colors.ENDC}")
    progress_bar(1.5, "Building prompt with retrieved context")

def simulate_response_generation():
    """Simulate response generation."""
    print_step(3, "Response Generation")
    
    # Prompt construction
    print(f"{Colors.CYAN}Constructing prompt with retrieved context...{Colors.ENDC}")
    progress_bar(1, "Optimizing prompt")
    
    # LLM generation
    print(f"\n{Colors.CYAN}Generating response with Ollama (llama3)...{Colors.ENDC}")
    
    response = """Based on the provided documents, the key performance metrics for the new product line are:

1. **Efficiency**: 27% increase compared to the previous generation (source: quarterly_report.txt)

2. **Processing Speed**: 45 units per minute, which exceeds the industry standard (source: product_specifications.csv)

3. **Energy Efficiency**: 0.8 kWh consumption rate, representing a 15% improvement (source: product_specifications.csv)

4. **Reliability Score**: 98.7%, which surpasses the target threshold of 95% (source: product_specifications.csv)

5. **Environmental Performance**: Consistent results across all operational environments, with all metrics exceeding target thresholds (source: technical_documentation.md)

These metrics indicate that the new product line is performing exceptionally well, particularly in terms of efficiency and reliability. The consistent performance across different operational environments also suggests robust design and implementation."""
    
    # Stream the response
    print(f"\n{Colors.GREEN}Response:{Colors.ENDC}")
    simulate_typing(response, 0.01)
    
    # Show citations
    print(f"\n{Colors.YELLOW}Sources:{Colors.ENDC}")
    print("1. quarterly_report.txt (Section: Q2 Performance Review)")
    print("2. product_specifications.csv (Rows: 45-48)")
    print("3. technical_documentation.md (Section: Performance Testing)")

def simulate_analytics():
    """Simulate analytics dashboard."""
    print_step(4, "Analytics")
    
    print(f"{Colors.CYAN}Generating analytics data...{Colors.ENDC}")
    
    # Query statistics
    print(f"\n{Colors.YELLOW}Query Statistics:{Colors.ENDC}")
    stats = {
        "total_queries": 1245,
        "avg_response_time": 9.8,
        "rag_usage_percentage": 78,
        "avg_tokens_per_response": 512,
        "top_document": "technical_documentation.md (used in 34% of responses)"
    }
    
    for key, value in stats.items():
        key_formatted = key.replace("_", " ").title()
        print(f"  {key_formatted}: {Colors.GREEN}{value}{Colors.ENDC}")
    
    # Performance metrics
    print(f"\n{Colors.YELLOW}Performance Metrics:{Colors.ENDC}")
    print(f"  Vector Search Time: {Colors.GREEN}0.45s (avg){Colors.ENDC}")
    print(f"  Context Assembly Time: {Colors.GREEN}0.12s (avg){Colors.ENDC}")
    print(f"  LLM Generation Time: {Colors.GREEN}9.2s (avg){Colors.ENDC}")
    print(f"  Total Response Time: {Colors.GREEN}9.8s (avg){Colors.ENDC}")
    
    # Document usage
    print(f"\n{Colors.YELLOW}Document Usage:{Colors.ENDC}")
    print(f"  Most Used Document: {Colors.GREEN}technical_documentation.md{Colors.ENDC}")
    print(f"  Most Relevant Section: {Colors.GREEN}Performance Testing{Colors.ENDC}")
    print(f"  Cache Hit Ratio: {Colors.GREEN}68%{Colors.ENDC}")

def main():
    """Run the demo presentation."""
    print_header("METIS RAG TECHNICAL DEMONSTRATION")
    
    print(f"{Colors.BOLD}Date:{Colors.ENDC} {datetime.now().strftime('%B %d, %Y')}")
    print(f"{Colors.BOLD}System:{Colors.ENDC} Metis RAG v1.2.0")
    print(f"{Colors.BOLD}Models:{Colors.ENDC} llama3 (LLM), nomic-embed-text (Embeddings)")
    
    input(f"\n{Colors.YELLOW}Press Enter to start the demonstration...{Colors.ENDC}")
    
    simulate_document_processing()
    input(f"\n{Colors.YELLOW}Press Enter to continue to query processing...{Colors.ENDC}")
    
    simulate_query_processing()
    input(f"\n{Colors.YELLOW}Press Enter to continue to response generation...{Colors.ENDC}")
    
    simulate_response_generation()
    input(f"\n{Colors.YELLOW}Press Enter to view analytics...{Colors.ENDC}")
    
    simulate_analytics()
    
    print_header("DEMONSTRATION COMPLETE")
    print(f"{Colors.GREEN}{Colors.BOLD}Thank you for attending the Metis RAG technical demonstration!{Colors.ENDC}")

if __name__ == "__main__":
    main()

================
File: scripts/demo/demo_tests.py
================
#!/usr/bin/env python3
"""
Demonstration script for Metis RAG testing strategy.
This script runs a subset of tests and displays the results in a user-friendly way.
Ideal for presentations and demonstrations.
"""

import os
import sys
import json
import logging
import subprocess
import time
import argparse
from datetime import datetime
import webbrowser
import shutil
import threading
import random

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("demo_tests")

# Demo test cases
DEMO_TESTS = [
    {
        "name": "Factual Accuracy Test",
        "description": "Tests if RAG responses contain expected facts from source documents",
        "command": ["pytest", "-xvs", "tests/test_rag_quality.py::test_factual_accuracy", "-v"],
        "report_file": "test_quality_results.json",
        "expected_duration": 10
    },
    {
        "name": "Multi-Document Retrieval Test",
        "description": "Tests retrieval across multiple documents",
        "command": ["pytest", "-xvs", "tests/test_rag_quality.py::test_multi_document_retrieval", "-v"],
        "report_file": "test_multi_doc_results.json",
        "expected_duration": 8
    },
    {
        "name": "File Type Support Test",
        "description": "Tests processing of different file types",
        "command": ["pytest", "-xvs", "tests/test_file_handling.py::test_file_type_support", "-v"],
        "report_file": "test_file_type_results.json",
        "expected_duration": 12
    },
    {
        "name": "Query Response Time Test",
        "description": "Measures response time for different query types",
        "command": ["pytest", "-xvs", "tests/test_performance.py::test_query_response_time", "-v"],
        "report_file": "test_response_time_results.json",
        "expected_duration": 15
    },
    {
        "name": "Special Characters Query Test",
        "description": "Tests queries with special characters, SQL injection, XSS, etc.",
        "command": ["pytest", "-xvs", "tests/test_edge_cases.py::test_special_characters_query", "-v"],
        "report_file": "test_special_queries_results.json",
        "expected_duration": 10
    }
]

def print_header(text):
    """Print a header with decoration"""
    width = 80
    print("\n" + "=" * width)
    print(f"{text.center(width)}")
    print("=" * width + "\n")

def print_test_info(test):
    """Print test information"""
    print(f"🧪 Test: {test['name']}")
    print(f"📝 Description: {test['description']}")
    print(f"⏱️  Expected Duration: {test['expected_duration']} seconds\n")

def print_progress_bar(iteration, total, prefix='', suffix='', length=50, fill='█'):
    """Print a progress bar"""
    percent = ("{0:.1f}").format(100 * (iteration / float(total)))
    filled_length = int(length * iteration // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end='\r')
    if iteration == total:
        print()

def animate_progress(duration, stop_event):
    """Animate a progress bar for the given duration"""
    steps = 100
    for i in range(steps + 1):
        if stop_event.is_set():
            # Fill the progress bar completely when done
            print_progress_bar(steps, steps, prefix='Progress:', suffix='Complete', length=50)
            break
        print_progress_bar(i, steps, prefix='Progress:', suffix='Running...', length=50)
        time.sleep(duration / steps)

def run_test(test):
    """Run a test and display progress"""
    print_test_info(test)
    
    # Create a stop event for the animation thread
    stop_event = threading.Event()
    
    # Start the animation in a separate thread
    animation_thread = threading.Thread(target=animate_progress, args=(test["expected_duration"], stop_event))
    animation_thread.start()
    
    # Run the test
    start_time = time.time()
    result = subprocess.run(test["command"], capture_output=True, text=True)
    end_time = time.time()
    
    # Stop the animation
    stop_event.set()
    animation_thread.join()
    
    # Print the result
    actual_duration = end_time - start_time
    print(f"\n⏱️  Actual Duration: {actual_duration:.2f} seconds")
    
    if result.returncode == 0:
        print("✅ Test PASSED\n")
    else:
        print("❌ Test FAILED\n")
        print("Error details:")
        print(result.stderr)
    
    # Check if report file exists
    if os.path.exists(test["report_file"]):
        print(f"📊 Report generated: {test['report_file']}")
        try:
            with open(test["report_file"], "r") as f:
                report_data = json.load(f)
                print(f"📈 Report contains {len(report_data) if isinstance(report_data, list) else 'structured'} data points")
        except json.JSONDecodeError:
            print("⚠️  Report file is not valid JSON")
    else:
        print("⚠️  No report file generated")
    
    print("\n" + "-" * 80 + "\n")
    
    return {
        "name": test["name"],
        "success": result.returncode == 0,
        "duration_seconds": actual_duration,
        "report_file": test["report_file"] if os.path.exists(test["report_file"]) else None
    }

def generate_summary(results):
    """Generate a summary of test results"""
    success_count = sum(1 for r in results if r["success"])
    total_count = len(results)
    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
    total_duration = sum(r["duration_seconds"] for r in results)
    
    print_header("TEST SUMMARY")
    print(f"Total Tests: {total_count}")
    print(f"Passed: {success_count}")
    print(f"Failed: {total_count - success_count}")
    print(f"Success Rate: {success_rate:.1f}%")
    print(f"Total Duration: {total_duration:.2f} seconds")
    
    # Print individual test results
    print("\nTest Results:")
    for i, result in enumerate(results):
        status = "✅ PASSED" if result["success"] else "❌ FAILED"
        print(f"{i+1}. {result['name']}: {status} ({result['duration_seconds']:.2f}s)")
    
    # Print report files
    print("\nReport Files:")
    for result in results:
        if result["report_file"]:
            print(f"- {result['report_file']}")
    
    return {
        "success_count": success_count,
        "total_count": total_count,
        "success_rate": success_rate,
        "total_duration": total_duration
    }

def open_reports():
    """Open HTML reports in the browser"""
    report_files = [
        "performance_benchmark_report.html",
        "edge_case_test_report.html",
        "metis_rag_test_report.html"
    ]
    
    for file in report_files:
        if os.path.exists(file):
            print(f"Opening {file} in browser...")
            webbrowser.open(f"file://{os.path.abspath(file)}")
            time.sleep(1)  # Delay to prevent browser from being overwhelmed

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Run Metis RAG demo tests")
    parser.add_argument("--test", type=int, help="Run a specific test (1-5)")
    parser.add_argument("--open-reports", action="store_true", help="Open HTML reports in browser")
    parser.add_argument("--random", action="store_true", help="Run tests in random order")
    return parser.parse_args()

def main():
    """Main function"""
    args = parse_args()
    
    print_header("METIS RAG TESTING DEMONSTRATION")
    print("This demonstration will run a subset of tests to showcase the testing framework.")
    print("Each test will display progress and results, and generate a report file.")
    print("At the end, a summary of all test results will be displayed.")
    
    # Select tests to run
    tests_to_run = DEMO_TESTS
    if args.test:
        if 1 <= args.test <= len(DEMO_TESTS):
            tests_to_run = [DEMO_TESTS[args.test - 1]]
        else:
            print(f"Error: Test number must be between 1 and {len(DEMO_TESTS)}")
            return 1
    
    # Randomize test order if requested
    if args.random:
        random.shuffle(tests_to_run)
    
    # Run tests
    results = []
    for test in tests_to_run:
        result = run_test(test)
        results.append(result)
    
    # Generate summary
    summary = generate_summary(results)
    
    # Open reports if requested
    if args.open_reports:
        open_reports()
    
    return 0 if summary["success_count"] == summary["total_count"] else 1

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/generation/generate_pdf.py
================
from reportlab.lib.pagesizes import letter
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
import os

def generate_test_pdf(output_path="sample_report.pdf"):
    """Generate a sample PDF report for testing Metis RAG"""
    
    # Create the PDF document
    doc = SimpleDocTemplate(output_path, pagesize=letter)
    styles = getSampleStyleSheet()
    
    # Create custom styles
    title_style = styles["Heading1"]
    title_style.alignment = 1  # Center alignment
    
    heading2_style = styles["Heading2"]
    heading3_style = styles["Heading3"]
    
    normal_style = styles["Normal"]
    normal_style.spaceAfter = 12
    
    # Create the content elements
    elements = []
    
    # Title
    elements.append(Paragraph("Quarterly Business Report", title_style))
    elements.append(Spacer(1, 24))
    
    # Executive Summary
    elements.append(Paragraph("Executive Summary", heading2_style))
    elements.append(Paragraph(
        """This quarterly report provides an overview of business performance for Q1 2025. 
        Overall, the company has seen strong growth in key metrics including revenue, customer 
        acquisition, and product engagement. This document summarizes the performance across 
        departments and outlines strategic initiatives for the upcoming quarter.""",
        normal_style
    ))
    elements.append(Spacer(1, 12))
    
    # Financial Performance
    elements.append(Paragraph("Financial Performance", heading2_style))
    elements.append(Paragraph(
        """The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. 
        Gross margin improved to 72%, up from 68% in the previous quarter. Operating expenses were 
        kept under control at $2.8M, resulting in a net profit of $1.4M.""",
        normal_style
    ))
    
    # Create a table for financial data
    financial_data = [
        ['Metric', 'Q1 2025', 'Q4 2024', 'Q1 2024', 'YoY Change'],
        ['Revenue', '$4.2M', '$3.8M', '$3.65M', '+15%'],
        ['Gross Margin', '72%', '68%', '65%', '+7%'],
        ['Operating Expenses', '$2.8M', '$2.7M', '$2.5M', '+12%'],
        ['Net Profit', '$1.4M', '$1.1M', '$0.9M', '+56%'],
    ]
    
    table = Table(financial_data, colWidths=[120, 80, 80, 80, 80])
    table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.black),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('GRID', (0, 0), (-1, -1), 1, colors.black),
    ]))
    
    elements.append(table)
    elements.append(Spacer(1, 24))
    
    # Product Development
    elements.append(Paragraph("Product Development", heading2_style))
    elements.append(Paragraph(
        """The product team successfully launched 3 major features this quarter:""",
        normal_style
    ))
    
    # Feature list
    features = [
        "Advanced Analytics Dashboard: Providing deeper insights into user behavior",
        "Mobile Application Redesign: Improving user experience and engagement",
        "API Integration Platform: Enabling third-party developers to build on our platform"
    ]
    
    for feature in features:
        elements.append(Paragraph(f"• {feature}", normal_style))
    
    elements.append(Spacer(1, 12))
    elements.append(Paragraph(
        """User engagement metrics show a 22% increase in daily active users following these releases. 
        The product roadmap for Q2 focuses on scalability improvements and enterprise features.""",
        normal_style
    ))
    elements.append(Spacer(1, 12))
    
    # Marketing and Sales
    elements.append(Paragraph("Marketing and Sales", heading2_style))
    elements.append(Paragraph(
        """The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from 
        the previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. 
        Customer acquisition cost (CAC) decreased by 15% to $350 per customer.""",
        normal_style
    ))
    
    # Customer Success
    elements.append(Paragraph("Customer Success", heading2_style))
    elements.append(Paragraph(
        """Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 
        42 to 48. The support team handled 3,200 tickets with an average response time of 2.5 hours 
        and a satisfaction rating of 4.8/5.""",
        normal_style
    ))
    
    # Strategic Initiatives for Q2
    elements.append(Paragraph("Strategic Initiatives for Q2", heading2_style))
    elements.append(Paragraph("The following initiatives are planned for Q2 2025:", normal_style))
    
    initiatives = [
        "International Expansion: Launch in European markets",
        "Enterprise Solution: Develop and release enterprise-grade features",
        "Strategic Partnerships: Form alliances with complementary service providers",
        "Operational Efficiency: Implement automation to reduce operational costs"
    ]
    
    for initiative in initiatives:
        elements.append(Paragraph(f"• {initiative}", normal_style))
    
    # Build the PDF
    doc.build(elements)
    
    print(f"PDF generated successfully at {os.path.abspath(output_path)}")
    return os.path.abspath(output_path)

if __name__ == "__main__":
    generate_test_pdf()

================
File: scripts/generation/generate_test_data.py
================
#!/usr/bin/env python3
"""
Test data generator for Metis RAG testing.
This script generates test documents with known facts for testing the RAG system.
"""

import os
import json
import argparse
import logging
import random
import string
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("generate_test_data")

# Test document templates
TEST_DOCUMENTS = {
    "technical_doc": {
        "filename": "technical_documentation.md",
        "content": """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
""",
        "tags": ["technical", "documentation", "architecture"],
        "folder": "/test"
    },
    "business_report": {
        "filename": "quarterly_report.txt",
        "content": """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
""",
        "tags": ["business", "report", "quarterly"],
        "folder": "/test"
    },
    "product_specs": {
        "filename": "product_specifications.csv",
        "content": """Product ID,Name,Category,Price,Features,Release Date
P001,MetisRAG Enterprise,Software,$4999,Advanced RAG capabilities|Multi-document retrieval|Enterprise security,2025-01-15
P002,MetisRAG Professional,Software,$1999,Standard RAG capabilities|Document management|API access,2025-01-15
P003,MetisRAG Basic,Software,$499,Basic RAG capabilities|Limited document storage|Web interface,2025-01-15
P004,MetisRAG API,Service,$0.10 per query,Pay-per-use|REST API|Developer documentation,2025-02-01
P005,MetisRAG Mobile,Mobile App,$9.99 per month,iOS and Android|Offline capabilities|Voice interface,2025-03-15
""",
        "tags": ["product", "specifications", "pricing"],
        "folder": "/test"
    },
    "user_guide": {
        "filename": "user_guide.md",
        "content": """# Metis RAG User Guide

## Getting Started

Welcome to Metis RAG, a powerful Retrieval-Augmented Generation system for enterprise knowledge management. This guide will help you get started with the system.

### Installation

To install Metis RAG, follow these steps:

1. Clone the repository: `git clone https://github.com/example/metis-rag.git`
2. Install dependencies: `pip install -r requirements.txt`
3. Configure environment variables: Copy `.env.example` to `.env` and update as needed
4. Start the application: `python -m app.main`

### First Steps

Once the application is running, you can access it at `http://localhost:8000`. The main interface provides the following sections:

- **Chat**: Interact with the RAG system through a chat interface
- **Documents**: Upload, manage, and organize your documents
- **System**: Configure system settings and monitor performance
- **Analytics**: View usage statistics and performance metrics

## Document Management

### Uploading Documents

To upload documents:

1. Navigate to the Documents page
2. Click the "Upload" button
3. Select one or more files from your computer
4. Click "Upload" to start the upload process

Supported file types include:
- PDF (.pdf)
- Text (.txt)
- Markdown (.md)
- CSV (.csv)

### Processing Documents

After uploading, documents need to be processed before they can be used for RAG:

1. Select the documents you want to process
2. Click the "Process" button
3. Wait for processing to complete

Processing includes:
- Text extraction
- Chunking
- Embedding generation
- Vector store indexing

### Organizing Documents

You can organize documents using folders and tags:

- **Folders**: Create a hierarchical structure for your documents
- **Tags**: Add labels to documents for flexible categorization

## Using the Chat Interface

### Basic Queries

To ask a question:

1. Type your question in the input field
2. Click "Send" or press Enter
3. View the response, including citations to source documents

### Advanced Options

The chat interface provides several advanced options:

- **RAG Toggle**: Enable or disable RAG for specific queries
- **Model Selection**: Choose different language models
- **Parameter Adjustment**: Fine-tune model parameters
- **Conversation History**: View and manage conversation history

## System Configuration

### Model Settings

You can configure the following model settings:

- **Default Model**: Set the default language model
- **Context Window**: Adjust the context window size
- **Temperature**: Control response randomness
- **Top-P**: Adjust nucleus sampling

### Vector Store Settings

Vector store settings include:

- **Embedding Model**: Choose the embedding model
- **Similarity Metric**: Select the similarity metric (cosine, dot product, etc.)
- **Top-K Results**: Set the number of results to retrieve

### Chunking Settings

Chunking settings include:

- **Chunking Strategy**: Choose between recursive, token, or markdown chunking
- **Chunk Size**: Set the chunk size in tokens or characters
- **Chunk Overlap**: Set the overlap between chunks

## Analytics

The analytics dashboard provides insights into:

- **Usage Metrics**: Queries per day, document uploads, etc.
- **Performance Metrics**: Response time, throughput, etc.
- **Document Metrics**: Document count, chunk count, etc.
- **Model Metrics**: Token usage, model performance, etc.

## Troubleshooting

### Common Issues

- **Slow Response Time**: Try reducing the number of retrieved chunks or using a smaller model
- **Poor Relevance**: Check if your documents are properly processed and indexed
- **Processing Errors**: Ensure your documents are in a supported format and not corrupted
- **Connection Issues**: Check if Ollama is running and accessible

### Getting Help

If you encounter issues not covered in this guide, please:

- Check the FAQ section
- Search the knowledge base
- Contact support at support@example.com
""",
        "tags": ["user guide", "documentation", "help"],
        "folder": "/test"
    },
    "api_documentation": {
        "filename": "api_documentation.md",
        "content": """# Metis RAG API Documentation

## Overview

The Metis RAG API provides programmatic access to the Retrieval-Augmented Generation system. This document describes the available endpoints, request/response formats, and authentication methods.

## Base URL

All API endpoints are relative to the base URL:

```
https://api.example.com/v1
```

## Authentication

The API uses API keys for authentication. To authenticate, include your API key in the `Authorization` header:

```
Authorization: Bearer YOUR_API_KEY
```

You can generate an API key in the System settings page.

## Endpoints

### Chat

#### Query

```
POST /chat/query
```

Submit a query to the RAG system.

**Request Body:**

```json
{
  "message": "What is the architecture of Metis RAG?",
  "conversation_id": "optional-conversation-id",
  "model": "optional-model-name",
  "use_rag": true,
  "stream": false,
  "model_parameters": {
    "temperature": 0.7,
    "top_p": 0.9
  },
  "metadata_filters": {
    "tags": ["technical", "documentation"]
  }
}
```

**Response:**

```json
{
  "message": "Metis RAG follows a modular architecture with the following components: Frontend Layer, API Layer, and RAG Engine. The RAG Engine consists of Document Processing, Vector Store, and LLM Integration. [1]",
  "conversation_id": "conversation-id",
  "citations": [
    {
      "document_id": "doc-id",
      "chunk_id": "chunk-id",
      "relevance_score": 0.92,
      "excerpt": "Metis RAG follows a modular architecture with the following components..."
    }
  ]
}
```

#### List Conversations

```
GET /chat/conversations
```

List all conversations.

**Response:**

```json
{
  "conversations": [
    {
      "id": "conversation-id",
      "created": "2025-01-15T12:34:56Z",
      "updated": "2025-01-15T13:45:67Z",
      "message_count": 10
    }
  ]
}
```

#### Get Conversation

```
GET /chat/conversations/{conversation_id}
```

Get a specific conversation.

**Response:**

```json
{
  "id": "conversation-id",
  "created": "2025-01-15T12:34:56Z",
  "updated": "2025-01-15T13:45:67Z",
  "messages": [
    {
      "content": "What is the architecture of Metis RAG?",
      "role": "user",
      "timestamp": "2025-01-15T12:34:56Z"
    },
    {
      "content": "Metis RAG follows a modular architecture with the following components...",
      "role": "assistant",
      "timestamp": "2025-01-15T12:35:00Z",
      "citations": [
        {
          "document_id": "doc-id",
          "chunk_id": "chunk-id",
          "relevance_score": 0.92,
          "excerpt": "Metis RAG follows a modular architecture with the following components..."
        }
      ]
    }
  ]
}
```

### Documents

#### Upload Document

```
POST /documents/upload
```

Upload a document.

**Request:**

Multipart form data with a `file` field.

**Response:**

```json
{
  "success": true,
  "document_id": "doc-id",
  "filename": "technical_documentation.md",
  "size": 12345
}
```

#### Process Documents

```
POST /documents/process
```

Process uploaded documents.

**Request Body:**

```json
{
  "document_ids": ["doc-id-1", "doc-id-2"],
  "force_reprocess": false,
  "chunking_strategy": "recursive",
  "chunk_size": 1000,
  "chunk_overlap": 200
}
```

**Response:**

```json
{
  "success": true,
  "processed_count": 2,
  "failed_count": 0,
  "failed_documents": []
}
```

#### List Documents

```
GET /documents/list
```

List all documents.

**Query Parameters:**

- `folder` (optional): Filter by folder
- `tags` (optional): Filter by tags (comma-separated)

**Response:**

```json
[
  {
    "id": "doc-id",
    "filename": "technical_documentation.md",
    "chunk_count": 10,
    "metadata": {
      "file_size": 12345,
      "file_type": "md",
      "created_at": 1642234567,
      "modified_at": 1642234567
    },
    "tags": ["technical", "documentation"],
    "folder": "/test",
    "uploaded": "2025-01-15T12:34:56Z"
  }
]
```

#### Get Document

```
GET /documents/{document_id}
```

Get a specific document.

**Response:**

```json
{
  "id": "doc-id",
  "filename": "technical_documentation.md",
  "content": "# Metis RAG Technical Documentation...",
  "chunk_count": 10,
  "metadata": {
    "file_size": 12345,
    "file_type": "md",
    "created_at": 1642234567,
    "modified_at": 1642234567
  },
  "tags": ["technical", "documentation"],
  "folder": "/test",
  "uploaded": "2025-01-15T12:34:56Z"
}
```

#### Delete Document

```
DELETE /documents/{document_id}
```

Delete a specific document.

**Response:**

```json
{
  "success": true
}
```

### System

#### Health Check

```
GET /system/health
```

Check system health.

**Response:**

```json
{
  "status": "ok",
  "version": "1.0.0",
  "components": {
    "api": "ok",
    "vector_store": "ok",
    "ollama": "ok"
  }
}
```

#### Models

```
GET /system/models
```

List available models.

**Response:**

```json
{
  "models": [
    {
      "name": "llama2",
      "description": "Llama 2 7B",
      "parameters": 7000000000,
      "context_window": 4096
    },
    {
      "name": "mistral",
      "description": "Mistral 7B",
      "parameters": 7000000000,
      "context_window": 8192
    }
  ]
}
```

### Analytics

#### Query Analytics

```
GET /analytics/queries
```

Get query analytics.

**Query Parameters:**

- `start_date` (optional): Start date (ISO format)
- `end_date` (optional): End date (ISO format)
- `model` (optional): Filter by model

**Response:**

```json
{
  "total_queries": 1234,
  "average_response_time_ms": 567,
  "queries_per_day": [
    {
      "date": "2025-01-15",
      "count": 123
    },
    {
      "date": "2025-01-16",
      "count": 456
    }
  ],
  "models": [
    {
      "name": "llama2",
      "count": 789
    },
    {
      "name": "mistral",
      "count": 345
    }
  ]
}
```

## Error Handling

The API uses standard HTTP status codes to indicate success or failure:

- 200: Success
- 400: Bad Request
- 401: Unauthorized
- 404: Not Found
- 500: Internal Server Error

Error responses include a JSON body with details:

```json
{
  "error": {
    "code": "invalid_request",
    "message": "Invalid request parameters",
    "details": {
      "message": "This field is required"
    }
  }
}
```

## Rate Limiting

The API is rate limited to 100 requests per minute per API key. If you exceed this limit, you'll receive a 429 Too Many Requests response.

## Pagination

List endpoints support pagination using the `offset` and `limit` query parameters:

```
GET /documents/list?offset=0&limit=10
```

Paginated responses include pagination metadata:

```json
{
  "data": [...],
  "pagination": {
    "total": 123,
    "offset": 0,
    "limit": 10,
    "next": "/documents/list?offset=10&limit=10",
    "previous": null
  }
}
```
""",
        "tags": ["api", "documentation", "reference"],
        "folder": "/test"
    }
}

# Test queries with expected facts
TEST_QUERIES = [
    {
        "query": "What is the architecture of Metis RAG?",
        "expected_facts": [
            "modular architecture",
            "Frontend Layer",
            "API Layer",
            "RAG Engine",
            "HTML, CSS, and JavaScript",
            "FastAPI"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What was the revenue reported in Q1 2025?",
        "expected_facts": [
            "$4.2M",
            "15% increase",
            "year-over-year",
            "net profit of $1.4M"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What are the components of the RAG engine?",
        "expected_facts": [
            "Document Processing",
            "Vector Store",
            "LLM Integration",
            "chunking",
            "embeddings",
            "Ollama"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What are the strategic initiatives for Q2?",
        "expected_facts": [
            "International Expansion",
            "European markets",
            "Enterprise Solution",
            "Strategic Partnerships",
            "Operational Efficiency"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What products are available and at what price points?",
        "expected_facts": [
            "MetisRAG Enterprise",
            "$4999",
            "MetisRAG Professional",
            "$1999",
            "MetisRAG Basic",
            "$499"
        ],
        "document_ids": ["product_specs"]
    },
    {
        "query": "How do I upload documents to the system?",
        "expected_facts": [
            "Navigate to the Documents page",
            "Click the \"Upload\" button",
            "Select one or more files",
            "Click \"Upload\" to start the upload process"
        ],
        "document_ids": ["user_guide"]
    },
    {
        "query": "What authentication method does the API use?",
        "expected_facts": [
            "API keys",
            "Authorization header",
            "Bearer YOUR_API_KEY"
        ],
        "document_ids": ["api_documentation"]
    }
]

def generate_random_string(length):
    """Generate a random string of specified length"""
    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))

def generate_test_documents(output_dir, count=None, specific_docs=None):
    """Generate test documents"""
    os.makedirs(output_dir, exist_ok=True)
    
    documents = []
    
    # Determine which documents to generate
    doc_ids = []
    if specific_docs:
        doc_ids = specific_docs
    elif count:
        doc_ids = random.sample(list(TEST_DOCUMENTS.keys()), min(count, len(TEST_DOCUMENTS)))
    else:
        doc_ids = list(TEST_DOCUMENTS.keys())
    
    # Generate documents
    for doc_id in doc_ids:
        doc_info = TEST_DOCUMENTS[doc_id]
        
        # Create document file
        file_path = os.path.join(output_dir, doc_info["filename"])
        with open(file_path, "w") as f:
            f.write(doc_info["content"])
        
        # Store document metadata
        documents.append({
            "id": doc_id,
            "filename": doc_info["filename"],
            "path": file_path,
            "tags": doc_info["tags"],
            "folder": doc_info["folder"]
        })
        
        logger.info(f"Generated document: {file_path}")
    
    # Generate metadata file
    metadata_path = os.path.join(output_dir, "metadata.json")
    with open(metadata_path, "w") as f:
        json.dump({
            "documents": documents,
            "generated_at": datetime.now().isoformat()
        }, f, indent=2)
    
    logger.info(f"Generated metadata: {metadata_path}")
    
    return documents

def generate_test_queries(output_dir, count=None, specific_queries=None):
    """Generate test queries"""
    # Determine which queries to generate
    queries = []
    if specific_queries:
        queries = [q for i, q in enumerate(TEST_QUERIES) if i in specific_queries]
    elif count:
        queries = random.sample(TEST_QUERIES, min(count, len(TEST_QUERIES)))
    else:
        queries = TEST_QUERIES
    
    # Generate queries file
    queries_path = os.path.join(output_dir, "test_queries.json")
    with open(queries_path, "w") as f:
        json.dump({
            "queries": queries,
            "generated_at": datetime.now().isoformat()
        }, f, indent=2)
    
    logger.info(f"Generated queries: {queries_path}")
    
    return queries

def generate_large_document(output_dir, size_kb):
    """Generate a large document of specified size in KB"""
    # Generate random content
    content = generate_random_string(size_kb * 1024)
    
    # Create document file
    file_path = os.path.join(output_dir, f"large_document_{size_kb}kb.txt")
    with open(file_path, "w") as f:
        f.write(content)
    
    logger.info(f"Generated large document: {file_path} ({size_kb} KB)")
    
    return file_path

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Generate test data for Metis RAG testing")
    parser.add_argument("--output-dir", type=str, default="test_data", help="Output directory for test data")
    parser.add_argument("--document-count", type=int, help="Number of documents to generate")
    parser.add_argument("--query-count", type=int, help="Number of queries to generate")
    parser.add_argument("--specific-docs", type=str, help="Specific documents to generate (comma-separated)")
    parser.add_argument("--specific-queries", type=str, help="Specific queries to generate (comma-separated)")
    parser.add_argument("--large-document", type=int, help="Generate a large document of specified size in KB")
    return parser.parse_args()

def main():
    """Main function"""
    args = parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Parse specific documents
    specific_docs = None
    if args.specific_docs:
        specific_docs = args.specific_docs.split(",")
    
    # Parse specific queries
    specific_queries = None
    if args.specific_queries:
        specific_queries = [int(i) for i in args.specific_queries.split(",")]
    
    # Generate test documents
    documents = generate_test_documents(args.output_dir, args.document_count, specific_docs)
    
    # Generate test queries
    queries = generate_test_queries(args.output_dir, args.query_count, specific_queries)
    
    # Generate large document if requested
    if args.large_document:
        large_doc_path = generate_large_document(args.output_dir, args.large_document)
    
    logger.info(f"Test data generation complete. Generated {len(documents)} documents and {len(queries)} queries.")

if __name__ == "__main__":
    main()

================
File: scripts/maintenance/clear_cache.py
================
#!/usr/bin/env python3
"""
Script to clear the vector store cache to ensure the system uses updated settings.
"""

import asyncio
from app.rag.vector_store import VectorStore

async def clear_cache():
    """Clear the vector store cache"""
    print("Clearing vector store cache...")
    vector_store = VectorStore()
    vector_store.clear_cache()
    print("Vector store cache cleared successfully!")

if __name__ == "__main__":
    asyncio.run(clear_cache())

================
File: scripts/maintenance/clear_database.py
================
#!/usr/bin/env python3
"""
Script to clear the Metis RAG database and start fresh.
This script will:
1. Delete the ChromaDB directory
2. Delete the uploads directory (optional)
"""

import os
import shutil
import argparse
from pathlib import Path

# Get the base directory
BASE_DIR = Path(__file__).resolve().parent

# Default paths
CHROMA_DB_DIR = BASE_DIR / "chroma_db"
UPLOAD_DIR = BASE_DIR / "uploads"

def clear_database(clear_uploads=False):
    """Clear the database and optionally the uploads directory"""
    print(f"Clearing ChromaDB directory: {CHROMA_DB_DIR}")
    
    if os.path.exists(CHROMA_DB_DIR):
        try:
            shutil.rmtree(CHROMA_DB_DIR)
            print("✅ ChromaDB directory deleted successfully")
        except Exception as e:
            print(f"❌ Error deleting ChromaDB directory: {str(e)}")
            print("   Make sure the application is not running")
            return False
    else:
        print("ℹ️ ChromaDB directory does not exist, nothing to delete")
    
    # Recreate the directory
    os.makedirs(CHROMA_DB_DIR, exist_ok=True)
    print("✅ Created fresh ChromaDB directory")
    
    if clear_uploads:
        print(f"Clearing uploads directory: {UPLOAD_DIR}")
        if os.path.exists(UPLOAD_DIR):
            try:
                # Delete all contents but keep the directory
                for item in os.listdir(UPLOAD_DIR):
                    item_path = os.path.join(UPLOAD_DIR, item)
                    if os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                    else:
                        os.remove(item_path)
                print("✅ Uploads directory cleared successfully")
            except Exception as e:
                print(f"❌ Error clearing uploads directory: {str(e)}")
                return False
        else:
            print("ℹ️ Uploads directory does not exist, nothing to delete")
            os.makedirs(UPLOAD_DIR, exist_ok=True)
            print("✅ Created fresh uploads directory")
    
    print("\n✅ Database cleared successfully!")
    print("\nYou can now restart the application with a fresh database.")
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Clear the Metis RAG database and start fresh")
    parser.add_argument("--clear-uploads", action="store_true", help="Also clear the uploads directory")
    args = parser.parse_args()
    
    clear_database(args.clear_uploads)

================
File: scripts/maintenance/reprocess_documents.py
================
#!/usr/bin/env python3
"""
Script to reprocess all documents in the Metis RAG system with updated chunking settings.
This is useful after changing chunking parameters to ensure all documents use the new settings.
"""

import os
import asyncio
import logging
from pathlib import Path

from app.models.document import Document
from app.rag.document_processor import DocumentProcessor
from app.rag.vector_store import VectorStore
from app.core.config import UPLOAD_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("reprocess_documents")

async def reprocess_documents():
    """Reprocess all documents with updated chunking settings"""
    logger.info("Starting document reprocessing")
    
    # Initialize document processor with updated settings
    processor = DocumentProcessor(
        chunking_strategy="recursive"  # This will use the updated chunk size for text files
    )
    
    # Initialize vector store
    vector_store = VectorStore()
    
    # Get all document directories in the uploads folder
    upload_path = Path(UPLOAD_DIR)
    if not upload_path.exists():
        logger.error(f"Uploads directory {UPLOAD_DIR} does not exist")
        return
    
    document_dirs = [d for d in upload_path.iterdir() if d.is_dir()]
    logger.info(f"Found {len(document_dirs)} document directories")
    
    # Process each document
    for doc_dir in document_dirs:
        try:
            # Get the document ID from the directory name
            document_id = doc_dir.name
            
            # Find the document file(s) in the directory
            files = list(doc_dir.iterdir())
            if not files:
                logger.warning(f"No files found in document directory {document_id}")
                continue
            
            # Assume the first file is the document (in a real system, you might need to be more sophisticated)
            document_file = files[0]
            
            logger.info(f"Reprocessing document: {document_file.name} (ID: {document_id})")
            
            # Read the file content
            try:
                with open(document_file, 'rb') as f:
                    content = f.read().decode('utf-8', errors='ignore')
            except Exception as e:
                logger.error(f"Error reading file {document_file}: {str(e)}")
                continue
                
            # Create a Document object
            document = Document(
                id=document_id,
                filename=document_file.name,
                content=content,  # Add the content field
                size=document_file.stat().st_size,
                folder="/",  # Default folder
                tags=[]  # No tags by default
            )
            
            # Delete existing document from vector store
            try:
                vector_store.delete_document(document_id)
                logger.info(f"Deleted existing document {document_id} from vector store")
            except Exception as e:
                logger.warning(f"Error deleting document {document_id} from vector store: {str(e)}")
            
            # Process the document with new chunking settings
            processed_document = await processor.process_document(document)
            
            # Add the document to the vector store
            await vector_store.add_document(processed_document)
            
            logger.info(f"Successfully reprocessed document {document_id} with {len(processed_document.chunks)} chunks")
        
        except Exception as e:
            logger.error(f"Error reprocessing document in directory {doc_dir}: {str(e)}")
    
    logger.info("Document reprocessing complete")

if __name__ == "__main__":
    asyncio.run(reprocess_documents())

================
File: scripts/maintenance/run_tests.py
================
#!/usr/bin/env python3
"""
Test runner for Metis RAG testing strategy.
This script executes all test suites and generates a comprehensive report.
"""

import os
import sys
import json
import logging
import subprocess
import time
import argparse
from datetime import datetime
import webbrowser
import shutil

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_runner")

# Test suites
TEST_SUITES = [
    {
        "name": "RAG Quality Tests",
        "module": "tests.test_rag_quality",
        "description": "Tests for factual accuracy, relevance, and citation quality",
        "report_files": [
            "test_quality_results.json",
            "test_multi_doc_results.json",
            "test_citation_results.json",
            "test_api_integration_results.json"
        ]
    },
    {
        "name": "File Handling Tests",
        "module": "tests.test_file_handling",
        "description": "Tests for different file types, multiple file uploads, and large files",
        "report_files": [
            "test_file_type_results.json",
            "test_chunking_strategy_results.json",
            "test_large_file_results.json",
            "test_api_upload_results.json",
            "test_multiple_upload_results.json"
        ]
    },
    {
        "name": "Performance Tests",
        "module": "tests.test_performance",
        "description": "Tests for response time, throughput, and resource utilization",
        "report_files": [
            "test_response_time_results.json",
            "test_throughput_results.json",
            "test_resource_utilization_results.json",
            "test_api_performance_results.json",
            "performance_benchmark_report.json",
            "performance_benchmark_report.html"
        ]
    },
    {
        "name": "Edge Case Tests",
        "module": "tests.test_edge_cases",
        "description": "Tests for unusual inputs, error handling, and system resilience",
        "report_files": [
            "test_special_queries_results.json",
            "test_concurrent_processing_results.json",
            "test_invalid_files_results.json",
            "test_malformed_requests_results.json",
            "test_vector_store_resilience_results.json",
            "edge_case_test_report.json",
            "edge_case_test_report.html"
        ]
    }
]

def run_test_suite(suite, args):
    """Run a test suite using pytest"""
    logger.info(f"Running {suite['name']}...")
    
    # Build pytest command
    cmd = [sys.executable, "-m", "pytest", "-xvs"]
    
    # Add specific test options
    if args.failfast:
        cmd.append("-xvs")
    
    # Add the module to test
    cmd.append(suite["module"])
    
    # Run the command
    start_time = time.time()
    result = subprocess.run(cmd, capture_output=True, text=True)
    end_time = time.time()
    
    # Log the result
    if result.returncode == 0:
        logger.info(f"{suite['name']} completed successfully in {end_time - start_time:.2f} seconds")
    else:
        logger.error(f"{suite['name']} failed with return code {result.returncode}")
        logger.error(f"STDOUT: {result.stdout}")
        logger.error(f"STDERR: {result.stderr}")
    
    return {
        "name": suite["name"],
        "success": result.returncode == 0,
        "duration_seconds": end_time - start_time,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "return_code": result.returncode
    }

def collect_reports(suite):
    """Collect report files for a test suite"""
    reports = {}
    
    for report_file in suite["report_files"]:
        if os.path.exists(report_file):
            try:
                with open(report_file, "r") as f:
                    reports[report_file] = json.load(f)
            except json.JSONDecodeError:
                logger.warning(f"Could not parse {report_file} as JSON")
                with open(report_file, "r") as f:
                    reports[report_file] = f.read()
        else:
            logger.warning(f"Report file {report_file} not found")
    
    return reports

def generate_master_report(results, args):
    """Generate a master report from all test results"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "test_suites": results
    }
    
    # Save report
    report_path = "metis_rag_test_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Master report saved to {os.path.abspath(report_path)}")
    
    # Generate HTML report
    html_report = generate_html_report(report)
    html_report_path = "metis_rag_test_report.html"
    
    with open(html_report_path, "w") as f:
        f.write(html_report)
    
    logger.info(f"HTML report saved to {os.path.abspath(html_report_path)}")
    
    # Open the report in a browser if requested
    if args.open_report:
        webbrowser.open(f"file://{os.path.abspath(html_report_path)}")
    
    return report_path, html_report_path

def generate_html_report(report):
    """Generate an HTML report from the test results"""
    # Count successful test suites
    success_count = sum(1 for suite in report["test_suites"] if suite["result"]["success"])
    total_count = len(report["test_suites"])
    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
    
    # Calculate total duration
    total_duration = sum(suite["result"]["duration_seconds"] for suite in report["test_suites"])
    
    # Generate HTML
    html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Test Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        .section {{ margin-bottom: 30px; }}
        .summary {{ display: flex; justify-content: space-between; margin-bottom: 20px; }}
        .summary-box {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; width: 30%; text-align: center; }}
        .success {{ color: green; }}
        .failure {{ color: red; }}
        table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .details {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-top: 10px; }}
        .details pre {{ white-space: pre-wrap; overflow-x: auto; }}
        .toggle-btn {{ background-color: #4CAF50; color: white; padding: 5px 10px; border: none; border-radius: 3px; cursor: pointer; }}
        .toggle-btn:hover {{ background-color: #45a049; }}
    </style>
    <script>
        function toggleDetails(id) {{
            var details = document.getElementById(id);
            if (details.style.display === "none") {{
                details.style.display = "block";
            }} else {{
                details.style.display = "none";
            }}
        }}
    </script>
</head>
<body>
    <h1>Metis RAG Test Report</h1>
    <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="summary">
        <div class="summary-box">
            <h3>Test Suites</h3>
            <p class="{('success' if success_rate == 100 else 'failure')}">{success_count}/{total_count} ({success_rate:.1f}%)</p>
        </div>
        <div class="summary-box">
            <h3>Total Duration</h3>
            <p>{total_duration:.2f} seconds</p>
        </div>
        <div class="summary-box">
            <h3>Timestamp</h3>
            <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
    
    <div class="section">
        <h2>Test Suite Results</h2>
        <table>
            <tr>
                <th>Test Suite</th>
                <th>Description</th>
                <th>Status</th>
                <th>Duration</th>
                <th>Details</th>
            </tr>
            {"".join([f'''
            <tr>
                <td>{suite["name"]}</td>
                <td>{suite["description"]}</td>
                <td class="{'success' if suite['result']['success'] else 'failure'}">{('Success' if suite['result']['success'] else 'Failure')}</td>
                <td>{suite["result"]["duration_seconds"]:.2f} seconds</td>
                <td><button class="toggle-btn" onclick="toggleDetails('details-{i}')">Toggle Details</button></td>
            </tr>
            <tr>
                <td colspan="5">
                    <div id="details-{i}" class="details" style="display: none;">
                        <h4>Output:</h4>
                        <pre>{suite["result"]["stdout"]}</pre>
                        <h4>Error Output:</h4>
                        <pre>{suite["result"]["stderr"]}</pre>
                        <h4>Reports:</h4>
                        <ul>
                            {"".join([f'<li><a href="{report_file}">{report_file}</a></li>' for report_file in suite["reports"].keys()])}
                        </ul>
                    </div>
                </td>
            </tr>
            ''' for i, suite in enumerate(report["test_suites"])])}
        </table>
    </div>
    
    <div class="section">
        <h2>Individual Test Reports</h2>
        <p>The following reports were generated by the test suites:</p>
        <ul>
            {"".join([f'<li><a href="{report_file}">{report_file}</a></li>' for suite in report["test_suites"] for report_file in suite["reports"].keys()])}
        </ul>
    </div>
    
    <div class="section">
        <h2>Specific Report Links</h2>
        <h3>Performance Report</h3>
        <p><a href="performance_benchmark_report.html">Performance Benchmark Report</a></p>
        
        <h3>Edge Case Report</h3>
        <p><a href="edge_case_test_report.html">Edge Case Test Report</a></p>
    </div>
</body>
</html>
"""
    
    return html

def create_reports_directory():
    """Create a directory for test reports"""
    reports_dir = "test_reports"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    reports_dir = f"{reports_dir}_{timestamp}"
    
    os.makedirs(reports_dir, exist_ok=True)
    return reports_dir

def copy_reports_to_directory(reports_dir):
    """Copy all report files to the reports directory"""
    report_files = []
    for suite in TEST_SUITES:
        report_files.extend(suite["report_files"])
    
    report_files.extend(["metis_rag_test_report.json", "metis_rag_test_report.html"])
    
    for file in report_files:
        if os.path.exists(file):
            shutil.copy(file, os.path.join(reports_dir, file))
            logger.info(f"Copied {file} to {reports_dir}")

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Run Metis RAG tests")
    parser.add_argument("--suite", type=str, help="Run a specific test suite (rag_quality, file_handling, performance, edge_cases)")
    parser.add_argument("--failfast", action="store_true", help="Stop on first failure")
    parser.add_argument("--open-report", action="store_true", help="Open the HTML report in a browser")
    parser.add_argument("--save-reports", action="store_true", help="Save reports to a timestamped directory")
    return parser.parse_args()

def main():
    """Main function"""
    args = parse_args()
    
    # Run tests
    results = []
    
    # Filter test suites if a specific one is requested
    suites_to_run = TEST_SUITES
    if args.suite:
        suite_map = {
            "rag_quality": "RAG Quality Tests",
            "file_handling": "File Handling Tests",
            "performance": "Performance Tests",
            "edge_cases": "Edge Case Tests"
        }
        if args.suite in suite_map:
            suite_name = suite_map[args.suite]
            suites_to_run = [suite for suite in TEST_SUITES if suite["name"] == suite_name]
            if not suites_to_run:
                logger.error(f"Test suite {args.suite} not found")
                return 1
        else:
            logger.error(f"Unknown test suite: {args.suite}")
            logger.info(f"Available suites: {', '.join(suite_map.keys())}")
            return 1
    
    # Run each test suite
    for suite in suites_to_run:
        # Run the test suite
        result = run_test_suite(suite, args)
        
        # Collect reports
        reports = collect_reports(suite)
        
        # Store results
        results.append({
            "name": suite["name"],
            "description": suite["description"],
            "result": result,
            "reports": reports
        })
        
        # Stop on first failure if requested
        if args.failfast and not result["success"]:
            logger.error(f"Stopping due to failure in {suite['name']}")
            break
    
    # Generate master report
    report_path, html_report_path = generate_master_report(results, args)
    
    # Save reports to a directory if requested
    if args.save_reports:
        reports_dir = create_reports_directory()
        copy_reports_to_directory(reports_dir)
        logger.info(f"Reports saved to {reports_dir}")
    
    # Return success if all test suites passed
    return 0 if all(suite["result"]["success"] for suite in results) else 1

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/maintenance/test_rag_retrieval.py
================
#!/usr/bin/env python3
"""
Test script for Metis RAG retrieval functionality.
This script creates test documents, uploads and processes them,
and then tests the RAG retrieval with specific queries.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_rag_retrieval")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.models.document import Document, Chunk

# Test document content
MARKDOWN_CONTENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

PDF_CONTENT = """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
"""

async def create_test_documents():
    """Create test documents for RAG testing"""
    logger.info("Creating test documents...")
    
    # Create directories for test documents
    os.makedirs("test_docs", exist_ok=True)
    
    # Create Markdown document
    markdown_path = os.path.join("test_docs", "technical_documentation.md")
    with open(markdown_path, "w") as f:
        f.write(MARKDOWN_CONTENT)
    
    # Create PDF-like text document (since we can't easily create a real PDF programmatically)
    pdf_path = os.path.join("test_docs", "quarterly_report.txt")
    with open(pdf_path, "w") as f:
        f.write(PDF_CONTENT)
    
    logger.info(f"Created test documents in {os.path.abspath('test_docs')}")
    return markdown_path, pdf_path

async def process_documents(vector_store, markdown_path, pdf_path):
    """Process the test documents and add them to the vector store"""
    logger.info("Processing test documents...")
    
    # Read file contents first
    with open(markdown_path, "r") as f:
        markdown_content = f.read()
    
    with open(pdf_path, "r") as f:
        pdf_content = f.read()
    
    # Create Document objects
    markdown_doc = Document(
        id=str(uuid.uuid4()),
        filename="technical_documentation.md",
        content=markdown_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    pdf_doc = Document(
        id=str(uuid.uuid4()),
        filename="quarterly_report.txt",
        content=pdf_content,
        tags=["business", "report", "quarterly"],
        folder="/test"
    )
    
    # Fix for ChromaDB metadata issue - convert tags to string
    # We'll modify the add_document method in vector_store.py to handle this properly
    class CustomVectorStore(VectorStore):
        async def add_document(self, document: Document) -> None:
            """Override to fix tags handling"""
            try:
                logger.info(f"Adding document {document.id} to vector store")
                
                # Make sure we have an Ollama client
                if self.ollama_client is None:
                    self.ollama_client = OllamaClient()
                
                # Prepare chunks for batch processing
                chunks_to_embed = [chunk for chunk in document.chunks if not chunk.embedding]
                chunk_contents = [chunk.content for chunk in chunks_to_embed]
                
                # Create embeddings in batch if possible
                if chunk_contents:
                    try:
                        # Batch embedding
                        embeddings = await self._batch_create_embeddings(chunk_contents)
                        
                        # Assign embeddings to chunks
                        for i, chunk in enumerate(chunks_to_embed):
                            chunk.embedding = embeddings[i]
                    except Exception as batch_error:
                        logger.warning(f"Batch embedding failed: {str(batch_error)}. Falling back to sequential embedding.")
                        # Fall back to sequential embedding
                        for chunk in chunks_to_embed:
                            chunk.embedding = await self.ollama_client.create_embedding(
                                text=chunk.content,
                                model=self.embedding_model
                            )
                
                # Add chunks to the collection
                for chunk in document.chunks:
                    if not chunk.embedding:
                        logger.warning(f"Chunk {chunk.id} has no embedding, skipping")
                        continue
                        
                    # Convert tags to string to avoid ChromaDB error
                    tags_str = ",".join(document.tags) if document.tags else ""
                    
                    self.collection.add(
                        ids=[chunk.id],
                        embeddings=[chunk.embedding],
                        documents=[chunk.content],
                        metadatas=[{
                            "document_id": document.id,
                            "chunk_index": chunk.metadata.get("index", 0),
                            "filename": document.filename,
                            "tags_str": tags_str,  # Use string instead of list
                            "folder": document.folder,
                            **chunk.metadata
                        }]
                    )
                
                logger.info(f"Added {len(document.chunks)} chunks to vector store for document {document.id}")
            except Exception as e:
                logger.error(f"Error adding document {document.id} to vector store: {str(e)}")
                raise
    
    # Use our custom vector store
    vector_store = CustomVectorStore()
    
    # Create chunks for Markdown document
    markdown_chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=markdown_content,
            metadata={
                "index": 0,
                "source": markdown_path
            }
        )
    ]
    
    # Create chunks for PDF document
    pdf_chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=pdf_content,
            metadata={
                "index": 0,
                "source": pdf_path
            }
        )
    ]
    
    # Assign chunks to documents
    markdown_doc.chunks = markdown_chunks
    pdf_doc.chunks = pdf_chunks
    
    # Add documents to vector store
    await vector_store.add_document(markdown_doc)
    await vector_store.add_document(pdf_doc)
    
    logger.info(f"Added documents to vector store: {markdown_doc.id}, {pdf_doc.id}")
    return markdown_doc, pdf_doc, vector_store

async def test_queries(rag_engine):
    """Test RAG retrieval with specific queries"""
    logger.info("Testing RAG retrieval with specific queries...")
    
    test_queries = [
        "What is the architecture of Metis RAG?",
        "What was the revenue reported in Q1 2025?",
        "What are the components of the RAG engine?",
        "What are the strategic initiatives for Q2?",
        "How does the vector store work?",
        "What was the customer retention rate?",
    ]
    
    results = []
    for query in test_queries:
        logger.info(f"Testing query: {query}")
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Log results
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        logger.info(f"Query: {query}")
        logger.info(f"Answer: {answer[:100]}...")
        logger.info(f"Sources: {len(sources)} chunks retrieved")
        
        # Store results for analysis
        results.append({
            "query": query,
            "answer": answer,
            "sources": [
                {
                    "document_id": s.document_id,
                    "chunk_id": s.chunk_id,
                    "relevance_score": s.relevance_score,
                    "excerpt": s.excerpt[:100] + "..." if len(s.excerpt) > 100 else s.excerpt
                }
                for s in sources
            ] if sources else []
        })
    
    # Save results to file
    results_path = os.path.join("test_docs", "rag_test_results.json")
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Test results saved to {os.path.abspath(results_path)}")
    return results

async def analyze_results(results):
    """Analyze the test results"""
    logger.info("Analyzing test results...")
    
    # Check if each query has sources
    queries_with_sources = sum(1 for r in results if r["sources"])
    logger.info(f"{queries_with_sources}/{len(results)} queries returned sources")
    
    # Check if answers contain source references
    answers_with_references = sum(1 for r in results if "[" in r["answer"] and "]" in r["answer"])
    logger.info(f"{answers_with_references}/{len(results)} answers contain source references")
    
    # Check for specific content in answers
    architecture_query = next((r for r in results if "architecture" in r["query"].lower()), None)
    revenue_query = next((r for r in results if "revenue" in r["query"].lower()), None)
    
    if architecture_query:
        has_architecture_info = any(keyword in architecture_query["answer"].lower() 
                                   for keyword in ["frontend", "api", "rag engine", "modular"])
        logger.info(f"Architecture query contains relevant information: {has_architecture_info}")
    
    if revenue_query:
        has_revenue_info = any(keyword in revenue_query["answer"].lower() 
                              for keyword in ["4.2", "million", "15%", "increase"])
        logger.info(f"Revenue query contains relevant information: {has_revenue_info}")
    
    # Overall assessment
    success_rate = (queries_with_sources / len(results)) * 100
    logger.info(f"Overall RAG retrieval success rate: {success_rate:.1f}%")
    
    return {
        "queries_with_sources": queries_with_sources,
        "total_queries": len(results),
        "answers_with_references": answers_with_references,
        "success_rate": success_rate
    }

async def main():
    """Main test function"""
    logger.info("Starting RAG retrieval test...")
    
    try:
        # Create test documents
        markdown_path, pdf_path = await create_test_documents()
        
        # Process documents - this now creates and returns our custom vector store
        markdown_doc, pdf_doc, vector_store = await process_documents(None, markdown_path, pdf_path)
        
        # Initialize RAG engine with our custom vector store
        rag_engine = RAGEngine(vector_store=vector_store)
        
        # Test queries
        results = await test_queries(rag_engine)
        
        # Analyze results
        analysis = await analyze_results(results)
        
        logger.info("RAG retrieval test completed successfully")
        logger.info(f"Success rate: {analysis['success_rate']:.1f}%")
        
    except Exception as e:
        logger.error(f"Error during RAG retrieval test: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: scripts/utils/create_test_user_simple.py
================
#!/usr/bin/env python3
"""
Create a test user for authentication testing

This script creates a test user with a known password directly in the database,
bypassing the API to ensure we have a valid user for testing.
"""

import asyncio
import sys
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from datetime import datetime
import uuid

# Add the project root to the Python path
sys.path.append('.')

from app.db.session import AsyncSessionLocal, init_db
from app.db.models import User
from app.core.security import get_password_hash

async def create_test_user():
    """Create a test user in the database"""
    # Initialize the database
    await init_db()
    
    # Create a session
    async with AsyncSessionLocal() as session:
        # Check if the test user already exists
        result = await session.execute(select(User).where(User.username == 'testuser'))
        existing_user = result.scalars().first()
        
        if existing_user:
            print(f"Test user already exists with ID: {existing_user.id}")
            print(f"Updating password...")
            
            # Update the password
            existing_user.password_hash = get_password_hash("Test@password123")
            await session.commit()
            
            print(f"Password updated for user: {existing_user.username}")
            return
        
        # Create a new test user
        test_user = User(
            id=uuid.uuid4(),
            username="testuser",
            email="testuser@example.com",
            password_hash=get_password_hash("Test@password123"),
            full_name="Test User",
            is_active=True,
            is_admin=False,
            created_at=datetime.utcnow()
        )
        
        # Add the user to the database
        session.add(test_user)
        await session.commit()
        
        print(f"Test user created with ID: {test_user.id}")
        print(f"Username: {test_user.username}")
        print(f"Password: Test@password123")

if __name__ == "__main__":
    asyncio.run(create_test_user())

================
File: scripts/utils/simple_app.py
================
import os
import uuid
from typing import Dict, Any, Optional, List
from pydantic import BaseModel
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# Create FastAPI app
app = FastAPI(title="Metis RAG (Simplified)")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
app.mount("/static", StaticFiles(directory="app/static"), name="static")

# Setup templates
templates = Jinja2Templates(directory="app/templates")

# API prefix
API_V1_STR = "/api"

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """
    Root endpoint that returns the main HTML page
    """
    return templates.TemplateResponse("chat.html", {"request": request})

@app.get("/documents", response_class=HTMLResponse)
async def documents_page(request: Request):
    """
    Documents management page
    """
    return templates.TemplateResponse("documents.html", {"request": request})

@app.get("/system", response_class=HTMLResponse)
async def system_page(request: Request):
    """
    System management page
    """
    return templates.TemplateResponse("system.html", {"request": request})

@app.get("/analytics", response_class=HTMLResponse)
async def analytics_page(request: Request):
    """
    Analytics dashboard page
    """
    return templates.TemplateResponse("analytics.html", {"request": request})

@app.get("/tasks", response_class=HTMLResponse)
async def tasks_page(request: Request):
    """
    Background tasks management page
    """
    return templates.TemplateResponse("tasks.html", {"request": request})

# Pydantic models for API
class ChatQuery(BaseModel):
    message: str
    conversation_id: Optional[str] = None
    model: Optional[str] = None
    use_rag: bool = True
    stream: bool = False
    model_parameters: Optional[Dict[str, Any]] = None
    user_id: Optional[str] = None
    metadata_filters: Optional[Dict[str, Any]] = None

class ChatResponse(BaseModel):
    message: str
    conversation_id: str
    citations: Optional[List[Dict[str, Any]]] = None

# API endpoints
@app.post(f"{API_V1_STR}/chat/query")
async def chat_query(query: ChatQuery):
    """
    Chat query endpoint (mock)
    """
    # Generate a conversation ID if not provided
    conversation_id = query.conversation_id or str(uuid.uuid4())
    
    # Return a mock response
    return ChatResponse(
        message=f"This is a mock response to: {query.message}",
        conversation_id=conversation_id,
        citations=[]
    )

@app.get(f"{API_V1_STR}/documents/list")
async def list_documents():
    """
    List documents endpoint (mock)
    """
    return {"documents": []}

@app.get(f"{API_V1_STR}/documents/tags")
async def get_tags():
    """
    Get tags endpoint (mock)
    """
    return {"tags": []}

@app.get(f"{API_V1_STR}/documents/folders")
async def get_folders():
    """
    Get folders endpoint (mock)
    """
    return {"folders": ["/"]}

@app.get(f"{API_V1_STR}/system/models")
async def get_models():
    """
    Get models endpoint (mock)
    """
    return {"models": [{"name": "gemma3:12b", "description": "Gemma 3 12B model"}]}

@app.get("/health")
async def health_check():
    """
    Health check endpoint
    """
    return {"status": "healthy", "message": "Simplified Metis RAG is running"}

if __name__ == "__main__":
    print("Starting simplified Metis RAG application...")
    uvicorn.run(app, host="127.0.0.1", port=8000)

================
File: scripts/utils/view_visualization.py
================
#!/usr/bin/env python3
"""
Simple script to open the Metis_RAG visualization in a browser.
"""
import os
import webbrowser
import sys
from pathlib import Path

def main():
    """Open the Metis_RAG visualization in a browser."""
    # Get the directory of this script
    script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
    
    # Path to the HTML file
    html_path = script_dir / "metis_rag_visualization.html"
    
    if not html_path.exists():
        print(f"Error: Could not find {html_path}")
        sys.exit(1)
    
    # Convert to URL format
    url = f"file://{html_path.absolute()}"
    
    print(f"Opening Metis_RAG visualization in browser...")
    webbrowser.open(url)
    print(f"If the browser doesn't open automatically, you can open this file manually:")
    print(f"{html_path.absolute()}")

if __name__ == "__main__":
    main()

================
File: scripts/benchmark_database_performance.py
================
#!/usr/bin/env python3
"""
Database Performance Benchmarking Script for Metis RAG

This script benchmarks SQLite vs PostgreSQL performance for various operations:
1. Document insertion and retrieval
2. Chunk storage and retrieval
3. Query performance with different database backends
4. Batch processing performance

Usage:
    python benchmark_database_performance.py --db-type sqlite|postgresql [--html] [--runs 5]
"""
import os
import sys
import json
import time
import asyncio
import argparse
import statistics
import tempfile
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.db.session import Base, engine, SessionLocal
from app.db.repositories.document_repository import DocumentRepository
from app.db.repositories.analytics_repository import AnalyticsRepository
from app.models.document import Document, Chunk
from app.core.config import SETTINGS, DATABASE_TYPE, DATABASE_URL
import uuid
from app.db.models import Document as DBDocument, Chunk as DBChunk
from app.db.adapters import to_uuid_or_str

# Custom adapter function to ensure metadata is a simple dictionary
def custom_pydantic_chunk_to_sqlalchemy(chunk, document_id):
    """
    Convert Pydantic Chunk to SQLAlchemy Chunk with proper metadata handling.
    
    Args:
        chunk: Pydantic Chunk model
        document_id: ID of the parent document
        
    Returns:
        SQLAlchemy Chunk model
    """
    # Handle UUID conversion based on database type
    chunk_id = to_uuid_or_str(chunk.id)
    doc_id = to_uuid_or_str(document_id)
    
    # Extract index from metadata or default to 0
    index = chunk.metadata.get('index', 0) if chunk.metadata else 0
    
    # Ensure metadata is a simple dictionary
    if not isinstance(chunk.metadata, dict):
        metadata = dict(chunk.metadata) if chunk.metadata else {}
    else:
        metadata = chunk.metadata
    
    sqlalchemy_chunk = DBChunk(
        id=chunk_id,
        document_id=doc_id,
        content=chunk.content,
        chunk_metadata=metadata,  # Use our sanitized metadata
        index=index,
        embedding_quality=None  # No embedding quality for benchmark
    )
    return sqlalchemy_chunk

# Custom function to save document with chunks
def custom_save_document_with_chunks(session, document):
    """
    Save a document with its chunks using custom adapter function
    
    Args:
        session: SQLAlchemy session
        document: Pydantic Document model
        
    Returns:
        Saved document (Pydantic model)
    """
    from app.db.adapters import (
        pydantic_document_to_sqlalchemy,
        sqlalchemy_document_to_pydantic,
        is_sqlalchemy_model
    )
    
    try:
        # Convert to SQLAlchemy model if needed
        if not is_sqlalchemy_model(document):
            db_document = pydantic_document_to_sqlalchemy(document)
        else:
            db_document = document
        
        # Check if document exists
        existing = session.query(DBDocument).filter(
            DBDocument.id == db_document.id
        ).first()
        
        if existing:
            # Update existing document
            existing.filename = db_document.filename
            existing.content = db_document.content
            existing.doc_metadata = db_document.doc_metadata
            existing.folder = db_document.folder
            existing.last_accessed = datetime.utcnow()
            existing.processing_status = db_document.processing_status
            existing.processing_strategy = db_document.processing_strategy
            
            # Delete existing chunks
            session.query(DBChunk).filter(
                DBChunk.document_id == existing.id
            ).delete()
            
            # Add new chunks
            if hasattr(document, 'chunks') and document.chunks:
                for i, chunk in enumerate(document.chunks):
                    # Create new chunk using our custom adapter
                    new_chunk = custom_pydantic_chunk_to_sqlalchemy(chunk, existing.id)
                    session.add(new_chunk)
                    
            target_document = existing
        else:
            # For new documents, we need to ensure we don't have ID conflicts
            # First check if a document with this ID already exists
            check_existing = session.query(DBDocument).filter(
                DBDocument.id == db_document.id
            ).first()
            
            if check_existing:
                # Generate a new ID
                db_document.id = uuid.uuid4()
            
            # Add new document
            session.add(db_document)
            session.flush()  # Flush to get the document ID
            
            # Add chunks
            if hasattr(document, 'chunks') and document.chunks:
                for i, chunk in enumerate(document.chunks):
                    # Create new chunk using our custom adapter
                    new_chunk = custom_pydantic_chunk_to_sqlalchemy(chunk, db_document.id)
                    session.add(new_chunk)
                    
            target_document = db_document
        
        # Commit changes
        session.commit()
        
        # Refresh and convert back to Pydantic
        session.refresh(target_document)
        return sqlalchemy_document_to_pydantic(target_document)
    except Exception as e:
        session.rollback()
        print(f"Error saving document with chunks: {str(e)}")
        raise

# Test configuration
TEST_FILE_SIZES = [
    ("small", 5),      # 5 KB
    ("medium", 50),    # 50 KB
    ("large", 500),    # 500 KB
    ("xlarge", 2000)   # 2 MB
]

TEST_CHUNKING_STRATEGIES = [
    "recursive",
    "token"
]

TEST_QUERIES = [
    "What is the main purpose of the RAG system?",
    "How does the document analysis service work?",
    "What are the key components of the LangGraph integration?",
    "Explain the database schema for documents and chunks",
    "How does the memory integration enhance the system?"
]

# Batch sizes for testing
BATCH_SIZES = [1, 5, 10, 20]

class SimpleDocumentProcessor:
    """Simple document processor for benchmarking"""
    
    def __init__(self, chunk_size=1500, chunk_overlap=150):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    async def process_document(self, document: Document) -> Document:
        """Process a document by splitting it into chunks"""
        # Create a simple chunking strategy
        content = document.content
        chunks = []
        
        # Simple chunking by character count with overlap
        for i in range(0, len(content), self.chunk_size - self.chunk_overlap):
            chunk_content = content[i:i + self.chunk_size]
            if chunk_content:
                # Create a simple dictionary for metadata
                chunk_metadata = {
                    "document_id": str(document.id),
                    "index": len(chunks),
                    "strategy": "simple"
                }
                
                chunks.append(
                    Chunk(
                        content=chunk_content,
                        metadata=chunk_metadata
                    )
                )
        
        # Add chunks to document
        document.chunks = chunks
        
        # Update document metadata
        if not document.metadata:
            document.metadata = {}
        document.metadata["chunking"] = {
            "strategy": "simple",
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
            "chunk_count": len(chunks)
        }
        
        return document

class DatabaseBenchmark:
    """Database performance benchmarking class"""
    
    def __init__(self, db_type: str, num_runs: int = 3):
        self.db_type = db_type
        self.num_runs = num_runs
        self.results = {
            "metadata": {
                "db_type": db_type,
                "timestamp": datetime.now().isoformat(),
                "num_runs": num_runs
            },
            "document_operations": [],
            "chunk_operations": [],
            "query_performance": [],
            "batch_processing": []
        }
        
        # Initialize database session
        self.db_session = SessionLocal()
        
        # Initialize repositories and services
        self.document_repository = DocumentRepository(self.db_session)
        self.analytics_repository = AnalyticsRepository(self.db_session)
        self.document_processor = SimpleDocumentProcessor()
        
        # Create folders in database
        self._ensure_folders_exist()
        
        print(f"Initialized benchmark for {db_type} database")
        print(f"Database URL: {DATABASE_URL}")
        
    def _ensure_folders_exist(self):
        """Ensure the root and benchmark folders exist in the database"""
        try:
            # Import Folder model
            from app.db.models import Folder
            
            # Check if root folder exists
            root_folder = self.db_session.query(Folder).filter(Folder.path == "/").first()
            if not root_folder:
                # Create root folder
                root_folder = Folder(
                    path="/",
                    name="Root",
                    parent_path=None
                )
                self.db_session.add(root_folder)
                self.db_session.commit()
                print("Created root folder in database")
            
            # Check if benchmark folder exists
            benchmark_folder = self.db_session.query(Folder).filter(Folder.path == "/benchmark").first()
            if not benchmark_folder:
                # Create benchmark folder
                benchmark_folder = Folder(
                    path="/benchmark",
                    name="Benchmark",
                    parent_path="/"
                )
                self.db_session.add(benchmark_folder)
                self.db_session.commit()
                print("Created benchmark folder in database")
        except Exception as e:
            self.db_session.rollback()
            print(f"Error creating folders: {e}")
        
    def cleanup(self):
        """Clean up resources"""
        self.db_session.close()
        
    def generate_test_file(self, size_kb: int, file_type: str = "txt"):
        """Generate a test file of the specified size"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix=f".{file_type}")
        
        try:
            # Generate content
            content = ""
            if file_type == "txt":
                # Generate paragraphs of text
                paragraph = "This is a test paragraph for document processing performance testing. " * 10
                paragraphs_needed = (size_kb * 1024) // len(paragraph)
                content = "\n\n".join([paragraph for _ in range(paragraphs_needed)])
            elif file_type == "md":
                # Generate markdown content
                paragraph = "This is a test paragraph for document processing performance testing. " * 10
                paragraphs_needed = (size_kb * 1024) // (len(paragraph) + 20)  # Account for markdown syntax
                
                content = "# Test Document\n\n"
                for i in range(paragraphs_needed):
                    content += f"## Section {i+1}\n\n{paragraph}\n\n"
            
            # Write content to file
            with os.fdopen(fd, 'w') as f:
                f.write(content)
            
            return path
        except Exception as e:
            os.close(fd)
            os.unlink(path)
            raise e
    
    async def benchmark_document_operations(self):
        """Benchmark document CRUD operations"""
        print("\nBenchmarking document operations...")
        
        results = []
        
        # Ensure test folder exists
        self.document_repository._ensure_folder_exists("/benchmark")
        
        for size_name, size_kb in TEST_FILE_SIZES:
            # Generate test file
            file_path = self.generate_test_file(size_kb, "txt")
            file_name = f"benchmark_{size_name}_{self.db_type}.txt"
            
            try:
                # Read file content
                with open(file_path, 'r') as f:
                    content = f.read()
                
                # Benchmark document creation
                create_times = []
                document_ids = []
                
                for _ in range(self.num_runs):
                    # Measure document creation time
                    start_time = time.time()
                    
                    document = self.document_repository.create_document(
                        filename=file_name,
                        content=content,
                        metadata={"file_type": "txt", "test_size": size_name, "benchmark": True},
                        folder="/benchmark"
                    )
                    
                    create_time = time.time() - start_time
                    create_times.append(create_time)
                    document_ids.append(str(document.id))
                
                # Benchmark document retrieval
                retrieve_times = []
                
                for doc_id in document_ids:
                    # Measure document retrieval time
                    start_time = time.time()
                    
                    document = self.document_repository.get_document_with_chunks(doc_id)
                    
                    retrieve_time = time.time() - start_time
                    retrieve_times.append(retrieve_time)
                
                # Benchmark document update
                update_times = []
                
                for doc_id in document_ids:
                    # Measure document update time
                    start_time = time.time()
                    
                    document = self.document_repository.get_document_with_chunks(doc_id)
                    # Create a copy of metadata and update it
                    updated_metadata = document.metadata.copy()
                    updated_metadata["updated"] = True
                    # Use update_document with the document ID
                    self.document_repository.update_document(
                        document_id=doc_id,
                        metadata=updated_metadata
                    )
                    
                    update_time = time.time() - start_time
                    update_times.append(update_time)
                
                # Benchmark document deletion
                delete_times = []
                
                for doc_id in document_ids:
                    # Measure document deletion time
                    start_time = time.time()
                    
                    self.document_repository.delete_document(doc_id)
                    
                    delete_time = time.time() - start_time
                    delete_times.append(delete_time)
                
                # Calculate average metrics
                avg_create_time = statistics.mean(create_times)
                avg_retrieve_time = statistics.mean(retrieve_times)
                avg_update_time = statistics.mean(update_times)
                avg_delete_time = statistics.mean(delete_times)
                
                results.append({
                    "size_name": size_name,
                    "size_kb": size_kb,
                    "avg_create_time": avg_create_time,
                    "avg_retrieve_time": avg_retrieve_time,
                    "avg_update_time": avg_update_time,
                    "avg_delete_time": avg_delete_time,
                    "total_avg_time": avg_create_time + avg_retrieve_time + avg_update_time + avg_delete_time,
                    "num_runs": self.num_runs
                })
                
                print(f"  {size_name} ({size_kb} KB): Create: {avg_create_time:.4f}s, Retrieve: {avg_retrieve_time:.4f}s, Update: {avg_update_time:.4f}s, Delete: {avg_delete_time:.4f}s")
                
            finally:
                # Clean up test file
                try:
                    os.unlink(file_path)
                except:
                    pass
        
        self.results["document_operations"] = results
        return results
    
    async def benchmark_chunk_operations(self):
        """Benchmark chunk operations"""
        print("\nBenchmarking chunk operations...")
        
        results = []
        
        # Ensure test folder exists
        self.document_repository._ensure_folder_exists("/benchmark")
        
        for size_name, size_kb in TEST_FILE_SIZES:
            for strategy in TEST_CHUNKING_STRATEGIES:
                # Generate test file
                file_path = self.generate_test_file(size_kb, "txt")
                file_name = f"benchmark_chunk_{size_name}_{strategy}_{self.db_type}.txt"
                
                try:
                    # Read file content
                    with open(file_path, 'r') as f:
                        content = f.read()
                    
                    # Create document in the database
                    document = self.document_repository.create_document(
                        filename=file_name,
                        content=content,
                        metadata={"file_type": "txt", "test_size": size_name, "benchmark": True},
                        folder="/benchmark"
                    )
                    
                    # Get document ID as string
                    document_id_str = str(document.id)
                    
                    # Process document to generate chunks
                    processed_document = await self.document_processor.process_document(document)
                    
                    # Ensure all chunk metadata is a simple dictionary
                    for chunk in processed_document.chunks:
                        if not isinstance(chunk.metadata, dict):
                            chunk.metadata = dict(chunk.metadata) if chunk.metadata else {}
                    
                    # Benchmark chunk insertion
                    start_time = time.time()
                    
                    # Save document with chunks using our custom function
                    custom_save_document_with_chunks(self.db_session, processed_document)
                    
                    chunk_insert_time = time.time() - start_time
                    
                    # Count chunks
                    chunk_count = len(processed_document.chunks)
                    
                    # Benchmark chunk retrieval
                    start_time = time.time()
                    
                    # Get document with chunks
                    retrieved_document = self.document_repository.get_document_with_chunks(document_id_str)
                    
                    chunk_retrieve_time = time.time() - start_time
                    
                    # Benchmark chunk update
                    start_time = time.time()
                    
                    # Update chunks
                    for i, chunk in enumerate(retrieved_document.chunks):
                        chunk.metadata["updated"] = True
                        self.document_repository.update_chunk(chunk)
                    
                    chunk_update_time = time.time() - start_time
                    
                    # Calculate per-chunk times
                    per_chunk_insert_time = chunk_insert_time / chunk_count if chunk_count > 0 else 0
                    per_chunk_retrieve_time = chunk_retrieve_time / chunk_count if chunk_count > 0 else 0
                    per_chunk_update_time = chunk_update_time / chunk_count if chunk_count > 0 else 0
                    
                    results.append({
                        "size_name": size_name,
                        "size_kb": size_kb,
                        "chunking_strategy": strategy,
                        "chunk_count": chunk_count,
                        "chunk_insert_time": chunk_insert_time,
                        "chunk_retrieve_time": chunk_retrieve_time,
                        "chunk_update_time": chunk_update_time,
                        "per_chunk_insert_time": per_chunk_insert_time,
                        "per_chunk_retrieve_time": per_chunk_retrieve_time,
                        "per_chunk_update_time": per_chunk_update_time
                    })
                    
                    print(f"  {size_name} ({size_kb} KB), {strategy}: Chunks: {chunk_count}, Insert: {chunk_insert_time:.4f}s, Retrieve: {chunk_retrieve_time:.4f}s, Update: {chunk_update_time:.4f}s")
                    
                    # Clean up - delete document and chunks
                    self.document_repository.delete_document(document_id_str)
                    
                finally:
                    # Clean up test file
                    try:
                        os.unlink(file_path)
                    except:
                        pass
        
        self.results["chunk_operations"] = results
        return results
    
    async def benchmark_query_performance(self):
        """Benchmark query performance"""
        print("\nBenchmarking query performance...")
        
        results = []
        
        # Load test documents if needed
        await self._load_test_documents()
        
        # Run queries
        for query in TEST_QUERIES:
            query_times = []
            token_counts = []
            source_counts = []
            
            for _ in range(self.num_runs):
                # Measure query time
                start_time = time.time()
                
                # Simple search instead of RAG query
                documents = self.document_repository.search_documents(query, limit=10)
                
                query_time = time.time() - start_time
                query_times.append(query_time)
                token_counts.append(len(query.split()))
                source_counts.append(len(documents))
            
            # Calculate average metrics
            avg_query_time = statistics.mean(query_times)
            avg_token_count = statistics.mean(token_counts)
            avg_source_count = statistics.mean(source_counts)
            
            results.append({
                "query": query,
                "avg_query_time": avg_query_time,
                "avg_token_count": avg_token_count,
                "avg_source_count": avg_source_count,
                "num_runs": self.num_runs
            })
            
            print(f"  Query: '{query[:50]}...': Time: {avg_query_time:.4f}s, Tokens: {avg_token_count:.0f}, Sources: {avg_source_count:.0f}")
        
        self.results["query_performance"] = results
        return results
    
    async def benchmark_batch_processing(self):
        """Benchmark batch document processing"""
        print("\nBenchmarking batch processing...")
        
        results = []
        
        # Ensure test folder exists
        self.document_repository._ensure_folder_exists("/benchmark")
        
        for batch_size in BATCH_SIZES:
            # Generate documents for the batch
            document_ids = []
            file_paths = []
            
            for i in range(batch_size):
                # Generate a medium-sized test file
                file_path = self.generate_test_file(50, "txt")  # 50 KB
                file_name = f"benchmark_batch_{i}_{self.db_type}.txt"
                file_paths.append(file_path)
                
                # Read file content
                with open(file_path, 'r') as f:
                    content = f.read()
                
                # Create document in the database
                document = self.document_repository.create_document(
                    filename=file_name,
                    content=content,
                    metadata={"file_type": "txt", "batch_test": True},
                    folder="/benchmark"
                )
                
                # Get document ID as string
                document_id_str = str(document.id)
                document_ids.append(document_id_str)
            
            try:
                # Measure batch processing time
                start_time = time.time()
                
                # Process each document
                for doc_id in document_ids:
                    document = self.document_repository.get_document_with_chunks(doc_id)
                    processed_document = await self.document_processor.process_document(document)
                    
                    # Ensure all chunk metadata is a simple dictionary
                    for chunk in processed_document.chunks:
                        if not isinstance(chunk.metadata, dict):
                            chunk.metadata = dict(chunk.metadata) if chunk.metadata else {}
                    
                    custom_save_document_with_chunks(self.db_session, processed_document)
                
                batch_time = time.time() - start_time
                
                # Calculate per-document time
                per_document_time = batch_time / batch_size
                
                results.append({
                    "batch_size": batch_size,
                    "total_time": batch_time,
                    "per_document_time": per_document_time
                })
                
                print(f"  Batch size {batch_size}: Total: {batch_time:.4f}s, Per document: {per_document_time:.4f}s")
                
                # Clean up - delete documents
                for doc_id in document_ids:
                    self.document_repository.delete_document(doc_id)
                
            finally:
                # Clean up test files
                for file_path in file_paths:
                    try:
                        os.unlink(file_path)
                    except:
                        pass
        
        self.results["batch_processing"] = results
        return results
    
    async def _load_test_documents(self):
        """Load test documents for query benchmarking if needed"""
        # Check if we have documents in the database
        # Use search_documents instead of list_documents
        documents = self.document_repository.search_documents("", limit=5)
        
        if len(documents) >= 5:
            print("  Using existing documents for query benchmarking")
            return
        
        print("  Loading test documents for query benchmarking...")
        
        # Ensure test folder exists
        self.document_repository._ensure_folder_exists("/benchmark")
        
        # Generate and process test documents
        for i, (size_name, size_kb) in enumerate(TEST_FILE_SIZES[:2]):  # Use only small and medium
            # Generate test files in different formats
            for file_type in ["txt", "md"]:
                file_path = self.generate_test_file(size_kb, file_type)
                file_name = f"benchmark_query_{i}_{size_name}.{file_type}"
                
                try:
                    # Read file content
                    with open(file_path, 'r') as f:
                        content = f.read()
                    
                    # Create document in the database
                    document = self.document_repository.create_document(
                        filename=file_name,
                        content=content,
                        metadata={"file_type": file_type, "test_size": size_name, "benchmark": True},
                        folder="/benchmark"
                    )
                    
                    # Get document ID as string
                    document_id_str = str(document.id)
                    
                    # Process document
                    processed_document = await self.document_processor.process_document(document)
                    
                    # Ensure all chunk metadata is a simple dictionary
                    for chunk in processed_document.chunks:
                        if not isinstance(chunk.metadata, dict):
                            chunk.metadata = dict(chunk.metadata) if chunk.metadata else {}
                    
                    custom_save_document_with_chunks(self.db_session, processed_document)
                    
                finally:
                    # Clean up test file
                    try:
                        os.unlink(file_path)
                    except:
                        pass
    
    def save_results(self, output_dir: str = None):
        """Save benchmark results to a JSON file"""
        if output_dir is None:
            output_dir = os.path.join(project_root, "tests", "results")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Create filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"db_benchmark_{self.db_type}_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)
        
        # Save results to file
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"\nResults saved to: {filepath}")
        return filepath

def generate_html_report(sqlite_results, postgres_results, output_path):
    """Generate HTML comparison report"""
    # Create HTML report
    html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Database Performance Comparison: SQLite vs PostgreSQL</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            h1, h2, h3 { color: #333; }
            table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; }
            tr:nth-child(even) { background-color: #f9f9f9; }
            .chart-container { width: 100%; height: 400px; margin-bottom: 30px; }
            .winner { font-weight: bold; color: green; }
            .loser { color: #d9534f; }
            .summary { background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
        </style>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <h1>Database Performance Comparison: SQLite vs PostgreSQL</h1>
        <p>Generated on: """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """</p>
        
        <div class="summary">
            <h2>Executive Summary</h2>
            <p>This report compares the performance of SQLite and PostgreSQL for the Metis RAG system across various operations:</p>
            <ul>
                <li>Document CRUD operations</li>
                <li>Chunk storage and retrieval</li>
                <li>Query performance</li>
                <li>Batch processing</li>
            </ul>
        </div>
        
        <h2>1. Document Operations</h2>
        <div class="chart-container">
            <canvas id="documentChart"></canvas>
        </div>
        
        <h3>Document Operations Comparison</h3>
        <table>
            <tr>
                <th>File Size</th>
                <th>Operation</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    """
    
    # Add document operations comparison
    sqlite_doc_ops = {f"{r['size_name']}": r for r in sqlite_results["document_operations"]}
    postgres_doc_ops = {f"{r['size_name']}": r for r in postgres_results["document_operations"]}
    
    for size_name, size_kb in TEST_FILE_SIZES:
        if size_name in sqlite_doc_ops and size_name in postgres_doc_ops:
            sqlite_data = sqlite_doc_ops[size_name]
            postgres_data = postgres_doc_ops[size_name]
            
            operations = [
                ("Create", "avg_create_time"),
                ("Retrieve", "avg_retrieve_time"),
                ("Update", "avg_update_time"),
                ("Delete", "avg_delete_time"),
                ("Total", "total_avg_time")
            ]
            
            for op_name, op_key in operations:
                sqlite_time = sqlite_data[op_key]
                postgres_time = postgres_data[op_key]
                
                # Calculate difference and determine winner
                diff_pct = ((postgres_time - sqlite_time) / sqlite_time) * 100
                faster_db = "SQLite" if sqlite_time < postgres_time else "PostgreSQL"
                
                # Format cells with winner/loser styling
                sqlite_cell = f'<td class="winner">{sqlite_time:.4f}</td>' if faster_db == "SQLite" else f'<td class="loser">{sqlite_time:.4f}</td>'
                postgres_cell = f'<td class="winner">{postgres_time:.4f}</td>' if faster_db == "PostgreSQL" else f'<td class="loser">{postgres_time:.4f}</td>'
                
                html += f"""
                <tr>
                    <td>{size_name} ({size_kb} KB)</td>
                    <td>{op_name}</td>
                    {sqlite_cell}
                    {postgres_cell}
                    <td>{abs(diff_pct):.2f}%</td>
                    <td>{faster_db}</td>
                </tr>
                """
    
    html += """
        </table>
        
        <h2>2. Chunk Operations</h2>
        <div class="chart-container">
            <canvas id="chunkChart"></canvas>
        </div>
        
        <h3>Chunk Operations Comparison</h3>
        <table>
            <tr>
                <th>File Size</th>
                <th>Strategy</th>
                <th>Operation</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    """
    
    # Add chunk operations comparison
    sqlite_chunk_ops = sqlite_results["chunk_operations"]
    postgres_chunk_ops = postgres_results["chunk_operations"]
    
    # Create lookup dictionaries
    sqlite_chunk_dict = {f"{r['size_name']}_{r['chunking_strategy']}": r for r in sqlite_chunk_ops}
    postgres_chunk_dict = {f"{r['size_name']}_{r['chunking_strategy']}": r for r in postgres_chunk_ops}
    
    for size_name, size_kb in TEST_FILE_SIZES:
        for strategy in TEST_CHUNKING_STRATEGIES:
            key = f"{size_name}_{strategy}"
            
            if key in sqlite_chunk_dict and key in postgres_chunk_dict:
                sqlite_data = sqlite_chunk_dict[key]
                postgres_data = postgres_chunk_dict[key]
                
                operations = [
                    ("Insert", "chunk_insert_time"),
                    ("Retrieve", "chunk_retrieve_time"),
                    ("Update", "chunk_update_time"),
                    ("Per-Chunk Insert", "per_chunk_insert_time"),
                    ("Per-Chunk Retrieve", "per_chunk_retrieve_time"),
                    ("Per-Chunk Update", "per_chunk_update_time")
                ]
                
                for op_name, op_key in operations:
                    sqlite_time = sqlite_data[op_key]
                    postgres_time = postgres_data[op_key]
                    
                    # Calculate difference and determine winner
                    if sqlite_time > 0:  # Avoid division by zero
                        diff_pct = ((postgres_time - sqlite_time) / sqlite_time) * 100
                    else:
                        diff_pct = 0
                        
                    faster_db = "SQLite" if sqlite_time < postgres_time else "PostgreSQL"
                    
                    # Format cells with winner/loser styling
                    sqlite_cell = f'<td class="winner">{sqlite_time:.4f}</td>' if faster_db == "SQLite" else f'<td class="loser">{sqlite_time:.4f}</td>'
                    postgres_cell = f'<td class="winner">{postgres_time:.4f}</td>' if faster_db == "PostgreSQL" else f'<td class="loser">{postgres_time:.4f}</td>'
                    
                    html += f"""
                    <tr>
                        <td>{size_name} ({size_kb} KB)</td>
                        <td>{strategy}</td>
                        <td>{op_name}</td>
                        {sqlite_cell}
                        {postgres_cell}
                        <td>{abs(diff_pct):.2f}%</td>
                        <td>{faster_db}</td>
                    </tr>
                    """
    
    html += """
        </table>
        
        <h2>3. Query Performance</h2>
        <div class="chart-container">
            <canvas id="queryChart"></canvas>
        </div>
        
        <h3>Query Performance Comparison</h3>
        <table>
            <tr>
                <th>Query</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    """
    
    # Add query performance comparison
    sqlite_query_ops = {r["query"]: r for r in sqlite_results["query_performance"]}
    postgres_query_ops = {r["query"]: r for r in postgres_results["query_performance"]}
    
    for query in TEST_QUERIES:
        if query in sqlite_query_ops and query in postgres_query_ops:
            sqlite_data = sqlite_query_ops[query]
            postgres_data = postgres_query_ops[query]
            
            sqlite_time = sqlite_data["avg_query_time"]
            postgres_time = postgres_data["avg_query_time"]
            
            # Calculate difference and determine winner
            diff_pct = ((postgres_time - sqlite_time) / sqlite_time) * 100
            faster_db = "SQLite" if sqlite_time < postgres_time else "PostgreSQL"
            
            # Format cells with winner/loser styling
            sqlite_cell = f'<td class="winner">{sqlite_time:.4f}</td>' if faster_db == "SQLite" else f'<td class="loser">{sqlite_time:.4f}</td>'
            postgres_cell = f'<td class="winner">{postgres_time:.4f}</td>' if faster_db == "PostgreSQL" else f'<td class="loser">{postgres_time:.4f}</td>'
            
            # Truncate query if too long
            query_display = query[:50] + "..." if len(query) > 50 else query
            
            html += f"""
            <tr>
                <td>{query_display}</td>
                {sqlite_cell}
                {postgres_cell}
                <td>{abs(diff_pct):.2f}%</td>
                <td>{faster_db}</td>
            </tr>
            """
    
    html += """
        </table>
        
        <h2>4. Batch Processing</h2>
        <div class="chart-container">
            <canvas id="batchChart"></canvas>
        </div>
        
        <h3>Batch Processing Comparison</h3>
        <table>
            <tr>
                <th>Batch Size</th>
                <th>Metric</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    """
    
    # Add batch processing comparison
    sqlite_batch_ops = {r["batch_size"]: r for r in sqlite_results["batch_processing"]}
    postgres_batch_ops = {r["batch_size"]: r for r in postgres_results["batch_processing"]}
    
    for batch_size in BATCH_SIZES:
        if batch_size in sqlite_batch_ops and batch_size in postgres_batch_ops:
            sqlite_data = sqlite_batch_ops[batch_size]
            postgres_data = postgres_batch_ops[batch_size]
            
            metrics = [
                ("Total Time", "total_time"),
                ("Per-Document Time", "per_document_time")
            ]
            
            for metric_name, metric_key in metrics:
                sqlite_time = sqlite_data[metric_key]
                postgres_time = postgres_data[metric_key]
                
                # Calculate difference and determine winner
                diff_pct = ((postgres_time - sqlite_time) / sqlite_time) * 100
                faster_db = "SQLite" if sqlite_time < postgres_time else "PostgreSQL"
                
                # Format cells with winner/loser styling
                sqlite_cell = f'<td class="winner">{sqlite_time:.4f}</td>' if faster_db == "SQLite" else f'<td class="loser">{sqlite_time:.4f}</td>'
                postgres_cell = f'<td class="winner">{postgres_time:.4f}</td>' if faster_db == "PostgreSQL" else f'<td class="loser">{postgres_time:.4f}</td>'
                
                html += f"""
                <tr>
                    <td>{batch_size}</td>
                    <td>{metric_name}</td>
                    {sqlite_cell}
                    {postgres_cell}
                    <td>{abs(diff_pct):.2f}%</td>
                    <td>{faster_db}</td>
                </tr>
                """
    
    # Prepare data for charts
    sqlite_doc_create_times = [sqlite_doc_ops.get(size[0], {}).get("avg_create_time", 0) for size in TEST_FILE_SIZES]
    postgres_doc_create_times = [postgres_doc_ops.get(size[0], {}).get("avg_create_time", 0) for size in TEST_FILE_SIZES]
    
    sqlite_doc_retrieve_times = [sqlite_doc_ops.get(size[0], {}).get("avg_retrieve_time", 0) for size in TEST_FILE_SIZES]
    postgres_doc_retrieve_times = [postgres_doc_ops.get(size[0], {}).get("avg_retrieve_time", 0) for size in TEST_FILE_SIZES]
    
    # Prepare query data
    query_labels = [f"Query {i+1}" for i in range(len(TEST_QUERIES))]
    sqlite_query_times = [sqlite_query_ops.get(query, {}).get("avg_query_time", 0) for query in TEST_QUERIES]
    postgres_query_times = [postgres_query_ops.get(query, {}).get("avg_query_time", 0) for query in TEST_QUERIES]
    
    # Prepare batch data
    sqlite_batch_times = [sqlite_batch_ops.get(size, {}).get("total_time", 0) for size in BATCH_SIZES]
    postgres_batch_times = [postgres_batch_ops.get(size, {}).get("total_time", 0) for size in BATCH_SIZES]
    
    # Calculate overall recommendations
    sqlite_doc_total = sum(sqlite_doc_create_times) + sum(sqlite_doc_retrieve_times)
    postgres_doc_total = sum(postgres_doc_create_times) + sum(postgres_doc_retrieve_times)
    
    sqlite_chunk_total = sum([r.get("chunk_insert_time", 0) for r in sqlite_chunk_ops])
    postgres_chunk_total = sum([r.get("chunk_insert_time", 0) for r in postgres_chunk_ops])
    
    sqlite_query_total = sum(sqlite_query_times)
    postgres_query_total = sum(postgres_query_times)
    
    sqlite_batch_total = sum(sqlite_batch_times)
    postgres_batch_total = sum(postgres_batch_times)
    
    doc_winner = "SQLite" if sqlite_doc_total < postgres_doc_total else "PostgreSQL"
    chunk_winner = "SQLite" if sqlite_chunk_total < postgres_chunk_total else "PostgreSQL"
    query_winner = "SQLite" if sqlite_query_total < postgres_query_total else "PostgreSQL"
    batch_winner = "SQLite" if sqlite_batch_total < postgres_batch_total else "PostgreSQL"
    
    overall_sqlite = sqlite_doc_total + sqlite_chunk_total + sqlite_query_total + sqlite_batch_total
    overall_postgres = postgres_doc_total + postgres_chunk_total + postgres_query_total + postgres_batch_total
    overall_winner = "SQLite" if overall_sqlite < overall_postgres else "PostgreSQL"
    
    # Convert data to JSON strings for JavaScript
    file_size_labels = [size[0] for size in TEST_FILE_SIZES]
    
    html += f"""
        </table>
        
        <h2>5. Recommendations</h2>
        <div class="summary">
            <p>Based on the benchmark results, here are some recommendations:</p>
            <ul>
                <li><strong>Document Operations:</strong> {doc_winner} performs better for basic document operations.</li>
                <li><strong>Chunk Operations:</strong> {chunk_winner} is more efficient for chunk storage and retrieval.</li>
                <li><strong>Query Performance:</strong> {query_winner} provides faster query response times.</li>
                <li><strong>Batch Processing:</strong> {batch_winner} handles batch processing more efficiently.</li>
            </ul>
            <p><strong>Overall Recommendation:</strong> {overall_winner} appears to be the better choice for this workload based on overall performance.</p>
        </div>
        
        <script>
            // Document operations chart
            const docCtx = document.getElementById('documentChart').getContext('2d');
            const docChart = new Chart(docCtx, {{
                type: 'bar',
                data: {{
                    labels: {json.dumps(file_size_labels)},
                    datasets: [
                        {{
                            label: 'SQLite - Create',
                            data: {json.dumps(sqlite_doc_create_times)},
                            backgroundColor: 'rgba(54, 162, 235, 0.6)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }},
                        {{
                            label: 'PostgreSQL - Create',
                            data: {json.dumps(postgres_doc_create_times)},
                            backgroundColor: 'rgba(255, 99, 132, 0.6)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        }},
                        {{
                            label: 'SQLite - Retrieve',
                            data: {json.dumps(sqlite_doc_retrieve_times)},
                            backgroundColor: 'rgba(75, 192, 192, 0.6)',
                            borderColor: 'rgba(75, 192, 192, 1)',
                            borderWidth: 1
                        }},
                        {{
                            label: 'PostgreSQL - Retrieve',
                            data: {json.dumps(postgres_doc_retrieve_times)},
                            backgroundColor: 'rgba(255, 159, 64, 0.6)',
                            borderColor: 'rgba(255, 159, 64, 1)',
                            borderWidth: 1
                        }}
                    ]
                }},
                options: {{
                    responsive: true,
                    plugins: {{
                        title: {{
                            display: true,
                            text: 'Document Operation Times by File Size'
                        }},
                    }},
                    scales: {{
                        y: {{
                            beginAtZero: true,
                            title: {{
                                display: true,
                                text: 'Time (seconds)'
                            }}
                        }},
                        x: {{
                            title: {{
                                display: true,
                                text: 'File Size'
                            }}
                        }}
                    }}
                }}
            }});
            
            // Query performance chart
            const queryCtx = document.getElementById('queryChart').getContext('2d');
            const queryChart = new Chart(queryCtx, {{
                type: 'bar',
                data: {{
                    labels: {json.dumps(query_labels)},
                    datasets: [
                        {{
                            label: 'SQLite',
                            data: {json.dumps(sqlite_query_times)},
                            backgroundColor: 'rgba(54, 162, 235, 0.6)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }},
                        {{
                            label: 'PostgreSQL',
                            data: {json.dumps(postgres_query_times)},
                            backgroundColor: 'rgba(255, 99, 132, 0.6)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        }}
                    ]
                }},
                options: {{
                    responsive: true,
                    plugins: {{
                        title: {{
                            display: true,
                            text: 'Query Response Times'
                        }},
                    }},
                    scales: {{
                        y: {{
                            beginAtZero: true,
                            title: {{
                                display: true,
                                text: 'Time (seconds)'
                            }}
                        }},
                        x: {{
                            title: {{
                                display: true,
                                text: 'Query'
                            }}
                        }}
                    }}
                }}
            }});
            
            // Batch processing chart
            const batchCtx = document.getElementById('batchChart').getContext('2d');
            const batchChart = new Chart(batchCtx, {{
                type: 'bar',
                data: {{
                    labels: {json.dumps(BATCH_SIZES)},
                    datasets: [
                        {{
                            label: 'SQLite',
                            data: {json.dumps(sqlite_batch_times)},
                            backgroundColor: 'rgba(54, 162, 235, 0.6)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }},
                        {{
                            label: 'PostgreSQL',
                            data: {json.dumps(postgres_batch_times)},
                            backgroundColor: 'rgba(255, 99, 132, 0.6)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        }}
                    ]
                }},
                options: {{
                    responsive: true,
                    plugins: {{
                        title: {{
                            display: true,
                            text: 'Batch Processing Times'
                        }},
                    }},
                    scales: {{
                        y: {{
                            beginAtZero: true,
                            title: {{
                                display: true,
                                text: 'Time (seconds)'
                            }}
                        }},
                        x: {{
                            title: {{
                                display: true,
                                text: 'Batch Size'
                            }}
                        }}
                    }}
                }}
            }});
        </script>
    </body>
    </html>
    """
    
    # Save HTML report
    with open(output_path, 'w') as f:
        f.write(html)
    
    print(f"\nHTML comparison report saved to: {output_path}")

async def run_benchmark(args):
    """Run the database benchmark"""
    print(f"\nRunning database performance benchmark for {args.db_type}...")
    
    # Create benchmark instance
    benchmark = DatabaseBenchmark(args.db_type, args.runs)
    
    try:
        # Run benchmarks
        await benchmark.benchmark_document_operations()
        await benchmark.benchmark_chunk_operations()
        await benchmark.benchmark_query_performance()
        await benchmark.benchmark_batch_processing()
        
        # Save results
        results_file = benchmark.save_results()
        
        # Store results file path for HTML report generation
        if args.db_type == "sqlite":
            args.sqlite_results = results_file
        else:
            args.postgres_results = results_file
        
        print("\nBenchmark completed successfully!")
        return 0
    except Exception as e:
        print(f"Error running benchmark: {e}")
        return 1
    finally:
        benchmark.cleanup()

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Database Performance Benchmarking for Metis RAG")
    parser.add_argument("--db-type", type=str, choices=["sqlite", "postgresql"],
                        help="Database type to benchmark")
    parser.add_argument("--html", action="store_true", help="Generate HTML report")
    parser.add_argument("--runs", type=int, default=3, help="Number of runs for each test")
    parser.add_argument("--sqlite-results", type=str, help="Path to SQLite results JSON file (for HTML report)")
    parser.add_argument("--postgres-results", type=str, help="Path to PostgreSQL results JSON file (for HTML report)")
    parser.add_argument("--report-only", action="store_true", help="Generate HTML report only without running benchmarks")
    args = parser.parse_args()
    
    # Check if we're only generating a report
    if args.report_only:
        if not args.sqlite_results or not args.postgres_results:
            print("Error: --sqlite-results and --postgres-results are required with --report-only")
            return 1
            
        # Load results
        try:
            with open(args.sqlite_results, 'r') as f:
                sqlite_results = json.load(f)
            
            with open(args.postgres_results, 'r') as f:
                postgres_results = json.load(f)
                
            # Create results directory if it doesn't exist
            results_dir = os.path.join(project_root, "tests", "results")
            os.makedirs(results_dir, exist_ok=True)
            
            # Create HTML report filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_path = os.path.join(results_dir, f"db_comparison_report_{timestamp}.html")
            
            # Generate HTML report
            generate_html_report(sqlite_results, postgres_results, html_path)
            return 0
        except Exception as e:
            print(f"Error generating report: {e}")
            return 1
    
    # Otherwise, run the benchmark
    if not args.db_type:
        print("Error: --db-type is required when running benchmarks")
        return 1
        
    # Run benchmark
    result = asyncio.run(run_benchmark(args))
    
    # Generate HTML report if requested and both result files are available
    if args.html and hasattr(args, 'sqlite_results') and hasattr(args, 'postgres_results'):
        # Load results
        with open(args.sqlite_results, 'r') as f:
            sqlite_results = json.load(f)
        
        with open(args.postgres_results, 'r') as f:
            postgres_results = json.load(f)
        
        # Create results directory if it doesn't exist
        results_dir = os.path.join(project_root, "tests", "results")
        os.makedirs(results_dir, exist_ok=True)
        
        # Create HTML report filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        html_path = os.path.join(results_dir, f"db_comparison_report_{timestamp}.html")
        
        # Generate HTML report
        generate_html_report(sqlite_results, postgres_results, html_path)
    
    return result

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/clear_cache_and_test.py
================
#!/usr/bin/env python3
"""
Script to clear the response cache and test the improved system prompt.
"""

import os
import sys
import requests
import json
import uuid
import time
from datetime import datetime

# API endpoints
BASE_URL = "http://localhost:8000"
API_URL = f"{BASE_URL}/api/chat/query"
AUTH_URL = f"{BASE_URL}/api/auth/token"
REGISTER_URL = f"{BASE_URL}/api/auth/register"

# Test user credentials
TEST_USERNAME = f"test_user_{uuid.uuid4().hex[:8]}"
TEST_PASSWORD = "Test@123456"
TEST_EMAIL = f"test_{uuid.uuid4().hex[:8]}@example.com"

# Test queries that won't trigger code detection and won't be in cache
TEST_QUERIES = [
    "What is the tallest mountain in the world?",
    "Tell me about the solar system",
    "What are some traditional dishes from Italy?",
    "Who wrote the novel Pride and Prejudice?",
    "What is the Great Barrier Reef?"
]

def register_user():
    """Register a test user."""
    headers = {
        "Content-Type": "application/json"
    }
    
    payload = {
        "username": TEST_USERNAME,
        "password": TEST_PASSWORD,
        "email": TEST_EMAIL,
        "full_name": "Test User"
    }
    
    try:
        response = requests.post(REGISTER_URL, headers=headers, json=payload)
        if response.status_code == 200:
            print(f"Successfully registered user: {TEST_USERNAME}")
            return True
        else:
            print(f"Failed to register user: {response.status_code} - {response.text}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"Error registering user: {e}")
        return False

def get_access_token():
    """Get an access token for the test user."""
    data = {
        "username": TEST_USERNAME,
        "password": TEST_PASSWORD
    }
    
    try:
        response = requests.post(
            AUTH_URL, 
            data=data,
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        if response.status_code == 200:
            token_data = response.json()
            print("Successfully obtained access token")
            return token_data["access_token"]
        else:
            print(f"Failed to get token: {response.status_code} - {response.text}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"Error getting token: {e}")
        return None

def send_chat_request(message, conversation_id=None, access_token=None):
    """Send a chat request to the API."""
    headers = {
        "Content-Type": "application/json"
    }
    
    # Add authorization header if token is provided
    if access_token:
        headers["Authorization"] = f"Bearer {access_token}"
    
    payload = {
        "message": message,
        "use_rag": True,  # Enable RAG
        "stream": False,  # Don't stream the response
        "skip_cache": True  # Skip the cache to force a new response
    }
    
    if conversation_id:
        payload["conversation_id"] = conversation_id
    
    try:
        response = requests.post(API_URL, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error sending request: {e}")
        return None

def run_test():
    """Run the test queries and print the responses."""
    print(f"Starting test at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Register a test user
    if not register_user():
        print("Failed to register test user. Exiting.")
        return
    
    # Get an access token
    access_token = get_access_token()
    if not access_token:
        print("Failed to get access token. Exiting.")
        return
    
    print(f"Testing with {len(TEST_QUERIES)} queries")
    print("-" * 80)
    
    conversation_id = None
    
    for i, query in enumerate(TEST_QUERIES):
        print(f"Query {i+1}: {query}")
        
        # Send the query with the access token
        result = send_chat_request(query, conversation_id, access_token)
        
        if result:
            # Save the conversation ID for the next query
            conversation_id = result.get("conversation_id")
            
            # Print the response
            print(f"Response: {result.get('message')}")
            
            # Print the sources if available
            sources = result.get("citations", [])
            if sources:
                print(f"Sources ({len(sources)}):")
                for source in sources:
                    print(f"  - {source}")
            else:
                print("No sources provided")
        else:
            print("Failed to get response")
        
        print("-" * 80)
        
        # Wait a bit between requests
        if i < len(TEST_QUERIES) - 1:
            time.sleep(1)
    
    print("Test completed")

if __name__ == "__main__":
    # Check if the API is available
    try:
        # Just try to connect to the server
        response = requests.get(BASE_URL)
        if response.status_code in [200, 404]:  # Either is fine, just checking if server is up
            run_test()
        else:
            print(f"API health check failed with status code {response.status_code}")
    except requests.exceptions.ConnectionError:
        print("Could not connect to the API. Make sure the application is running.")
        sys.exit(1)

================
File: scripts/create_admin_user.py
================
import asyncio
import sys
import os
from sqlalchemy import text

# Add the project root directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.db.session import AsyncSessionLocal
from app.db.repositories.user_repository import UserRepository
from app.models.user import UserCreate, UserUpdate

async def create_admin_user():
    """Create an admin user or update an existing user to be an admin"""
    
    admin_username = "admin"
    admin_email = "admin@example.com"
    admin_password = "admin123"
    
    async with AsyncSessionLocal() as session:
        user_repository = UserRepository(session)
        
        # Check if admin user already exists
        admin_user = await user_repository.get_by_username(admin_username)
        
        if admin_user:
            print(f"Admin user already exists: {admin_user.username}")
            
            # Make sure the user is an admin
            if not admin_user.is_admin:
                print(f"Updating {admin_user.username} to be an admin...")
                update_data = UserUpdate(is_admin=True)
                updated_user = await user_repository.update_user(admin_user.id, update_data)
                print(f"User updated: {updated_user.username} (admin: {updated_user.is_admin})")
        else:
            print(f"Creating admin user: {admin_username}")
            
            # Create admin user
            admin_user_data = UserCreate(
                username=admin_username,
                email=admin_email,
                password=admin_password,
                full_name="Admin User",
                is_active=True,
                is_admin=True
            )
            
            try:
                user = await user_repository.create_user(admin_user_data)
                print(f"Admin user created successfully: {user.username} (email: {user.email}, admin: {user.is_admin})")
                print(f"Username: {admin_username}")
                print(f"Password: {admin_password}")
            except ValueError as e:
                print(f"Error creating admin user: {str(e)}")
                
                # Try with a different email if the username is taken but email is different
                try:
                    existing_user = await user_repository.get_by_username(admin_username)
                    if existing_user and existing_user.email != admin_email:
                        admin_user_data.email = f"admin_{os.urandom(4).hex()}@example.com"
                        user = await user_repository.create_user(admin_user_data)
                        print(f"Admin user created with alternative email: {user.username} (email: {user.email}, admin: {user.is_admin})")
                        print(f"Username: {admin_username}")
                        print(f"Password: {admin_password}")
                except Exception as e2:
                    print(f"Error creating admin user with alternative email: {str(e2)}")

if __name__ == "__main__":
    asyncio.run(create_admin_user())

================
File: scripts/create_default_roles.py
================
#!/usr/bin/env python3
"""
Script to create default roles in the database.
This script creates the basic roles (admin, editor, viewer) with their respective permissions.
"""

import asyncio
import sys
import os
import logging
from uuid import UUID
from sqlalchemy import select

# Add the parent directory to the path so we can import the app modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.db.session import AsyncSessionLocal, init_db
from app.db.models import Role, UserRole, User
from app.db.repositories.role_repository import RoleRepository
from app.models.role import RoleCreate

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Define default roles and their permissions
DEFAULT_ROLES = [
    {
        "name": "admin",
        "description": "Administrator with full access to all features",
        "permissions": {
            "read": True,
            "write": True,
            "delete": True,
            "share": True,
            "manage_users": True,
            "manage_roles": True,
            "manage_system": True
        }
    },
    {
        "name": "editor",
        "description": "Editor with access to create and edit documents",
        "permissions": {
            "read": True,
            "write": True,
            "delete": True,
            "share": True,
            "manage_users": False,
            "manage_roles": False,
            "manage_system": False
        }
    },
    {
        "name": "viewer",
        "description": "Viewer with read-only access to documents",
        "permissions": {
            "read": True,
            "write": False,
            "delete": False,
            "share": False,
            "manage_users": False,
            "manage_roles": False,
            "manage_system": False
        }
    }
]


async def create_default_roles():
    """Create default roles in the database"""
    logger.info("Initializing database connection...")
    await init_db()
    
    async with AsyncSessionLocal() as session:
        role_repo = RoleRepository(session)
        
        for role_data in DEFAULT_ROLES:
            # Check if role already exists
            existing_role = await role_repo.get_by_name(role_data["name"])
            
            if existing_role:
                logger.info(f"Role '{role_data['name']}' already exists, updating permissions...")
                # Update existing role
                await role_repo.update_role(
                    existing_role.id,
                    {
                        "description": role_data["description"],
                        "permissions": role_data["permissions"]
                    }
                )
            else:
                logger.info(f"Creating role '{role_data['name']}'...")
                # Create new role
                role_create = RoleCreate(
                    name=role_data["name"],
                    description=role_data["description"],
                    permissions=role_data["permissions"]
                )
                await role_repo.create_role(role_create)
        
        logger.info("Default roles created/updated successfully")
        
        # Assign admin role to admin users
        logger.info("Assigning admin role to admin users...")
        
        # Get admin role
        admin_role = await role_repo.get_by_name("admin")
        if not admin_role:
            logger.error("Admin role not found, cannot assign to users")
            return
        
        # Get all admin users
        stmt = select(User).where(User.is_admin == True)
        admin_users = await session.execute(stmt)
        admin_users = admin_users.scalars().all()
        
        if not admin_users:
            logger.warning("No admin users found in the database")
            return
        
        # Assign admin role to each admin user
        for user in admin_users:
            logger.info(f"Assigning admin role to user '{user.username}'...")
            try:
                await role_repo.assign_role_to_user(str(user.id), admin_role.id)
                logger.info(f"Admin role assigned to user '{user.username}' successfully")
            except ValueError as e:
                # This might happen if the user already has the role
                logger.info(f"Note: {str(e)}")
        
        logger.info("Admin role assignment completed")


if __name__ == "__main__":
    asyncio.run(create_default_roles())

================
File: scripts/create_password_reset_table.py
================
import asyncio
import sys
import os
from sqlalchemy import text

# Add the project root directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.db.session import AsyncSessionLocal

async def create_password_reset_table():
    """Create the password_reset_tokens table"""
    
    # SQL statements for creating the password_reset_tokens table and indexes
    sql_statements = [
        """
        CREATE TABLE IF NOT EXISTS password_reset_tokens (
            id UUID PRIMARY KEY,
            user_id UUID NOT NULL REFERENCES users(id),
            token VARCHAR NOT NULL UNIQUE,
            created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            expires_at TIMESTAMP WITHOUT TIME ZONE NOT NULL,
            is_used BOOLEAN DEFAULT FALSE
        )
        """,
        """
        CREATE INDEX IF NOT EXISTS ix_password_reset_tokens_token ON password_reset_tokens(token)
        """,
        """
        CREATE INDEX IF NOT EXISTS ix_password_reset_tokens_user_id ON password_reset_tokens(user_id)
        """,
        """
        CREATE INDEX IF NOT EXISTS ix_password_reset_tokens_expires_at ON password_reset_tokens(expires_at)
        """
    ]
    
    # Connect to the database and execute each SQL statement
    async with AsyncSessionLocal() as session:
        for sql in sql_statements:
            await session.execute(text(sql))
        await session.commit()
        print("Password reset tokens table created successfully")

if __name__ == "__main__":
    asyncio.run(create_password_reset_table())

================
File: scripts/create_system_user.py
================
#!/usr/bin/env python3
"""
Create a system user for document ownership when users are deleted.
"""

import asyncio
import uuid
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select, text

from app.db.models import User
from app.core.security import get_password_hash


async def create_system_user():
    """Create a system user if it doesn't exist."""
    engine = create_async_engine('postgresql+asyncpg://postgres:postgres@localhost:5432/metis_rag')
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
    
    async with async_session() as session:
        # Check if system user already exists
        stmt = select(User).where(User.username == 'system')
        result = await session.execute(stmt)
        system_user = result.scalars().first()
        
        if not system_user:
            # Create system user
            system_user_id = uuid.uuid4()
            system_user = User(
                id=system_user_id,
                username='system',
                email='system@metisrag.internal',
                password_hash=get_password_hash('not_accessible'),
                full_name='System User',
                is_active=True,
                is_admin=True
            )
            session.add(system_user)
            await session.commit()
            print(f'System user created with ID: {system_user_id}')
        else:
            print(f'System user already exists with ID: {system_user.id}')


if __name__ == "__main__":
    asyncio.run(create_system_user())

================
File: scripts/create_tables.py
================
#!/usr/bin/env python3
"""
Create database tables directly using SQLAlchemy
"""
import os
import sys
import asyncio
from pathlib import Path

# Add the project root directory to the Python path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from app.db.session import Base, engine
from app.db.models import Document, Chunk, Tag, Folder, Conversation, Message, Citation, ProcessingJob, AnalyticsQuery, BackgroundTask
from app.models.memory import Memory

async def create_tables():
    """
    Create database tables
    """
    print("Creating database tables...")
    
    # Create tables using async engine
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    print("Database tables created successfully!")
    return 0

if __name__ == "__main__":
    sys.exit(asyncio.run(create_tables()))

================
File: scripts/create_test_folders.py
================
#!/usr/bin/env python3
"""
Create test folders in the database
"""
import os
import sys
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.db.session import SessionLocal
from app.db.repositories.document_repository import DocumentRepository
from app.db.models import Folder

def create_test_folders():
    """Create test folders in the database"""
    # Create database session
    db_session = SessionLocal()
    
    try:
        # Create document repository
        document_repository = DocumentRepository(db_session)
        
        # Create root folder if it doesn't exist
        root_folder = db_session.query(Folder).filter(Folder.path == "/").first()
        if not root_folder:
            root_folder = Folder(path="/", name="Root", parent_path=None)
            db_session.add(root_folder)
            db_session.commit()
            print("Created root folder '/'")
        else:
            print("Root folder '/' already exists")
        
        # Create test folder if it doesn't exist (without trailing slash)
        test_folder = db_session.query(Folder).filter(Folder.path == "/test").first()
        if not test_folder:
            test_folder = Folder(
                path="/test",  # No trailing slash
                name="test",
                parent_path="/",
                document_count=0
            )
            db_session.add(test_folder)
            db_session.commit()
            print("Created test folder '/test'")
        else:
            print("Test folder '/test' already exists")
        
        return 0
    except Exception as e:
        print(f"Error creating test folders: {e}")
        return 1
    finally:
        db_session.close()

if __name__ == "__main__":
    sys.exit(create_test_folders())

================
File: scripts/create_test_user.py
================
import asyncio
import sys
import os
from sqlalchemy import text

# Add the project root directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.db.session import AsyncSessionLocal
from app.db.repositories.user_repository import UserRepository
from app.models.user import UserCreate

async def create_test_user():
    """Check if there are any users in the database and create a test user if needed"""
    
    async with AsyncSessionLocal() as session:
        # Check if there are any users
        user_repository = UserRepository(session)
        users = await user_repository.get_all_users()
        
        if users:
            print(f"Found {len(users)} users in the database:")
            for user in users:
                print(f"  - {user.username} (email: {user.email}, admin: {user.is_admin})")
        else:
            print("No users found in the database. Creating a test user...")
            
            # Create a test user
            test_user = UserCreate(
                username="testuser",
                email="test@example.com",
                password="password123",
                full_name="Test User",
                is_active=True,
                is_admin=True
            )
            
            try:
                user = await user_repository.create_user(test_user)
                print(f"Test user created successfully: {user.username} (email: {user.email}, admin: {user.is_admin})")
            except Exception as e:
                print(f"Error creating test user: {str(e)}")

if __name__ == "__main__":
    asyncio.run(create_test_user())

================
File: scripts/grant_permissions.sql
================
-- Change ownership of tables to postgres user
ALTER TABLE documents OWNER TO postgres;
ALTER TABLE conversations OWNER TO postgres;
ALTER TABLE chunks OWNER TO postgres;
ALTER TABLE document_tags OWNER TO postgres;
ALTER TABLE tags OWNER TO postgres;
ALTER TABLE messages OWNER TO postgres;
ALTER TABLE citations OWNER TO postgres;
ALTER TABLE processing_jobs OWNER TO postgres;
ALTER TABLE analytics_queries OWNER TO postgres;
ALTER TABLE background_tasks OWNER TO postgres;
ALTER TABLE folders OWNER TO postgres;

-- Change ownership of sequences (if any)
DO $$
DECLARE
    seq_name text;
BEGIN
    FOR seq_name IN SELECT sequence_name FROM information_schema.sequences WHERE sequence_schema = 'public'
    LOOP
        EXECUTE 'ALTER SEQUENCE ' || seq_name || ' OWNER TO postgres';
    END LOOP;
END $$;

-- Grant permissions on sequences (if any)
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO postgres;

================
File: scripts/implement_database_enhancements.py
================
#!/usr/bin/env python3
"""
Database-Specific Enhancements Script for Metis RAG

This script implements database-specific enhancements for PostgreSQL and SQLite:
1. PostgreSQL: JSONB operators, full-text search, connection pooling
2. SQLite: Optimized indexes, pragma settings for concurrent access

Usage:
    python implement_database_enhancements.py --db-type sqlite|postgresql [--apply]
"""
import os
import sys
import argparse
import asyncio
from typing import List, Dict, Any, Optional

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.db.session import Base, engine, SessionLocal
from app.core.config import DATABASE_TYPE, DATABASE_URL
from sqlalchemy import text

class DatabaseEnhancer:
    """Database enhancement implementation class"""
    
    def __init__(self, db_type: str, apply: bool = False):
        self.db_type = db_type
        self.apply = apply
        
        # Initialize database session
        self.db_session = SessionLocal()
        
        print(f"Initialized database enhancer for {db_type} database")
        print(f"Database URL: {DATABASE_URL}")
        print(f"Apply mode: {'ON' if apply else 'OFF (dry run)'}")
        
    def cleanup(self):
        """Clean up resources"""
        self.db_session.close()
    
    def execute_sql(self, sql: str, description: str) -> bool:
        """Execute SQL statement with proper error handling"""
        try:
            print(f"  {description}")
            print(f"    SQL: {sql}")
            
            if self.apply:
                self.db_session.execute(text(sql))
                self.db_session.commit()
                print("    ✓ Applied successfully")
                return True
            else:
                print("    ✓ Validated (not applied - dry run mode)")
                return True
        except Exception as e:
            self.db_session.rollback()
            print(f"    ✗ Error: {str(e)}")
            return False
    
    def enhance_postgresql(self) -> List[Dict[str, Any]]:
        """Implement PostgreSQL-specific enhancements"""
        print("\nImplementing PostgreSQL enhancements...")
        
        enhancements = []
        
        # 1. Create GIN index on document metadata for faster JSONB queries
        sql = """
        CREATE INDEX IF NOT EXISTS idx_documents_metadata_gin 
        ON documents USING GIN (metadata);
        """
        success = self.execute_sql(sql, "Creating GIN index on document metadata")
        enhancements.append({
            "name": "GIN index on document metadata",
            "applied": success,
            "description": "Enables efficient querying of JSONB metadata fields using operators like @>, ?, and ?&"
        })
        
        # 2. Create GIN index on chunk metadata for faster JSONB queries
        sql = """
        CREATE INDEX IF NOT EXISTS idx_chunks_metadata_gin 
        ON chunks USING GIN (metadata);
        """
        success = self.execute_sql(sql, "Creating GIN index on chunk metadata")
        enhancements.append({
            "name": "GIN index on chunk metadata",
            "applied": success,
            "description": "Enables efficient querying of JSONB metadata fields in chunks"
        })
        
        # 3. Create full-text search index on document content
        sql = """
        -- Create tsvector column if it doesn't exist
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1 FROM information_schema.columns 
                WHERE table_name = 'documents' AND column_name = 'content_tsv'
            ) THEN
                ALTER TABLE documents ADD COLUMN content_tsv tsvector;
                
                -- Create index on the tsvector column
                CREATE INDEX idx_documents_content_tsv ON documents USING GIN (content_tsv);
                
                -- Create trigger to update tsvector column
                CREATE OR REPLACE FUNCTION documents_content_trigger() RETURNS trigger AS $$
                BEGIN
                    NEW.content_tsv := to_tsvector('english', COALESCE(NEW.content, ''));
                    RETURN NEW;
                END
                $$ LANGUAGE plpgsql;
                
                CREATE TRIGGER tsvector_update_documents_content 
                BEFORE INSERT OR UPDATE OF content ON documents
                FOR EACH ROW EXECUTE FUNCTION documents_content_trigger();
                
                -- Update existing records
                UPDATE documents SET content_tsv = to_tsvector('english', COALESCE(content, ''));
            END IF;
        END
        $$;
        """
        success = self.execute_sql(sql, "Creating full-text search index on document content")
        enhancements.append({
            "name": "Full-text search on document content",
            "applied": success,
            "description": "Enables efficient full-text search using PostgreSQL's tsvector type"
        })
        
        # 4. Create full-text search index on chunk content
        sql = """
        -- Create tsvector column if it doesn't exist
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1 FROM information_schema.columns 
                WHERE table_name = 'chunks' AND column_name = 'content_tsv'
            ) THEN
                ALTER TABLE chunks ADD COLUMN content_tsv tsvector;
                
                -- Create index on the tsvector column
                CREATE INDEX idx_chunks_content_tsv ON chunks USING GIN (content_tsv);
                
                -- Create trigger to update tsvector column
                CREATE OR REPLACE FUNCTION chunks_content_trigger() RETURNS trigger AS $$
                BEGIN
                    NEW.content_tsv := to_tsvector('english', COALESCE(NEW.content, ''));
                    RETURN NEW;
                END
                $$ LANGUAGE plpgsql;
                
                CREATE TRIGGER tsvector_update_chunks_content 
                BEFORE INSERT OR UPDATE OF content ON chunks
                FOR EACH ROW EXECUTE FUNCTION chunks_content_trigger();
                
                -- Update existing records
                UPDATE chunks SET content_tsv = to_tsvector('english', COALESCE(content, ''));
            END IF;
        END
        $$;
        """
        success = self.execute_sql(sql, "Creating full-text search index on chunk content")
        enhancements.append({
            "name": "Full-text search on chunk content",
            "applied": success,
            "description": "Enables efficient full-text search on chunk content"
        })
        
        # 5. Create function for semantic search using embeddings
        sql = """
        -- Create extension if it doesn't exist
        CREATE EXTENSION IF NOT EXISTS vector;
        
        -- Create function for cosine similarity
        CREATE OR REPLACE FUNCTION cosine_similarity(a real[], b real[]) 
        RETURNS real AS $$
        DECLARE
            dot_product real := 0;
            norm_a real := 0;
            norm_b real := 0;
        BEGIN
            FOR i IN 1..array_length(a, 1) LOOP
                dot_product := dot_product + (a[i] * b[i]);
                norm_a := norm_a + (a[i] * a[i]);
                norm_b := norm_b + (b[i] * b[i]);
            END LOOP;
            
            IF norm_a = 0 OR norm_b = 0 THEN
                RETURN 0;
            ELSE
                RETURN dot_product / (sqrt(norm_a) * sqrt(norm_b));
            END IF;
        END;
        $$ LANGUAGE plpgsql IMMUTABLE;
        """
        success = self.execute_sql(sql, "Creating function for semantic search")
        enhancements.append({
            "name": "Semantic search function",
            "applied": success,
            "description": "Enables efficient semantic search using vector embeddings"
        })
        
        # 6. Optimize connection pooling settings
        sql = """
        -- Set statement timeout to prevent long-running queries
        ALTER DATABASE current_database() SET statement_timeout = '30s';
        
        -- Set idle in transaction timeout
        ALTER DATABASE current_database() SET idle_in_transaction_session_timeout = '60s';
        """
        success = self.execute_sql(sql, "Optimizing connection pooling settings")
        enhancements.append({
            "name": "Connection pooling optimization",
            "applied": success,
            "description": "Prevents connection leaks and improves connection reuse"
        })
        
        # 7. Create materialized view for frequently accessed data
        sql = """
        -- Create materialized view for document statistics
        CREATE MATERIALIZED VIEW IF NOT EXISTS document_stats AS
        SELECT 
            d.id,
            d.filename,
            d.folder,
            d.uploaded,
            d.processing_status,
            COUNT(c.id) AS chunk_count,
            AVG(LENGTH(c.content)) AS avg_chunk_size,
            SUM(LENGTH(c.content)) AS total_content_size
        FROM 
            documents d
        LEFT JOIN 
            chunks c ON d.id = c.document_id
        GROUP BY 
            d.id, d.filename, d.folder, d.uploaded, d.processing_status;
            
        -- Create index on the materialized view
        CREATE UNIQUE INDEX IF NOT EXISTS idx_document_stats_id ON document_stats (id);
        """
        success = self.execute_sql(sql, "Creating materialized view for document statistics")
        enhancements.append({
            "name": "Document statistics materialized view",
            "applied": success,
            "description": "Improves performance for frequently accessed document statistics"
        })
        
        # 8. Create function to refresh materialized view
        sql = """
        -- Create function to refresh materialized view
        CREATE OR REPLACE FUNCTION refresh_document_stats()
        RETURNS void AS $$
        BEGIN
            REFRESH MATERIALIZED VIEW CONCURRENTLY document_stats;
        END;
        $$ LANGUAGE plpgsql;
        """
        success = self.execute_sql(sql, "Creating function to refresh materialized view")
        enhancements.append({
            "name": "Materialized view refresh function",
            "applied": success,
            "description": "Allows for concurrent refreshing of the document statistics view"
        })
        
        return enhancements
    
    def enhance_sqlite(self) -> List[Dict[str, Any]]:
        """Implement SQLite-specific enhancements"""
        print("\nImplementing SQLite enhancements...")
        
        enhancements = []
        
        # 1. Enable WAL mode for better concurrency
        sql = "PRAGMA journal_mode = WAL;"
        success = self.execute_sql(sql, "Enabling WAL mode for better concurrency")
        enhancements.append({
            "name": "WAL mode",
            "applied": success,
            "description": "Enables Write-Ahead Logging for better concurrency and performance"
        })
        
        # 2. Set busy timeout for concurrent access
        sql = "PRAGMA busy_timeout = 5000;"
        success = self.execute_sql(sql, "Setting busy timeout for concurrent access")
        enhancements.append({
            "name": "Busy timeout",
            "applied": success,
            "description": "Sets timeout to 5 seconds when database is locked"
        })
        
        # 3. Enable foreign key constraints
        sql = "PRAGMA foreign_keys = ON;"
        success = self.execute_sql(sql, "Enabling foreign key constraints")
        enhancements.append({
            "name": "Foreign key constraints",
            "applied": success,
            "description": "Ensures referential integrity in the database"
        })
        
        # 4. Set synchronous mode to NORMAL for better performance
        sql = "PRAGMA synchronous = NORMAL;"
        success = self.execute_sql(sql, "Setting synchronous mode to NORMAL")
        enhancements.append({
            "name": "Synchronous mode",
            "applied": success,
            "description": "Balances durability and performance"
        })
        
        # 5. Create index on document content for faster text search
        sql = """
        CREATE INDEX IF NOT EXISTS idx_documents_content ON documents(content);
        """
        success = self.execute_sql(sql, "Creating index on document content")
        enhancements.append({
            "name": "Document content index",
            "applied": success,
            "description": "Improves performance for text search operations"
        })
        
        # 6. Create index on chunk content for faster text search
        sql = """
        CREATE INDEX IF NOT EXISTS idx_chunks_content ON chunks(content);
        """
        success = self.execute_sql(sql, "Creating index on chunk content")
        enhancements.append({
            "name": "Chunk content index",
            "applied": success,
            "description": "Improves performance for text search operations on chunks"
        })
        
        # 7. Create index on document metadata for faster JSON queries
        sql = """
        CREATE INDEX IF NOT EXISTS idx_documents_metadata ON documents(metadata);
        """
        success = self.execute_sql(sql, "Creating index on document metadata")
        enhancements.append({
            "name": "Document metadata index",
            "applied": success,
            "description": "Improves performance for JSON queries on document metadata"
        })
        
        # 8. Create index on chunk metadata for faster JSON queries
        sql = """
        CREATE INDEX IF NOT EXISTS idx_chunks_metadata ON chunks(metadata);
        """
        success = self.execute_sql(sql, "Creating index on chunk metadata")
        enhancements.append({
            "name": "Chunk metadata index",
            "applied": success,
            "description": "Improves performance for JSON queries on chunk metadata"
        })
        
        # 9. Optimize database
        sql = "PRAGMA optimize;"
        success = self.execute_sql(sql, "Optimizing database")
        enhancements.append({
            "name": "Database optimization",
            "applied": success,
            "description": "Runs internal optimizations on the database"
        })
        
        return enhancements
    
    def implement_enhancements(self) -> Dict[str, Any]:
        """Implement database-specific enhancements"""
        results = {
            "database_type": self.db_type,
            "apply_mode": self.apply,
            "enhancements": []
        }
        
        if self.db_type == "postgresql":
            results["enhancements"] = self.enhance_postgresql()
        elif self.db_type == "sqlite":
            results["enhancements"] = self.enhance_sqlite()
        
        # Print summary
        print("\nEnhancement Summary:")
        applied_count = sum(1 for e in results["enhancements"] if e["applied"])
        total_count = len(results["enhancements"])
        print(f"  Applied: {applied_count}/{total_count} enhancements")
        
        if not self.apply:
            print("\nTo apply these enhancements, run again with --apply flag")
        
        return results

async def run_enhancements(args):
    """Run the database enhancements"""
    print(f"\nRunning database enhancements for {args.db_type}...")
    
    # Create enhancer instance
    enhancer = DatabaseEnhancer(args.db_type, args.apply)
    
    try:
        # Implement enhancements
        results = enhancer.implement_enhancements()
        
        print("\nEnhancements completed successfully!")
        return 0
    except Exception as e:
        print(f"Error implementing enhancements: {e}")
        return 1
    finally:
        enhancer.cleanup()

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Database-Specific Enhancements for Metis RAG")
    parser.add_argument("--db-type", type=str, choices=["sqlite", "postgresql"], required=True,
                        help="Database type to enhance")
    parser.add_argument("--apply", action="store_true", help="Apply enhancements (default is dry run)")
    args = parser.parse_args()
    
    # Run enhancements
    result = asyncio.run(run_enhancements(args))
    
    return result

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/implement_postgres_enhancements.py
================
#!/usr/bin/env python3
"""
PostgreSQL-Specific Enhancements for Metis RAG

This script implements PostgreSQL-specific optimizations:
1. JSONB operators for metadata queries
2. Full-text search capabilities
3. Connection pooling configuration
4. Index optimization

Usage:
    python implement_postgres_enhancements.py [--apply]
"""
import os
import sys
import json
import argparse
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.db.session import Base, engine, SessionLocal
from app.core.config import DATABASE_TYPE, DATABASE_URL

# SQL scripts for PostgreSQL enhancements
POSTGRES_ENHANCEMENTS = {
    "jsonb_operators": """
-- Add indexes for JSONB operators on metadata fields
CREATE INDEX IF NOT EXISTS idx_documents_metadata_gin ON documents USING GIN (metadata);
CREATE INDEX IF NOT EXISTS idx_chunks_metadata_gin ON chunks USING GIN (metadata);

-- Example of how to query using JSONB operators:
-- SELECT * FROM documents WHERE metadata @> '{"file_type": "pdf"}'::jsonb;
-- SELECT * FROM chunks WHERE metadata @> '{"tags": ["important"]}'::jsonb;
""",

    "full_text_search": """
-- Add tsvector columns for full-text search
ALTER TABLE documents ADD COLUMN IF NOT EXISTS content_tsv tsvector;
ALTER TABLE chunks ADD COLUMN IF NOT EXISTS content_tsv tsvector;

-- Create update triggers for tsvector columns
CREATE OR REPLACE FUNCTION documents_tsvector_update_trigger() RETURNS trigger AS $$
BEGIN
  NEW.content_tsv := to_tsvector('english', coalesce(NEW.content, ''));
  RETURN NEW;
END
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION chunks_tsvector_update_trigger() RETURNS trigger AS $$
BEGIN
  NEW.content_tsv := to_tsvector('english', coalesce(NEW.content, ''));
  RETURN NEW;
END
$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS documents_tsvector_update ON documents;
CREATE TRIGGER documents_tsvector_update BEFORE INSERT OR UPDATE
ON documents FOR EACH ROW EXECUTE FUNCTION documents_tsvector_update_trigger();

DROP TRIGGER IF EXISTS chunks_tsvector_update ON chunks;
CREATE TRIGGER chunks_tsvector_update BEFORE INSERT OR UPDATE
ON chunks FOR EACH ROW EXECUTE FUNCTION chunks_tsvector_update_trigger();

-- Create GIN indexes for full-text search
CREATE INDEX IF NOT EXISTS idx_documents_content_tsv ON documents USING GIN (content_tsv);
CREATE INDEX IF NOT EXISTS idx_chunks_content_tsv ON chunks USING GIN (content_tsv);

-- Example of how to use full-text search:
-- SELECT * FROM documents WHERE content_tsv @@ to_tsquery('english', 'search & terms');
-- SELECT * FROM chunks WHERE content_tsv @@ to_tsquery('english', 'search & terms');
""",

    "index_optimization": """
-- Add additional indexes for common query patterns
CREATE INDEX IF NOT EXISTS idx_documents_filename_folder ON documents (filename, folder);
CREATE INDEX IF NOT EXISTS idx_documents_uploaded ON documents (uploaded);
CREATE INDEX IF NOT EXISTS idx_chunks_document_id_index ON chunks (document_id, index);

-- Add partial indexes for common filters
CREATE INDEX IF NOT EXISTS idx_documents_processing_status_pending ON documents (id) 
WHERE processing_status = 'pending';

CREATE INDEX IF NOT EXISTS idx_documents_processing_status_completed ON documents (id) 
WHERE processing_status = 'completed';

-- Add index for document type filtering
CREATE INDEX IF NOT EXISTS idx_documents_file_type ON documents ((metadata->>'file_type'));

-- Add index for tag filtering
CREATE INDEX IF NOT EXISTS idx_documents_tags ON documents USING GIN ((metadata->'tags_list'));
""",

    "connection_pooling": """
-- PostgreSQL connection pooling settings (to be added to postgresql.conf)
-- max_connections = 100
-- shared_buffers = 256MB
-- effective_cache_size = 768MB
-- work_mem = 8MB
-- maintenance_work_mem = 64MB
-- max_worker_processes = 8
-- max_parallel_workers_per_gather = 4
-- max_parallel_workers = 8
-- random_page_cost = 1.1
-- effective_io_concurrency = 200
-- checkpoint_completion_target = 0.9
-- wal_buffers = 16MB
-- default_statistics_target = 100
"""
}

# Python code for implementing PostgreSQL-specific repository methods
POSTGRES_REPOSITORY_METHODS = '''
    def search_documents_with_jsonb(self, metadata_query: Dict[str, Any]) -> List[Document]:
        """
        Search documents using PostgreSQL JSONB operators
        
        Args:
            metadata_query: Dictionary of metadata key-value pairs to search for
            
        Returns:
            List of matching documents
        """
        if DATABASE_TYPE != 'postgresql':
            raise ValueError("This method is only available for PostgreSQL")
        
        # Convert Python dict to JSONB query string
        jsonb_query = json.dumps(metadata_query)
        
        # Execute query using JSONB containment operator
        stmt = select(self.model).where(text(f"doc_metadata @> '{jsonb_query}'::jsonb"))
        result = self.db.execute(stmt).scalars().all()
        
        # Convert to Pydantic models
        return [sqlalchemy_document_to_pydantic(doc) for doc in result]
    
    def full_text_search_documents(self, query: str) -> List[Document]:
        """
        Search documents using PostgreSQL full-text search
        
        Args:
            query: Search query string
            
        Returns:
            List of matching documents
        """
        if DATABASE_TYPE != 'postgresql':
            raise ValueError("This method is only available for PostgreSQL")
        
        # Convert query to tsquery format
        ts_query = " & ".join(query.split())
        
        # Execute query using full-text search
        stmt = select(self.model).where(text(f"content_tsv @@ to_tsquery('english', '{ts_query}')"))
        result = self.db.execute(stmt).scalars().all()
        
        # Convert to Pydantic models
        return [sqlalchemy_document_to_pydantic(doc) for doc in result]
    
    def full_text_search_chunks(self, query: str, limit: int = 10) -> List[Chunk]:
        """
        Search chunks using PostgreSQL full-text search
        
        Args:
            query: Search query string
            limit: Maximum number of results to return
            
        Returns:
            List of matching chunks
        """
        if DATABASE_TYPE != 'postgresql':
            raise ValueError("This method is only available for PostgreSQL")
        
        # Convert query to tsquery format
        ts_query = " & ".join(query.split())
        
        # Execute query using full-text search with ranking
        stmt = text(f"""
            SELECT c.*, ts_rank(c.content_tsv, to_tsquery('english', :query)) AS rank
            FROM chunks c
            WHERE c.content_tsv @@ to_tsquery('english', :query)
            ORDER BY rank DESC
            LIMIT :limit
        """)
        
        result = self.db.execute(stmt, {"query": ts_query, "limit": limit}).all()
        
        # Convert to Pydantic models
        chunks = []
        for row in result:
            chunk = Chunk(
                id=str(row.id),
                content=row.content,
                metadata=row.chunk_metadata
            )
            chunks.append(chunk)
        
        return chunks
'''

# Connection pooling configuration for app/db/session.py
CONNECTION_POOLING_CONFIG = """
# Create SQLAlchemy engine with optimized connection pooling for PostgreSQL
if DATABASE_TYPE == 'postgresql':
    engine = create_engine(
        DATABASE_URL,
        pool_size=DATABASE_POOL_SIZE,
        max_overflow=DATABASE_MAX_OVERFLOW,
        pool_pre_ping=True,  # Verify connections before using them
        pool_recycle=3600,   # Recycle connections after 1 hour
        pool_timeout=30,     # Connection timeout
        pool_use_lifo=True,  # Use LIFO to improve locality of reference
        echo_pool=False      # Set to True for debugging connection pool
    )
else:
    engine = create_engine(
        DATABASE_URL,
        pool_size=DATABASE_POOL_SIZE,
        max_overflow=DATABASE_MAX_OVERFLOW,
        pool_pre_ping=True,
        pool_recycle=3600
    )
"""

# API endpoint for PostgreSQL-specific features
POSTGRES_API_ENDPOINT = '''
@router.get("/search/advanced", response_model=List[DocumentInfo])
async def advanced_search(
    query: str = Query(None, description="Full-text search query"),
    metadata: str = Query(None, description="JSON metadata query"),
    db: Session = Depends(get_db)
):
    """
    Advanced search endpoint using PostgreSQL-specific features
    """
    if DATABASE_TYPE != 'postgresql':
        raise HTTPException(
            status_code=400,
            detail="Advanced search is only available with PostgreSQL backend"
        )
    
    document_repository = DocumentRepository(db)
    results = []
    
    # Full-text search
    if query:
        results.extend(document_repository.full_text_search_documents(query))
    
    # JSONB metadata search
    if metadata:
        try:
            metadata_query = json.loads(metadata)
            results.extend(document_repository.search_documents_with_jsonb(metadata_query))
        except json.JSONDecodeError:
            raise HTTPException(
                status_code=400,
                detail="Invalid JSON in metadata query"
            )
    
    # Convert to DocumentInfo and deduplicate
    seen_ids = set()
    unique_results = []
    
    for doc in results:
        if doc.id not in seen_ids:
            seen_ids.add(doc.id)
            unique_results.append(DocumentInfo(
                id=doc.id,
                filename=doc.filename,
                chunk_count=len(doc.chunks) if hasattr(doc, 'chunks') else 0,
                metadata=doc.metadata,
                tags=doc.tags if hasattr(doc, 'tags') else [],
                folder=doc.folder,
                uploaded=doc.uploaded
            ))
    
    return unique_results
'''

def check_postgres_connection():
    """Check if PostgreSQL is available and configured"""
    if DATABASE_TYPE != 'postgresql':
        print("Current database is not PostgreSQL. These enhancements are only applicable to PostgreSQL.")
        return False
    
    try:
        # Try to connect to PostgreSQL
        db = SessionLocal()
        db.execute("SELECT 1")
        db.close()
        print(f"Successfully connected to PostgreSQL at {DATABASE_URL}")
        return True
    except Exception as e:
        print(f"Error connecting to PostgreSQL: {e}")
        return False

def apply_sql_enhancements(db, enhancement_name):
    """Apply SQL enhancements to the database"""
    print(f"Applying {enhancement_name} enhancements...")
    
    sql = POSTGRES_ENHANCEMENTS[enhancement_name]
    statements = [stmt.strip() for stmt in sql.split(';') if stmt.strip()]
    
    for stmt in statements:
        try:
            db.execute(stmt)
            db.commit()
            print(f"  Successfully executed: {stmt[:60]}...")
        except Exception as e:
            db.rollback()
            print(f"  Error executing statement: {e}")
            print(f"  Statement: {stmt}")

def modify_repository_file():
    """Add PostgreSQL-specific methods to document repository"""
    repo_file = os.path.join(project_root, "app", "db", "repositories", "document_repository.py")
    
    try:
        with open(repo_file, 'r') as f:
            content = f.read()
        
        # Check if methods already exist
        if "search_documents_with_jsonb" in content:
            print("PostgreSQL-specific repository methods already exist.")
            return
        
        # Find the end of the class definition
        import_section = "from app.db.adapters import sqlalchemy_document_to_pydantic, pydantic_document_to_sqlalchemy\n"
        if import_section not in content:
            import_section = "from app.db.adapters import "
        
        # Add imports if needed
        if "import json" not in content:
            content = content.replace(import_section, "import json\n" + import_section)
        
        if "from sqlalchemy import text" not in content:
            content = content.replace(import_section, "from sqlalchemy import text\n" + import_section)
        
        if "from app.core.config import DATABASE_TYPE" not in content:
            content = content.replace(import_section, "from app.core.config import DATABASE_TYPE\n" + import_section)
        
        # Find the end of the class definition
        class_end = content.rfind("}")
        if class_end == -1:
            class_end = content.rfind("return documents")
        
        if class_end != -1:
            # Insert new methods
            new_content = content[:class_end] + "\n" + POSTGRES_REPOSITORY_METHODS + "\n" + content[class_end:]
            
            # Write back to file
            with open(repo_file, 'w') as f:
                f.write(new_content)
            
            print(f"Added PostgreSQL-specific methods to {repo_file}")
        else:
            print(f"Could not find appropriate location to add methods in {repo_file}")
    
    except Exception as e:
        print(f"Error modifying repository file: {e}")

def modify_session_file():
    """Modify session.py to add connection pooling configuration"""
    session_file = os.path.join(project_root, "app", "db", "session.py")
    
    try:
        with open(session_file, 'r') as f:
            content = f.read()
        
        # Check if connection pooling config already exists
        if "pool_use_lifo=True" in content:
            print("Connection pooling configuration already exists.")
            return
        
        # Find the engine creation
        engine_creation = "engine = create_engine("
        if engine_creation in content:
            # Find the end of the engine creation
            engine_end = content.find(")", content.find(engine_creation))
            
            # Replace the engine creation with the new configuration
            new_content = content.replace(
                content[content.find(engine_creation):engine_end+1],
                CONNECTION_POOLING_CONFIG
            )
            
            # Write back to file
            with open(session_file, 'w') as f:
                f.write(new_content)
            
            print(f"Added connection pooling configuration to {session_file}")
        else:
            print(f"Could not find engine creation in {session_file}")
    
    except Exception as e:
        print(f"Error modifying session file: {e}")

def add_api_endpoint():
    """Add PostgreSQL-specific API endpoint"""
    api_file = os.path.join(project_root, "app", "api", "documents.py")
    
    try:
        with open(api_file, 'r') as f:
            content = f.read()
        
        # Check if endpoint already exists
        if "def advanced_search" in content:
            print("PostgreSQL-specific API endpoint already exists.")
            return
        
        # Find the imports
        import_section = "from fastapi import APIRouter, Depends, HTTPException, Query, File, UploadFile, Form, status\n"
        if import_section not in content:
            import_section = "from fastapi import "
        
        # Add imports if needed
        if "import json" not in content:
            content = content.replace(import_section, "import json\n" + import_section)
        
        if "from app.core.config import DATABASE_TYPE" not in content:
            content = content.replace(import_section, "from app.core.config import DATABASE_TYPE\n" + import_section)
        
        # Find the end of the file
        file_end = content.rfind("@router.delete")
        if file_end == -1:
            file_end = len(content)
        
        # Find the last route definition
        last_route = content.rfind("@router")
        if last_route != -1:
            # Find the end of the last route function
            last_route_end = content.find("\n\n", last_route)
            if last_route_end == -1:
                last_route_end = len(content)
            
            # Insert new endpoint
            new_content = content[:last_route_end] + "\n\n" + POSTGRES_API_ENDPOINT + content[last_route_end:]
            
            # Write back to file
            with open(api_file, 'w') as f:
                f.write(new_content)
            
            print(f"Added PostgreSQL-specific API endpoint to {api_file}")
        else:
            print(f"Could not find appropriate location to add endpoint in {api_file}")
    
    except Exception as e:
        print(f"Error adding API endpoint: {e}")

def create_migration_script():
    """Create a migration script for PostgreSQL enhancements"""
    migrations_dir = os.path.join(project_root, "alembic", "versions")
    os.makedirs(migrations_dir, exist_ok=True)
    
    # Create a new migration file
    timestamp = int(time.time())
    migration_file = os.path.join(migrations_dir, f"postgres_enhancements_{timestamp}.py")
    
    migration_content = f"""\"\"\"PostgreSQL enhancements

Revision ID: postgres_enhancements_{timestamp}
Revises: 
Create Date: {datetime.datetime.now().isoformat()}

\"\"\"
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB, TSVECTOR
from app.core.config import DATABASE_TYPE

# revision identifiers, used by Alembic.
revision = 'postgres_enhancements_{timestamp}'
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    # Skip if not PostgreSQL
    if DATABASE_TYPE != 'postgresql':
        return
    
    # Add tsvector columns for full-text search
    op.add_column('documents', sa.Column('content_tsv', TSVECTOR))
    op.add_column('chunks', sa.Column('content_tsv', TSVECTOR))
    
    # Create GIN indexes for JSONB operators
    op.execute(\"\"\"
    CREATE INDEX IF NOT EXISTS idx_documents_metadata_gin ON documents USING GIN (metadata);
    CREATE INDEX IF NOT EXISTS idx_chunks_metadata_gin ON chunks USING GIN (metadata);
    \"\"\")
    
    # Create GIN indexes for full-text search
    op.execute(\"\"\"
    CREATE INDEX IF NOT EXISTS idx_documents_content_tsv ON documents USING GIN (content_tsv);
    CREATE INDEX IF NOT EXISTS idx_chunks_content_tsv ON chunks USING GIN (content_tsv);
    \"\"\")
    
    # Create update triggers for tsvector columns
    op.execute(\"\"\"
    CREATE OR REPLACE FUNCTION documents_tsvector_update_trigger() RETURNS trigger AS $$
    BEGIN
      NEW.content_tsv := to_tsvector('english', coalesce(NEW.content, ''));
      RETURN NEW;
    END
    $$ LANGUAGE plpgsql;
    
    CREATE OR REPLACE FUNCTION chunks_tsvector_update_trigger() RETURNS trigger AS $$
    BEGIN
      NEW.content_tsv := to_tsvector('english', coalesce(NEW.content, ''));
      RETURN NEW;
    END
    $$ LANGUAGE plpgsql;
    
    DROP TRIGGER IF EXISTS documents_tsvector_update ON documents;
    CREATE TRIGGER documents_tsvector_update BEFORE INSERT OR UPDATE
    ON documents FOR EACH ROW EXECUTE FUNCTION documents_tsvector_update_trigger();
    
    DROP TRIGGER IF EXISTS chunks_tsvector_update ON chunks;
    CREATE TRIGGER chunks_tsvector_update BEFORE INSERT OR UPDATE
    ON chunks FOR EACH ROW EXECUTE FUNCTION chunks_tsvector_update_trigger();
    \"\"\")
    
    # Add additional indexes for common query patterns
    op.create_index('idx_documents_filename_folder', 'documents', ['filename', 'folder'])
    op.create_index('idx_documents_uploaded', 'documents', ['uploaded'])
    op.create_index('idx_chunks_document_id_index', 'chunks', ['document_id', 'index'])
    
    # Add partial indexes for common filters
    op.execute(\"\"\"
    CREATE INDEX IF NOT EXISTS idx_documents_processing_status_pending ON documents (id) 
    WHERE processing_status = 'pending';
    
    CREATE INDEX IF NOT EXISTS idx_documents_processing_status_completed ON documents (id) 
    WHERE processing_status = 'completed';
    \"\"\")
    
    # Add index for document type filtering
    op.execute(\"\"\"
    CREATE INDEX IF NOT EXISTS idx_documents_file_type ON documents ((metadata->>'file_type'));
    \"\"\")
    
    # Add index for tag filtering
    op.execute(\"\"\"
    CREATE INDEX IF NOT EXISTS idx_documents_tags ON documents USING GIN ((metadata->'tags_list'));
    \"\"\")


def downgrade():
    # Skip if not PostgreSQL
    if DATABASE_TYPE != 'postgresql':
        return
    
    # Drop triggers
    op.execute(\"\"\"
    DROP TRIGGER IF EXISTS documents_tsvector_update ON documents;
    DROP TRIGGER IF EXISTS chunks_tsvector_update ON chunks;
    DROP FUNCTION IF EXISTS documents_tsvector_update_trigger();
    DROP FUNCTION IF EXISTS chunks_tsvector_update_trigger();
    \"\"\")
    
    # Drop indexes
    op.execute(\"\"\"
    DROP INDEX IF EXISTS idx_documents_metadata_gin;
    DROP INDEX IF EXISTS idx_chunks_metadata_gin;
    DROP INDEX IF EXISTS idx_documents_content_tsv;
    DROP INDEX IF EXISTS idx_chunks_content_tsv;
    DROP INDEX IF EXISTS idx_documents_filename_folder;
    DROP INDEX IF EXISTS idx_documents_uploaded;
    DROP INDEX IF EXISTS idx_chunks_document_id_index;
    DROP INDEX IF EXISTS idx_documents_processing_status_pending;
    DROP INDEX IF EXISTS idx_documents_processing_status_completed;
    DROP INDEX IF EXISTS idx_documents_file_type;
    DROP INDEX IF EXISTS idx_documents_tags;
    \"\"\")
    
    # Drop columns
    op.drop_column('documents', 'content_tsv')
    op.drop_column('chunks', 'content_tsv')
"""
    
    try:
        with open(migration_file, 'w') as f:
            f.write(migration_content)
        
        print(f"Created migration script: {migration_file}")
    except Exception as e:
        print(f"Error creating migration script: {e}")

def create_postgres_config_guide():
    """Create a guide for PostgreSQL configuration"""
    guide_file = os.path.join(project_root, "docs", "deployment", "postgresql_optimization.md")
    os.makedirs(os.path.dirname(guide_file), exist_ok=True)
    
    guide_content = """# PostgreSQL Optimization Guide for Metis RAG

This guide provides recommendations for optimizing PostgreSQL for use with the Metis RAG system.

## Connection Pooling

The Metis RAG system uses SQLAlchemy's built-in connection pooling, which is configured in `app/db/session.py`. The following settings are used:

```python
engine = create_engine(
    DATABASE_URL,
    pool_size=DATABASE_POOL_SIZE,  # Default: 5
    max_overflow=DATABASE_MAX_OVERFLOW,  # Default: 10
    pool_pre_ping=True,  # Verify connections before using them
    pool_recycle=3600,   # Recycle connections after 1 hour
    pool_timeout=30,     # Connection timeout
    pool_use_lifo=True,  # Use LIFO to improve locality of reference
)
```

For production deployments, you may want to adjust these settings based on your expected load:

- `pool_size`: The number of connections to keep open in the pool. For high-traffic applications, increase this to 10-20.
- `max_overflow`: The maximum number of connections to create beyond the pool size. For high-traffic applications, increase this to 20-30.
- `pool_recycle`: The number of seconds after which a connection is recycled. The default of 3600 (1 hour) is usually appropriate.

## PostgreSQL Configuration

For optimal performance, consider the following settings in your `postgresql.conf` file:

```
# Connection settings
max_connections = 100

# Memory settings
shared_buffers = 256MB  # 25% of available RAM, up to 8GB
effective_cache_size = 768MB  # 75% of available RAM
work_mem = 8MB  # Increase for complex queries
maintenance_work_mem = 64MB  # Increase for maintenance operations

# Parallel query settings
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8

# Cost-based optimizer settings
random_page_cost = 1.1  # For SSDs
effective_io_concurrency = 200  # For SSDs

# Checkpoint settings
checkpoint_completion_target = 0.9
wal_buffers = 16MB

# Query planning
default_statistics_target = 100
```

Adjust these settings based on your server's available resources.

## JSONB Operators

PostgreSQL's JSONB type provides powerful operators for querying JSON data. The Metis RAG system uses JSONB for storing document and chunk metadata.

Example queries:

```sql
-- Find documents with a specific file type
SELECT * FROM documents WHERE metadata @> '{"file_type": "pdf"}'::jsonb;

-- Find chunks with specific tags
SELECT * FROM chunks WHERE metadata @> '{"tags": ["important"]}'::jsonb;

-- Find documents with a specific metadata field
SELECT * FROM documents WHERE metadata ? 'author';

-- Find documents with a specific metadata value
SELECT * FROM documents WHERE metadata->>'status' = 'processed';
```

## Full-Text Search

PostgreSQL's full-text search capabilities are used for searching document and chunk content.

Example queries:

```sql
-- Search for documents containing specific terms
SELECT * FROM documents 
WHERE content_tsv @@ to_tsquery('english', 'search & terms');

-- Search for chunks containing specific terms, ranked by relevance
SELECT c.*, ts_rank(c.content_tsv, to_tsquery('english', 'search & terms')) AS rank
FROM chunks c
WHERE c.content_tsv @@ to_tsquery('english', 'search & terms')
ORDER BY rank DESC
LIMIT 10;
```

## Indexes

The following indexes are created to optimize common query patterns:

- GIN indexes on JSONB metadata fields for efficient metadata queries
- GIN indexes on tsvector columns for efficient full-text search
- B-tree indexes on common query fields (filename, folder, uploaded)
- Partial indexes for filtering by processing status
- Expression indexes for filtering by file type and tags

## API Endpoints

The PostgreSQL-specific features are exposed through the following API endpoints:

- `GET /api/documents/search/advanced`: Advanced search using full-text search and JSONB operators

Example usage:

```
GET /api/documents/search/advanced?query=important%20information&metadata={"file_type":"pdf","tags":["important"]}
```

## Monitoring

For monitoring PostgreSQL performance, consider using the following tools:

- pg_stat_statements: For tracking query performance
- pgBadger: For analyzing PostgreSQL logs
- pgAdmin: For general PostgreSQL administration and monitoring

## Maintenance

Regular maintenance tasks:

- VACUUM ANALYZE: To update statistics and reclaim space
- REINDEX: To rebuild indexes for optimal performance
- pg_dump: For regular backups

Consider setting up automated maintenance tasks using cron or a similar scheduler.
"""
    
    try:
        with open(guide_file, 'w') as f:
            f.write(guide_content)
        
        print(f"Created PostgreSQL optimization guide: {guide_file}")
    except Exception as e:
        print(f"Error creating guide: {e}")

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="PostgreSQL-Specific Enhancements for Metis RAG")
    parser.add_argument("--apply", action="store_true", help="Apply the enhancements")
    args = parser.parse_args()
    
    print("PostgreSQL-Specific Enhancements for Metis RAG")
    print("==============================================")
    
    # Check if PostgreSQL is available
    if not check_postgres_connection():
        print("\nSkipping enhancements as PostgreSQL is not available.")
        return 1
    
    if args.apply:
        print("\nApplying PostgreSQL enhancements...")
        
        # Apply SQL enhancements
        db = SessionLocal()
        try:
            for enhancement in POSTGRES_ENHANCEMENTS:
                apply_sql_enhancements(db, enhancement)
        finally:
            db.close()
        
        # Modify repository file
        modify_repository_file()
        
        # Modify session file
        modify_session_file()
        
        # Add API endpoint
        add_api_endpoint()
        
        # Create migration script
        create_migration_script()
        
        # Create PostgreSQL configuration guide
        create_postgres_config_guide()
        
        print("\nPostgreSQL enhancements applied successfully!")
    else:
        print("\nThis script will apply the following enhancements:")
        print("  1. Add JSONB operators for metadata queries")
        print("  2. Add full-text search capabilities")
        print("  3. Optimize connection pooling")
        print("  4. Add index optimizations")
        print("  5. Add PostgreSQL-specific repository methods")
        print("  6. Add PostgreSQL-specific API endpoints")
        print("  7. Create a migration script")
        print("  8. Create a PostgreSQL configuration guide")
        print("\nRun with --apply to apply these enhancements.")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/initialize_test_chroma.py
================
#!/usr/bin/env python3
"""
Initialize a fresh ChromaDB database for testing.
This script creates a new ChromaDB database in the test_e2e_chroma directory.
"""

import os
import sys
import shutil
import logging
import chromadb
from chromadb.config import Settings

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("initialize_test_chroma")

def initialize_chroma_db(db_dir: str):
    """
    Initialize a fresh ChromaDB database.
    
    Args:
        db_dir: Directory to store the ChromaDB database
    """
    try:
        logger.info(f"Initializing ChromaDB in directory: {db_dir}")
        
        # Remove existing directory if it exists
        if os.path.exists(db_dir):
            logger.info(f"Removing existing directory: {db_dir}")
            shutil.rmtree(db_dir)
        
        # Create directory
        os.makedirs(db_dir, exist_ok=True)
        
        # Initialize ChromaDB
        chroma_client = chromadb.PersistentClient(
            path=db_dir,
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        
        # Create a test collection
        collection = chroma_client.create_collection(
            name="documents",
            metadata={"description": "Test collection for documents"}
        )
        
        logger.info(f"Created collection: {collection.name}")
        logger.info(f"ChromaDB initialized successfully in {db_dir}")
        
        return True
    except Exception as e:
        logger.error(f"Error initializing ChromaDB: {str(e)}")
        return False

def main():
    """Main function"""
    logger.info("Starting ChromaDB initialization...")
    
    # Initialize ChromaDB in test_e2e_chroma directory
    db_dir = "test_e2e_chroma"
    
    if initialize_chroma_db(db_dir):
        logger.info("ChromaDB initialization completed successfully")
        return 0
    else:
        logger.error("ChromaDB initialization failed")
        return 1

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/make_user_admin.py
================
#!/usr/bin/env python
"""
Script to make a user an admin
"""
import asyncio
import sys
import os
from uuid import UUID

# Add the parent directory to the path so we can import from app
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.db.session import get_session
from app.db.repositories.user_repository import UserRepository
from app.models.user import UserUpdate


async def make_user_admin(username: str):
    """
    Make a user an admin
    
    Args:
        username: Username of the user to make admin
    """
    print(f"Making user '{username}' an admin...")
    
    async for session in get_session():
        # Get the user repository
        user_repo = UserRepository(session)
        
        # Get the user by username
        user = await user_repo.get_by_username(username)
        
        if not user:
            print(f"User '{username}' not found")
            return
        
        # Check if user is already an admin
        if user.is_admin:
            print(f"User '{username}' is already an admin")
            return
        
        # Update the user to make them an admin
        user_data = UserUpdate(is_admin=True)
        updated_user = await user_repo.update_user(user.id, user_data)
        
        if updated_user:
            print(f"User '{username}' is now an admin")
        else:
            print(f"Failed to update user '{username}'")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python make_user_admin.py <username>")
        sys.exit(1)
    
    username = sys.argv[1]
    asyncio.run(make_user_admin(username))

================
File: scripts/migrate_database.py
================
#!/usr/bin/env python3
"""
Database Migration Script for Metis RAG

This script migrates data between SQLite and PostgreSQL databases:
1. Export data from source database to JSON
2. Import data to target database from JSON
3. Verify data integrity after migration

Usage:
    python migrate_database.py --source sqlite|postgresql --target sqlite|postgresql [--export-file data/export.json]
"""
import os
import sys
import json
import argparse
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.db.session import Base, engine, SessionLocal
from app.core.config import DATABASE_TYPE, DATABASE_URL
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

class DatabaseMigrator:
    """Database migration class"""
    
    def __init__(self, source_type: str, target_type: str, export_file: str):
        self.source_type = source_type
        self.target_type = target_type
        self.export_file = export_file
        
        # Initialize source database session
        self.source_url = self._get_database_url(source_type)
        self.source_engine = create_engine(self.source_url)
        self.SourceSession = sessionmaker(autocommit=False, autoflush=False, bind=self.source_engine)
        self.source_session = self.SourceSession()
        
        # Initialize target database session
        self.target_url = self._get_database_url(target_type)
        self.target_engine = create_engine(self.target_url)
        self.TargetSession = sessionmaker(autocommit=False, autoflush=False, bind=self.target_engine)
        self.target_session = self.TargetSession()
        
        print(f"Initialized database migrator")
        print(f"Source database: {source_type} ({self.source_url})")
        print(f"Target database: {target_type} ({self.target_url})")
        print(f"Export file: {export_file}")
    
    def _get_database_url(self, db_type: str) -> str:
        """Get database URL based on type"""
        if db_type == "sqlite":
            return "sqlite:///./data/metis_rag.db"
        elif db_type == "postgresql":
            # Use environment variables or default values
            pg_user = os.environ.get("POSTGRES_USER", "postgres")
            pg_password = os.environ.get("POSTGRES_PASSWORD", "postgres")
            pg_host = os.environ.get("POSTGRES_HOST", "localhost")
            pg_port = os.environ.get("POSTGRES_PORT", "5432")
            pg_db = os.environ.get("POSTGRES_DB", "metis_rag")
            return f"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}"
        else:
            raise ValueError(f"Unsupported database type: {db_type}")
    
    def cleanup(self):
        """Clean up resources"""
        self.source_session.close()
        self.target_session.close()
    
    def export_data(self) -> Dict[str, Any]:
        """Export data from source database to JSON"""
        print("\nExporting data from source database...")
        
        data = {
            "metadata": {
                "source_type": self.source_type,
                "timestamp": datetime.now().isoformat(),
                "version": "1.0"
            },
            "tables": {}
        }
        
        # Get list of tables
        if self.source_type == "sqlite":
            tables_query = "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';"
        else:
            tables_query = "SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname='public';"
        
        tables_result = self.source_session.execute(text(tables_query))
        tables = [row[0] for row in tables_result]
        
        # Export data from each table
        for table in tables:
            print(f"  Exporting table: {table}")
            
            # Get table data
            data_query = f"SELECT * FROM {table};"
            data_result = self.source_session.execute(text(data_query))
            
            # Get column names
            columns = data_result.keys()
            
            # Convert to list of dictionaries
            rows = []
            for row in data_result:
                # Convert row to dictionary with proper JSON serialization
                row_dict = {}
                for i, column in enumerate(columns):
                    value = row[i]
                    # Handle special types
                    if isinstance(value, datetime):
                        row_dict[column] = value.isoformat()
                    elif hasattr(value, '__str__'):
                        row_dict[column] = str(value)
                    else:
                        row_dict[column] = value
                rows.append(row_dict)
            
            # Add to data
            data["tables"][table] = {
                "columns": list(columns),
                "rows": rows
            }
            
            print(f"    Exported {len(rows)} rows")
        
        # Write to file
        os.makedirs(os.path.dirname(self.export_file), exist_ok=True)
        with open(self.export_file, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"Data exported to: {self.export_file}")
        return data
    
    def import_data(self) -> Dict[str, Any]:
        """Import data to target database from JSON"""
        print("\nImporting data to target database...")
        
        # Read data from file
        with open(self.export_file, 'r') as f:
            data = json.load(f)
        
        results = {
            "tables": {},
            "total_rows": 0
        }
        
        # Import data to each table
        for table, table_data in data["tables"].items():
            print(f"  Importing table: {table}")
            
            # Skip alembic_version table
            if table == "alembic_version":
                print(f"    Skipping alembic_version table")
                continue
            
            # Clear existing data
            clear_query = f"DELETE FROM {table};"
            try:
                self.target_session.execute(text(clear_query))
                self.target_session.commit()
                print(f"    Cleared existing data")
            except Exception as e:
                self.target_session.rollback()
                print(f"    Error clearing data: {str(e)}")
            
            # Import rows
            rows = table_data["rows"]
            imported_count = 0
            
            for row in rows:
                # Build insert query
                columns = ", ".join(row.keys())
                placeholders = ", ".join([f":{col}" for col in row.keys()])
                insert_query = f"INSERT INTO {table} ({columns}) VALUES ({placeholders});"
                
                try:
                    self.target_session.execute(text(insert_query), row)
                    imported_count += 1
                except Exception as e:
                    self.target_session.rollback()
                    print(f"    Error importing row: {str(e)}")
            
            # Commit changes
            self.target_session.commit()
            
            # Add to results
            results["tables"][table] = {
                "total_rows": len(rows),
                "imported_rows": imported_count
            }
            results["total_rows"] += imported_count
            
            print(f"    Imported {imported_count}/{len(rows)} rows")
        
        print(f"Data import completed: {results['total_rows']} total rows imported")
        return results
    
    def verify_data(self) -> Dict[str, Any]:
        """Verify data integrity after migration"""
        print("\nVerifying data integrity...")
        
        # Read data from file
        with open(self.export_file, 'r') as f:
            data = json.load(f)
        
        results = {
            "tables": {},
            "success": True
        }
        
        # Verify each table
        for table, table_data in data["tables"].items():
            print(f"  Verifying table: {table}")
            
            # Skip alembic_version table
            if table == "alembic_version":
                print(f"    Skipping alembic_version table")
                continue
            
            # Get row count
            count_query = f"SELECT COUNT(*) FROM {table};"
            source_count = len(table_data["rows"])
            
            try:
                target_count_result = self.target_session.execute(text(count_query))
                target_count = target_count_result.scalar()
                
                # Verify count
                count_match = source_count == target_count
                
                # Add to results
                results["tables"][table] = {
                    "source_count": source_count,
                    "target_count": target_count,
                    "count_match": count_match
                }
                
                if not count_match:
                    results["success"] = False
                
                print(f"    Row count: {target_count}/{source_count} {'✓' if count_match else '✗'}")
            except Exception as e:
                results["tables"][table] = {
                    "source_count": source_count,
                    "target_count": 0,
                    "count_match": False,
                    "error": str(e)
                }
                results["success"] = False
                print(f"    Error verifying table: {str(e)}")
        
        # Print summary
        print("\nVerification Summary:")
        if results["success"]:
            print("  ✓ All tables verified successfully")
        else:
            print("  ✗ Some tables failed verification")
        
        return results
    
    def migrate_database(self) -> Dict[str, Any]:
        """Migrate data between databases"""
        results = {
            "source_type": self.source_type,
            "target_type": self.target_type,
            "export_file": self.export_file,
            "timestamp": datetime.now().isoformat()
        }
        
        # Export data
        export_data = self.export_data()
        results["export"] = {
            "tables": len(export_data["tables"]),
            "total_rows": sum(len(table_data["rows"]) for table_data in export_data["tables"].values())
        }
        
        # Import data
        import_results = self.import_data()
        results["import"] = import_results
        
        # Verify data
        verify_results = self.verify_data()
        results["verify"] = verify_results
        
        # Print summary
        print("\nMigration Summary:")
        print(f"  Source: {self.source_type}")
        print(f"  Target: {self.target_type}")
        print(f"  Tables: {results['export']['tables']}")
        print(f"  Rows: {results['import']['total_rows']}/{results['export']['total_rows']}")
        print(f"  Verification: {'Successful' if verify_results['success'] else 'Failed'}")
        
        return results

async def run_migration(args):
    """Run the database migration"""
    print(f"\nRunning database migration from {args.source} to {args.target}...")
    
    # Create migrator instance
    migrator = DatabaseMigrator(args.source, args.target, args.export_file)
    
    try:
        # Migrate database
        results = migrator.migrate_database()
        
        print("\nMigration completed successfully!")
        return 0
    except Exception as e:
        print(f"Error migrating database: {e}")
        return 1
    finally:
        migrator.cleanup()

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Database Migration for Metis RAG")
    parser.add_argument("--source", type=str, choices=["sqlite", "postgresql"], required=True,
                        help="Source database type")
    parser.add_argument("--target", type=str, choices=["sqlite", "postgresql"], required=True,
                        help="Target database type")
    parser.add_argument("--export-file", type=str, default="data/export.json",
                        help="Export file path (default: data/export.json)")
    args = parser.parse_args()
    
    # Check if source and target are the same
    if args.source == args.target:
        print(f"Error: Source and target databases are the same: {args.source}")
        return 1
    
    # Run migration
    result = asyncio.run(run_migration(args))
    
    return result

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/migrate_to_async_database_tool.py
================
#!/usr/bin/env python
"""
Migration script to transition from the old synchronous DatabaseTool to the new async version.

This script:
1. Renames the old DatabaseTool to DatabaseToolLegacy
2. Renames the new DatabaseToolAsync to DatabaseTool
3. Updates imports in the codebase

Usage:
    python scripts/migrate_to_async_database_tool.py

"""
import os
import re
import shutil
from pathlib import Path

def main():
    """Main migration function"""
    print("Starting migration to async DatabaseTool...")
    
    # Get the project root directory
    project_root = Path(__file__).parent.parent
    
    # Step 1: Rename the old DatabaseTool to DatabaseToolLegacy
    old_tool_path = project_root / "app" / "rag" / "tools" / "database_tool.py"
    legacy_tool_path = project_root / "app" / "rag" / "tools" / "database_tool_legacy.py"
    
    if old_tool_path.exists():
        print(f"Renaming {old_tool_path} to {legacy_tool_path}")
        
        # Read the old file
        with open(old_tool_path, "r") as f:
            content = f.read()
        
        # Update class name
        content = content.replace("class DatabaseTool(Tool):", "class DatabaseToolLegacy(Tool):")
        
        # Write to legacy file
        with open(legacy_tool_path, "w") as f:
            f.write(content)
        
        print(f"Created legacy version at {legacy_tool_path}")
    else:
        print(f"Warning: Old tool file {old_tool_path} not found")
    
    # Step 2: Rename the new DatabaseToolAsync to DatabaseTool
    new_tool_path = project_root / "app" / "rag" / "tools" / "database_tool_async.py"
    
    if new_tool_path.exists():
        print(f"Renaming {new_tool_path} to {old_tool_path}")
        
        # Read the new file
        with open(new_tool_path, "r") as f:
            content = f.read()
        
        # Write to the original location
        with open(old_tool_path, "w") as f:
            f.write(content)
        
        print(f"Installed new async version at {old_tool_path}")
    else:
        print(f"Error: New tool file {new_tool_path} not found")
        return
    
    # Step 3: Update imports in the codebase
    print("Updating imports in the codebase...")
    
    # Directories to search for imports
    dirs_to_search = [
        project_root / "app",
        project_root / "tests"
    ]
    
    # Files to exclude
    exclude_files = [
        str(legacy_tool_path),
        str(old_tool_path)
    ]
    
    # Patterns to search for
    import_patterns = [
        (r"from app\.rag\.tools\.database_tool import DatabaseTool", 
         "from app.rag.tools.database_tool import DatabaseTool  # Async version"),
        (r"from app\.rag\.tools import database_tool", 
         "from app.rag.tools import database_tool  # Contains async DatabaseTool")
    ]
    
    # Counter for modified files
    modified_files = 0
    
    # Walk through directories
    for directory in dirs_to_search:
        for root, _, files in os.walk(directory):
            for file in files:
                if not file.endswith(".py"):
                    continue
                
                file_path = os.path.join(root, file)
                
                # Skip excluded files
                if file_path in exclude_files:
                    continue
                
                # Read file content
                with open(file_path, "r") as f:
                    content = f.read()
                
                # Check if any pattern matches
                original_content = content
                for pattern, replacement in import_patterns:
                    content = re.sub(pattern, replacement, content)
                
                # If content changed, write back to file
                if content != original_content:
                    with open(file_path, "w") as f:
                        f.write(content)
                    print(f"Updated imports in {file_path}")
                    modified_files += 1
    
    print(f"Updated imports in {modified_files} files")
    
    # Step 4: Create a backup of the original file
    backup_dir = project_root / "backups" / "database_tool"
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    backup_path = backup_dir / "database_tool_original.py"
    if legacy_tool_path.exists():
        print(f"Creating backup at {backup_path}")
        shutil.copy2(legacy_tool_path, backup_path)
    
    print("\nMigration completed successfully!")
    print("\nNext steps:")
    print("1. Run tests to ensure everything works correctly:")
    print("   pytest tests/unit/test_database_tool_async.py")
    print("2. Update your code to use the new async features if needed")
    print("3. If you encounter any issues, the original implementation is available at:")
    print(f"   {legacy_tool_path}")
    print("   and a backup is stored at:")
    print(f"   {backup_path}")

if __name__ == "__main__":
    main()

================
File: scripts/optimize_chunking_strategy.py
================
#!/usr/bin/env python3
"""
Chunking Strategy Optimization Script for Metis RAG

This script tests different chunking strategies and parameters to find the optimal
configuration for each database backend (SQLite and PostgreSQL).

Usage:
    python optimize_chunking_strategy.py --db-type sqlite|postgresql [--runs 3] [--output-file chunking_recommendations.json]
"""
import os
import sys
import json
import time
import asyncio
import argparse
import statistics
import tempfile
import uuid
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.db.session import Base, engine, SessionLocal
from app.db.repositories.document_repository import DocumentRepository
from app.models.document import Document, Chunk
from app.core.config import SETTINGS, DATABASE_TYPE, DATABASE_URL
from app.db.models import Document as DBDocument, Chunk as DBChunk
from app.db.adapters import to_uuid_or_str

# Import custom functions from benchmark script
from scripts.benchmark_database_performance import custom_pydantic_chunk_to_sqlalchemy, custom_save_document_with_chunks

# Test configuration
TEST_FILE_SIZES = [
    ("small", 5),      # 5 KB
    ("medium", 50),    # 50 KB
    ("large", 500),    # 500 KB
    ("xlarge", 2000)   # 2 MB
]

# Chunking strategies to test
CHUNKING_STRATEGIES = [
    "recursive",
    "token",
    "markdown",
    "semantic"
]

# Chunk size parameters to test
CHUNK_SIZES = [
    500,    # Small chunks
    1000,   # Medium chunks
    2000,   # Large chunks
    4000    # Extra large chunks
]

# Chunk overlap parameters to test
CHUNK_OVERLAPS = [
    50,     # Minimal overlap
    100,    # Small overlap
    200,    # Medium overlap
    400     # Large overlap
]

class ChunkingStrategyOptimizer:
    """Chunking strategy optimization class"""
    
    def __init__(self, db_type: str, num_runs: int = 3):
        self.db_type = db_type
        self.num_runs = num_runs
        self.results = {
            "metadata": {
                "db_type": db_type,
                "timestamp": datetime.now().isoformat(),
                "num_runs": num_runs
            },
            "strategy_results": [],
            "recommendations": {}
        }
        
        # Initialize database session
        self.db_session = SessionLocal()
        
        # Initialize repositories
        self.document_repository = DocumentRepository(self.db_session)
        
        # Create folders in database
        self._ensure_folders_exist()
        
        print(f"Initialized chunking optimizer for {db_type} database")
        print(f"Database URL: {DATABASE_URL}")
        
    def _ensure_folders_exist(self):
        """Ensure the root and benchmark folders exist in the database"""
        try:
            # Import Folder model
            from app.db.models import Folder
            
            # Check if root folder exists
            root_folder = self.db_session.query(Folder).filter(Folder.path == "/").first()
            if not root_folder:
                # Create root folder
                root_folder = Folder(
                    path="/",
                    name="Root",
                    parent_path=None
                )
                self.db_session.add(root_folder)
                self.db_session.commit()
                print("Created root folder in database")
            
            # Check if optimization folder exists
            opt_folder = self.db_session.query(Folder).filter(Folder.path == "/optimization").first()
            if not opt_folder:
                # Create optimization folder
                opt_folder = Folder(
                    path="/optimization",
                    name="Optimization",
                    parent_path="/"
                )
                self.db_session.add(opt_folder)
                self.db_session.commit()
                print("Created optimization folder in database")
        except Exception as e:
            self.db_session.rollback()
            print(f"Error creating folders: {e}")
        
    def cleanup(self):
        """Clean up resources"""
        self.db_session.close()
        
    def generate_test_file(self, size_kb: int, file_type: str = "txt"):
        """Generate a test file of the specified size"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix=f".{file_type}")
        
        try:
            # Generate content
            content = ""
            if file_type == "txt":
                # Generate paragraphs of text
                paragraph = "This is a test paragraph for document processing performance testing. " * 10
                paragraphs_needed = (size_kb * 1024) // len(paragraph)
                content = "\n\n".join([paragraph for _ in range(paragraphs_needed)])
            elif file_type == "md":
                # Generate markdown content
                paragraph = "This is a test paragraph for document processing performance testing. " * 10
                paragraphs_needed = (size_kb * 1024) // (len(paragraph) + 20)  # Account for markdown syntax
                
                content = "# Test Document\n\n"
                for i in range(paragraphs_needed):
                    content += f"## Section {i+1}\n\n{paragraph}\n\n"
            
            # Write content to file
            with os.fdopen(fd, 'w') as f:
                f.write(content)
            
            return path
        except Exception as e:
            os.close(fd)
            os.unlink(path)
            raise e
    
    class CustomChunkingStrategy:
        """Custom chunking strategy for testing different parameters"""
        
        def __init__(self, strategy: str, chunk_size: int, chunk_overlap: int):
            self.strategy = strategy
            self.chunk_size = chunk_size
            self.chunk_overlap = chunk_overlap
            
        async def process_document(self, document: Document) -> Document:
            """Process a document by splitting it into chunks"""
            from app.rag.document_processor import DocumentProcessor
            
            # Create a document processor with the specified parameters
            processor = DocumentProcessor(
                chunking_strategy=self.strategy,
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
            
            # Process the document
            processed_document = await processor.process_document(document)
            
            # Ensure all chunk metadata is a simple dictionary
            for chunk in processed_document.chunks:
                if not isinstance(chunk.metadata, dict):
                    chunk.metadata = dict(chunk.metadata) if chunk.metadata else {}
            
            return processed_document
    
    async def test_chunking_strategy(self, strategy: str, chunk_size: int, chunk_overlap: int, file_size: Tuple[str, int]) -> Dict[str, Any]:
        """Test a specific chunking strategy with given parameters"""
        size_name, size_kb = file_size
        
        # Generate test files in different formats
        results = []
        
        for file_type in ["txt", "md"]:
            # Generate test file
            file_path = self.generate_test_file(size_kb, file_type)
            file_name = f"opt_{size_name}_{strategy}_{chunk_size}_{chunk_overlap}_{self.db_type}.{file_type}"
            
            try:
                # Read file content
                with open(file_path, 'r') as f:
                    content = f.read()
                
                # Create document in the database
                document = self.document_repository.create_document(
                    filename=file_name,
                    content=content,
                    metadata={
                        "file_type": file_type, 
                        "test_size": size_name, 
                        "optimization": True,
                        "strategy": strategy,
                        "chunk_size": chunk_size,
                        "chunk_overlap": chunk_overlap
                    },
                    folder="/optimization"
                )
                
                # Get document ID as string
                document_id_str = str(document.id)
                
                # Create chunking strategy
                chunking_strategy = self.CustomChunkingStrategy(strategy, chunk_size, chunk_overlap)
                
                # Process times
                process_times = []
                chunk_counts = []
                insert_times = []
                retrieve_times = []
                
                for _ in range(self.num_runs):
                    # Measure document processing time
                    start_time = time.time()
                    processed_document = await chunking_strategy.process_document(document)
                    process_time = time.time() - start_time
                    
                    # Record chunk count
                    chunk_count = len(processed_document.chunks)
                    
                    # Measure chunk insertion time
                    start_time = time.time()
                    custom_save_document_with_chunks(self.db_session, processed_document)
                    insert_time = time.time() - start_time
                    
                    # Measure chunk retrieval time
                    start_time = time.time()
                    retrieved_document = self.document_repository.get_document_with_chunks(document_id_str)
                    retrieve_time = time.time() - start_time
                    
                    # Record times
                    process_times.append(process_time)
                    chunk_counts.append(chunk_count)
                    insert_times.append(insert_time)
                    retrieve_times.append(retrieve_time)
                
                # Calculate average metrics
                avg_process_time = statistics.mean(process_times)
                avg_chunk_count = statistics.mean(chunk_counts)
                avg_insert_time = statistics.mean(insert_times)
                avg_retrieve_time = statistics.mean(retrieve_times)
                
                # Calculate per-chunk times
                per_chunk_process_time = avg_process_time / avg_chunk_count if avg_chunk_count > 0 else 0
                per_chunk_insert_time = avg_insert_time / avg_chunk_count if avg_chunk_count > 0 else 0
                per_chunk_retrieve_time = avg_retrieve_time / avg_chunk_count if avg_chunk_count > 0 else 0
                
                # Calculate overall score (lower is better)
                # Weight factors can be adjusted based on importance
                process_weight = 0.3
                insert_weight = 0.4
                retrieve_weight = 0.3
                
                overall_score = (
                    process_weight * per_chunk_process_time +
                    insert_weight * per_chunk_insert_time +
                    retrieve_weight * per_chunk_retrieve_time
                )
                
                result = {
                    "file_type": file_type,
                    "size_name": size_name,
                    "size_kb": size_kb,
                    "strategy": strategy,
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap,
                    "avg_chunk_count": avg_chunk_count,
                    "avg_process_time": avg_process_time,
                    "avg_insert_time": avg_insert_time,
                    "avg_retrieve_time": avg_retrieve_time,
                    "per_chunk_process_time": per_chunk_process_time,
                    "per_chunk_insert_time": per_chunk_insert_time,
                    "per_chunk_retrieve_time": per_chunk_retrieve_time,
                    "overall_score": overall_score
                }
                
                results.append(result)
                
                print(f"  {file_type}, {size_name} ({size_kb} KB), {strategy}, size={chunk_size}, overlap={chunk_overlap}: " +
                      f"Chunks: {avg_chunk_count:.1f}, Process: {avg_process_time:.4f}s, " +
                      f"Insert: {avg_insert_time:.4f}s, Retrieve: {avg_retrieve_time:.4f}s, " +
                      f"Score: {overall_score:.6f}")
                
                # Clean up - delete document and chunks
                self.document_repository.delete_document(document_id_str)
                
            finally:
                # Clean up test file
                try:
                    os.unlink(file_path)
                except:
                    pass
        
        return {
            "size_name": size_name,
            "size_kb": size_kb,
            "strategy": strategy,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "results": results
        }
    
    async def optimize_chunking_strategies(self):
        """Test different chunking strategies and parameters"""
        print("\nOptimizing chunking strategies...")
        
        # Ensure test folder exists
        self.document_repository._ensure_folder_exists("/optimization")
        
        # Test each combination of parameters
        for size_tuple in TEST_FILE_SIZES:
            for strategy in CHUNKING_STRATEGIES:
                for chunk_size in CHUNK_SIZES:
                    for chunk_overlap in CHUNK_OVERLAPS:
                        # Skip invalid combinations
                        if chunk_overlap >= chunk_size:
                            continue
                            
                        # Test this combination
                        result = await self.test_chunking_strategy(strategy, chunk_size, chunk_overlap, size_tuple)
                        self.results["strategy_results"].append(result)
        
        # Generate recommendations
        self._generate_recommendations()
        
        return self.results
    
    def _generate_recommendations(self):
        """Generate recommendations based on test results"""
        # Group results by file size and type
        grouped_results = {}
        
        for result in self.results["strategy_results"]:
            for file_result in result["results"]:
                key = f"{file_result['size_name']}_{file_result['file_type']}"
                if key not in grouped_results:
                    grouped_results[key] = []
                grouped_results[key].append(file_result)
        
        # Find best strategy for each file size and type
        recommendations = {}
        
        for key, results in grouped_results.items():
            # Sort by overall score (lower is better)
            sorted_results = sorted(results, key=lambda x: x["overall_score"])
            
            # Get best result
            best_result = sorted_results[0]
            
            # Create recommendation
            recommendation = {
                "file_type": best_result["file_type"],
                "size_name": best_result["size_name"],
                "size_kb": best_result["size_kb"],
                "strategy": best_result["strategy"],
                "chunk_size": best_result["chunk_size"],
                "chunk_overlap": best_result["chunk_overlap"],
                "avg_chunk_count": best_result["avg_chunk_count"],
                "overall_score": best_result["overall_score"],
                "justification": f"This configuration provides the best balance of processing speed, " +
                                f"database insertion performance, and retrieval performance for " +
                                f"{best_result['size_name']} {best_result['file_type']} files."
            }
            
            recommendations[key] = recommendation
        
        # Add recommendations to results
        self.results["recommendations"] = recommendations
        
        # Print recommendations
        print("\nRecommended chunking strategies:")
        for key, recommendation in recommendations.items():
            print(f"  {recommendation['size_name']} {recommendation['file_type']}: " +
                  f"Strategy: {recommendation['strategy']}, " +
                  f"Size: {recommendation['chunk_size']}, " +
                  f"Overlap: {recommendation['chunk_overlap']}")
    
    def save_results(self, output_file: str = None):
        """Save optimization results to a JSON file"""
        if output_file is None:
            # Create results directory if it doesn't exist
            results_dir = os.path.join(project_root, "tests", "results")
            os.makedirs(results_dir, exist_ok=True)
            
            # Create filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(results_dir, f"chunking_optimization_{self.db_type}_{timestamp}.json")
        
        # Save results to file
        with open(output_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"\nResults saved to: {output_file}")
        return output_file

async def run_optimization(args):
    """Run the chunking strategy optimization"""
    print(f"\nRunning chunking strategy optimization for {args.db_type}...")
    
    # Create optimizer instance
    optimizer = ChunkingStrategyOptimizer(args.db_type, args.runs)
    
    try:
        # Run optimization
        await optimizer.optimize_chunking_strategies()
        
        # Save results
        optimizer.save_results(args.output_file)
        
        print("\nOptimization completed successfully!")
        return 0
    except Exception as e:
        print(f"Error running optimization: {e}")
        return 1
    finally:
        optimizer.cleanup()

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Chunking Strategy Optimization for Metis RAG")
    parser.add_argument("--db-type", type=str, choices=["sqlite", "postgresql"], required=True,
                        help="Database type to optimize for")
    parser.add_argument("--runs", type=int, default=3, help="Number of runs for each test")
    parser.add_argument("--output-file", type=str, help="Output file path for optimization results")
    args = parser.parse_args()
    
    # Run optimization
    result = asyncio.run(run_optimization(args))
    
    return result

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/optimize_sqlite_for_concurrency.py
================
#!/usr/bin/env python3
"""
SQLite Optimization Script for Metis RAG

This script implements SQLite-specific optimizations:
1. Configure WAL (Write-Ahead Logging) mode for better concurrency
2. Optimize pragma settings for performance
3. Add indexes for common query patterns
4. Implement connection pooling optimizations

Usage:
    python optimize_sqlite_for_concurrency.py [--apply]
"""
import os
import sys
import json
import time
import argparse
import sqlite3
from pathlib import Path
from typing import List, Dict, Any, Optional

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.core.config import DATABASE_URL, DATABASE_TYPE

# SQLite optimization settings
SQLITE_PRAGMAS = {
    "journal_mode": "WAL",       # Use Write-Ahead Logging for better concurrency
    "synchronous": "NORMAL",     # Balance between safety and performance
    "cache_size": "-5000",       # Use 5MB of memory for database cache
    "foreign_keys": "ON",        # Enforce foreign key constraints
    "temp_store": "MEMORY",      # Store temporary tables in memory
    "mmap_size": "30000000",     # Memory-mapped I/O (30MB)
    "busy_timeout": "5000",      # Wait 5 seconds when database is locked
    "auto_vacuum": "INCREMENTAL" # Incremental vacuum to reclaim space
}

# SQL statements for index optimization
SQLITE_INDEXES = [
    "CREATE INDEX IF NOT EXISTS idx_documents_filename_folder ON documents (filename, folder)",
    "CREATE INDEX IF NOT EXISTS idx_documents_uploaded ON documents (uploaded)",
    "CREATE INDEX IF NOT EXISTS idx_chunks_document_id_index ON chunks (document_id, \"index\")",
    "CREATE INDEX IF NOT EXISTS idx_documents_processing_status ON documents (processing_status)",
    "CREATE INDEX IF NOT EXISTS idx_documents_file_type ON documents (json_extract(metadata, '$.file_type'))",
    "CREATE INDEX IF NOT EXISTS idx_chunks_embedding_quality ON chunks (embedding_quality)"
]

# Connection pooling configuration for app/db/session.py
SQLITE_CONNECTION_POOLING = """
# SQLite-specific engine configuration
if DATABASE_TYPE == 'sqlite':
    engine = create_engine(
        DATABASE_URL,
        connect_args={
            'check_same_thread': False,  # Allow multithreaded access
            'timeout': 30,               # Connection timeout in seconds
            'isolation_level': None      # Use autocommit mode
        },
        pool_size=DATABASE_POOL_SIZE,
        max_overflow=DATABASE_MAX_OVERFLOW,
        pool_pre_ping=True,
        pool_recycle=3600
    )
else:
    engine = create_engine(
        DATABASE_URL,
        pool_size=DATABASE_POOL_SIZE,
        max_overflow=DATABASE_MAX_OVERFLOW,
        pool_pre_ping=True,
        pool_recycle=3600
    )
"""

# SQLite repository methods for optimized queries
SQLITE_REPOSITORY_METHODS = '''
    def search_documents_with_json(self, metadata_query: Dict[str, Any]) -> List[Document]:
        """
        Search documents using SQLite JSON functions
        
        Args:
            metadata_query: Dictionary of metadata key-value pairs to search for
            
        Returns:
            List of matching documents
        """
        if DATABASE_TYPE != 'sqlite':
            raise ValueError("This method is only available for SQLite")
        
        # Build query conditions
        conditions = []
        params = {}
        
        for i, (key, value) in enumerate(metadata_query.items()):
            param_name = f"value_{i}"
            conditions.append(f"json_extract(doc_metadata, '$.{key}') = :{param_name}")
            params[param_name] = value if not isinstance(value, list) else json.dumps(value)
        
        # Combine conditions
        where_clause = " AND ".join(conditions)
        
        # Execute query
        stmt = text(f"SELECT * FROM documents WHERE {where_clause}")
        result = self.db.execute(stmt, params).scalars().all()
        
        # Convert to Pydantic models
        return [sqlalchemy_document_to_pydantic(doc) for doc in result]
    
    def full_text_search_documents(self, query: str) -> List[Document]:
        """
        Search documents using SQLite FTS (if available) or LIKE
        
        Args:
            query: Search query string
            
        Returns:
            List of matching documents
        """
        if DATABASE_TYPE != 'sqlite':
            raise ValueError("This method is only available for SQLite")
        
        # Check if FTS is available
        try:
            # Try to use FTS
            stmt = text("""
                SELECT d.* FROM documents d
                WHERE d.content LIKE :query
                ORDER BY length(d.content) ASC
                LIMIT 20
            """)
            result = self.db.execute(stmt, {"query": f"%{query}%"}).scalars().all()
        except Exception:
            # Fall back to simple LIKE query
            stmt = select(self.model).where(self.model.content.like(f"%{query}%"))
            result = self.db.execute(stmt).scalars().all()
        
        # Convert to Pydantic models
        return [sqlalchemy_document_to_pydantic(doc) for doc in result]
'''

def extract_database_path():
    """Extract the database file path from DATABASE_URL"""
    if DATABASE_TYPE != 'sqlite':
        print("Current database is not SQLite. These optimizations are only applicable to SQLite.")
        return None
    
    # Extract path from sqlite:/// URL
    if DATABASE_URL.startswith('sqlite:///'):
        db_path = DATABASE_URL[10:]
        return os.path.abspath(db_path)
    else:
        print(f"Unexpected SQLite URL format: {DATABASE_URL}")
        return None

def check_sqlite_version():
    """Check SQLite version and features"""
    try:
        conn = sqlite3.connect(':memory:')
        cursor = conn.cursor()
        
        # Get SQLite version
        cursor.execute('SELECT sqlite_version()')
        version = cursor.fetchone()[0]
        print(f"SQLite version: {version}")
        
        # Check if WAL mode is supported
        cursor.execute("PRAGMA journal_mode=WAL")
        journal_mode = cursor.fetchone()[0]
        wal_supported = journal_mode.upper() == 'WAL'
        print(f"WAL mode supported: {wal_supported}")
        
        # Check if JSON functions are available
        try:
            cursor.execute("SELECT json_extract('{\"a\":1}', '$.a')")
            json_supported = cursor.fetchone()[0] == 1
            print(f"JSON functions supported: {json_supported}")
        except sqlite3.OperationalError:
            json_supported = False
            print("JSON functions not supported")
        
        conn.close()
        
        return {
            'version': version,
            'wal_supported': wal_supported,
            'json_supported': json_supported
        }
    except Exception as e:
        print(f"Error checking SQLite version: {e}")
        return None

def optimize_sqlite_database(db_path):
    """Apply SQLite optimizations to the database"""
    if not os.path.exists(db_path):
        print(f"Database file not found: {db_path}")
        return False
    
    try:
        # Connect to the database
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Apply PRAGMA settings
        print("\nApplying PRAGMA settings:")
        for pragma, value in SQLITE_PRAGMAS.items():
            try:
                cursor.execute(f"PRAGMA {pragma}={value}")
                result = cursor.execute(f"PRAGMA {pragma}").fetchone()
                print(f"  {pragma} = {result[0]}")
            except sqlite3.OperationalError as e:
                print(f"  Error setting {pragma}: {e}")
        
        # Create indexes
        print("\nCreating indexes:")
        for index_stmt in SQLITE_INDEXES:
            try:
                cursor.execute(index_stmt)
                print(f"  {index_stmt[:60]}...")
            except sqlite3.OperationalError as e:
                print(f"  Error creating index: {e}")
        
        # Run ANALYZE to update statistics
        print("\nRunning ANALYZE to update statistics...")
        cursor.execute("ANALYZE")
        
        # Commit changes and close connection
        conn.commit()
        conn.close()
        
        print("\nSQLite optimizations applied successfully!")
        return True
    except Exception as e:
        print(f"Error optimizing SQLite database: {e}")
        return False

def modify_session_file():
    """Modify session.py to add SQLite-specific connection pooling configuration"""
    session_file = os.path.join(project_root, "app", "db", "session.py")
    
    try:
        with open(session_file, 'r') as f:
            content = f.read()
        
        # Check if SQLite-specific config already exists
        if "check_same_thread': False" in content:
            print("SQLite-specific connection pooling configuration already exists.")
            return
        
        # Find the engine creation
        engine_creation = "engine = create_engine("
        if engine_creation in content:
            # Find the end of the engine creation
            engine_end = content.find(")", content.find(engine_creation))
            
            # Replace the engine creation with the new configuration
            new_content = content.replace(
                content[content.find(engine_creation):engine_end+1],
                SQLITE_CONNECTION_POOLING
            )
            
            # Write back to file
            with open(session_file, 'w') as f:
                f.write(new_content)
            
            print(f"Added SQLite-specific connection pooling configuration to {session_file}")
        else:
            print(f"Could not find engine creation in {session_file}")
    
    except Exception as e:
        print(f"Error modifying session file: {e}")

def modify_repository_file():
    """Add SQLite-specific methods to document repository"""
    repo_file = os.path.join(project_root, "app", "db", "repositories", "document_repository.py")
    
    try:
        with open(repo_file, 'r') as f:
            content = f.read()
        
        # Check if methods already exist
        if "search_documents_with_json" in content:
            print("SQLite-specific repository methods already exist.")
            return
        
        # Find the end of the class definition
        import_section = "from app.db.adapters import sqlalchemy_document_to_pydantic, pydantic_document_to_sqlalchemy\n"
        if import_section not in content:
            import_section = "from app.db.adapters import "
        
        # Add imports if needed
        if "import json" not in content:
            content = content.replace(import_section, "import json\n" + import_section)
        
        if "from sqlalchemy import text" not in content:
            content = content.replace(import_section, "from sqlalchemy import text\n" + import_section)
        
        if "from app.core.config import DATABASE_TYPE" not in content:
            content = content.replace(import_section, "from app.core.config import DATABASE_TYPE\n" + import_section)
        
        # Find the end of the class definition
        class_end = content.rfind("}")
        if class_end == -1:
            class_end = content.rfind("return documents")
        
        if class_end != -1:
            # Insert new methods
            new_content = content[:class_end] + "\n" + SQLITE_REPOSITORY_METHODS + "\n" + content[class_end:]
            
            # Write back to file
            with open(repo_file, 'w') as f:
                f.write(new_content)
            
            print(f"Added SQLite-specific methods to {repo_file}")
        else:
            print(f"Could not find appropriate location to add methods in {repo_file}")
    
    except Exception as e:
        print(f"Error modifying repository file: {e}")

def create_sqlite_config_guide():
    """Create a guide for SQLite configuration"""
    guide_file = os.path.join(project_root, "docs", "deployment", "sqlite_optimization.md")
    os.makedirs(os.path.dirname(guide_file), exist_ok=True)
    
    guide_content = """# SQLite Optimization Guide for Metis RAG

This guide provides recommendations for optimizing SQLite for use with the Metis RAG system, particularly for concurrent access patterns.

## Write-Ahead Logging (WAL)

SQLite's default journal mode can cause contention when multiple processes try to write to the database. The Write-Ahead Logging (WAL) journal mode allows multiple readers to coexist with a single writer, significantly improving concurrency.

The Metis RAG system configures SQLite to use WAL mode with the following PRAGMA settings:

```sql
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
PRAGMA busy_timeout=5000;
```

These settings provide a good balance between performance, concurrency, and data safety.

## Memory Optimization

SQLite performance can be significantly improved by allocating more memory for its cache and operations:

```sql
PRAGMA cache_size=-5000;  -- Use 5MB of memory for database cache
PRAGMA temp_store=MEMORY; -- Store temporary tables in memory
PRAGMA mmap_size=30000000; -- Memory-mapped I/O (30MB)
```

Adjust these values based on your server's available memory.

## Connection Pooling

The Metis RAG system uses SQLAlchemy's connection pooling with SQLite-specific optimizations:

```python
engine = create_engine(
    DATABASE_URL,
    connect_args={
        'check_same_thread': False,  # Allow multithreaded access
        'timeout': 30,               # Connection timeout in seconds
        'isolation_level': None      # Use autocommit mode
    },
    pool_size=5,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=3600
)
```

The `check_same_thread=False` option allows connections to be used across threads, which is necessary for web applications. The `timeout` option specifies how long to wait for a locked database before giving up.

## Indexes

The following indexes are created to optimize common query patterns:

```sql
CREATE INDEX IF NOT EXISTS idx_documents_filename_folder ON documents (filename, folder);
CREATE INDEX IF NOT EXISTS idx_documents_uploaded ON documents (uploaded);
CREATE INDEX IF NOT EXISTS idx_chunks_document_id_index ON chunks (document_id, "index");
CREATE INDEX IF NOT EXISTS idx_documents_processing_status ON documents (processing_status);
CREATE INDEX IF NOT EXISTS idx_documents_file_type ON documents (json_extract(metadata, '$.file_type'));
CREATE INDEX IF NOT EXISTS idx_chunks_embedding_quality ON chunks (embedding_quality);
```

## JSON Support

SQLite 3.9.0 and later include built-in JSON functions that can be used to query JSON data stored in the database. The Metis RAG system uses these functions to query document and chunk metadata:

```sql
-- Find documents with a specific file type
SELECT * FROM documents WHERE json_extract(metadata, '$.file_type') = 'pdf';

-- Find chunks with specific tags
SELECT * FROM chunks WHERE json_extract(metadata, '$.tags') LIKE '%important%';
```

## Maintenance

Regular maintenance tasks:

1. **Vacuum**: Reclaim unused space and defragment the database
   ```sql
   VACUUM;
   ```

2. **Analyze**: Update statistics for the query planner
   ```sql
   ANALYZE;
   ```

3. **Integrity Check**: Verify database integrity
   ```sql
   PRAGMA integrity_check;
   ```

4. **Backup**: Create a backup of the database
   ```bash
   sqlite3 metis_rag.db ".backup metis_rag_backup.db"
   ```

## Concurrency Limitations

Despite the optimizations, SQLite still has limitations for highly concurrent workloads:

1. Only one writer can modify the database at a time
2. Readers can block writers in certain scenarios
3. Long-running transactions can cause contention

For high-concurrency production deployments, consider using PostgreSQL instead.

## Monitoring

Monitor SQLite performance using the following queries:

```sql
-- Check if WAL mode is enabled
PRAGMA journal_mode;

-- Check current cache size
PRAGMA cache_size;

-- List all indexes
SELECT * FROM sqlite_master WHERE type='index';

-- Check database size
PRAGMA page_count * PRAGMA page_size;
```

## Troubleshooting

Common issues and solutions:

1. **Database is locked**: Increase the busy timeout or reduce transaction duration
   ```sql
   PRAGMA busy_timeout=10000;  -- Wait 10 seconds for locks
   ```

2. **Slow queries**: Analyze the query plan
   ```sql
   EXPLAIN QUERY PLAN SELECT * FROM documents WHERE ...;
   ```

3. **Database corruption**: Run integrity check and recover from backup
   ```sql
   PRAGMA integrity_check;
   ```
"""
    
    try:
        with open(guide_file, 'w') as f:
            f.write(guide_content)
        
        print(f"Created SQLite optimization guide: {guide_file}")
    except Exception as e:
        print(f"Error creating guide: {e}")

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="SQLite Optimization for Metis RAG")
    parser.add_argument("--apply", action="store_true", help="Apply the optimizations")
    args = parser.parse_args()
    
    print("SQLite Optimization for Metis RAG")
    print("================================")
    
    # Check SQLite version and features
    sqlite_info = check_sqlite_version()
    if not sqlite_info:
        return 1
    
    # Extract database path
    db_path = extract_database_path()
    if not db_path:
        return 1
    
    print(f"\nDatabase path: {db_path}")
    
    if args.apply:
        print("\nApplying SQLite optimizations...")
        
        # Optimize SQLite database
        if not optimize_sqlite_database(db_path):
            return 1
        
        # Modify session file
        modify_session_file()
        
        # Modify repository file
        modify_repository_file()
        
        # Create SQLite configuration guide
        create_sqlite_config_guide()
        
        print("\nSQLite optimizations applied successfully!")
    else:
        print("\nThis script will apply the following optimizations:")
        print("  1. Configure WAL (Write-Ahead Logging) mode for better concurrency")
        print("  2. Optimize PRAGMA settings for performance")
        print("  3. Add indexes for common query patterns")
        print("  4. Implement connection pooling optimizations")
        print("  5. Add SQLite-specific repository methods")
        print("  6. Create a SQLite configuration guide")
        print("\nRun with --apply to apply these optimizations.")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/README_BACKGROUND_TASKS.md
================
# Background Task System Testing

This directory contains scripts for testing the Background Task System implementation in the Metis_RAG project.

## Prerequisites

Before running the tests, make sure you have the following dependencies installed:

```bash
pip install httpx psutil
```

## Running the Tests

### 1. Run Database Migrations

First, run the database migrations to create the necessary tables for the Background Task System:

```bash
python scripts/run_migrations.py
```

This will run the Alembic migrations to create the `background_tasks` table in the database.

### 2. Start the Application

Start the Metis_RAG application using the provided script:

```bash
python scripts/run_app.py
```

This script will:
1. Start the application with uvicorn
2. Wait for the application to start
3. Open a browser window to http://localhost:8000
4. Display application logs in the terminal

Alternatively, you can start the application manually:

```bash
uvicorn app.main:app --reload
```

The application should be running on http://localhost:8000.

### 3. Run the Tests

You can run all tests for the Background Task System using the provided script:

```bash
python scripts/run_background_task_tests.py
```

This script will:

1. Run database migrations
2. Start the application
3. Run the test_background_tasks.py script
4. Run pytest tests for the Background Task System
5. Stop the application

If you want to skip the database migrations, you can use the `--skip-migrations` flag:

```bash
python scripts/run_background_task_tests.py --skip-migrations
```

Alternatively, you can run the tests manually:

```bash
# In a separate terminal, run the test script
python scripts/test_background_tasks.py
```

The test_background_tasks.py script will:

1. Submit various types of tasks (document processing, vector store updates, report generation, system maintenance)
2. Test task dependencies
3. Test task priorities
4. Test scheduled tasks
5. Test task cancellation
6. Test concurrent task execution

## Viewing the Results

You can view the results of the tests in the terminal output. Additionally, you can access the Background Task System dashboard at:

```
http://localhost:8000/tasks
```

This dashboard provides a visual interface for monitoring tasks, viewing task details, and managing the Background Task System.

## Test Descriptions

### Document Processing Test

Tests the document processing task type, which processes a document and extracts its content.

### Vector Store Update Test

Tests the vector store update task type, which updates the vector store with new document embeddings.

### Report Generation Test

Tests the report generation task type, which generates reports based on document analysis.

### System Maintenance Test

Tests the system maintenance task type, which performs system maintenance operations like cleanup, optimization, and backups.

### Task Dependencies Test

Tests the task dependency system, which ensures that tasks are executed in the correct order based on their dependencies.

### Task Priorities Test

Tests the task priority system, which ensures that higher-priority tasks are executed before lower-priority tasks.

### Scheduled Tasks Test

Tests the scheduled task system, which allows tasks to be scheduled for execution at a specific time.

### Task Cancellation Test

Tests the task cancellation system, which allows tasks to be cancelled before or during execution.

### Concurrent Tasks Test

Tests the concurrent task execution system, which allows multiple tasks to be executed simultaneously.

## Troubleshooting

If you encounter any issues while running the tests, check the following:

1. Make sure the application is running on http://localhost:8000
2. Make sure the database migrations have been applied
3. Check the application logs for any errors
4. Make sure the required dependencies are installed

If you continue to experience issues, please refer to the Background Task System documentation or contact the development team.

================
File: scripts/README_SYSTEM_PROMPT_TESTING.md
================
# System Prompt Testing for Metis RAG

This directory contains scripts for testing different system prompts for the Metis RAG system.

## Overview

The `test_system_prompts.py` script allows you to compare the behavior of different system prompts by running the same set of queries against both the original complex prompt and the simplified prompt.

## Features

- Tests both individual queries and multi-turn conversations
- Runs predefined test scenarios to evaluate specific issues:
  - Empty Vector Store Test
  - Non-existent Document Test
  - User Information Test
  - Citation Test
- Generates detailed HTML reports with side-by-side comparisons
- Saves all results in JSON format for further analysis

## Usage

```bash
# Run all tests
python scripts/test_system_prompts.py

# Run only test queries
python scripts/test_system_prompts.py --test-queries

# Run only test scenarios
python scripts/test_system_prompts.py --test-scenarios

# Specify a custom output directory
python scripts/test_system_prompts.py --output-dir custom_results
```

## Test Queries

The script includes the following test queries based on the example chat:

1. "hello"
2. "where is Paris in comparison to Madrid"
3. "distance and direction"
4. "how can I get there from the US?"
5. "I will be leaving from Washington DC"

## Test Scenarios

The script includes the following test scenarios:

1. **Empty Vector Store Test**: Tests how the system handles queries when no documents exist
2. **Non-existent Document Test**: Tests how the system handles queries about specific documents that don't exist
3. **User Information Test**: Tests if the system remembers user information across the conversation
4. **Citation Test**: Tests how the system uses citations when documents are available

## Results

The script generates the following output:

1. JSON files with detailed test results
2. HTML reports with side-by-side comparisons of responses
3. Console output with test progress

Results are saved in the `test_results` directory by default, with a timestamp to distinguish different test runs.

## Customization

You can customize the script by:

1. Modifying the `TEST_QUERIES` list to add your own test queries
2. Adding new test scenarios to the `TEST_SCENARIOS` list
3. Implementing custom setup functions in the `setup_scenario` method

## Issues to Look For

When analyzing the results, pay attention to:

1. **Document Hallucination**: Does the system claim to have documents it doesn't have?
2. **Memory Loss**: Does the system remember user information across the conversation?
3. **Content Fabrication**: Does the system generate text instead of admitting it doesn't have information?
4. **Citation Misuse**: Does the system use citation markers correctly?
5. **Empty Results Handling**: Does the system clearly communicate when no documents are found?
6. **Response Repetition**: Does the system repeat previous responses or greetings?

## Example Issues from Sample Chat

The sample chat with the simplified prompt showed several issues:

1. **Greeting Repetition**: The system repeated "Hello there! How can I help you today? [1]" in multiple responses
2. **Citation Misuse**: The system used citation markers [1] even when it didn't seem to have actual documents
3. **Unclear Sourcing**: The system provided information without clearly indicating if it was from retrieved documents or general knowledge

================
File: scripts/run_app.py
================
#!/usr/bin/env python3
"""
Run the Metis_RAG application
"""
import os
import sys
import subprocess
import time
import webbrowser

def run_app():
    """
    Run the Metis_RAG application
    """
    print("Starting Metis_RAG application...")
    
    # Get the project root directory
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Change to the project root directory
    os.chdir(project_root)
    
    # Run the application
    try:
        # Start the application
        process = subprocess.Popen(
            ["uvicorn", "app.main:app", "--reload"],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )
        
        # Wait for the application to start
        print("Waiting for application to start...")
        started = False
        for line in process.stdout:
            print(line, end="")
            if "Application startup complete" in line:
                started = True
                break
        
        if started:
            print("\nApplication started successfully!")
            
            # Open the browser
            print("Opening browser...")
            webbrowser.open("http://localhost:8000")
            
            # Keep the application running
            print("\nPress Ctrl+C to stop the application")
            try:
                while True:
                    line = process.stdout.readline()
                    if line:
                        print(line, end="")
                    if process.poll() is not None:
                        break
            except KeyboardInterrupt:
                print("\nStopping application...")
            finally:
                process.terminate()
                process.wait()
        else:
            print("Error: Application failed to start")
            return 1
        
        return 0
    except Exception as e:
        print(f"Error running application: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(run_app())

================
File: scripts/run_background_task_tests.py
================
#!/usr/bin/env python3
"""
Run all tests for the Background Task System
"""
import os
import sys
import subprocess
import time
import signal
import argparse

def run_tests(skip_migrations=False):
    """
    Run all tests for the Background Task System
    
    Args:
        skip_migrations: Skip running migrations
        
    Returns:
        Exit code
    """
    print("=== Background Task System Tests ===")
    
    # Get the project root directory
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Change to the project root directory
    os.chdir(project_root)
    
    # Run migrations if not skipped
    if not skip_migrations:
        print("\n1. Running database migrations...")
        try:
            migration_result = subprocess.run(
                ["python", "scripts/run_migrations.py"],
                check=True,
                capture_output=True,
                text=True
            )
            print(migration_result.stdout)
            print("Migrations completed successfully!")
        except subprocess.CalledProcessError as e:
            print(f"Error running migrations: {e}")
            print(e.stdout)
            print(e.stderr)
            return 1
    else:
        print("\n1. Skipping database migrations...")
    
    # Start the application
    print("\n2. Starting the application...")
    try:
        app_process = subprocess.Popen(
            ["uvicorn", "app.main:app", "--reload"],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )
        
        # Wait for the application to start
        print("Waiting for application to start...")
        started = False
        for _ in range(30):  # Wait up to 30 seconds
            try:
                # Check if the application is running
                response = subprocess.run(
                    ["curl", "-s", "http://localhost:8000/api/v1/system/health"],
                    check=True,
                    capture_output=True,
                    text=True
                )
                if "status" in response.stdout:
                    started = True
                    break
            except subprocess.CalledProcessError:
                pass
            
            time.sleep(1)
        
        if not started:
            print("Error: Application failed to start")
            app_process.terminate()
            app_process.wait()
            return 1
        
        print("Application started successfully!")
        
        # Run the tests
        print("\n3. Running background task tests...")
        try:
            test_result = subprocess.run(
                ["python", "scripts/test_background_tasks.py"],
                check=True,
                capture_output=True,
                text=True
            )
            print(test_result.stdout)
            print("Tests completed successfully!")
            
            # Run pytest tests
            print("\n4. Running pytest tests...")
            pytest_result = subprocess.run(
                ["pytest", "tests/test_background_tasks.py", "-v"],
                check=True,
                capture_output=True,
                text=True
            )
            print(pytest_result.stdout)
            print("Pytest tests completed successfully!")
            
            return 0
        except subprocess.CalledProcessError as e:
            print(f"Error running tests: {e}")
            print(e.stdout)
            print(e.stderr)
            return 1
        finally:
            # Stop the application
            print("\nStopping application...")
            app_process.terminate()
            app_process.wait()
    except Exception as e:
        print(f"Error: {str(e)}")
        return 1

def parse_args():
    """
    Parse command line arguments
    
    Returns:
        Parsed arguments
    """
    parser = argparse.ArgumentParser(description="Run Background Task System tests")
    parser.add_argument(
        "--skip-migrations",
        action="store_true",
        help="Skip running database migrations"
    )
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    try:
        sys.exit(run_tests(skip_migrations=args.skip_migrations))
    except KeyboardInterrupt:
        print("\nTests interrupted by user")
        # Ensure any running processes are terminated
        for proc in subprocess.Popen(['ps', '-A'], stdout=subprocess.PIPE).communicate()[0].splitlines():
            if b'uvicorn' in proc:
                pid = int(proc.split(None, 1)[0])
                os.kill(pid, signal.SIGKILL)
        sys.exit(1)

================
File: scripts/run_document_processing_performance_tests.py
================
#!/usr/bin/env python3
"""
Run document processing performance tests and save results
"""
import os
import sys
import json
import asyncio
import argparse
from datetime import datetime
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from app.db.session import SessionLocal
from app.db.repositories.document_repository import DocumentRepository
from app.rag.document_processor import DocumentProcessor
from app.rag.document_analysis_service import DocumentAnalysisService
from app.rag.vector_store import VectorStore
from tests.test_document_processing_performance import (
    run_processing_tests,
    run_analysis_tests,
    TEST_FILE_SIZES,
    TEST_CHUNKING_STRATEGIES,
    ensure_test_folder_exists
)

def save_results_to_file(results, test_type):
    """Save test results to a JSON file"""
    # Create results directory if it doesn't exist
    results_dir = os.path.join(project_root, "tests", "results")
    os.makedirs(results_dir, exist_ok=True)
    
    # Create filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{test_type}_results_{timestamp}.json"
    filepath = os.path.join(results_dir, filename)
    
    # Save results to file
    with open(filepath, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nResults saved to: {filepath}")
    return filepath

def generate_html_report(processing_results, analysis_results, output_path):
    """Generate HTML report from test results"""
    # Create HTML report
    html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Document Processing Performance Report</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            h1, h2, h3 { color: #333; }
            table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; }
            tr:nth-child(even) { background-color: #f9f9f9; }
            .chart-container { width: 100%; height: 400px; margin-bottom: 30px; }
        </style>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <h1>Document Processing Performance Report</h1>
        <p>Generated on: """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """</p>
        
        <h2>Document Processing Performance</h2>
        <div class="chart-container">
            <canvas id="processingChart"></canvas>
        </div>
        
        <h3>Processing Results by File Size and Strategy</h3>
        <table>
            <tr>
                <th>File</th>
                <th>Size</th>
                <th>Strategy</th>
                <th>Avg Time (s)</th>
                <th>Avg Chunks</th>
            </tr>
    """
    
    # Add processing results to table
    for result in processing_results:
        html += f"""
            <tr>
                <td>{result['filename']}</td>
                <td>{result['size_kb']} KB</td>
                <td>{result['chunking_strategy']}</td>
                <td>{result['avg_elapsed_time']:.2f}</td>
                <td>{int(result['avg_chunk_count'])}</td>
            </tr>
        """
    
    html += """
        </table>
        
        <h2>Document Analysis Performance</h2>
        <div class="chart-container">
            <canvas id="analysisChart"></canvas>
        </div>
        
        <h3>Analysis Results by File Size</h3>
        <table>
            <tr>
                <th>File</th>
                <th>Size</th>
                <th>Avg Time (s)</th>
                <th>Recommended Strategy</th>
            </tr>
    """
    
    # Add analysis results to table
    for result in analysis_results:
        html += f"""
            <tr>
                <td>{result['filename']}</td>
                <td>{result['size_kb']} KB</td>
                <td>{result['avg_elapsed_time']:.2f}</td>
                <td>{result['recommended_strategy']}</td>
            </tr>
        """
    
    # Prepare data for charts
    strategies = list(set([r['chunking_strategy'] for r in processing_results]))
    file_sizes = list(set([r['size_name'] for r in processing_results]))
    
    # Create JavaScript for charts
    html += """
        </table>
        
        <script>
            // Processing chart
            const processingCtx = document.getElementById('processingChart').getContext('2d');
            const processingChart = new Chart(processingCtx, {
                type: 'bar',
                data: {
                    labels: """ + str(file_sizes) + """,
                    datasets: [
    """
    
    # Add datasets for each strategy
    for i, strategy in enumerate(strategies):
        strategy_results = [r for r in processing_results if r['chunking_strategy'] == strategy]
        data_by_size = {}
        for result in strategy_results:
            if result['size_name'] not in data_by_size:
                data_by_size[result['size_name']] = []
            data_by_size[result['size_name']].append(result['avg_elapsed_time'])
        
        # Average times for each size
        avg_times = []
        for size in file_sizes:
            if size in data_by_size and data_by_size[size]:
                avg_times.append(sum(data_by_size[size]) / len(data_by_size[size]))
            else:
                avg_times.append(0)
        
        # Generate random color
        color = f"hsl({i * 360 // len(strategies)}, 70%, 60%)"
        
        html += f"""
                        {{
                            label: '{strategy}',
                            data: {avg_times},
                            backgroundColor: '{color}',
                            borderColor: '{color}',
                            borderWidth: 1
                        }},
        """
    
    html += """
                    ]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Average Processing Time by File Size and Strategy'
                        },
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Time (seconds)'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'File Size'
                            }
                        }
                    }
                }
            });
            
            // Analysis chart
            const analysisCtx = document.getElementById('analysisChart').getContext('2d');
            const analysisChart = new Chart(analysisCtx, {
                type: 'bar',
                data: {
                    labels: """ + str(file_sizes) + """,
                    datasets: [{
                        label: 'Analysis Time',
                        data: [
    """
    
    # Add analysis data
    analysis_data = {}
    for result in analysis_results:
        if result['size_name'] not in analysis_data:
            analysis_data[result['size_name']] = []
        analysis_data[result['size_name']].append(result['avg_elapsed_time'])
    
    for size in file_sizes:
        if size in analysis_data and analysis_data[size]:
            html += f"{sum(analysis_data[size]) / len(analysis_data[size])},"
        else:
            html += "0,"
    
    html += """
                        ],
                        backgroundColor: 'rgba(54, 162, 235, 0.6)',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Average Analysis Time by File Size'
                        },
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Time (seconds)'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'File Size'
                            }
                        }
                    }
                }
            });
        </script>
    </body>
    </html>
    """
    
    # Save HTML report
    with open(output_path, 'w') as f:
        f.write(html)
    
    print(f"\nHTML report saved to: {output_path}")

async def run_tests(args):
    """Run the performance tests"""
    print(f"\nRunning document processing performance tests...")
    print(f"File sizes: {[size[0] for size in TEST_FILE_SIZES]}")
    print(f"Chunking strategies: {TEST_CHUNKING_STRATEGIES}")
    
    # Create database session
    db_session = SessionLocal()
    
    try:
        # Create repositories and services
        document_repository = DocumentRepository(db_session)
        document_processor = DocumentProcessor()
        document_analysis_service = DocumentAnalysisService()
        vector_store = VectorStore()
        
        # Ensure test folder exists
        ensure_test_folder_exists(document_repository)
        
        # Run processing tests
        print("\nRunning document processing tests...")
        processing_results = await run_processing_tests(
            document_repository, 
            document_processor, 
            vector_store
        )
        
        # Save processing results
        processing_file = save_results_to_file(processing_results, "document_processing")
        
        # Run analysis tests
        print("\nRunning document analysis tests...")
        analysis_results = await run_analysis_tests(
            document_repository, 
            document_analysis_service
        )
        
        # Save analysis results
        analysis_file = save_results_to_file(analysis_results, "document_analysis")
        
        # Generate HTML report
        if args.html:
            # Create results directory if it doesn't exist
            results_dir = os.path.join(project_root, "tests", "results")
            os.makedirs(results_dir, exist_ok=True)
            
            # Create HTML report filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_path = os.path.join(results_dir, f"document_processing_report_{timestamp}.html")
            
            # Generate HTML report
            generate_html_report(processing_results, analysis_results, html_path)
        
        print("\nTests completed successfully!")
        return 0
    except Exception as e:
        print(f"Error running tests: {e}")
        return 1
    finally:
        db_session.close()

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Run document processing performance tests")
    parser.add_argument("--html", action="store_true", help="Generate HTML report")
    args = parser.parse_args()
    
    return asyncio.run(run_tests(args))

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/run_grant_permissions.py
================
#!/usr/bin/env python3
"""
Run SQL script to grant permissions to the postgres user
"""
import os
import sys
import subprocess

def run_grant_permissions():
    """
    Run SQL script to grant permissions
    """
    print("Running SQL script to grant permissions to the postgres user...")
    
    # Get the project root directory
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Change to the project root directory
    os.chdir(project_root)
    
    # Path to the SQL script
    sql_script_path = os.path.join(project_root, "scripts", "grant_permissions.sql")
    
    # Run psql command
    try:
        # Note: This should be run as the charleshoward user who owns the tables
        result = subprocess.run(
            ["psql", "-d", "metis_rag", "-f", sql_script_path],
            check=True,
            capture_output=True,
            text=True
        )
        print(result.stdout)
        print("Permissions granted successfully!")
        return 0
    except subprocess.CalledProcessError as e:
        print(f"Error running SQL script: {e}")
        print(e.stdout)
        print(e.stderr)
        return 1

if __name__ == "__main__":
    sys.exit(run_grant_permissions())

================
File: scripts/run_memory_migration.py
================
#!/usr/bin/env python
"""
Run the migration to add the memories table
"""
import asyncio
import logging
import os
import sys

# Add the parent directory to the path so we can import app modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from alembic import command
from alembic.config import Config
from app.core.config import SETTINGS

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

async def run_migration():
    """Run the migration to add the memories table"""
    try:
        logger.info("Starting memory table migration")
        
        # Get the alembic.ini path
        alembic_ini_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'alembic.ini')
        
        # Create Alembic config
        alembic_cfg = Config(alembic_ini_path)
        
        # Run the migration
        command.upgrade(alembic_cfg, "head")
        
        logger.info("Memory table migration completed successfully")
    except Exception as e:
        logger.error(f"Error running memory table migration: {str(e)}")
        raise

if __name__ == "__main__":
    logger.info(f"Using database URL: {SETTINGS.database_url}")
    asyncio.run(run_migration())

================
File: scripts/run_migrations.py
================
#!/usr/bin/env python3
"""
Run Alembic migrations for the Background Task System
"""
import os
import sys
import subprocess

def run_migration():
    """
    Run Alembic migrations
    """
    print("Running Alembic migrations for the Background Task System...")
    
    # Get the project root directory
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Change to the project root directory
    os.chdir(project_root)
    
    # Run Alembic upgrade
    try:
        result = subprocess.run(
            ["alembic", "upgrade", "heads"],
            check=True,
            capture_output=True,
            text=True
        )
        print(result.stdout)
        print("Migration completed successfully!")
        return 0
    except subprocess.CalledProcessError as e:
        print(f"Error running migration: {e}")
        print(e.stdout)
        print(e.stderr)
        return 1

if __name__ == "__main__":
    sys.exit(run_migration())

================
File: scripts/run_phase4_migrations.py
================
#!/usr/bin/env python3
"""
Script to run the Phase 4 migrations for the Metis RAG Authentication Implementation Plan.
This script runs the Alembic migrations to add the roles, user-role associations, 
notifications, organizations, and organization-member tables.
"""

import asyncio
import sys
import os
import logging
import subprocess
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Get the project root directory
project_root = Path(__file__).parent.parent.absolute()


async def run_migrations():
    """Run the Alembic migrations"""
    logger.info("Running Alembic migrations for Phase 4...")
    
    # Change to the project root directory
    os.chdir(project_root)
    
    try:
        # Run the migrations
        result = subprocess.run(
            ["alembic", "upgrade", "head"],
            check=True,
            capture_output=True,
            text=True
        )
        
        logger.info(f"Migration output:\n{result.stdout}")
        
        if result.stderr:
            logger.warning(f"Migration warnings/errors:\n{result.stderr}")
        
        logger.info("Migrations completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Migration failed with exit code {e.returncode}")
        logger.error(f"Error output:\n{e.stderr}")
        return False


async def main():
    """Main function"""
    logger.info("Starting Phase 4 migrations...")
    
    # Run the migrations
    success = await run_migrations()
    
    if success:
        logger.info("Phase 4 migrations completed successfully")
        logger.info("The following tables have been added:")
        logger.info("  - roles: For role-based access control")
        logger.info("  - user_roles: For user-role associations")
        logger.info("  - notifications: For the notification system")
        logger.info("  - organizations: For multi-tenant isolation")
        logger.info("  - organization_members: For organization membership")
        logger.info("The documents table has been updated with an organization_id column")
    else:
        logger.error("Phase 4 migrations failed")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

================
File: scripts/run_query_refinement_test.sh
================
#!/bin/bash

# Script to run the query refinement test and verify the fixes

# Set the working directory to the project root
cd "$(dirname "$0")/.."

# Print header
echo "====================================================="
echo "  Metis RAG Query Refinement Fix Test"
echo "====================================================="
echo

# Check if Python is available
if ! command -v python3 &> /dev/null; then
    echo "Error: Python 3 is required but not found."
    exit 1
fi

# Check if the virtual environment exists
if [ -d "venv_py310" ]; then
    echo "Activating virtual environment..."
    source venv_py310/bin/activate
else
    echo "Warning: Virtual environment not found. Using system Python."
fi

# Check if the test file exists
if [ ! -f "tests/test_query_refinement_fix.py" ]; then
    echo "Error: Test file not found: tests/test_query_refinement_fix.py"
    exit 1
fi

# Run the test
echo "Running query refinement test..."
echo
python3 tests/test_query_refinement_fix.py

# Check the exit code
if [ $? -eq 0 ]; then
    echo
    echo "Test completed successfully!"
    echo
    echo "For more information about the fixes, see:"
    echo "docs/technical/query_refinement_fix.md"
else
    echo
    echo "Test failed. Please check the output for errors."
fi

echo
echo "====================================================="

================
File: scripts/setup.sh
================
#!/bin/bash
# Setup script for Metis RAG

# Create virtual environment
echo "Creating virtual environment..."
python3 -m venv venv
source venv/bin/activate

# Install dependencies
echo "Installing dependencies..."
pip install -r requirements.txt

# Create necessary directories
echo "Creating necessary directories..."
mkdir -p uploads chroma_db

# Create .env file if it doesn't exist
if [ ! -f .env ]; then
    echo "Creating .env file..."
    cp .env.example .env
fi

echo "Setup complete! You can now run the application with:"
echo "uvicorn app.main:app --reload"

================
File: scripts/test_adapter_functions.py
================
"""
Test script for adapter functions.

This script demonstrates the adapter functions working correctly by:
1. Creating a Pydantic Document
2. Converting it to a SQLAlchemy Document
3. Converting it back to a Pydantic Document
4. Verifying that the data is preserved
"""
import sys
import os
import uuid
from datetime import datetime

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.document import Document as PydanticDocument, Chunk as PydanticChunk
from app.db.models import Document as DBDocument, Chunk as DBChunk
from app.db.adapters import (
    pydantic_document_to_sqlalchemy,
    sqlalchemy_document_to_pydantic,
    pydantic_chunk_to_sqlalchemy,
    sqlalchemy_chunk_to_pydantic,
    to_str_id,
    to_uuid_or_str
)

def test_adapter_functions():
    """Test the adapter functions"""
    print("Testing adapter functions...")
    
    # Create a Pydantic Document with Chunks
    doc_id = str(uuid.uuid4())
    pydantic_doc = PydanticDocument(
        id=doc_id,
        filename="test.txt",
        content="This is a test document.",
        metadata={
            "file_type": "txt",
            "test_size": "small",
            "processing_status": "pending"
        },
        folder="/test",
        chunks=[
            PydanticChunk(
                id=str(uuid.uuid4()),
                content="This is chunk 1.",
                metadata={"index": 0}
            ),
            PydanticChunk(
                id=str(uuid.uuid4()),
                content="This is chunk 2.",
                metadata={"index": 1}
            )
        ]
    )
    
    print(f"Created Pydantic Document: {pydantic_doc.id}")
    print(f"Number of chunks: {len(pydantic_doc.chunks)}")
    
    # Convert to SQLAlchemy Document
    db_doc = pydantic_document_to_sqlalchemy(pydantic_doc)
    
    print(f"Converted to SQLAlchemy Document: {db_doc.id}")
    print(f"Number of chunks: {len(db_doc.chunks)}")
    print(f"Metadata: {db_doc.doc_metadata}")
    
    # Verify that the data is preserved
    # For SQLite, the ID might be a string, while for PostgreSQL it might be a UUID
    # So we convert both to strings for comparison
    assert str(db_doc.id) == str(pydantic_doc.id)
    assert db_doc.filename == pydantic_doc.filename
    assert db_doc.content == pydantic_doc.content
    assert db_doc.doc_metadata == pydantic_doc.metadata
    assert db_doc.folder == pydantic_doc.folder
    assert len(db_doc.chunks) == len(pydantic_doc.chunks)
    
    # Convert back to Pydantic Document
    pydantic_doc2 = sqlalchemy_document_to_pydantic(db_doc)
    
    print(f"Converted back to Pydantic Document: {pydantic_doc2.id}")
    print(f"Number of chunks: {len(pydantic_doc2.chunks)}")
    print(f"Metadata: {pydantic_doc2.metadata}")
    
    # Verify that the data is preserved
    # For SQLite, the ID might be a string, while for PostgreSQL it might be a UUID
    # So we convert both to strings for comparison
    assert str(pydantic_doc2.id) == str(pydantic_doc.id)
    assert pydantic_doc2.filename == pydantic_doc.filename
    assert pydantic_doc2.content == pydantic_doc.content
    assert pydantic_doc2.metadata == pydantic_doc.metadata
    assert pydantic_doc2.folder == pydantic_doc.folder
    assert len(pydantic_doc2.chunks) == len(pydantic_doc.chunks)
    
    print("All tests passed!")
    
    return True

if __name__ == "__main__":
    test_adapter_functions()

================
File: scripts/test_api_directly.py
================
#!/usr/bin/env python3
"""
Direct API testing script for Metis RAG.
This script tests the Metis RAG API directly using the requests library.
"""

import os
import sys
import json
import logging
import uuid
import time
from typing import Dict, Any, List, Optional
import requests
from io import BytesIO

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_api_directly")

# Base URL for the API
BASE_URL = "http://localhost:8000"

# Test documents - paths relative to project root
TEST_DOCUMENTS = {
    "technical_specs": {
        "path": "data/test_docs/smart_home_technical_specs.pdf",
        "type": "pdf",
        "content_type": "application/pdf"
    },
    "user_guide": {
        "path": "data/test_docs/smart_home_user_guide.txt",
        "type": "txt",
        "content_type": "text/plain"
    },
    "device_comparison": {
        "path": "data/test_docs/smart_home_device_comparison.csv",
        "type": "csv",
        "content_type": "text/csv"
    },
    "developer_reference": {
        "path": "data/test_docs/smart_home_developer_reference.md",
        "type": "md",
        "content_type": "text/markdown"
    }
}

# Test queries
TEST_QUERIES = [
    "What are the specifications of the SmartHome Hub?",
    "How do I troubleshoot when devices won't connect?",
    "What is the battery life of the motion sensor?",
    "Compare the Motion Sensor and Door Sensor specifications."
]

def verify_test_documents():
    """Verify that all test documents exist"""
    missing_docs = []
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        if not os.path.exists(doc_info["path"]):
            missing_docs.append(doc_info["path"])
    
    if missing_docs:
        logger.error(f"Missing test documents: {', '.join(missing_docs)}")
        logger.error("Please ensure all test documents are created before running the test.")
        return False
    else:
        logger.info("All test documents verified.")
        return True

def authenticate():
    """
    Authenticate with the API and return the access token.
    
    Returns:
        Access token if authentication successful, None otherwise
    """
    logger.info("Authenticating with API...")
    
    # Create a unique test user for this test run
    test_username = f"testuser_{uuid.uuid4().hex[:8]}"
    test_password = "testpassword"
    
    try:
        # Register the new test user
        logger.info(f"Registering new test user: {test_username}")
        register_response = requests.post(
            f"{BASE_URL}/api/auth/register",
            json={
                "username": test_username,
                "email": f"{test_username}@example.com",
                "password": test_password,
                "full_name": "Test User",
                "is_active": True,
                "is_admin": False
            }
        )
        
        if register_response.status_code != 200:
            logger.error(f"Registration failed: {register_response.status_code} - {register_response.text}")
            return None
            
        logger.info(f"User {test_username} registered successfully")
        
        # Authenticate with the new user
        login_response = requests.post(
            f"{BASE_URL}/api/auth/token",
            data={
                "username": test_username,
                "password": test_password,
                "grant_type": "password"
            }
        )
        
        if login_response.status_code != 200:
            logger.error(f"Login failed: {login_response.status_code} - {login_response.text}")
            return None
            
        token_data = login_response.json()
        access_token = token_data.get("access_token")
        
        if not access_token:
            logger.error("No access token in response")
            return None
        
        logger.info(f"Successfully obtained token: {access_token[:10]}...")
        return access_token
            
    except Exception as e:
        logger.error(f"Authentication error: {str(e)}")
        return None

def upload_and_process_documents(access_token):
    """
    Upload and process all test documents.
    
    Args:
        access_token: Access token for authentication
        
    Returns:
        Dictionary of document IDs keyed by document type
    """
    logger.info("Uploading and processing documents...")
    
    # Set up headers with authentication token
    headers = {"Authorization": f"Bearer {access_token}"}
    
    uploaded_docs = {}
    results = []
    
    # Upload and process each document
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        logger.info(f"Testing upload and processing for {doc_id} ({doc_info['path']})")
        
        # Read file content
        try:
            with open(doc_info['path'], 'rb') as f:
                file_content = f.read()
        except Exception as e:
            logger.error(f"Failed to read file {doc_info['path']}: {str(e)}")
            continue
        
        # Upload the file
        try:
            files = {"file": (os.path.basename(doc_info['path']), file_content, doc_info['content_type'])}
            upload_response = requests.post(
                f"{BASE_URL}/api/documents/upload",
                headers=headers,
                files=files
            )
            
            if upload_response.status_code != 200:
                logger.error(f"Upload failed: {upload_response.status_code} - {upload_response.text}")
                continue
                
            upload_data = upload_response.json()
            if not upload_data.get("success"):
                logger.error(f"Upload response indicates failure: {upload_data}")
                continue
                
            document_id = upload_data.get("document_id")
            if not document_id:
                logger.error("No document_id in upload response")
                continue
                
            uploaded_docs[doc_id] = document_id
            logger.info(f"Successfully uploaded {doc_id}, document_id: {document_id}")
            
            # Process the document
            process_response = requests.post(
                f"{BASE_URL}/api/documents/process",
                headers=headers,
                json={"document_ids": [document_id]}
            )
            
            if process_response.status_code != 200:
                logger.error(f"Processing failed: {process_response.status_code} - {process_response.text}")
                continue
                
            process_data = process_response.json()
            if not process_data.get("success"):
                logger.error(f"Process response indicates failure: {process_data}")
                continue
                
            logger.info(f"Successfully processed {doc_id}")
            
            # Get document info
            info_response = requests.get(
                f"{BASE_URL}/api/documents/{document_id}",
                headers=headers
            )
            
            if info_response.status_code != 200:
                logger.error(f"Info request failed: {info_response.status_code} - {info_response.text}")
                continue
                
            doc_info_data = info_response.json()
            
            # Store results
            results.append({
                "document_id": document_id,
                "document_type": doc_info['type'],
                "filename": os.path.basename(doc_info['path']),
                "success": True,
                "chunk_count": doc_info_data.get("chunk_count", 0) if isinstance(doc_info_data, dict) else 0
            })
            
        except Exception as e:
            logger.error(f"Error in upload/processing test for {doc_id}: {str(e)}")
            results.append({
                "document_id": doc_id,
                "document_type": doc_info['type'],
                "filename": os.path.basename(doc_info['path']),
                "success": False,
                "error": str(e)
            })
    
    # Save results to file
    results_path = "test_results/api_test_upload_results.json"
    os.makedirs(os.path.dirname(results_path), exist_ok=True)
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Upload and processing results saved to {os.path.abspath(results_path)}")
    
    return uploaded_docs

def test_queries(access_token, uploaded_docs):
    """
    Test queries against the uploaded documents.
    
    Args:
        access_token: Access token for authentication
        uploaded_docs: Dictionary of document IDs keyed by document type
    """
    logger.info("Testing queries...")
    
    # Set up headers with authentication token
    headers = {"Authorization": f"Bearer {access_token}"}
    
    results = []
    
    # Test each query
    for query in TEST_QUERIES:
        logger.info(f"Testing query: '{query}'")
        
        try:
            # Execute query with RAG
            query_response = requests.post(
                f"{BASE_URL}/api/chat/query",
                headers=headers,
                json={
                    "message": query,
                    "use_rag": True,
                    "stream": False
                }
            )
            
            if query_response.status_code != 200:
                logger.error(f"Query failed: {query_response.status_code} - {query_response.text}")
                continue
                
            response_data = query_response.json()
            if "message" not in response_data:
                logger.error("No message in query response")
                continue
                
            answer = response_data["message"]
            logger.info(f"Answer: {answer[:100]}...")
            
            # Store results
            results.append({
                "query": query,
                "answer": answer,
                "success": True
            })
            
        except Exception as e:
            logger.error(f"Error in query test for '{query}': {str(e)}")
            results.append({
                "query": query,
                "success": False,
                "error": str(e)
            })
    
    # Save results to file
    results_path = "test_results/api_test_query_results.json"
    os.makedirs(os.path.dirname(results_path), exist_ok=True)
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Query results saved to {os.path.abspath(results_path)}")

def cleanup_documents(access_token, uploaded_docs):
    """
    Clean up uploaded documents.
    
    Args:
        access_token: Access token for authentication
        uploaded_docs: Dictionary of document IDs keyed by document type
    """
    logger.info("Cleaning up documents...")
    
    # Set up headers with authentication token
    headers = {"Authorization": f"Bearer {access_token}"}
    
    # Delete each document
    for doc_type, doc_id in uploaded_docs.items():
        logger.info(f"Deleting document: {doc_type} (ID: {doc_id})")
        
        try:
            delete_response = requests.delete(
                f"{BASE_URL}/api/documents/{doc_id}",
                headers=headers
            )
            
            if delete_response.status_code != 200:
                logger.error(f"Delete failed: {delete_response.status_code} - {delete_response.text}")
                continue
                
            delete_data = delete_response.json()
            if not delete_data.get("success"):
                logger.error(f"Delete response indicates failure: {delete_data}")
                continue
                
            logger.info(f"Successfully deleted document: {doc_type} (ID: {doc_id})")
            
        except Exception as e:
            logger.error(f"Error deleting document {doc_type} (ID: {doc_id}): {str(e)}")

def main():
    """Main function"""
    logger.info("Starting direct API test...")
    
    # Verify test documents
    if not verify_test_documents():
        logger.error("Test documents verification failed. Aborting test.")
        return 1
    
    # Authenticate
    access_token = authenticate()
    if not access_token:
        logger.error("Authentication failed. Aborting test.")
        return 1
    
    # Upload and process documents
    uploaded_docs = upload_and_process_documents(access_token)
    if not uploaded_docs:
        logger.error("Document upload and processing failed. Aborting test.")
        return 1
    
    # Test queries
    test_queries(access_token, uploaded_docs)
    
    # Clean up documents
    cleanup_documents(access_token, uploaded_docs)
    
    logger.info("Direct API test completed successfully.")
    return 0

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/test_api_endpoints.py
================
#!/usr/bin/env python3
"""
Test API endpoints to verify they're working correctly with database repositories
"""
import os
import sys
import json
import asyncio
import argparse
import requests
from datetime import datetime
from uuid import uuid4
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Base URL for API
BASE_URL = "http://localhost:8000/api/v1"

def test_health_endpoint():
    """Test the health check endpoint"""
    print("\n=== Testing Health Check Endpoint ===")
    
    try:
        # Test health check endpoint
        response = requests.get(f"{BASE_URL}/health")
        response.raise_for_status()
        
        # Parse response
        data = response.json()
        
        # Print results
        print(f"Status: {data['status']}")
        print("Components:")
        for component, info in data['components'].items():
            print(f"  {component}: {info['status']}")
        
        return True
    except Exception as e:
        print(f"Error testing health check endpoint: {e}")
        return False

def test_document_endpoints():
    """Test document API endpoints"""
    print("\n=== Testing Document API Endpoints ===")
    
    try:
        # Test list documents endpoint
        print("\nTesting list documents endpoint...")
        response = requests.get(f"{BASE_URL}/documents/list")
        response.raise_for_status()
        documents = response.json()
        print(f"Found {len(documents)} documents")
        
        # Test document upload endpoint
        print("\nTesting document upload endpoint...")
        # Create a test file
        test_file_path = os.path.join(project_root, "test_upload.txt")
        with open(test_file_path, "w") as f:
            f.write("This is a test document for API endpoint testing.")
        
        # Upload the file
        with open(test_file_path, "rb") as f:
            files = {"file": (os.path.basename(test_file_path), f)}
            data = {"tags": "test,api", "folder": "/test"}
            response = requests.post(f"{BASE_URL}/documents/upload", files=files, data=data)
        
        response.raise_for_status()
        upload_result = response.json()
        document_id = upload_result["document_id"]
        print(f"Uploaded document with ID: {document_id}")
        
        # Test get document endpoint
        print("\nTesting get document endpoint...")
        response = requests.get(f"{BASE_URL}/documents/{document_id}")
        response.raise_for_status()
        document = response.json()
        print(f"Retrieved document: {document['filename']}")
        
        # Test update document tags endpoint
        print("\nTesting update document tags endpoint...")
        tags = ["test", "api", "updated"]
        response = requests.put(
            f"{BASE_URL}/documents/{document_id}/tags",
            json={"tags": tags}
        )
        response.raise_for_status()
        tag_result = response.json()
        print(f"Updated tags: {tag_result['tags']}")
        
        # Test update document folder endpoint
        print("\nTesting update document folder endpoint...")
        folder = "/test/updated"
        response = requests.put(
            f"{BASE_URL}/documents/{document_id}/folder",
            json={"folder": folder}
        )
        response.raise_for_status()
        folder_result = response.json()
        print(f"Updated folder: {folder_result['folder']}")
        
        # Test document processing endpoint
        print("\nTesting document processing endpoint...")
        response = requests.post(
            f"{BASE_URL}/documents/process",
            json={
                "document_ids": [document_id],
                "chunking_strategy": "recursive",
                "force_reprocess": True
            }
        )
        response.raise_for_status()
        process_result = response.json()
        print(f"Processing result: {process_result['message']}")
        
        # Test filter documents endpoint
        print("\nTesting filter documents endpoint...")
        response = requests.post(
            f"{BASE_URL}/documents/filter",
            json={"tags": ["test"], "folder": "/test/updated"}
        )
        response.raise_for_status()
        filter_result = response.json()
        print(f"Found {len(filter_result)} documents matching filter")
        
        # Test get all tags endpoint
        print("\nTesting get all tags endpoint...")
        response = requests.get(f"{BASE_URL}/documents/tags")
        response.raise_for_status()
        tags_result = response.json()
        print(f"Found tags: {tags_result['tags']}")
        
        # Test get all folders endpoint
        print("\nTesting get all folders endpoint...")
        response = requests.get(f"{BASE_URL}/documents/folders")
        response.raise_for_status()
        folders_result = response.json()
        print(f"Found folders: {folders_result['folders']}")
        
        # Test delete document endpoint
        print("\nTesting delete document endpoint...")
        response = requests.delete(f"{BASE_URL}/documents/{document_id}")
        response.raise_for_status()
        delete_result = response.json()
        print(f"Delete result: {delete_result['message']}")
        
        # Clean up test file
        os.remove(test_file_path)
        
        return True
    except Exception as e:
        print(f"Error testing document endpoints: {e}")
        return False

def test_chat_endpoints():
    """Test chat API endpoints"""
    print("\n=== Testing Chat API Endpoints ===")
    
    try:
        # Test chat query endpoint
        print("\nTesting chat query endpoint...")
        query = {
            "message": "What is RAG?",
            "conversation_id": None,
            "user_id": "test_user",
            "model": None,
            "use_rag": True,
            "stream": False
        }
        response = requests.post(f"{BASE_URL}/chat/query", json=query)
        response.raise_for_status()
        query_result = response.json()
        conversation_id = query_result["conversation_id"]
        print(f"Created conversation with ID: {conversation_id}")
        print(f"Response: {query_result['message'][:100]}...")
        
        # Test get conversation history endpoint
        print("\nTesting get conversation history endpoint...")
        response = requests.get(f"{BASE_URL}/chat/history?conversation_id={conversation_id}")
        response.raise_for_status()
        history_result = response.json()
        print(f"Found {len(history_result['messages'])} messages in conversation")
        
        # Test list conversations endpoint
        print("\nTesting list conversations endpoint...")
        response = requests.get(f"{BASE_URL}/chat/list?user_id=test_user")
        response.raise_for_status()
        list_result = response.json()
        print(f"Found {len(list_result['conversations'])} conversations for user")
        
        # Test save conversation endpoint
        print("\nTesting save conversation endpoint...")
        response = requests.post(f"{BASE_URL}/chat/save?conversation_id={conversation_id}")
        response.raise_for_status()
        save_result = response.json()
        print(f"Save result: {save_result['message']}")
        
        # Test clear conversation endpoint
        print("\nTesting clear conversation endpoint...")
        response = requests.delete(f"{BASE_URL}/chat/clear?conversation_id={conversation_id}")
        response.raise_for_status()
        clear_result = response.json()
        print(f"Clear result: {clear_result['message']}")
        
        return True
    except Exception as e:
        print(f"Error testing chat endpoints: {e}")
        return False

def test_analytics_endpoints():
    """Test analytics API endpoints"""
    print("\n=== Testing Analytics API Endpoints ===")
    
    try:
        # Test record query endpoint
        print("\nTesting record query endpoint...")
        query_data = {
            "query": "Test query for analytics",
            "model": "test_model",
            "use_rag": True,
            "response_time_ms": 500,
            "token_count": 100,
            "document_ids": [str(uuid4())],
            "query_type": "test",
            "successful": True
        }
        response = requests.post(f"{BASE_URL}/analytics/record_query", json=query_data)
        response.raise_for_status()
        record_result = response.json()
        print(f"Record result: {record_result['message']}")
        
        # Test query stats endpoint
        print("\nTesting query stats endpoint...")
        response = requests.get(f"{BASE_URL}/analytics/query_stats?time_period=all")
        response.raise_for_status()
        stats_result = response.json()
        print(f"Query count: {stats_result['query_count']}")
        print(f"Average response time: {stats_result['avg_response_time_ms']} ms")
        
        # Test document usage endpoint
        print("\nTesting document usage endpoint...")
        response = requests.get(f"{BASE_URL}/analytics/document_usage?time_period=all")
        response.raise_for_status()
        usage_result = response.json()
        print(f"Document count: {usage_result['document_count']}")
        
        # Test system stats endpoint
        print("\nTesting system stats endpoint...")
        response = requests.get(f"{BASE_URL}/analytics/system_stats")
        response.raise_for_status()
        system_result = response.json()
        print(f"Document count: {system_result['document_count']}")
        print(f"Query count: {system_result['query_count']}")
        
        # Test model performance endpoint
        print("\nTesting model performance endpoint...")
        response = requests.get(f"{BASE_URL}/analytics/model_performance?time_period=all")
        response.raise_for_status()
        model_result = response.json()
        print(f"Found {len(model_result['models'])} models")
        
        # Test query types endpoint
        print("\nTesting query types endpoint...")
        response = requests.get(f"{BASE_URL}/analytics/query_types?time_period=all")
        response.raise_for_status()
        types_result = response.json()
        print(f"Found {len(types_result['query_types'])} query types")
        
        return True
    except Exception as e:
        print(f"Error testing analytics endpoints: {e}")
        return False

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Test API endpoints")
    parser.add_argument("--health", action="store_true", help="Test health check endpoint only")
    parser.add_argument("--documents", action="store_true", help="Test document endpoints only")
    parser.add_argument("--chat", action="store_true", help="Test chat endpoints only")
    parser.add_argument("--analytics", action="store_true", help="Test analytics endpoints only")
    args = parser.parse_args()
    
    # If no specific tests are requested, run all tests
    run_all = not (args.health or args.documents or args.chat or args.analytics)
    
    results = {}
    
    # Test health check endpoint
    if run_all or args.health:
        results["health"] = test_health_endpoint()
    
    # Test document endpoints
    if run_all or args.documents:
        results["documents"] = test_document_endpoints()
    
    # Test chat endpoints
    if run_all or args.chat:
        results["chat"] = test_chat_endpoints()
    
    # Test analytics endpoints
    if run_all or args.analytics:
        results["analytics"] = test_analytics_endpoints()
    
    # Print summary
    print("\n=== Test Summary ===")
    all_passed = True
    for test, passed in results.items():
        status = "PASSED" if passed else "FAILED"
        print(f"{test}: {status}")
        all_passed = all_passed and passed
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/test_authentication.py
================
#!/usr/bin/env python3
"""
Authentication System Test Script

This script tests the various authentication endpoints of the Metis RAG application.
It covers user registration, login, password reset, and admin functionality.
"""

import argparse
import json
import sys
import time
import uuid
from typing import Dict, Tuple, Optional, List, Any

import requests

# Default settings
DEFAULT_BASE_URL = "http://localhost:8000/api"
DEFAULT_ADMIN_USERNAME = "admin"
DEFAULT_ADMIN_PASSWORD = "admin123"
DEFAULT_TEST_USERNAME = f"testuser_{uuid.uuid4().hex[:8]}"
DEFAULT_TEST_EMAIL = f"testuser_{uuid.uuid4().hex[:8]}@example.com"
DEFAULT_TEST_PASSWORD = "password123"


class AuthTester:
    """Test the authentication system of Metis RAG"""

    def __init__(self, base_url: str):
        self.base_url = base_url
        self.admin_token = None
        self.user_token = None
        self.test_user_id = None
        self.reset_token = None

    def register_user(self, username: str, email: str, password: str, full_name: Optional[str] = None) -> Tuple[Dict, int]:
        """Register a new user"""
        print(f"\n[*] Registering user: {username}")
        
        data = {
            "username": username,
            "email": email,
            "password": password
        }
        
        if full_name:
            data["full_name"] = full_name
            
        response = requests.post(
            f"{self.base_url}/auth/register",
            json=data
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        if response.status_code == 200:
            print(f"User created: {result.get('username')} (ID: {result.get('id')})")
            self.test_user_id = result.get('id')
        else:
            print(f"Error: {result}")
            
        return result, response.status_code

    def login(self, username: str, password: str) -> Tuple[Dict, int]:
        """Login and get access token"""
        print(f"\n[*] Logging in as: {username}")
        
        response = requests.post(
            f"{self.base_url}/auth/token",
            data={
                "username": username,
                "password": password
            }
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        if response.status_code == 200 and "access_token" in result:
            print(f"Login successful. Token received.")
            if username == DEFAULT_ADMIN_USERNAME:
                self.admin_token = result["access_token"]
            else:
                self.user_token = result["access_token"]
        else:
            print(f"Login failed: {result}")
            
        return result, response.status_code

    def get_current_user(self, token: str) -> Tuple[Dict, int]:
        """Get current user information"""
        print("\n[*] Getting current user info")
        
        response = requests.get(
            f"{self.base_url}/auth/me",
            headers={"Authorization": f"Bearer {token}"}
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        if response.status_code == 200:
            print(f"User: {result.get('username')} (ID: {result.get('id')})")
        else:
            print(f"Error: {result}")
            
        return result, response.status_code

    def request_password_reset(self, email: str) -> Tuple[Dict, int]:
        """Request a password reset"""
        print(f"\n[*] Requesting password reset for: {email}")
        
        response = requests.post(
            f"{self.base_url}/password-reset/request-reset",
            json={"email": email}
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        print(f"Response: {result}")
        print("Note: Check server logs for the reset token")
            
        return result, response.status_code

    def reset_password(self, token: str, new_password: str) -> Tuple[Dict, int]:
        """Reset password using token"""
        print(f"\n[*] Resetting password with token")
        
        response = requests.post(
            f"{self.base_url}/password-reset/reset-password",
            json={
                "token": token,
                "password": new_password,
                "confirm_password": new_password
            }
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        print(f"Response: {result}")
            
        return result, response.status_code

    def list_users(self, admin_token: str, search: Optional[str] = None) -> Tuple[List[Dict], int]:
        """List all users (admin only)"""
        print("\n[*] Listing all users")
        
        url = f"{self.base_url}/admin/users"
        if search:
            url += f"?search={search}"
            
        response = requests.get(
            url,
            headers={"Authorization": f"Bearer {admin_token}"}
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        if response.status_code == 200:
            print(f"Found {len(result)} users")
            for user in result:
                print(f"  - {user.get('username')} ({user.get('email')})")
        else:
            print(f"Error: {result}")
            
        return result, response.status_code

    def get_user(self, admin_token: str, user_id: str) -> Tuple[Dict, int]:
        """Get a specific user (admin only)"""
        print(f"\n[*] Getting user with ID: {user_id}")
        
        response = requests.get(
            f"{self.base_url}/admin/users/{user_id}",
            headers={"Authorization": f"Bearer {admin_token}"}
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        if response.status_code == 200:
            print(f"User: {result.get('username')} (Email: {result.get('email')})")
        else:
            print(f"Error: {result}")
            
        return result, response.status_code

    def create_user(self, admin_token: str, user_data: Dict[str, Any]) -> Tuple[Dict, int]:
        """Create a new user (admin only)"""
        print(f"\n[*] Creating new user: {user_data.get('username')}")
        
        response = requests.post(
            f"{self.base_url}/admin/users",
            headers={"Authorization": f"Bearer {admin_token}"},
            json=user_data
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        if response.status_code == 200:
            print(f"User created: {result.get('username')} (ID: {result.get('id')})")
        else:
            print(f"Error: {result}")
            
        return result, response.status_code

    def update_user(self, admin_token: str, user_id: str, user_data: Dict[str, Any]) -> Tuple[Dict, int]:
        """Update a user (admin only)"""
        print(f"\n[*] Updating user with ID: {user_id}")
        
        response = requests.put(
            f"{self.base_url}/admin/users/{user_id}",
            headers={"Authorization": f"Bearer {admin_token}"},
            json=user_data
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        if response.status_code == 200:
            print(f"User updated: {result.get('username')} (Email: {result.get('email')})")
        else:
            print(f"Error: {result}")
            
        return result, response.status_code

    def delete_user(self, admin_token: str, user_id: str) -> Tuple[Dict, int]:
        """Delete a user (admin only)"""
        print(f"\n[*] Deleting user with ID: {user_id}")
        
        response = requests.delete(
            f"{self.base_url}/admin/users/{user_id}",
            headers={"Authorization": f"Bearer {admin_token}"}
        )
        
        try:
            result = response.json()
        except json.JSONDecodeError:
            result = {"error": "Invalid JSON response", "text": response.text}
            
        print(f"Status: {response.status_code}")
        print(f"Response: {result}")
            
        return result, response.status_code

    def run_basic_tests(self):
        """Run basic authentication tests"""
        print("\n=== Running Basic Authentication Tests ===\n")
        
        # 1. Register a test user
        self.register_user(
            DEFAULT_TEST_USERNAME, 
            DEFAULT_TEST_EMAIL, 
            DEFAULT_TEST_PASSWORD, 
            "Test User"
        )
        
        # 2. Login as the test user
        self.login(DEFAULT_TEST_USERNAME, DEFAULT_TEST_PASSWORD)
        
        # 3. Get current user info
        if self.user_token:
            self.get_current_user(self.user_token)
        
        # 4. Request password reset
        self.request_password_reset(DEFAULT_TEST_EMAIL)
        
        print("\n=== Basic Authentication Tests Completed ===")

    def run_admin_tests(self):
        """Run admin functionality tests"""
        print("\n=== Running Admin Functionality Tests ===\n")
        
        # 1. Login as admin
        self.login(DEFAULT_ADMIN_USERNAME, DEFAULT_ADMIN_PASSWORD)
        
        if not self.admin_token:
            print("Admin login failed. Skipping admin tests.")
            return
        
        # 2. List all users
        self.list_users(self.admin_token)
        
        # 3. Get specific user
        if self.test_user_id:
            self.get_user(self.admin_token, self.test_user_id)
        
        # 4. Create a new user
        new_user_data = {
            "username": f"adminuser_{uuid.uuid4().hex[:8]}",
            "email": f"adminuser_{uuid.uuid4().hex[:8]}@example.com",
            "password": "adminpass123",
            "full_name": "Admin Created User",
            "is_active": True,
            "is_admin": False
        }
        
        user_result, _ = self.create_user(self.admin_token, new_user_data)
        created_user_id = user_result.get('id')
        
        # 5. Update the user
        if created_user_id:
            update_data = {
                "full_name": "Updated User Name",
                "is_admin": True
            }
            self.update_user(self.admin_token, created_user_id, update_data)
        
        # 6. Delete the user
        if created_user_id:
            self.delete_user(self.admin_token, created_user_id)
        
        print("\n=== Admin Functionality Tests Completed ===")

    def run_all_tests(self):
        """Run all authentication tests"""
        self.run_basic_tests()
        self.run_admin_tests()
        
        print("\n=== All Authentication Tests Completed ===")


def main():
    """Main function"""
    # Declare globals at the beginning of the function
    global DEFAULT_ADMIN_USERNAME, DEFAULT_ADMIN_PASSWORD
    global DEFAULT_TEST_USERNAME, DEFAULT_TEST_EMAIL, DEFAULT_TEST_PASSWORD
    
    parser = argparse.ArgumentParser(description='Test Metis RAG Authentication System')
    parser.add_argument('--url', default=DEFAULT_BASE_URL, help='Base API URL')
    parser.add_argument('--admin-user', default=DEFAULT_ADMIN_USERNAME, help='Admin username')
    parser.add_argument('--admin-pass', default=DEFAULT_ADMIN_PASSWORD, help='Admin password')
    parser.add_argument('--test-user', default=DEFAULT_TEST_USERNAME, help='Test username')
    parser.add_argument('--test-email', default=DEFAULT_TEST_EMAIL, help='Test email')
    parser.add_argument('--test-pass', default=DEFAULT_TEST_PASSWORD, help='Test password')
    parser.add_argument('--basic-only', action='store_true', help='Run only basic tests')
    parser.add_argument('--admin-only', action='store_true', help='Run only admin tests')
    
    args = parser.parse_args()
    
    tester = AuthTester(args.url)
    
    # Override defaults if provided
    
    DEFAULT_ADMIN_USERNAME = args.admin_user
    DEFAULT_ADMIN_PASSWORD = args.admin_pass
    DEFAULT_TEST_USERNAME = args.test_user
    DEFAULT_TEST_EMAIL = args.test_email
    DEFAULT_TEST_PASSWORD = args.test_pass
    
    try:
        if args.basic_only:
            tester.run_basic_tests()
        elif args.admin_only:
            tester.run_admin_tests()
        else:
            tester.run_all_tests()
    except KeyboardInterrupt:
        print("\nTest interrupted by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\nError during testing: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()

================
File: scripts/test_background_tasks.py
================
#!/usr/bin/env python3
"""
Test script for the Background Task System

This script demonstrates the functionality of the Background Task System
by submitting various types of tasks and monitoring their execution.
"""
import asyncio
import json
import sys
import time
from datetime import datetime, timedelta

import httpx

# API base URL
BASE_URL = "http://localhost:8000/api/v1"

async def submit_task(name, task_type, params=None, priority="normal"):
    """
    Submit a task to the Background Task System
    
    Args:
        name: Task name
        task_type: Task type
        params: Task parameters
        priority: Task priority
        
    Returns:
        Task ID
    """
    url = f"{BASE_URL}/tasks"
    data = {
        "name": name,
        "task_type": task_type,
        "priority": priority,
        "params": params or {}
    }
    
    async with httpx.AsyncClient() as client:
        response = await client.post(url, json=data)
        response.raise_for_status()
        return response.json()["id"]

async def get_task(task_id):
    """
    Get task details
    
    Args:
        task_id: Task ID
        
    Returns:
        Task details
    """
    url = f"{BASE_URL}/tasks/{task_id}"
    
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        response.raise_for_status()
        return response.json()

async def get_task_stats():
    """
    Get task statistics
    
    Returns:
        Task statistics
    """
    url = f"{BASE_URL}/tasks/stats"
    
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        response.raise_for_status()
        return response.json()

async def wait_for_task_completion(task_id, timeout_seconds=60):
    """
    Wait for a task to complete
    
    Args:
        task_id: Task ID
        timeout_seconds: Timeout in seconds
        
    Returns:
        Task details
    """
    start_time = time.time()
    while time.time() - start_time < timeout_seconds:
        task = await get_task(task_id)
        if task["status"] in ("completed", "failed", "cancelled"):
            return task
        
        # Print progress
        print(f"Task {task_id} - Status: {task['status']}, Progress: {task['progress']:.1f}%")
        
        # Wait before checking again
        await asyncio.sleep(1)
    
    raise TimeoutError(f"Task {task_id} did not complete within {timeout_seconds} seconds")

async def test_document_processing():
    """
    Test document processing task
    """
    print("\n=== Testing Document Processing Task ===")
    
    # Submit task
    task_id = await submit_task(
        name="Process Test Document",
        task_type="document_processing",
        params={"document_id": "test123"},
        priority="high"
    )
    
    print(f"Submitted document processing task: {task_id}")
    
    # Wait for completion
    task = await wait_for_task_completion(task_id)
    
    # Print result
    print(f"Task completed with status: {task['status']}")
    if task["result"]:
        print(f"Result: {json.dumps(task['result'], indent=2)}")
    if task["error"]:
        print(f"Error: {task['error']}")

async def test_vector_store_update():
    """
    Test vector store update task
    """
    print("\n=== Testing Vector Store Update Task ===")
    
    # Submit task
    task_id = await submit_task(
        name="Update Vector Store",
        task_type="vector_store_update",
        params={"document_ids": ["doc1", "doc2", "doc3"]},
        priority="normal"
    )
    
    print(f"Submitted vector store update task: {task_id}")
    
    # Wait for completion
    task = await wait_for_task_completion(task_id)
    
    # Print result
    print(f"Task completed with status: {task['status']}")
    if task["result"]:
        print(f"Result: {json.dumps(task['result'], indent=2)}")
    if task["error"]:
        print(f"Error: {task['error']}")

async def test_report_generation():
    """
    Test report generation task
    """
    print("\n=== Testing Report Generation Task ===")
    
    # Submit task
    task_id = await submit_task(
        name="Generate Monthly Report",
        task_type="report_generation",
        params={
            "report_type": "comprehensive",
            "document_ids": ["doc1", "doc2", "doc3", "doc4", "doc5"]
        },
        priority="normal"
    )
    
    print(f"Submitted report generation task: {task_id}")
    
    # Wait for completion
    task = await wait_for_task_completion(task_id)
    
    # Print result
    print(f"Task completed with status: {task['status']}")
    if task["result"]:
        print(f"Result: {json.dumps(task['result'], indent=2)}")
    if task["error"]:
        print(f"Error: {task['error']}")

async def test_system_maintenance():
    """
    Test system maintenance task
    """
    print("\n=== Testing System Maintenance Task ===")
    
    # Submit task
    task_id = await submit_task(
        name="Database Optimization",
        task_type="system_maintenance",
        params={"maintenance_type": "optimization"},
        priority="low"
    )
    
    print(f"Submitted system maintenance task: {task_id}")
    
    # Wait for completion
    task = await wait_for_task_completion(task_id)
    
    # Print result
    print(f"Task completed with status: {task['status']}")
    if task["result"]:
        print(f"Result: {json.dumps(task['result'], indent=2)}")
    if task["error"]:
        print(f"Error: {task['error']}")

async def test_task_dependencies():
    """
    Test task dependencies
    """
    print("\n=== Testing Task Dependencies ===")
    
    # Submit first task
    first_task_id = await submit_task(
        name="First Task",
        task_type="document_processing",
        params={"document_id": "dep_test_1"},
        priority="normal"
    )
    
    print(f"Submitted first task: {first_task_id}")
    
    # Submit second task that depends on first task
    second_task_id = await submit_task(
        name="Second Task",
        task_type="vector_store_update",
        params={
            "document_ids": ["dep_test_1"],
            "dependencies": [{"task_id": first_task_id, "required_status": "completed"}]
        },
        priority="normal"
    )
    
    print(f"Submitted second task (depends on first): {second_task_id}")
    
    # Submit third task that depends on second task
    third_task_id = await submit_task(
        name="Third Task",
        task_type="report_generation",
        params={
            "report_type": "summary",
            "document_ids": ["dep_test_1"],
            "dependencies": [{"task_id": second_task_id, "required_status": "completed"}]
        },
        priority="normal"
    )
    
    print(f"Submitted third task (depends on second): {third_task_id}")
    
    # Wait for first task to complete
    print("\nWaiting for first task to complete...")
    await wait_for_task_completion(first_task_id)
    
    # Wait for second task to complete
    print("\nWaiting for second task to complete...")
    await wait_for_task_completion(second_task_id)
    
    # Wait for third task to complete
    print("\nWaiting for third task to complete...")
    await wait_for_task_completion(third_task_id)
    
    print("\nAll dependent tasks completed successfully!")

async def test_task_priorities():
    """
    Test task priorities
    """
    print("\n=== Testing Task Priorities ===")
    
    # Submit low priority task
    low_task_id = await submit_task(
        name="Low Priority Task",
        task_type="system_maintenance",
        params={"maintenance_type": "cleanup"},
        priority="low"
    )
    
    print(f"Submitted low priority task: {low_task_id}")
    
    # Submit normal priority task
    normal_task_id = await submit_task(
        name="Normal Priority Task",
        task_type="document_processing",
        params={"document_id": "priority_test_1"},
        priority="normal"
    )
    
    print(f"Submitted normal priority task: {normal_task_id}")
    
    # Submit high priority task
    high_task_id = await submit_task(
        name="High Priority Task",
        task_type="vector_store_update",
        params={"document_ids": ["priority_test_1"]},
        priority="high"
    )
    
    print(f"Submitted high priority task: {high_task_id}")
    
    # Submit critical priority task
    critical_task_id = await submit_task(
        name="Critical Priority Task",
        task_type="report_generation",
        params={"report_type": "summary", "document_ids": ["priority_test_1"]},
        priority="critical"
    )
    
    print(f"Submitted critical priority task: {critical_task_id}")
    
    # Wait for all tasks to complete
    print("\nWaiting for all tasks to complete...")
    await asyncio.gather(
        wait_for_task_completion(low_task_id),
        wait_for_task_completion(normal_task_id),
        wait_for_task_completion(high_task_id),
        wait_for_task_completion(critical_task_id)
    )
    
    print("\nAll priority tasks completed!")

async def test_scheduled_tasks():
    """
    Test scheduled tasks
    """
    print("\n=== Testing Scheduled Tasks ===")
    
    # Schedule a task for 10 seconds in the future
    schedule_time = datetime.now() + timedelta(seconds=10)
    
    # Submit scheduled task
    task_id = await submit_task(
        name="Scheduled Task",
        task_type="system_maintenance",
        params={
            "maintenance_type": "backup",
            "schedule_time": schedule_time.isoformat()
        },
        priority="normal"
    )
    
    print(f"Submitted scheduled task: {task_id}")
    print(f"Scheduled for: {schedule_time.isoformat()}")
    
    # Wait for completion
    task = await wait_for_task_completion(task_id, timeout_seconds=30)
    
    # Print result
    print(f"Task completed with status: {task['status']}")
    if task["result"]:
        print(f"Result: {json.dumps(task['result'], indent=2)}")
    if task["error"]:
        print(f"Error: {task['error']}")

async def test_task_cancellation():
    """
    Test task cancellation
    """
    print("\n=== Testing Task Cancellation ===")
    
    # Submit a long-running task
    task_id = await submit_task(
        name="Long Running Task",
        task_type="report_generation",
        params={
            "report_type": "comprehensive",
            "document_ids": ["doc1", "doc2", "doc3", "doc4", "doc5", "doc6", "doc7", "doc8", "doc9", "doc10"]
        },
        priority="low"
    )
    
    print(f"Submitted long-running task: {task_id}")
    
    # Wait a bit for the task to start
    await asyncio.sleep(2)
    
    # Get task status
    task = await get_task(task_id)
    print(f"Task status before cancellation: {task['status']}")
    
    # Cancel the task
    url = f"{BASE_URL}/tasks/{task_id}/cancel"
    async with httpx.AsyncClient() as client:
        response = await client.post(url)
        response.raise_for_status()
    
    print(f"Cancelled task: {task_id}")
    
    # Get task status after cancellation
    task = await get_task(task_id)
    print(f"Task status after cancellation: {task['status']}")

async def test_concurrent_tasks():
    """
    Test concurrent task execution
    """
    print("\n=== Testing Concurrent Task Execution ===")
    
    # Number of tasks to submit
    num_tasks = 10
    
    # Submit multiple tasks
    task_ids = []
    for i in range(num_tasks):
        task_id = await submit_task(
            name=f"Concurrent Task {i+1}",
            task_type="document_processing",
            params={"document_id": f"concurrent_test_{i+1}"},
            priority="normal"
        )
        task_ids.append(task_id)
        print(f"Submitted task {i+1}: {task_id}")
    
    # Wait for all tasks to complete
    print(f"\nWaiting for all {num_tasks} tasks to complete...")
    start_time = time.time()
    
    # Wait for tasks to complete
    await asyncio.gather(*[wait_for_task_completion(task_id) for task_id in task_ids])
    
    # Calculate elapsed time
    elapsed_time = time.time() - start_time
    
    print(f"\nAll {num_tasks} tasks completed in {elapsed_time:.2f} seconds")
    print(f"Average time per task: {elapsed_time / num_tasks:.2f} seconds")

async def main():
    """
    Main function
    """
    print("=== Background Task System Test ===")
    
    try:
        # Get initial task stats
        stats = await get_task_stats()
        print(f"Initial task stats: {json.dumps(stats, indent=2)}")
        
        # Run tests
        await test_document_processing()
        await test_vector_store_update()
        await test_report_generation()
        await test_system_maintenance()
        await test_task_dependencies()
        await test_task_priorities()
        await test_scheduled_tasks()
        await test_task_cancellation()
        await test_concurrent_tasks()
        
        # Get final task stats
        stats = await get_task_stats()
        print(f"\nFinal task stats: {json.dumps(stats, indent=2)}")
        
        print("\n=== All tests completed successfully! ===")
    except Exception as e:
        print(f"Error: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(asyncio.run(main()))

================
File: scripts/test_chat_api.py
================
#!/usr/bin/env python3
"""
Simple script to test the chat API with different queries.
This will help us compare the responses with the simplified system prompt.
"""

import requests
import json
import sys
import time
import os
import uuid
from datetime import datetime

# API endpoints
BASE_URL = "http://localhost:8000"
API_URL = f"{BASE_URL}/api/chat/query"
AUTH_URL = f"{BASE_URL}/api/auth/token"
REGISTER_URL = f"{BASE_URL}/api/auth/register"

# Test user credentials
TEST_USERNAME = f"test_user_{uuid.uuid4().hex[:8]}"
TEST_PASSWORD = "Test@123456"
TEST_EMAIL = f"test_{uuid.uuid4().hex[:8]}@example.com"

# Test queries based on the example chat
TEST_QUERIES = [
    "hello",
    "where is Paris in comparison to Madrid",
    "distance and direction",
    "how can I get there from the US?",
    "I will be leaving from Washington DC"
]
# No longer skipping authentication since we're handling it properly
SKIP_AUTH = True

def register_user():
    """Register a test user."""
    headers = {
        "Content-Type": "application/json"
    }
    
    payload = {
        "username": TEST_USERNAME,
        "password": TEST_PASSWORD,
        "email": TEST_EMAIL,
        "full_name": "Test User"
    }
    
    try:
        response = requests.post(REGISTER_URL, headers=headers, json=payload)
        if response.status_code == 200:
            print(f"Successfully registered user: {TEST_USERNAME}")
            return True
        else:
            print(f"Failed to register user: {response.status_code} - {response.text}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"Error registering user: {e}")
        return False

def get_access_token():
    """Get an access token for the test user."""
    data = {
        "username": TEST_USERNAME,
        "password": TEST_PASSWORD
    }
    
    try:
        response = requests.post(
            AUTH_URL,
            data=data,
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        if response.status_code == 200:
            token_data = response.json()
            print("Successfully obtained access token")
            return token_data["access_token"]
        else:
            print(f"Failed to get token: {response.status_code} - {response.text}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"Error getting token: {e}")
        return None

def send_chat_request(message, conversation_id=None, access_token=None):
    """Send a chat request to the API."""
    headers = {
        "Content-Type": "application/json"
    }
    
    # Add authorization header if token is provided
    if access_token:
        headers["Authorization"] = f"Bearer {access_token}"
    
    payload = {
        "message": message,
        "use_rag": True,  # Enable RAG
        "stream": False   # Don't stream the response
    }
    
    if conversation_id:
        payload["conversation_id"] = conversation_id
    
    try:
        response = requests.post(API_URL, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error sending request: {e}")
        return None

def run_test():
    """Run the test queries and print the responses."""
    print(f"Starting test at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Register a test user
    if not register_user():
        print("Failed to register test user. Exiting.")
        return
    
    # Get an access token
    access_token = get_access_token()
    if not access_token:
        print("Failed to get access token. Exiting.")
        return
    
    print(f"Testing with {len(TEST_QUERIES)} queries")
    print("-" * 80)
    
    conversation_id = None
    
    for i, query in enumerate(TEST_QUERIES):
        print(f"Query {i+1}: {query}")
        
        # Send the query with the access token
        result = send_chat_request(query, conversation_id, access_token)
        
        if result:
            # Save the conversation ID for the next query
            conversation_id = result.get("conversation_id")
            
            # Print the response
            print(f"Response: {result.get('message')}")
            
            # Print the sources if available
            sources = result.get("citations", [])
            if sources:
                print(f"Sources ({len(sources)}):")
                for source in sources:
                    print(f"  - {source}")
            else:
                print("No sources provided")
        else:
            print("Failed to get response")
        
        print("-" * 80)
        
        # Wait a bit between requests
        if i < len(TEST_QUERIES) - 1:
            time.sleep(1)
    
    print("Test completed")

if __name__ == "__main__":
    # Check if the API is available
    try:
        # Just try to connect to the server
        response = requests.get(BASE_URL)
        if response.status_code in [200, 404]:  # Either is fine, just checking if server is up
            run_test()
        else:
            print(f"API health check failed with status code {response.status_code}")
    except requests.exceptions.ConnectionError:
        print("Could not connect to the API. Make sure the application is running.")
        sys.exit(1)

================
File: scripts/test_document_processing.py
================
"""
Test script for document processing with adapter functions.

This script demonstrates the document processor working with the adapter functions by:
1. Creating a Pydantic Document
2. Processing the document
3. Converting the processed document to a SQLAlchemy Document
4. Converting it back to a Pydantic Document
5. Verifying that the data is preserved
"""
import sys
import os
import uuid
import asyncio
import tempfile
from datetime import datetime

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.document import Document as PydanticDocument, Chunk as PydanticChunk
from app.db.models import Document as DBDocument, Chunk as DBChunk
from app.rag.document_processor import DocumentProcessor
from app.db.adapters import (
    pydantic_document_to_sqlalchemy,
    sqlalchemy_document_to_pydantic,
    pydantic_chunk_to_sqlalchemy,
    sqlalchemy_chunk_to_pydantic,
    to_str_id,
    to_uuid_or_str
)
from app.core.config import UPLOAD_DIR

async def test_document_processing():
    """Test document processing with adapter functions"""
    print("Testing document processing with adapter functions...")
    
    # Create a temporary directory for the test
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # Create a test document
            doc_id = str(uuid.uuid4())
            filename = "test.txt"
            
            # Create document directory
            document_dir = os.path.join(temp_dir, doc_id)
            os.makedirs(document_dir, exist_ok=True)
            
            # Create test file
            document_file_path = os.path.join(document_dir, filename)
            with open(document_file_path, 'w') as f:
                f.write("This is a test document.\n\n" * 10)
                
            print(f"Created test file at: {document_file_path}")
            
            # Create a Pydantic Document
            pydantic_doc = PydanticDocument(
                id=doc_id,
                filename=filename,
                content="",  # Content will be read during processing
                metadata={
                    "file_type": "txt",
                    "test_size": "small",
                    "processing_status": "pending"
                },
                folder="/test"
            )
            
            print(f"Created Pydantic Document: {pydantic_doc.id}")
            
            # Create a DocumentProcessor with simplified settings
            # Disable chunking judge and document analysis service
            import app.rag.document_processor
            original_use_chunking_judge = app.rag.document_processor.USE_CHUNKING_JUDGE
            app.rag.document_processor.USE_CHUNKING_JUDGE = False
            
            # Create a custom document processor that uses our temp directory
            class TestDocumentProcessor(DocumentProcessor):
                async def _load_document(self, file_path: str):
                    """Override to use the correct file path"""
                    print(f"Original file path: {file_path}")
                    
                    # Extract document ID and filename from the path
                    parts = file_path.split('/')
                    doc_id = parts[-2]
                    filename = parts[-1]
                    
                    # Use the temp directory instead
                    correct_path = os.path.join(temp_dir, doc_id, filename)
                    print(f"Corrected file path: {correct_path}")
                    
                    # Read the file content
                    with open(correct_path, 'r') as f:
                        content = f.read()
                    
                    # Return a LangchainDocument
                    from langchain.schema import Document as LangchainDocument
                    return [LangchainDocument(page_content=content, metadata={"source": correct_path})]
            
            try:
                processor = TestDocumentProcessor(
                    upload_dir=temp_dir,  # Use the temp directory directly
                    chunking_strategy="recursive",
                    chunk_size=1500,
                    chunk_overlap=150
                )
            finally:
                # Restore original settings
                app.rag.document_processor.USE_CHUNKING_JUDGE = original_use_chunking_judge
            
            # Process the document
            processed_doc = await processor.process_document(pydantic_doc)
            
            print(f"Processed document: {processed_doc.id}")
            print(f"Number of chunks: {len(processed_doc.chunks)}")
            
            # Convert to SQLAlchemy Document
            db_doc = pydantic_document_to_sqlalchemy(processed_doc)
            
            print(f"Converted to SQLAlchemy Document: {db_doc.id}")
            print(f"Number of chunks: {len(db_doc.chunks)}")
            
            # Convert back to Pydantic Document
            pydantic_doc2 = sqlalchemy_document_to_pydantic(db_doc)
            
            print(f"Converted back to Pydantic Document: {pydantic_doc2.id}")
            print(f"Number of chunks: {len(pydantic_doc2.chunks)}")
            
            # Verify that the data is preserved
            assert str(pydantic_doc2.id) == str(processed_doc.id)
            assert pydantic_doc2.filename == processed_doc.filename
            assert pydantic_doc2.content == processed_doc.content
            assert pydantic_doc2.metadata == processed_doc.metadata
            assert pydantic_doc2.folder == processed_doc.folder
            assert len(pydantic_doc2.chunks) == len(processed_doc.chunks)
            
            print("All tests passed!")
            
            return True
        finally:
            # No need to restore UPLOAD_DIR since we're using dependency injection
            pass

if __name__ == "__main__":
    asyncio.run(test_document_processing())

================
File: scripts/test_memory_buffer.py
================
#!/usr/bin/env python
"""
Test script for the memory buffer functionality
"""
import asyncio
import logging
import os
import sys
import uuid
from datetime import datetime

# Add the parent directory to the path so we can import app modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.db.session import get_session
from app.models.memory import Memory
from app.db.models import Conversation
from app.rag.memory_buffer import add_to_memory_buffer, get_memory_buffer, process_query

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

async def test_memory_buffer():
    """Test the memory buffer functionality"""
    try:
        logger.info("Starting memory buffer test")
        
        # Get a database session
        db = await anext(get_session())
        
        # Create a test conversation
        conversation = Conversation(message_count=0)
        db.add(conversation)
        await db.commit()
        await db.refresh(conversation)
        
        conversation_id = conversation.id
        logger.info(f"Created test conversation with ID: {conversation_id}")
        
        # Test adding a memory
        memory = await add_to_memory_buffer(
            conversation_id=conversation_id,
            content="This is a test memory",
            label="test_memory",
            db=db
        )
        logger.info(f"Added memory: {memory.id}")
        
        # Test retrieving memories
        memories = await get_memory_buffer(
            conversation_id=conversation_id,
            db=db
        )
        logger.info(f"Retrieved {len(memories)} memories")
        for i, mem in enumerate(memories):
            logger.info(f"Memory {i+1}: {mem.content}")
        
        # Test processing a memory storage command
        user_id = str(uuid.uuid4())
        query = "Remember this phrase: The sky is blue and the grass is green."
        processed_query, memory_response, memory_operation = await process_query(
            query=query,
            user_id=user_id,
            conversation_id=conversation_id,
            db=db
        )
        logger.info(f"Processed query: {processed_query}")
        logger.info(f"Memory response: {memory_response}")
        logger.info(f"Memory operation: {memory_operation}")
        
        # Test processing a memory recall command
        query = "Recall what I asked you to remember."
        processed_query, memory_response, memory_operation = await process_query(
            query=query,
            user_id=user_id,
            conversation_id=conversation_id,
            db=db
        )
        logger.info(f"Processed query: {processed_query}")
        logger.info(f"Memory response: {memory_response}")
        logger.info(f"Memory operation: {memory_operation}")
        
        # Test processing a regular query
        query = "What is the capital of France?"
        processed_query, memory_response, memory_operation = await process_query(
            query=query,
            user_id=user_id,
            conversation_id=conversation_id,
            db=db
        )
        logger.info(f"Processed query: {processed_query}")
        logger.info(f"Memory response: {memory_response}")
        logger.info(f"Memory operation: {memory_operation}")
        
        logger.info("Memory buffer test completed successfully")
    except Exception as e:
        logger.error(f"Error testing memory buffer: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(test_memory_buffer())

================
File: scripts/test_schema_inspector.py
================
"""
Test script for the PostgreSQL schema inspector and tool
"""
import asyncio
import sys
import os
import logging
from pathlib import Path

# Add the project root to the Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.db.connection_manager import connection_manager
from app.db.schema_inspector import schema_inspector
from app.rag.tools.postgresql_tool import PostgreSQLTool

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("test_schema_inspector")

async def test_schema_inspector(connection_string: str):
    """
    Test the schema inspector with a PostgreSQL connection
    
    Args:
        connection_string: PostgreSQL connection string
    """
    logger.info("Testing schema inspector with connection: %s", connection_string)
    
    # Register connection
    conn_id = connection_manager.register_connection(connection_string)
    logger.info("Registered connection with ID: %s", conn_id)
    
    try:
        # Test get_schemas
        logger.info("Testing get_schemas...")
        schemas = await schema_inspector.get_schemas(conn_id)
        logger.info("Found %d schemas:", len(schemas))
        for schema in schemas:
            logger.info("  - %s (owner: %s)", schema["schema_name"], schema["owner"])
        
        # Test get_tables for the public schema
        logger.info("\nTesting get_tables for 'public' schema...")
        tables = await schema_inspector.get_tables(conn_id, "public")
        logger.info("Found %d tables in 'public' schema:", len(tables))
        for table in tables:
            logger.info("  - %s (%s, ~%d rows, %s)", 
                       table["table_name"], 
                       table["type"], 
                       table["row_estimate"], 
                       table["total_size"])
        
        # If tables were found, test get_columns for the first table
        if tables:
            first_table = tables[0]["table_name"]
            logger.info("\nTesting get_columns for table '%s'...", first_table)
            columns = await schema_inspector.get_columns(conn_id, first_table)
            logger.info("Found %d columns in table '%s':", len(columns), first_table)
            for column in columns:
                logger.info("  - %s (%s, nullable: %s)", 
                           column["column_name"], 
                           column["data_type"], 
                           column["is_nullable"])
            
            # Test get_indexes
            logger.info("\nTesting get_indexes for table '%s'...", first_table)
            indexes = await schema_inspector.get_indexes(conn_id, first_table)
            logger.info("Found %d indexes in table '%s':", len(indexes), first_table)
            for index in indexes:
                logger.info("  - %s (%s, unique: %s)", 
                           index["index_name"], 
                           index["index_type"], 
                           index["is_unique"])
            
            # Test get_constraints
            logger.info("\nTesting get_constraints for table '%s'...", first_table)
            constraints = await schema_inspector.get_constraints(conn_id, first_table)
            logger.info("Found %d constraints in table '%s':", len(constraints), first_table)
            for constraint in constraints:
                logger.info("  - %s (%s)", 
                           constraint["constraint_name"], 
                           constraint["constraint_type"])
            
            # Test get_table_structure
            logger.info("\nTesting get_table_structure for table '%s'...", first_table)
            structure = await schema_inspector.get_table_structure(conn_id, first_table)
            logger.info("Retrieved table structure for '%s'", first_table)
            logger.info("  - %d columns, %d indexes, %d constraints", 
                       len(structure["columns"]), 
                       len(structure["indexes"]), 
                       len(structure["constraints"]))
        
        # Test get_extensions
        logger.info("\nTesting get_extensions...")
        extensions = await schema_inspector.get_extension_info(conn_id)
        logger.info("Found %d extensions:", len(extensions))
        for ext in extensions:
            logger.info("  - %s (version: %s)", ext["name"], ext["version"])
        
        # Test check_extension_installed for pgvector
        logger.info("\nTesting check_extension_installed for 'vector'...")
        has_pgvector = await schema_inspector.check_extension_installed(conn_id, "vector")
        logger.info("pgvector installed: %s", has_pgvector)
        
        # Test get_pgvector_info
        logger.info("\nTesting get_pgvector_info...")
        pgvector_info = await schema_inspector.get_pgvector_info(conn_id)
        logger.info("pgvector info: %s", pgvector_info)
        
        return True
    except Exception as e:
        logger.error("Error testing schema inspector: %s", str(e))
        return False
    finally:
        # Close connection
        await connection_manager.close(conn_id)
        logger.info("Closed connection")

async def test_postgresql_tool(connection_string: str):
    """
    Test the PostgreSQL tool with a PostgreSQL connection
    
    Args:
        connection_string: PostgreSQL connection string
    """
    logger.info("\n\nTesting PostgreSQL tool with connection: %s", connection_string)
    
    # Register connection
    conn_id = connection_manager.register_connection(connection_string)
    logger.info("Registered connection with ID: %s", conn_id)
    
    # Create PostgreSQL tool
    pg_tool = PostgreSQLTool()
    
    try:
        # Test get_schemas operation
        logger.info("Testing get_schemas operation...")
        result = await pg_tool.execute({
            "operation": "get_schemas",
            "connection_id": conn_id
        })
        logger.info("Found %d schemas in %.2f seconds", 
                   len(result["schemas"]), 
                   result["execution_time"])
        
        # Test get_tables operation
        logger.info("\nTesting get_tables operation...")
        result = await pg_tool.execute({
            "operation": "get_tables",
            "connection_id": conn_id,
            "schema": "public"
        })
        logger.info("Found %d tables in 'public' schema in %.2f seconds", 
                   len(result["tables"]), 
                   result["execution_time"])
        
        # If tables were found, test get_columns operation for the first table
        if result["tables"]:
            first_table = result["tables"][0]["table_name"]
            
            # Test get_columns operation
            logger.info("\nTesting get_columns operation for table '%s'...", first_table)
            result = await pg_tool.execute({
                "operation": "get_columns",
                "connection_id": conn_id,
                "table_name": first_table,
                "schema": "public"
            })
            logger.info("Found %d columns in table '%s' in %.2f seconds", 
                       len(result["columns"]), 
                       first_table, 
                       result["execution_time"])
            
            # Test explain_query operation
            logger.info("\nTesting explain_query operation...")
            result = await pg_tool.execute({
                "operation": "explain_query",
                "connection_id": conn_id,
                "query": f"SELECT * FROM {first_table} LIMIT 10",
                "explain_type": "analyze"
            })
            logger.info("Explained query in %.2f seconds", result["execution_time"])
            logger.info("Plan:\n%s", result.get("plan_text", ""))
        
        # Test get_extensions operation
        logger.info("\nTesting get_extensions operation...")
        result = await pg_tool.execute({
            "operation": "get_extensions",
            "connection_id": conn_id
        })
        logger.info("Found %d extensions in %.2f seconds", 
                   len(result["extensions"]), 
                   result["execution_time"])
        
        # Test check_extension operation for pgvector
        logger.info("\nTesting check_extension operation for 'vector'...")
        result = await pg_tool.execute({
            "operation": "check_extension",
            "connection_id": conn_id,
            "extension_name": "vector"
        })
        logger.info("pgvector installed: %s (checked in %.2f seconds)", 
                   result["installed"], 
                   result["execution_time"])
        
        # Test get_pgvector_info operation
        logger.info("\nTesting get_pgvector_info operation...")
        result = await pg_tool.execute({
            "operation": "get_pgvector_info",
            "connection_id": conn_id
        })
        logger.info("pgvector info retrieved in %.2f seconds", result["execution_time"])
        if result["pgvector_info"].get("installed", False):
            logger.info("pgvector version: %s", result["pgvector_info"].get("version", "unknown"))
            logger.info("Vector columns: %d", len(result["pgvector_info"].get("vector_columns", [])))
            logger.info("Vector indexes: %d", len(result["pgvector_info"].get("vector_indexes", [])))
        
        return True
    except Exception as e:
        logger.error("Error testing PostgreSQL tool: %s", str(e))
        return False
    finally:
        # Close connection
        await connection_manager.close(conn_id)
        logger.info("Closed connection")

async def main():
    """Main function"""
    # Get PostgreSQL connection string from environment variable or use a default
    connection_string = os.environ.get(
        "POSTGRES_CONNECTION_STRING", 
        "postgresql://postgres:postgres@localhost:5432/postgres"
    )
    
    # Test schema inspector
    inspector_success = await test_schema_inspector(connection_string)
    
    # Test PostgreSQL tool
    tool_success = await test_postgresql_tool(connection_string)
    
    # Report results
    if inspector_success and tool_success:
        logger.info("\n\nAll tests passed successfully!")
        return 0
    else:
        logger.error("\n\nSome tests failed!")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

================
File: scripts/test_system_prompts.py
================
#!/usr/bin/env python3
"""
Script to test and compare different system prompts for the Metis RAG system.
This script runs the same set of queries against both the original and simplified
system prompts and compares the results.
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

try:
    from app.rag.rag_engine import RAGEngine
    from app.rag.system_prompts.rag import RAG_SYSTEM_PROMPT
    from app.models.conversation import Message
except ImportError:
    print("Error: Could not import required modules from the Metis RAG system.")
    print("Make sure you're running this script from the project root directory.")
    sys.exit(1)

# Define the original system prompt (saved before we modified it)
ORIGINAL_SYSTEM_PROMPT = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
- Do not use your general knowledge unless the context is insufficient, and clearly indicate when you're doing so.
- Analyze the context carefully to find the most relevant information for the user's question.
- If multiple sources provide different information, synthesize them and explain any discrepancies.
- If the context includes metadata like filenames, tags, or folders, use this to understand the source and relevance of the information.

WHEN INFORMATION IS LIMITED:
1. If you find SOME relevant information but it's not comprehensive, start with: "I've searched my knowledge base for information about [topic]. While I don't have comprehensive information on this topic, I did find some relevant documents that mention it."
2. Then present the limited information you have, with proper citations.
3. End with: "Please note this information is limited to what's in my document database. For more comprehensive information, consider consulting specialized resources."

WHEN NO INFORMATION IS FOUND:
1. Clearly state: "Based on the provided documents, I don't have information about [topic]."
2. Only after acknowledging the limitation, you may provide general knowledge with: "However, generally speaking..." to assist the user.

CITATION FORMATTING:
1. Always use numbered citations like [1], [2] that correspond to the sources provided.
2. At the end of your response, list your sources in a structured format:
   Sources:
   [1] Document ID: abc123... - "Document Title"
   [2] Document ID: def456... - "Document Title"

CONVERSATION HANDLING:
- IMPORTANT: Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.
- If no conversation history is provided, treat the query as a new, standalone question.
- Only maintain continuity with previous exchanges when conversation history is explicitly provided.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Always cite your sources with numbers in square brackets [1] when using information from the context.
"""

# Define the simplified system prompt (current one)
SIMPLIFIED_SYSTEM_PROMPT = RAG_SYSTEM_PROMPT

# Test queries based on the example chat
TEST_QUERIES = [
    "hello",
    "where is Paris in comparison to Madrid",
    "distance and direction",
    "how can I get there from the US?",
    "I will be leaving from Washington DC"
]

# Test scenarios to evaluate specific issues
TEST_SCENARIOS = [
    {
        "name": "Empty Vector Store Test",
        "description": "Test how the system handles queries when no documents exist",
        "setup": "clear_vector_store",
        "query": "Tell me about quantum computing"
    },
    {
        "name": "Non-existent Document Test",
        "description": "Test how the system handles queries about specific documents that don't exist",
        "setup": "standard",
        "query": "What does the document 'Introduction to China's Provinces' say about Beijing?"
    },
    {
        "name": "User Information Test",
        "description": "Test if the system remembers user information across the conversation",
        "setup": "standard",
        "conversation": [
            "My name is Charles",
            "What can you tell me about machine learning?",
            "Thank you for that information. Can you recommend some resources?"
        ]
    },
    {
        "name": "Citation Test",
        "description": "Test how the system uses citations when documents are available",
        "setup": "with_documents",
        "query": "What are the key principles of artificial intelligence?"
    }
]

class SystemPromptTester:
    """Class to test different system prompts for the Metis RAG system."""
    
    def __init__(self, output_dir: str = "test_results"):
        """Initialize the tester with output directory."""
        self.output_dir = output_dir
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = os.path.join(output_dir, f"system_prompt_test_{self.timestamp}")
        
        # Create output directory if it doesn't exist
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Initialize RAG engines with different system prompts
        self.original_rag_engine = None
        self.simplified_rag_engine = None
        
        print(f"Results will be saved to: {self.results_dir}")
    
    def initialize_rag_engines(self):
        """Initialize RAG engines with different system prompts."""
        try:
            # Initialize RAG engine with original system prompt
            self.original_rag_engine = RAGEngine()
            
            # Initialize RAG engine with simplified system prompt
            self.simplified_rag_engine = RAGEngine()
            
            print("RAG engines initialized successfully.")
            return True
        except Exception as e:
            print(f"Error initializing RAG engines: {str(e)}")
            return False
    
    def run_query(self, rag_engine, query: str, conversation_history: Optional[List[Message]] = None) -> Dict[str, Any]:
        """Run a query against a RAG engine and return the result."""
        try:
            # Convert conversation history to Message objects if provided as strings
            if conversation_history and isinstance(conversation_history[0], str):
                messages = []
                for i, msg in enumerate(conversation_history):
                    role = "user" if i % 2 == 0 else "assistant"
                    messages.append(Message(role=role, content=msg))
                conversation_history = messages
            
            # Run the query
            start_time = time.time()
            result = rag_engine.query(
                query=query,
                conversation_history=conversation_history,
                stream=False
            )
            end_time = time.time()
            
            # Extract relevant information from the result
            response = {
                "query": query,
                "response": result.get("response", ""),
                "sources": result.get("sources", []),
                "execution_time": end_time - start_time
            }
            
            return response
        except Exception as e:
            print(f"Error running query: {str(e)}")
            return {
                "query": query,
                "response": f"Error: {str(e)}",
                "sources": [],
                "execution_time": 0
            }
    
    def run_conversation(self, rag_engine, conversation: List[str]) -> List[Dict[str, Any]]:
        """Run a conversation (multiple queries) against a RAG engine."""
        results = []
        history = []
        
        for i, query in enumerate(conversation):
            # Run the query with conversation history
            result = self.run_query(rag_engine, query, history)
            results.append(result)
            
            # Update conversation history
            history.append(Message(role="user", content=query))
            history.append(Message(role="assistant", content=result["response"]))
        
        return results
    
    def run_test_queries(self):
        """Run the test queries against both system prompts."""
        if not self.initialize_rag_engines():
            return
        
        original_results = []
        simplified_results = []
        
        # Run each query against both system prompts
        for query in TEST_QUERIES:
            print(f"Running query: {query}")
            
            # Run with original system prompt
            original_result = self.run_query(self.original_rag_engine, query)
            original_results.append(original_result)
            
            # Run with simplified system prompt
            simplified_result = self.run_query(self.simplified_rag_engine, query)
            simplified_results.append(simplified_result)
        
        # Save results
        self.save_results("test_queries", original_results, simplified_results)
    
    def run_test_scenarios(self):
        """Run the test scenarios against both system prompts."""
        if not self.initialize_rag_engines():
            return
        
        scenario_results = []
        
        # Run each scenario
        for scenario in TEST_SCENARIOS:
            print(f"Running scenario: {scenario['name']}")
            
            # Setup the scenario
            self.setup_scenario(scenario["setup"])
            
            # Run the scenario
            if "query" in scenario:
                # Single query scenario
                original_result = self.run_query(self.original_rag_engine, scenario["query"])
                simplified_result = self.run_query(self.simplified_rag_engine, scenario["query"])
                
                scenario_result = {
                    "name": scenario["name"],
                    "description": scenario["description"],
                    "query": scenario["query"],
                    "original_result": original_result,
                    "simplified_result": simplified_result
                }
            elif "conversation" in scenario:
                # Conversation scenario
                original_results = self.run_conversation(self.original_rag_engine, scenario["conversation"])
                simplified_results = self.run_conversation(self.simplified_rag_engine, scenario["conversation"])
                
                scenario_result = {
                    "name": scenario["name"],
                    "description": scenario["description"],
                    "conversation": scenario["conversation"],
                    "original_results": original_results,
                    "simplified_results": simplified_results
                }
            
            scenario_results.append(scenario_result)
        
        # Save scenario results
        self.save_scenario_results(scenario_results)
    
    def setup_scenario(self, setup_type: str):
        """Setup a test scenario."""
        if setup_type == "clear_vector_store":
            # Clear the vector store (implementation depends on your system)
            print("Setting up scenario: clear_vector_store")
            # self.original_rag_engine.vector_store.clear()
            # self.simplified_rag_engine.vector_store.clear()
        elif setup_type == "with_documents":
            # Add test documents to the vector store
            print("Setting up scenario: with_documents")
            # Add implementation to add test documents
        else:
            # Standard setup (no special configuration)
            print("Setting up scenario: standard")
    
    def save_results(self, test_name: str, original_results: List[Dict[str, Any]], simplified_results: List[Dict[str, Any]]):
        """Save test results to files."""
        results = {
            "test_name": test_name,
            "timestamp": self.timestamp,
            "original_system_prompt": ORIGINAL_SYSTEM_PROMPT,
            "simplified_system_prompt": SIMPLIFIED_SYSTEM_PROMPT,
            "original_results": original_results,
            "simplified_results": simplified_results
        }
        
        # Save results to JSON file
        results_file = os.path.join(self.results_dir, f"{test_name}_results.json")
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2)
        
        # Generate HTML report
        self.generate_html_report(test_name, results)
        
        print(f"Results saved to: {results_file}")
    
    def save_scenario_results(self, scenario_results: List[Dict[str, Any]]):
        """Save scenario results to files."""
        results = {
            "test_name": "test_scenarios",
            "timestamp": self.timestamp,
            "original_system_prompt": ORIGINAL_SYSTEM_PROMPT,
            "simplified_system_prompt": SIMPLIFIED_SYSTEM_PROMPT,
            "scenario_results": scenario_results
        }
        
        # Save results to JSON file
        results_file = os.path.join(self.results_dir, "scenario_results.json")
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2)
        
        # Generate HTML report
        self.generate_scenario_html_report(results)
        
        print(f"Scenario results saved to: {results_file}")
    
    def generate_html_report(self, test_name: str, results: Dict[str, Any]):
        """Generate an HTML report for the test results."""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Metis RAG System Prompt Test Results - {test_name}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1, h2, h3 {{ color: #333; }}
                .query {{ background-color: #f5f5f5; padding: 10px; margin: 10px 0; border-left: 5px solid #007bff; }}
                .response {{ background-color: #f9f9f9; padding: 10px; margin: 10px 0; border-left: 5px solid #28a745; }}
                .sources {{ background-color: #f9f9f9; padding: 10px; margin: 10px 0; border-left: 5px solid #ffc107; }}
                .comparison {{ display: flex; }}
                .original, .simplified {{ flex: 1; margin: 10px; padding: 10px; border: 1px solid #ddd; }}
                .system-prompt {{ background-color: #f0f0f0; padding: 10px; white-space: pre-wrap; }}
                .metrics {{ margin-top: 20px; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>Metis RAG System Prompt Test Results</h1>
            <p>Test: {test_name}</p>
            <p>Timestamp: {results['timestamp']}</p>
            
            <h2>System Prompts</h2>
            <div class="comparison">
                <div class="original">
                    <h3>Original System Prompt</h3>
                    <div class="system-prompt">{results['original_system_prompt']}</div>
                </div>
                <div class="simplified">
                    <h3>Simplified System Prompt</h3>
                    <div class="system-prompt">{results['simplified_system_prompt']}</div>
                </div>
            </div>
            
            <h2>Test Results</h2>
        """
        
        # Add results for each query
        for i in range(len(results['original_results'])):
            original = results['original_results'][i]
            simplified = results['simplified_results'][i]
            
            html += f"""
            <h3>Query {i+1}: {original['query']}</h3>
            <div class="comparison">
                <div class="original">
                    <h4>Original System Prompt Response</h4>
                    <div class="response">{original['response']}</div>
                    <div class="sources">
                        <h5>Sources ({len(original['sources'])})</h5>
                        <ul>
            """
            
            for source in original['sources']:
                html += f"<li>{source}</li>"
            
            html += f"""
                        </ul>
                    </div>
                    <div class="metrics">
                        <p>Execution Time: {original['execution_time']:.2f} seconds</p>
                    </div>
                </div>
                <div class="simplified">
                    <h4>Simplified System Prompt Response</h4>
                    <div class="response">{simplified['response']}</div>
                    <div class="sources">
                        <h5>Sources ({len(simplified['sources'])})</h5>
                        <ul>
            """
            
            for source in simplified['sources']:
                html += f"<li>{source}</li>"
            
            html += f"""
                        </ul>
                    </div>
                    <div class="metrics">
                        <p>Execution Time: {simplified['execution_time']:.2f} seconds</p>
                    </div>
                </div>
            </div>
            """
        
        html += """
        </body>
        </html>
        """
        
        # Save HTML report
        report_file = os.path.join(self.results_dir, f"{test_name}_report.html")
        with open(report_file, "w") as f:
            f.write(html)
        
        print(f"HTML report saved to: {report_file}")
    
    def generate_scenario_html_report(self, results: Dict[str, Any]):
        """Generate an HTML report for the scenario test results."""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Metis RAG System Prompt Scenario Test Results</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1, h2, h3, h4 {{ color: #333; }}
                .scenario {{ background-color: #f5f5f5; padding: 15px; margin: 20px 0; border-left: 5px solid #007bff; }}
                .query {{ background-color: #f5f5f5; padding: 10px; margin: 10px 0; border-left: 5px solid #007bff; }}
                .response {{ background-color: #f9f9f9; padding: 10px; margin: 10px 0; border-left: 5px solid #28a745; }}
                .sources {{ background-color: #f9f9f9; padding: 10px; margin: 10px 0; border-left: 5px solid #ffc107; }}
                .comparison {{ display: flex; }}
                .original, .simplified {{ flex: 1; margin: 10px; padding: 10px; border: 1px solid #ddd; }}
                .system-prompt {{ background-color: #f0f0f0; padding: 10px; white-space: pre-wrap; }}
                .metrics {{ margin-top: 20px; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>Metis RAG System Prompt Scenario Test Results</h1>
            <p>Timestamp: {results['timestamp']}</p>
            
            <h2>System Prompts</h2>
            <div class="comparison">
                <div class="original">
                    <h3>Original System Prompt</h3>
                    <div class="system-prompt">{results['original_system_prompt']}</div>
                </div>
                <div class="simplified">
                    <h3>Simplified System Prompt</h3>
                    <div class="system-prompt">{results['simplified_system_prompt']}</div>
                </div>
            </div>
            
            <h2>Scenario Results</h2>
        """
        
        # Add results for each scenario
        for scenario in results['scenario_results']:
            html += f"""
            <div class="scenario">
                <h3>{scenario['name']}</h3>
                <p>{scenario['description']}</p>
            """
            
            if "query" in scenario:
                # Single query scenario
                html += f"""
                <h4>Query: {scenario['query']}</h4>
                <div class="comparison">
                    <div class="original">
                        <h4>Original System Prompt Response</h4>
                        <div class="response">{scenario['original_result']['response']}</div>
                        <div class="sources">
                            <h5>Sources ({len(scenario['original_result']['sources'])})</h5>
                            <ul>
                """
                
                for source in scenario['original_result']['sources']:
                    html += f"<li>{source}</li>"
                
                html += f"""
                            </ul>
                        </div>
                        <div class="metrics">
                            <p>Execution Time: {scenario['original_result']['execution_time']:.2f} seconds</p>
                        </div>
                    </div>
                    <div class="simplified">
                        <h4>Simplified System Prompt Response</h4>
                        <div class="response">{scenario['simplified_result']['response']}</div>
                        <div class="sources">
                            <h5>Sources ({len(scenario['simplified_result']['sources'])})</h5>
                            <ul>
                """
                
                for source in scenario['simplified_result']['sources']:
                    html += f"<li>{source}</li>"
                
                html += f"""
                            </ul>
                        </div>
                        <div class="metrics">
                            <p>Execution Time: {scenario['simplified_result']['execution_time']:.2f} seconds</p>
                        </div>
                    </div>
                </div>
                """
            elif "conversation" in scenario:
                # Conversation scenario
                html += f"""
                <h4>Conversation</h4>
                <table>
                    <tr>
                        <th>Turn</th>
                        <th>User Query</th>
                        <th>Original Response</th>
                        <th>Simplified Response</th>
                    </tr>
                """
                
                for i, query in enumerate(scenario['conversation']):
                    original_response = scenario['original_results'][i]['response'] if i < len(scenario['original_results']) else ""
                    simplified_response = scenario['simplified_results'][i]['response'] if i < len(scenario['simplified_results']) else ""
                    
                    html += f"""
                    <tr>
                        <td>{i+1}</td>
                        <td>{query}</td>
                        <td>{original_response}</td>
                        <td>{simplified_response}</td>
                    </tr>
                    """
                
                html += """
                </table>
                """
            
            html += """
            </div>
            """
        
        html += """
        </body>
        </html>
        """
        
        # Save HTML report
        report_file = os.path.join(self.results_dir, "scenario_report.html")
        with open(report_file, "w") as f:
            f.write(html)
        
        print(f"Scenario HTML report saved to: {report_file}")

def main():
    """Main function to run the system prompt tests."""
    parser = argparse.ArgumentParser(description="Test different system prompts for the Metis RAG system.")
    parser.add_argument("--output-dir", default="test_results", help="Directory to save test results")
    parser.add_argument("--test-queries", action="store_true", help="Run test queries")
    parser.add_argument("--test-scenarios", action="store_true", help="Run test scenarios")
    args = parser.parse_args()
    
    # Create tester
    tester = SystemPromptTester(output_dir=args.output_dir)
    
    # Run tests
    if args.test_queries:
        tester.run_test_queries()
    
    if args.test_scenarios:
        tester.run_test_scenarios()
    
    # If no tests specified, run all
    if not args.test_queries and not args.test_scenarios:
        tester.run_test_queries()
        tester.run_test_scenarios()
    
    print("System prompt tests completed.")

if __name__ == "__main__":
    main()

================
File: scripts/update_docker_config.py
================
#!/usr/bin/env python3
"""
Docker Configuration Update Script for Metis RAG

This script updates the Docker configuration to support both SQLite and PostgreSQL:
1. Updates docker-compose.yml to include PostgreSQL service
2. Creates database initialization scripts
3. Updates Dockerfile to install PostgreSQL client libraries
4. Creates environment-based configuration switching

Usage:
    python update_docker_config.py [--apply]
"""
import os
import sys
import argparse
import shutil
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

class DockerConfigUpdater:
    """Docker configuration update class"""
    
    def __init__(self, apply: bool = False):
        self.apply = apply
        self.config_dir = os.path.join(project_root, "config")
        
        print(f"Initialized Docker configuration updater")
        print(f"Apply mode: {'ON' if apply else 'OFF (dry run)'}")
        
    def update_file(self, file_path: str, new_content: str, description: str) -> bool:
        """Update a file with new content"""
        try:
            print(f"  {description}")
            
            # Check if file exists
            if not os.path.exists(file_path):
                print(f"    ✗ Error: File not found: {file_path}")
                return False
            
            # Backup file
            backup_path = f"{file_path}.bak"
            if self.apply:
                shutil.copy2(file_path, backup_path)
                print(f"    ✓ Created backup: {backup_path}")
            
            # Write new content
            if self.apply:
                with open(file_path, 'w') as f:
                    f.write(new_content)
                print(f"    ✓ Updated file: {file_path}")
                return True
            else:
                print(f"    ✓ Validated (not applied - dry run mode)")
                return True
        except Exception as e:
            print(f"    ✗ Error: {str(e)}")
            return False
    
    def create_file(self, file_path: str, content: str, description: str) -> bool:
        """Create a new file with content"""
        try:
            print(f"  {description}")
            
            # Check if file already exists
            if os.path.exists(file_path):
                print(f"    ✓ File already exists: {file_path}")
                return True
            
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # Write content
            if self.apply:
                with open(file_path, 'w') as f:
                    f.write(content)
                print(f"    ✓ Created file: {file_path}")
                return True
            else:
                print(f"    ✓ Validated (not applied - dry run mode)")
                return True
        except Exception as e:
            print(f"    ✗ Error: {str(e)}")
            return False
    
    def update_docker_compose(self) -> bool:
        """Update docker-compose.yml to include PostgreSQL service"""
        file_path = os.path.join(self.config_dir, "docker-compose.yml")
        
        new_content = """version: '3.8'

services:
  # Metis RAG API service
  metis-rag:
    build:
      context: ..
      dockerfile: config/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ../data:/app/data
      - ../uploads:/app/uploads
    environment:
      - DATABASE_TYPE=${DATABASE_TYPE:-sqlite}
      - DATABASE_URL=${DATABASE_URL:-sqlite:///./data/metis_rag.db}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-metis_rag}
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
    depends_on:
      - postgres
    restart: unless-stopped
    networks:
      - metis-network

  # PostgreSQL database service (optional)
  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-metis_rag}
    restart: unless-stopped
    networks:
      - metis-network

volumes:
  postgres-data:

networks:
  metis-network:
    driver: bridge
"""
        
        return self.update_file(file_path, new_content, "Updating docker-compose.yml")
    
    def update_dockerfile(self) -> bool:
        """Update Dockerfile to install PostgreSQL client libraries"""
        file_path = os.path.join(self.config_dir, "Dockerfile")
        
        new_content = """FROM python:3.10-slim

WORKDIR /app

# Install system dependencies including PostgreSQL client
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements file
COPY config/requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app/ /app/app/
COPY scripts/ /app/scripts/

# Create necessary directories
RUN mkdir -p /app/data /app/uploads

# Set environment variables
ENV PYTHONPATH=/app
ENV DATABASE_TYPE=sqlite
ENV DATABASE_URL=sqlite:///./data/metis_rag.db

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
"""
        
        return self.update_file(file_path, new_content, "Updating Dockerfile")
    
    def update_requirements(self) -> bool:
        """Update requirements.txt to include PostgreSQL dependencies"""
        file_path = os.path.join(self.config_dir, "requirements.txt")
        
        # Read existing requirements
        try:
            with open(file_path, 'r') as f:
                existing_content = f.read()
        except Exception as e:
            print(f"    ✗ Error reading requirements.txt: {str(e)}")
            return False
        
        # Check if psycopg2 is already included
        if "psycopg2" in existing_content:
            print("  PostgreSQL dependencies already in requirements.txt")
            return True
        
        # Add PostgreSQL dependencies
        new_content = existing_content.strip() + "\n\n# PostgreSQL dependencies\npsycopg2-binary>=2.9.5\n"
        
        return self.update_file(file_path, new_content, "Updating requirements.txt")
    
    def create_init_scripts(self) -> bool:
        """Create database initialization scripts"""
        init_dir = os.path.join(self.config_dir, "init-scripts")
        os.makedirs(init_dir, exist_ok=True)
        
        # Create init script
        init_script_path = os.path.join(init_dir, "01-init.sql")
        init_script_content = """-- PostgreSQL initialization script for Metis RAG

-- Create extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "vector";

-- Set configuration
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;

-- Create schema if it doesn't exist
CREATE SCHEMA IF NOT EXISTS public;

-- Grant privileges
GRANT ALL ON SCHEMA public TO postgres;
GRANT ALL ON SCHEMA public TO public;

-- Create user for application if not exists
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT FROM pg_catalog.pg_roles WHERE rolname = 'metis_app'
    ) THEN
        CREATE USER metis_app WITH PASSWORD 'metis_password';
    END IF;
END
$$;

-- Grant privileges to application user
GRANT ALL PRIVILEGES ON DATABASE metis_rag TO metis_app;
GRANT ALL PRIVILEGES ON SCHEMA public TO metis_app;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO metis_app;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO metis_app;
GRANT ALL PRIVILEGES ON ALL FUNCTIONS IN SCHEMA public TO metis_app;

-- Set default privileges for future objects
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO metis_app;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO metis_app;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON FUNCTIONS TO metis_app;

-- Create function for updating timestamps
CREATE OR REPLACE FUNCTION update_modified_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = now();
    RETURN NEW;
END;
$$ language 'plpgsql';
"""
        
        init_result = self.create_file(init_script_path, init_script_content, "Creating PostgreSQL initialization script")
        
        # Create functions script
        functions_script_path = os.path.join(init_dir, "02-functions.sql")
        functions_script_content = """-- PostgreSQL functions for Metis RAG

-- Create function for cosine similarity
CREATE OR REPLACE FUNCTION cosine_similarity(a real[], b real[]) 
RETURNS real AS $$
DECLARE
    dot_product real := 0;
    norm_a real := 0;
    norm_b real := 0;
BEGIN
    FOR i IN 1..array_length(a, 1) LOOP
        dot_product := dot_product + (a[i] * b[i]);
        norm_a := norm_a + (a[i] * a[i]);
        norm_b := norm_b + (b[i] * b[i]);
    END LOOP;
    
    IF norm_a = 0 OR norm_b = 0 THEN
        RETURN 0;
    ELSE
        RETURN dot_product / (sqrt(norm_a) * sqrt(norm_b));
    END IF;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Create function for semantic search
CREATE OR REPLACE FUNCTION semantic_search(
    query_embedding real[],
    match_threshold real,
    match_count integer
)
RETURNS TABLE(
    chunk_id uuid,
    document_id uuid,
    content text,
    similarity real
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        c.id::uuid as chunk_id,
        c.document_id::uuid,
        c.content,
        cosine_similarity(c.embedding, query_embedding) as similarity
    FROM
        chunks c
    WHERE
        c.embedding IS NOT NULL
    ORDER BY
        cosine_similarity(c.embedding, query_embedding) DESC
    LIMIT match_count;
END;
$$ LANGUAGE plpgsql;

-- Create function to refresh materialized views
CREATE OR REPLACE FUNCTION refresh_materialized_views()
RETURNS void AS $$
DECLARE
    view_name text;
BEGIN
    FOR view_name IN (SELECT matviewname FROM pg_matviews)
    LOOP
        EXECUTE 'REFRESH MATERIALIZED VIEW ' || view_name;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
"""
        
        functions_result = self.create_file(functions_script_path, functions_script_content, "Creating PostgreSQL functions script")
        
        return init_result and functions_result
    
    def create_env_files(self) -> bool:
        """Create environment configuration files"""
        # Create .env.sqlite
        sqlite_env_path = os.path.join(self.config_dir, ".env.sqlite")
        sqlite_env_content = """# SQLite configuration
DATABASE_TYPE=sqlite
DATABASE_URL=sqlite:///./data/metis_rag.db
"""
        
        sqlite_result = self.create_file(sqlite_env_path, sqlite_env_content, "Creating SQLite environment file")
        
        # Create .env.postgresql
        postgres_env_path = os.path.join(self.config_dir, ".env.postgresql")
        postgres_env_content = """# PostgreSQL configuration
DATABASE_TYPE=postgresql
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/metis_rag
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=metis_rag
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
"""
        
        postgres_result = self.create_file(postgres_env_path, postgres_env_content, "Creating PostgreSQL environment file")
        
        return sqlite_result and postgres_result
    
    def create_deployment_scripts(self) -> bool:
        """Create deployment scripts"""
        # Create start-sqlite.sh
        sqlite_script_path = os.path.join(self.config_dir, "start-sqlite.sh")
        sqlite_script_content = """#!/bin/bash
# Start Metis RAG with SQLite configuration

# Load SQLite environment variables
export $(grep -v '^#' .env.sqlite | xargs)

# Start the services
docker-compose up -d metis-rag

echo "Metis RAG started with SQLite configuration"
"""
        
        sqlite_result = self.create_file(sqlite_script_path, sqlite_script_content, "Creating SQLite startup script")
        
        # Create start-postgresql.sh
        postgres_script_path = os.path.join(self.config_dir, "start-postgresql.sh")
        postgres_script_content = """#!/bin/bash
# Start Metis RAG with PostgreSQL configuration

# Load PostgreSQL environment variables
export $(grep -v '^#' .env.postgresql | xargs)

# Start the services
docker-compose up -d

echo "Metis RAG started with PostgreSQL configuration"
"""
        
        postgres_result = self.create_file(postgres_script_path, postgres_script_content, "Creating PostgreSQL startup script")
        
        # Make scripts executable
        if self.apply:
            os.chmod(sqlite_script_path, 0o755)
            os.chmod(postgres_script_path, 0o755)
        
        return sqlite_result and postgres_result
    
    def create_documentation(self) -> bool:
        """Create deployment documentation"""
        docs_dir = os.path.join(project_root, "docs", "deployment")
        os.makedirs(docs_dir, exist_ok=True)
        
        doc_path = os.path.join(docs_dir, "database_deployment.md")
        doc_content = """# Metis RAG Database Deployment Guide

This guide explains how to deploy Metis RAG with different database backends.

## Database Options

Metis RAG supports two database backends:

1. **SQLite** - Simple file-based database, good for development and small deployments
2. **PostgreSQL** - Full-featured database, recommended for production and larger deployments

## Deployment with SQLite

SQLite is the simplest option and requires no additional setup:

```bash
cd config
./start-sqlite.sh
```

This will start the Metis RAG API service with SQLite as the database backend.

## Deployment with PostgreSQL

PostgreSQL provides better performance, concurrency, and advanced features:

```bash
cd config
./start-postgresql.sh
```

This will start both the Metis RAG API service and a PostgreSQL database service.

### PostgreSQL Configuration

You can customize the PostgreSQL configuration by editing the `.env.postgresql` file:

```
DATABASE_TYPE=postgresql
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/metis_rag
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=metis_rag
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
```

For production deployments, make sure to change the default passwords.

## Database Migration

To migrate from SQLite to PostgreSQL:

1. Export data from SQLite:
   ```bash
   python scripts/export_data.py --output-file data/export.json
   ```

2. Start PostgreSQL:
   ```bash
   cd config
   ./start-postgresql.sh
   ```

3. Import data to PostgreSQL:
   ```bash
   python scripts/import_data.py --input-file data/export.json --db-type postgresql
   ```

## Performance Considerations

- **SQLite**: Good for small to medium deployments with limited concurrent users
- **PostgreSQL**: Better for larger deployments with many concurrent users

PostgreSQL provides several advantages for production use:
- Better concurrency handling
- Full-text search capabilities
- JSONB operators for efficient metadata queries
- Connection pooling
- Better performance with large datasets

## Backup and Restore

### SQLite Backup
```bash
cp data/metis_rag.db data/metis_rag.db.backup
```

### PostgreSQL Backup
```bash
docker exec -t metis-rag_postgres_1 pg_dump -U postgres metis_rag > backup.sql
```

### PostgreSQL Restore
```bash
cat backup.sql | docker exec -i metis-rag_postgres_1 psql -U postgres -d metis_rag
```
"""
        
        return self.create_file(doc_path, doc_content, "Creating deployment documentation")
    
    def update_docker_config(self) -> Dict[str, Any]:
        """Update Docker configuration"""
        print("\nUpdating Docker configuration...")
        
        results = {
            "apply_mode": self.apply,
            "updates": []
        }
        
        # Update docker-compose.yml
        docker_compose_result = self.update_docker_compose()
        results["updates"].append({
            "name": "docker-compose.yml",
            "applied": docker_compose_result
        })
        
        # Update Dockerfile
        dockerfile_result = self.update_dockerfile()
        results["updates"].append({
            "name": "Dockerfile",
            "applied": dockerfile_result
        })
        
        # Update requirements.txt
        requirements_result = self.update_requirements()
        results["updates"].append({
            "name": "requirements.txt",
            "applied": requirements_result
        })
        
        # Create initialization scripts
        init_scripts_result = self.create_init_scripts()
        results["updates"].append({
            "name": "Initialization scripts",
            "applied": init_scripts_result
        })
        
        # Create environment files
        env_files_result = self.create_env_files()
        results["updates"].append({
            "name": "Environment files",
            "applied": env_files_result
        })
        
        # Create deployment scripts
        deployment_scripts_result = self.create_deployment_scripts()
        results["updates"].append({
            "name": "Deployment scripts",
            "applied": deployment_scripts_result
        })
        
        # Create documentation
        documentation_result = self.create_documentation()
        results["updates"].append({
            "name": "Deployment documentation",
            "applied": documentation_result
        })
        
        # Print summary
        print("\nUpdate Summary:")
        applied_count = sum(1 for u in results["updates"] if u["applied"])
        total_count = len(results["updates"])
        print(f"  Applied: {applied_count}/{total_count} updates")
        
        if not self.apply:
            print("\nTo apply these updates, run again with --apply flag")
        
        return results

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Docker Configuration Update for Metis RAG")
    parser.add_argument("--apply", action="store_true", help="Apply updates (default is dry run)")
    args = parser.parse_args()
    
    # Create updater instance
    updater = DockerConfigUpdater(args.apply)
    
    try:
        # Update Docker configuration
        results = updater.update_docker_config()
        
        print("\nDocker configuration update completed successfully!")
        return 0
    except Exception as e:
        print(f"Error updating Docker configuration: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())

================
File: scripts/update_docker_for_postgres.py
================
#!/usr/bin/env python3
"""
Update Docker Configuration for PostgreSQL Support

This script updates the Docker configuration files to support PostgreSQL:
1. Updates docker-compose.yml to include a PostgreSQL service
2. Updates Dockerfile to install PostgreSQL client libraries
3. Creates database initialization scripts
4. Updates environment configuration

Usage:
    python update_docker_for_postgres.py [--apply]
"""
import os
import sys
import yaml
import argparse
import shutil
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Docker Compose configuration for PostgreSQL
POSTGRES_DOCKER_COMPOSE = """
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: config/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./uploads:/app/uploads
      - ./chroma_db:/app/chroma_db
    environment:
      - DATABASE_TYPE=${DATABASE_TYPE:-postgresql}
      - DATABASE_USER=${DATABASE_USER:-postgres}
      - DATABASE_PASSWORD=${DATABASE_PASSWORD:-postgres}
      - DATABASE_HOST=${DATABASE_HOST:-postgres}
      - DATABASE_PORT=${DATABASE_PORT:-5432}
      - DATABASE_NAME=${DATABASE_NAME:-metis_rag}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-gemma3:12b}
      - DEFAULT_EMBEDDING_MODEL=${DEFAULT_EMBEDDING_MODEL:-nomic-embed-text}
    depends_on:
      - postgres
      - ollama
    restart: unless-stopped
    networks:
      - metis-network

  postgres:
    image: postgres:16-alpine
    environment:
      - POSTGRES_USER=${DATABASE_USER:-postgres}
      - POSTGRES_PASSWORD=${DATABASE_PASSWORD:-postgres}
      - POSTGRES_DB=${DATABASE_NAME:-metis_rag}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/postgres/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - metis-network

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      - metis-network

volumes:
  postgres-data:
  ollama-data:

networks:
  metis-network:
    driver: bridge
"""

# Docker Compose configuration for SQLite
SQLITE_DOCKER_COMPOSE = """
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: config/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./uploads:/app/uploads
      - ./chroma_db:/app/chroma_db
    environment:
      - DATABASE_TYPE=${DATABASE_TYPE:-sqlite}
      - DATABASE_URL=${DATABASE_URL:-sqlite:///./test.db}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-gemma3:12b}
      - DEFAULT_EMBEDDING_MODEL=${DEFAULT_EMBEDDING_MODEL:-nomic-embed-text}
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - metis-network

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      - metis-network

volumes:
  ollama-data:

networks:
  metis-network:
    driver: bridge
"""

# Dockerfile updates for PostgreSQL support
DOCKERFILE_POSTGRES_ADDITIONS = """
# Install PostgreSQL client libraries
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Install psycopg2 for PostgreSQL support
RUN pip install --no-cache-dir psycopg2-binary
"""

# PostgreSQL initialization script
POSTGRES_INIT_SCRIPT = """
#!/bin/bash
set -e

# Create extensions
psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" --dbname "$POSTGRES_DB" <<-EOSQL
    CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
    CREATE EXTENSION IF NOT EXISTS "pg_trgm";
EOSQL

echo "PostgreSQL initialized with extensions"
"""

# Environment configuration for PostgreSQL
POSTGRES_ENV_CONFIG = """
# Database settings
DATABASE_TYPE=postgresql
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_HOST=postgres
DATABASE_PORT=5432
DATABASE_NAME=metis_rag
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# Ollama settings
OLLAMA_BASE_URL=http://ollama:11434
DEFAULT_MODEL=gemma3:12b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# Document settings
UPLOAD_DIR=/app/uploads
CHROMA_DB_DIR=/app/chroma_db
CHUNK_SIZE=1500
CHUNK_OVERLAP=150
"""

# Environment configuration for SQLite
SQLITE_ENV_CONFIG = """
# Database settings
DATABASE_TYPE=sqlite
DATABASE_URL=sqlite:///./test.db
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# Ollama settings
OLLAMA_BASE_URL=http://ollama:11434
DEFAULT_MODEL=gemma3:12b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# Document settings
UPLOAD_DIR=/app/uploads
CHROMA_DB_DIR=/app/chroma_db
CHUNK_SIZE=1500
CHUNK_OVERLAP=150
"""

def update_docker_compose():
    """Update docker-compose.yml to include PostgreSQL service"""
    docker_compose_file = os.path.join(project_root, "config", "docker-compose.yml")
    
    # Create config directory if it doesn't exist
    os.makedirs(os.path.dirname(docker_compose_file), exist_ok=True)
    
    # Create PostgreSQL docker-compose.yml
    with open(docker_compose_file, 'w') as f:
        f.write(POSTGRES_DOCKER_COMPOSE.strip())
    
    print(f"Updated {docker_compose_file} with PostgreSQL service")
    
    # Create SQLite docker-compose.yml
    sqlite_docker_compose_file = os.path.join(project_root, "config", "docker-compose.sqlite.yml")
    with open(sqlite_docker_compose_file, 'w') as f:
        f.write(SQLITE_DOCKER_COMPOSE.strip())
    
    print(f"Created {sqlite_docker_compose_file} for SQLite configuration")

def update_dockerfile():
    """Update Dockerfile to install PostgreSQL client libraries"""
    dockerfile = os.path.join(project_root, "config", "Dockerfile")
    
    # Create config directory if it doesn't exist
    os.makedirs(os.path.dirname(dockerfile), exist_ok=True)
    
    # Check if Dockerfile exists
    if os.path.exists(dockerfile):
        with open(dockerfile, 'r') as f:
            content = f.read()
        
        # Check if PostgreSQL additions already exist
        if "libpq-dev" in content:
            print(f"Dockerfile already includes PostgreSQL client libraries")
            return
        
        # Find a good place to add PostgreSQL additions
        if "RUN pip install" in content:
            # Add before pip install
            new_content = content.replace(
                "RUN pip install",
                DOCKERFILE_POSTGRES_ADDITIONS + "\nRUN pip install"
            )
        else:
            # Add at the end
            new_content = content + "\n" + DOCKERFILE_POSTGRES_ADDITIONS
        
        with open(dockerfile, 'w') as f:
            f.write(new_content)
        
        print(f"Updated {dockerfile} with PostgreSQL client libraries")
    else:
        # Create a basic Dockerfile
        basic_dockerfile = """FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .

# Install PostgreSQL client libraries
RUN apt-get update && apt-get install -y --no-install-recommends \\
    libpq-dev \\
    && rm -rf /var/lib/apt/lists/*

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt
RUN pip install --no-cache-dir psycopg2-binary

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
"""
        
        with open(dockerfile, 'w') as f:
            f.write(basic_dockerfile)
        
        print(f"Created {dockerfile} with PostgreSQL support")

def create_postgres_init_scripts():
    """Create PostgreSQL initialization scripts"""
    init_dir = os.path.join(project_root, "config", "postgres", "init")
    os.makedirs(init_dir, exist_ok=True)
    
    # Create initialization script
    init_script = os.path.join(init_dir, "01-init-extensions.sh")
    with open(init_script, 'w') as f:
        f.write(POSTGRES_INIT_SCRIPT)
    
    # Make script executable
    os.chmod(init_script, 0o755)
    
    print(f"Created PostgreSQL initialization script: {init_script}")

def update_environment_config():
    """Update environment configuration files"""
    # Create PostgreSQL .env file
    postgres_env_file = os.path.join(project_root, "config", ".env.postgres")
    with open(postgres_env_file, 'w') as f:
        f.write(POSTGRES_ENV_CONFIG)
    
    print(f"Created PostgreSQL environment configuration: {postgres_env_file}")
    
    # Create SQLite .env file
    sqlite_env_file = os.path.join(project_root, "config", ".env.sqlite")
    with open(sqlite_env_file, 'w') as f:
        f.write(SQLITE_ENV_CONFIG)
    
    print(f"Created SQLite environment configuration: {sqlite_env_file}")
    
    # Create example .env file
    example_env_file = os.path.join(project_root, "config", ".env.example")
    with open(example_env_file, 'w') as f:
        f.write(POSTGRES_ENV_CONFIG + "\n# Uncomment for SQLite\n# DATABASE_TYPE=sqlite\n# DATABASE_URL=sqlite:///./test.db\n")
    
    print(f"Created example environment configuration: {example_env_file}")

def create_deployment_guide():
    """Create deployment guide for PostgreSQL"""
    guide_file = os.path.join(project_root, "docs", "deployment", "postgres_deployment.md")
    os.makedirs(os.path.dirname(guide_file), exist_ok=True)
    
    guide_content = """# PostgreSQL Deployment Guide for Metis RAG

This guide provides instructions for deploying the Metis RAG system with PostgreSQL.

## Prerequisites

- Docker and Docker Compose
- Git
- Basic knowledge of PostgreSQL

## Deployment Steps

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/metis-rag.git
cd metis-rag
```

### 2. Configure Environment Variables

Create a `.env` file in the project root directory:

```bash
cp config/.env.postgres .env
```

Edit the `.env` file to customize your deployment:

```
# Database settings
DATABASE_TYPE=postgresql
DATABASE_USER=postgres
DATABASE_PASSWORD=your_secure_password
DATABASE_HOST=postgres
DATABASE_PORT=5432
DATABASE_NAME=metis_rag
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# Ollama settings
OLLAMA_BASE_URL=http://ollama:11434
DEFAULT_MODEL=gemma3:12b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# Document settings
UPLOAD_DIR=/app/uploads
CHROMA_DB_DIR=/app/chroma_db
CHUNK_SIZE=1500
CHUNK_OVERLAP=150
```

### 3. Start the Services

```bash
docker-compose up -d
```

This will start the following services:
- PostgreSQL database
- Ollama for LLM inference
- Metis RAG application

### 4. Run Database Migrations

```bash
docker-compose exec app python -m scripts.run_migrations
```

### 5. Access the Application

The application will be available at http://localhost:8000

## PostgreSQL Configuration

The PostgreSQL service is configured with the following settings:

- Data is persisted in a Docker volume (`postgres-data`)
- The database is initialized with the UUID and pg_trgm extensions
- The database is accessible on port 5432

### Connecting to PostgreSQL

To connect to the PostgreSQL database:

```bash
docker-compose exec postgres psql -U postgres -d metis_rag
```

### Backup and Restore

To backup the database:

```bash
docker-compose exec postgres pg_dump -U postgres -d metis_rag > backup.sql
```

To restore the database:

```bash
cat backup.sql | docker-compose exec -T postgres psql -U postgres -d metis_rag
```

## Switching Between SQLite and PostgreSQL

The Metis RAG system supports both SQLite and PostgreSQL. To switch between them:

### Switch to SQLite

1. Update the `.env` file:
```
DATABASE_TYPE=sqlite
DATABASE_URL=sqlite:///./test.db
```

2. Use the SQLite Docker Compose configuration:
```bash
docker-compose -f config/docker-compose.sqlite.yml up -d
```

### Switch to PostgreSQL

1. Update the `.env` file:
```
DATABASE_TYPE=postgresql
DATABASE_USER=postgres
DATABASE_PASSWORD=your_secure_password
DATABASE_HOST=postgres
DATABASE_PORT=5432
DATABASE_NAME=metis_rag
```

2. Use the PostgreSQL Docker Compose configuration:
```bash
docker-compose up -d
```

## Production Deployment Considerations

For production deployments, consider the following:

### Security

- Change the default PostgreSQL password
- Use a separate PostgreSQL instance with proper backup and monitoring
- Configure SSL for PostgreSQL connections

### Performance

- Increase the PostgreSQL memory settings based on available resources
- Configure connection pooling for high-traffic deployments
- Use a managed PostgreSQL service (AWS RDS, Azure Database, etc.)

### High Availability

- Set up PostgreSQL replication
- Configure automated backups
- Implement a health check and monitoring system

## Troubleshooting

### Database Connection Issues

If the application cannot connect to the database:

1. Check if the PostgreSQL container is running:
```bash
docker-compose ps
```

2. Check the PostgreSQL logs:
```bash
docker-compose logs postgres
```

3. Verify the database connection settings in the `.env` file

### Migration Issues

If database migrations fail:

1. Check the application logs:
```bash
docker-compose logs app
```

2. Run migrations manually:
```bash
docker-compose exec app alembic upgrade head
```

3. Reset the database if needed:
```bash
docker-compose down -v  # This will delete all data!
docker-compose up -d
```
"""
    
    with open(guide_file, 'w') as f:
        f.write(guide_content)
    
    print(f"Created PostgreSQL deployment guide: {guide_file}")

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Update Docker Configuration for PostgreSQL Support")
    parser.add_argument("--apply", action="store_true", help="Apply the changes")
    args = parser.parse_args()
    
    print("Update Docker Configuration for PostgreSQL Support")
    print("================================================")
    
    if args.apply:
        print("\nApplying Docker configuration updates...")
        
        # Update docker-compose.yml
        update_docker_compose()
        
        # Update Dockerfile
        update_dockerfile()
        
        # Create PostgreSQL initialization scripts
        create_postgres_init_scripts()
        
        # Update environment configuration
        update_environment_config()
        
        # Create deployment guide
        create_deployment_guide()
        
        print("\nDocker configuration updates applied successfully!")
        print("\nTo deploy with PostgreSQL:")
        print("1. Copy the environment configuration: cp config/.env.postgres .env")
        print("2. Start the services: docker-compose up -d")
        print("3. Run migrations: docker-compose exec app python -m scripts.run_migrations")
    else:
        print("\nThis script will apply the following changes:")
        print("  1. Update docker-compose.yml to include a PostgreSQL service")
        print("  2. Update Dockerfile to install PostgreSQL client libraries")
        print("  3. Create database initialization scripts")
        print("  4. Update environment configuration")
        print("  5. Create a deployment guide")
        print("\nRun with --apply to apply these changes.")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

================
File: test_results/api_test_query_results.json
================
[
  {
    "query": "What are the specifications of the SmartHome Hub?",
    "answer": "According to the provided documents [1], the specifications of the SmartHome Hub are: \n\n* Protocol: RF\n* Battery Life: 1 year\n* Price: $79. 99\n* Indoor Range: 75 ft\n* Outdoor Range: 50 ft\n* Water Resistant: No\n* Voice Control: Yes\n* Hub Required: Yes\n\nThese specifications can be found in the context under the device ID \"SH-RC100\" [10].",
    "success": true
  },
  {
    "query": "How do I troubleshoot when devices won't connect?",
    "answer": "I've searched my knowledge base for information about those specific workarounds within the provided context. While I don't have comprehensive information on this topic, I did find some relevant documents that mention it.\n\nAccording to the sources I have access to:\n\n[1] if __name__ == '__main__':\n    app.run(port=5000)\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Authentication failures**\n   - Check that your client ID and secret are correct\n   - Verify that your... (Source: smart_home_developer_reference.md)\n\n[2] 2. Google Assistant\n   - Open the Google Home app\n   - Tap the \"+\" icon\n   - Select \"Set up device\"\n   - Choose \"Works with Google\"\n   - Find and select SmartHome\n   - Link your SmartHome account\n\n3. ... (Source: smart_home_user_guide.txt)\n\n[3] if __name__ == '__main__':\n    app.run(port=5000)\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Authentication failures**\n   - Check that your client ID and secret are correct\n   - Verify that your... (Source: smart_home_developer_reference.md)\n\nPlease note this information is limited to what's in my document database. For more comprehensive information, consider consulting specialized resources.",
    "success": true
  },
  {
    "query": "What is the battery life of the motion sensor?",
    "answer": "Based on the provided documents, I found information about the motion sensor's battery life. \n\nAccording to [1], the Motion Sensor (Device ID: SH-MS100) has a Battery Life of 2 years. \n\nSources: \n[1] Source: smart_home_device_comparison. csv, Tags: , Folder: / - \"Device Information\"",
    "success": true
  },
  {
    "query": "Compare the Motion Sensor and Door Sensor specifications.",
    "answer": "Based on the provided documents, I can compare the Motion Sensor and Door Sensor specifications. \n\n**Indoor Range**: The Motion Sensor has an indoor range of 40 ft [2], while the Door Sensor has an indoor range of 50 ft [1].\n\n**Outdoor Range**: The Motion Sensor has an outdoor range of 25 ft [2], whereas the Door Sensor has an outdoor range of 30 ft [1].\n\n**Price**: The Motion Sensor costs $24. 99 [2] compared to the Door Sensor, which is priced at $19. 99 [1].\n\n**Battery Life**: Both devices have a battery life of 2 years [1] and [2].\n\nThat's all the information I can provide based on the context. If you'd like more comprehensive information or comparisons with other devices, please note that this information is limited to what's in my document database. \n\nSources: \n\n[1] smart_home_device_comparison. csv - \"Device Comparison\"\n[2] smart_home_device_comparison. csv - \"Device Comparison\"",
    "success": true
  }
]

================
File: test_results/api_test_upload_results.json
================
[]

================
File: test_results/test_e2e_demo_comprehensive_report.json
================
{
  "test_name": "Metis RAG End-to-End Demo Test",
  "timestamp": "2025-03-25T12:09:33.978930",
  "summary": {
    "document_count": 4,
    "document_success_rate": 100.0,
    "query_count": 3,
    "query_success_rate": 66.66666666666666,
    "average_fact_percentage": 73.88888888888889
  },
  "results": {
    "document_processing": [
      {
        "document_id": "simulated_technical_specs",
        "document_type": "pdf",
        "filename": "smart_home_technical_specs.pdf",
        "success": true,
        "file_size_bytes": 9036,
        "processing_time_seconds": 0.09036,
        "chunk_count": 4
      },
      {
        "document_id": "simulated_user_guide",
        "document_type": "txt",
        "filename": "smart_home_user_guide.txt",
        "success": true,
        "file_size_bytes": 4177,
        "processing_time_seconds": 0.04177,
        "chunk_count": 2
      },
      {
        "document_id": "simulated_device_comparison",
        "document_type": "csv",
        "filename": "smart_home_device_comparison.csv",
        "success": true,
        "file_size_bytes": 1648,
        "processing_time_seconds": 0.01648,
        "chunk_count": 1
      },
      {
        "document_id": "simulated_developer_reference",
        "document_type": "md",
        "filename": "smart_home_developer_reference.md",
        "success": true,
        "file_size_bytes": 14247,
        "processing_time_seconds": 0.14247,
        "chunk_count": 7
      }
    ],
    "query_responses": [
      {
        "query": "What are the specifications of the SmartHome Hub?",
        "answer": "Here is information about your query: 'What are the specifications of the SmartHome Hub?'\n\n- ARM Cortex-A53\n- quad-core\n- 1.4GHz\n- 2GB RAM\n- 16GB eMMC\n- Wi-Fi\n- Bluetooth 5.0\n- Zigbee 3.0\n\nThis information comes from [1] technical_specs",
        "expected_facts": [
          "ARM Cortex-A53",
          "quad-core",
          "1.4GHz",
          "2GB RAM",
          "16GB eMMC",
          "Wi-Fi",
          "Bluetooth 5.0",
          "Zigbee 3.0",
          "Z-Wave",
          "5V DC"
        ],
        "facts_found": 8,
        "fact_percentage": 80.0,
        "processing_time_seconds": 1.5,
        "success": true
      },
      {
        "query": "How do I troubleshoot when devices won't connect?",
        "answer": "Here is information about your query: 'How do I troubleshoot when devices won't connect?'\n\n- within range\n- 30-50 feet\n- pairing mode\n\nThis information comes from [1] user_guide",
        "expected_facts": [
          "within range",
          "30-50 feet",
          "pairing mode",
          "compatible with SmartHome"
        ],
        "facts_found": 3,
        "fact_percentage": 75.0,
        "processing_time_seconds": 1.5,
        "success": true
      },
      {
        "query": "Compare the Motion Sensor and Door Sensor specifications and setup process.",
        "answer": "Here is information about your query: 'Compare the Motion Sensor and Door Sensor specifications and setup process.'\n\n- SH-MS100\n- SH-DS100\n- Zigbee\n- 2 years\n\nThis information comes from [1] device_comparison and [2] user_guide",
        "expected_facts": [
          "SH-MS100",
          "SH-DS100",
          "Zigbee",
          "2 years",
          "pairing mode",
          "Add Device"
        ],
        "facts_found": 4,
        "fact_percentage": 66.66666666666666,
        "processing_time_seconds": 2.0,
        "success": false
      }
    ]
  }
}

================
File: test_results/test_e2e_demo_query_results.json
================
[
  {
    "query": "What are the specifications of the SmartHome Hub?",
    "answer": "Here is information about your query: 'What are the specifications of the SmartHome Hub?'\n\n- ARM Cortex-A53\n- quad-core\n- 1.4GHz\n- 2GB RAM\n- 16GB eMMC\n- Wi-Fi\n- Bluetooth 5.0\n- Zigbee 3.0\n\nThis information comes from [1] technical_specs",
    "expected_facts": [
      "ARM Cortex-A53",
      "quad-core",
      "1.4GHz",
      "2GB RAM",
      "16GB eMMC",
      "Wi-Fi",
      "Bluetooth 5.0",
      "Zigbee 3.0",
      "Z-Wave",
      "5V DC"
    ],
    "facts_found": 8,
    "fact_percentage": 80.0,
    "processing_time_seconds": 1.5,
    "success": true
  },
  {
    "query": "How do I troubleshoot when devices won't connect?",
    "answer": "Here is information about your query: 'How do I troubleshoot when devices won't connect?'\n\n- within range\n- 30-50 feet\n- pairing mode\n\nThis information comes from [1] user_guide",
    "expected_facts": [
      "within range",
      "30-50 feet",
      "pairing mode",
      "compatible with SmartHome"
    ],
    "facts_found": 3,
    "fact_percentage": 75.0,
    "processing_time_seconds": 1.5,
    "success": true
  },
  {
    "query": "Compare the Motion Sensor and Door Sensor specifications and setup process.",
    "answer": "Here is information about your query: 'Compare the Motion Sensor and Door Sensor specifications and setup process.'\n\n- SH-MS100\n- SH-DS100\n- Zigbee\n- 2 years\n\nThis information comes from [1] device_comparison and [2] user_guide",
    "expected_facts": [
      "SH-MS100",
      "SH-DS100",
      "Zigbee",
      "2 years",
      "pairing mode",
      "Add Device"
    ],
    "facts_found": 4,
    "fact_percentage": 66.66666666666666,
    "processing_time_seconds": 2.0,
    "success": false
  }
]

================
File: test_results/test_e2e_demo_upload_results.json
================
[
  {
    "document_id": "simulated_technical_specs",
    "document_type": "pdf",
    "filename": "smart_home_technical_specs.pdf",
    "success": true,
    "file_size_bytes": 9036,
    "processing_time_seconds": 0.09036,
    "chunk_count": 4
  },
  {
    "document_id": "simulated_user_guide",
    "document_type": "txt",
    "filename": "smart_home_user_guide.txt",
    "success": true,
    "file_size_bytes": 4177,
    "processing_time_seconds": 0.04177,
    "chunk_count": 2
  },
  {
    "document_id": "simulated_device_comparison",
    "document_type": "csv",
    "filename": "smart_home_device_comparison.csv",
    "success": true,
    "file_size_bytes": 1648,
    "processing_time_seconds": 0.01648,
    "chunk_count": 1
  },
  {
    "document_id": "simulated_developer_reference",
    "document_type": "md",
    "filename": "smart_home_developer_reference.md",
    "success": true,
    "file_size_bytes": 14247,
    "processing_time_seconds": 0.14247,
    "chunk_count": 7
  }
]

================
File: tests/data/test_data.csv
================
id,name,department,role,years_experience,skills
1,John Smith,Engineering,Software Engineer,5,"Python, JavaScript, Docker"
2,Emily Johnson,Data Science,Data Scientist,3,"Python, R, Machine Learning, SQL"
3,Michael Brown,Engineering,DevOps Engineer,7,"Kubernetes, AWS, Terraform, Linux"
4,Sarah Davis,Product,Product Manager,4,"Agile, User Research, Roadmapping"
5,David Wilson,Engineering,Frontend Developer,2,"React, TypeScript, CSS"
6,Jennifer Miller,Data Science,ML Engineer,6,"TensorFlow, PyTorch, Python, MLOps"
7,Robert Taylor,Engineering,Backend Developer,8,"Java, Spring, Microservices, Kafka"
8,Lisa Anderson,Product,UX Designer,5,"Figma, User Testing, Wireframing"
9,James Thomas,Engineering,Security Engineer,4,"Penetration Testing, OWASP, Cryptography"
10,Patricia Martinez,Data Science,Data Analyst,2,"SQL, Tableau, Excel, Python"
11,Christopher Harris,Engineering,QA Engineer,3,"Selenium, Jest, Cypress, Test Planning"
12,Elizabeth Clark,Product,Technical Writer,6,"Documentation, API Reference, Tutorials"
13,Daniel Lewis,Engineering,Mobile Developer,4,"Swift, Kotlin, React Native"
14,Jessica Lee,Data Science,Data Engineer,5,"Spark, Hadoop, Airflow, SQL"
15,Matthew Walker,Engineering,Site Reliability Engineer,7,"Prometheus, Grafana, AWS, GCP"

================
File: tests/data/test_document.txt
================
This is a test document for authentication testing.

================
File: tests/data/test_models.html
================
<!DOCTYPE html>
<html>
<head>
    <title>Test Model Selection</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
        }
        .form-group {
            margin-bottom: 20px;
        }
        label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        select {
            width: 100%;
            padding: 8px;
            border: 1px solid #ccc;
            border-radius: 4px;
        }
        button {
            padding: 8px 16px;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        pre {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Test Model Selection</h1>
        
        <div class="form-group">
            <label for="model">Select Model:</label>
            <select id="model" name="model">
                <option value="llama3" selected>Llama 3</option>
                <!-- Other models will be loaded dynamically -->
            </select>
        </div>
        
        <button id="load-models">Load Models</button>
        <button id="check-models">Check API Response</button>
        
        <h2>API Response:</h2>
        <pre id="api-response">Click "Check API Response" to see the raw API response</pre>
        
        <h2>Selected Model:</h2>
        <pre id="selected-model">No model selected</pre>
        
        <script>
            document.addEventListener('DOMContentLoaded', function() {
                const modelSelect = document.getElementById('model');
                const loadModelsBtn = document.getElementById('load-models');
                const checkModelsBtn = document.getElementById('check-models');
                const apiResponseEl = document.getElementById('api-response');
                const selectedModelEl = document.getElementById('selected-model');
                
                // Load models when the page loads
                loadModels();
                
                // Load models when the button is clicked
                loadModelsBtn.addEventListener('click', loadModels);
                
                // Check API response
                checkModelsBtn.addEventListener('click', checkApiResponse);
                
                // Update selected model when changed
                modelSelect.addEventListener('change', function() {
                    selectedModelEl.textContent = `Selected model: ${modelSelect.value}`;
                });
                
                function loadModels() {
                    fetch('/api/system/models')
                        .then(response => response.json())
                        .then(models => {
                            console.log('Models:', models);
                            
                            // Clear the dropdown
                            modelSelect.innerHTML = '';
                            
                            // Add each model to the dropdown
                            models.forEach(model => {
                                const option = document.createElement('option');
                                option.value = model.name;
                                option.textContent = model.name;
                                modelSelect.appendChild(option);
                                console.log('Added model:', model.name);
                            });
                            
                            // Update selected model
                            selectedModelEl.textContent = `Selected model: ${modelSelect.value}`;
                        })
                        .catch(error => {
                            console.error('Error loading models:', error);
                            apiResponseEl.textContent = `Error: ${error.message}`;
                        });
                }
                
                function checkApiResponse() {
                    fetch('/api/system/models')
                        .then(response => response.json())
                        .then(data => {
                            apiResponseEl.textContent = JSON.stringify(data, null, 2);
                        })
                        .catch(error => {
                            apiResponseEl.textContent = `Error: ${error.message}`;
                        });
                }
            });
        </script>
    </div>
</body>
</html>

================
File: tests/data/test_upload_document.txt
================
This is a test document for upload testing.

================
File: tests/e2e/test_auth_flows.py
================
#!/usr/bin/env python3
"""
End-to-end tests for authentication flows in Metis RAG
"""

import pytest
import uuid
import requests
import json
import time
from typing import Dict, Tuple, List, Optional
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_auth_flows")


class TestAuthFlows:
    """End-to-end tests for authentication flows"""
    
    # Base URL for API
    base_url = "http://localhost:8000"
    
    def setup_method(self):
        """Setup method for each test"""
        # Create unique test users for each test
        self.test_users = []
    
    def teardown_method(self):
        """Teardown method for each test"""
        # Clean up test users
        for user in self.test_users:
            if "token" in user:
                try:
                    # Delete user if possible
                    headers = {"Authorization": f"Bearer {user['token']}"}
                    requests.delete(f"{self.base_url}/api/auth/users/{user['id']}", headers=headers)
                except Exception as e:
                    logger.warning(f"Failed to delete test user: {e}")
    
    def create_test_user(self) -> Dict:
        """Create a test user"""
        unique_id = uuid.uuid4().hex[:8]
        user_data = {
            "username": f"testuser_{unique_id}",
            "email": f"testuser_{unique_id}@example.com",
            "password": "testpassword123",
            "full_name": f"Test User {unique_id}",
            "is_active": True,
            "is_admin": False
        }
        
        # Register user
        response = requests.post(
            f"{self.base_url}/api/auth/register",
            json=user_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to create test user: {response.text}")
            raise Exception(f"Failed to create test user: {response.text}")
        
        # Add user to list for cleanup
        user_info = response.json()
        user_info["password"] = user_data["password"]
        self.test_users.append(user_info)
        
        return user_info
    
    def login_user(self, username: str, password: str) -> Tuple[Dict, requests.Session]:
        """Login a user and return token data and session"""
        session = requests.Session()
        
        # Login
        response = session.post(
            f"{self.base_url}/api/auth/token",
            data={
                "username": username,
                "password": password,
                "grant_type": "password"
            }
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to login user: {response.text}")
            raise Exception(f"Failed to login user: {response.text}")
        
        token_data = response.json()
        
        # Set token in session headers
        session.headers.update({"Authorization": f"Bearer {token_data['access_token']}"})
        
        # Update user info with token
        for user in self.test_users:
            if user["username"] == username:
                user["token"] = token_data["access_token"]
                user["refresh_token"] = token_data["refresh_token"]
                break
        
        return token_data, session
    
    def refresh_token(self, refresh_token: str, session: Optional[requests.Session] = None) -> Dict:
        """Refresh an access token"""
        if session is None:
            session = requests.Session()
        
        # Refresh token
        response = session.post(
            f"{self.base_url}/api/auth/refresh",
            json={"refresh_token": refresh_token}
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to refresh token: {response.text}")
            raise Exception(f"Failed to refresh token: {response.text}")
        
        token_data = response.json()
        
        # Update session headers with new token
        session.headers.update({"Authorization": f"Bearer {token_data['access_token']}"})
        
        return token_data
    
    def create_document(self, session: requests.Session, title: str, content: str, is_public: bool = False) -> Dict:
        """Create a document"""
        doc_data = {
            "title": title,
            "content": content,
            "is_public": is_public
        }
        
        response = session.post(
            f"{self.base_url}/api/documents",
            json=doc_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to create document: {response.text}")
            raise Exception(f"Failed to create document: {response.text}")
        
        return response.json()
    
    def share_document(self, session: requests.Session, document_id: str, user_id: str, permission_level: str) -> Dict:
        """Share a document with another user"""
        share_data = {
            "document_id": document_id,
            "user_id": user_id,
            "permission_level": permission_level
        }
        
        response = session.post(
            f"{self.base_url}/api/documents/{document_id}/share",
            json=share_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to share document: {response.text}")
            raise Exception(f"Failed to share document: {response.text}")
        
        return response.json()
    
    def test_complete_user_journey(self):
        """Test a complete user journey: Register -> Login -> Access Protected Resource -> Refresh Token -> Access Again -> Logout"""
        logger.info("Starting complete user journey test")
        
        # Step 1: Register a new user
        logger.info("Step 1: Registering a new user")
        user_info = self.create_test_user()
        assert "id" in user_info
        assert "username" in user_info
        logger.info(f"✓ User registered: {user_info['username']}")
        
        # Step 2: Login with the new user
        logger.info("Step 2: Logging in with the new user")
        token_data, session = self.login_user(user_info["username"], user_info["password"])
        assert "access_token" in token_data
        assert "refresh_token" in token_data
        logger.info("✓ Login successful")
        
        # Step 3: Access a protected resource (get user profile)
        logger.info("Step 3: Accessing a protected resource")
        profile_response = session.get(f"{self.base_url}/api/auth/me")
        assert profile_response.status_code == 200
        profile_data = profile_response.json()
        assert profile_data["username"] == user_info["username"]
        logger.info("✓ Protected resource accessed successfully")
        
        # Step 4: Create a document (another protected resource)
        logger.info("Step 4: Creating a document")
        doc_title = f"Test Document {uuid.uuid4().hex[:8]}"
        doc_content = "This is a test document created during the authentication flow test."
        doc_data = self.create_document(session, doc_title, doc_content)
        assert "id" in doc_data
        assert doc_data["title"] == doc_title
        logger.info(f"✓ Document created: {doc_data['title']}")
        
        # Step 5: Wait for token to be closer to expiry (in a real test, we might use a shorter expiry time)
        logger.info("Step 5: Waiting briefly before refreshing token")
        time.sleep(2)  # Just a short wait for demonstration
        
        # Step 6: Refresh the token
        logger.info("Step 6: Refreshing the token")
        new_token_data = self.refresh_token(token_data["refresh_token"], session)
        assert "access_token" in new_token_data
        assert new_token_data["access_token"] != token_data["access_token"]
        logger.info("✓ Token refreshed successfully")
        
        # Step 7: Access a protected resource with the new token
        logger.info("Step 7: Accessing a protected resource with the new token")
        docs_response = session.get(f"{self.base_url}/api/documents")
        assert docs_response.status_code == 200
        docs_data = docs_response.json()
        assert isinstance(docs_data, list)
        assert any(doc["id"] == doc_data["id"] for doc in docs_data)
        logger.info("✓ Protected resource accessed with refreshed token")
        
        # Step 8: Simulate logout (just clear the session)
        logger.info("Step 8: Logging out")
        session.headers.pop("Authorization", None)
        
        # Step 9: Verify that protected resources are no longer accessible
        logger.info("Step 9: Verifying protected resources are no longer accessible")
        unauth_response = session.get(f"{self.base_url}/api/auth/me")
        assert unauth_response.status_code == 401
        logger.info("✓ Protected resource correctly denied after logout")
        
        logger.info("Complete user journey test passed successfully")
    
    def test_permission_scenario(self):
        """Test a complex permission scenario with multiple users"""
        logger.info("Starting permission scenario test")
        
        # Step 1: Create two users (owner and collaborator)
        logger.info("Step 1: Creating two test users")
        owner_info = self.create_test_user()
        collaborator_info = self.create_test_user()
        logger.info(f"✓ Created owner user: {owner_info['username']}")
        logger.info(f"✓ Created collaborator user: {collaborator_info['username']}")
        
        # Step 2: Login as owner
        logger.info("Step 2: Logging in as owner")
        owner_token_data, owner_session = self.login_user(owner_info["username"], owner_info["password"])
        logger.info("✓ Owner login successful")
        
        # Step 3: Login as collaborator
        logger.info("Step 3: Logging in as collaborator")
        collaborator_token_data, collaborator_session = self.login_user(collaborator_info["username"], collaborator_info["password"])
        logger.info("✓ Collaborator login successful")
        
        # Step 4: Owner creates a private document
        logger.info("Step 4: Owner creates a private document")
        private_doc_title = f"Private Document {uuid.uuid4().hex[:8]}"
        private_doc_content = "This is a private document that will be shared with the collaborator."
        private_doc = self.create_document(owner_session, private_doc_title, private_doc_content, is_public=False)
        logger.info(f"✓ Owner created private document: {private_doc['title']}")
        
        # Step 5: Owner creates a public document
        logger.info("Step 5: Owner creates a public document")
        public_doc_title = f"Public Document {uuid.uuid4().hex[:8]}"
        public_doc_content = "This is a public document that anyone can see."
        public_doc = self.create_document(owner_session, public_doc_title, public_doc_content, is_public=True)
        logger.info(f"✓ Owner created public document: {public_doc['title']}")
        
        # Step 6: Collaborator tries to access owner's private document (should fail)
        logger.info("Step 6: Collaborator tries to access owner's private document")
        private_doc_response = collaborator_session.get(f"{self.base_url}/api/documents/{private_doc['id']}")
        assert private_doc_response.status_code == 404
        logger.info("✓ Collaborator correctly denied access to private document")
        
        # Step 7: Collaborator accesses owner's public document (should succeed)
        logger.info("Step 7: Collaborator accesses owner's public document")
        public_doc_response = collaborator_session.get(f"{self.base_url}/api/documents/{public_doc['id']}")
        assert public_doc_response.status_code == 200
        public_doc_data = public_doc_response.json()
        assert public_doc_data["id"] == public_doc["id"]
        logger.info("✓ Collaborator successfully accessed public document")
        
        # Step 8: Owner shares private document with collaborator (read permission)
        logger.info("Step 8: Owner shares private document with collaborator (read permission)")
        share_result = self.share_document(owner_session, private_doc["id"], collaborator_info["id"], "read")
        logger.info("✓ Owner shared private document with collaborator")
        
        # Step 9: Collaborator accesses the now-shared private document (should succeed)
        logger.info("Step 9: Collaborator accesses the now-shared private document")
        shared_doc_response = collaborator_session.get(f"{self.base_url}/api/documents/{private_doc['id']}")
        assert shared_doc_response.status_code == 200
        shared_doc_data = shared_doc_response.json()
        assert shared_doc_data["id"] == private_doc["id"]
        logger.info("✓ Collaborator successfully accessed shared document")
        
        # Step 10: Collaborator tries to update the shared document (should fail with read permission)
        logger.info("Step 10: Collaborator tries to update the shared document")
        update_data = {"title": f"Updated {private_doc_title}"}
        update_response = collaborator_session.put(f"{self.base_url}/api/documents/{private_doc['id']}", json=update_data)
        assert update_response.status_code in [403, 404]  # Either forbidden or not found
        logger.info("✓ Collaborator correctly denied update permission")
        
        # Step 11: Owner upgrades collaborator's permission to write
        logger.info("Step 11: Owner upgrades collaborator's permission to write")
        upgrade_result = self.share_document(owner_session, private_doc["id"], collaborator_info["id"], "write")
        logger.info("✓ Owner upgraded collaborator's permission to write")
        
        # Step 12: Collaborator updates the shared document (should succeed now)
        logger.info("Step 12: Collaborator updates the shared document")
        update_data = {"title": f"Updated {private_doc_title}"}
        update_response = collaborator_session.put(f"{self.base_url}/api/documents/{private_doc['id']}", json=update_data)
        assert update_response.status_code == 200
        updated_doc = update_response.json()
        assert updated_doc["title"] == update_data["title"]
        logger.info("✓ Collaborator successfully updated shared document")
        
        # Step 13: Owner revokes collaborator's access
        logger.info("Step 13: Owner revokes collaborator's access")
        revoke_response = owner_session.delete(f"{self.base_url}/api/documents/{private_doc['id']}/share/{collaborator_info['id']}")
        assert revoke_response.status_code == 200
        logger.info("✓ Owner revoked collaborator's access")
        
        # Step 14: Collaborator tries to access the document again (should fail)
        logger.info("Step 14: Collaborator tries to access the document again")
        final_response = collaborator_session.get(f"{self.base_url}/api/documents/{private_doc['id']}")
        assert final_response.status_code == 404
        logger.info("✓ Collaborator correctly denied access after revocation")
        
        logger.info("Permission scenario test passed successfully")


if __name__ == "__main__":
    # Run the tests
    test = TestAuthFlows()
    try:
        test.test_complete_user_journey()
        test.test_permission_scenario()
        print("All tests passed!")
    except Exception as e:
        print(f"Test failed: {e}")
    finally:
        test.teardown_method()

================
File: tests/e2e/test_permission_scenarios.py
================
#!/usr/bin/env python3
"""
End-to-end tests for complex permission scenarios in Metis RAG
"""

import pytest
import uuid
import requests
import json
import time
from typing import Dict, Tuple, List, Optional
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_permission_scenarios")


class TestPermissionScenarios:
    """End-to-end tests for complex permission scenarios"""
    
    # Base URL for API
    base_url = "http://localhost:8000"
    
    def setup_method(self):
        """Setup method for each test"""
        # Create unique test users for each test
        self.test_users = []
        self.test_organizations = []
        self.test_roles = []
    
    def teardown_method(self):
        """Teardown method for each test"""
        # Clean up test users, organizations, and roles
        admin_token = None
        
        # Try to find an admin token for cleanup
        for user in self.test_users:
            if user.get("is_admin") and "token" in user:
                admin_token = user["token"]
                break
        
        if admin_token:
            headers = {"Authorization": f"Bearer {admin_token}"}
            
            # Clean up organizations
            for org in self.test_organizations:
                try:
                    requests.delete(f"{self.base_url}/api/organizations/{org['id']}", headers=headers)
                except Exception as e:
                    logger.warning(f"Failed to delete test organization: {e}")
            
            # Clean up roles
            for role in self.test_roles:
                try:
                    requests.delete(f"{self.base_url}/api/roles/{role['id']}", headers=headers)
                except Exception as e:
                    logger.warning(f"Failed to delete test role: {e}")
        
        # Clean up users
        for user in self.test_users:
            if "token" in user:
                try:
                    headers = {"Authorization": f"Bearer {user['token']}"}
                    requests.delete(f"{self.base_url}/api/auth/users/{user['id']}", headers=headers)
                except Exception as e:
                    logger.warning(f"Failed to delete test user: {e}")
    
    def create_test_user(self, is_admin: bool = False) -> Dict:
        """Create a test user"""
        unique_id = uuid.uuid4().hex[:8]
        user_data = {
            "username": f"testuser_{unique_id}",
            "email": f"testuser_{unique_id}@example.com",
            "password": "testpassword123",
            "full_name": f"Test User {unique_id}",
            "is_active": True,
            "is_admin": is_admin
        }
        
        # Register user
        response = requests.post(
            f"{self.base_url}/api/auth/register",
            json=user_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to create test user: {response.text}")
            raise Exception(f"Failed to create test user: {response.text}")
        
        # Add user to list for cleanup
        user_info = response.json()
        user_info["password"] = user_data["password"]
        self.test_users.append(user_info)
        
        return user_info
    
    def login_user(self, username: str, password: str) -> Tuple[Dict, requests.Session]:
        """Login a user and return token data and session"""
        session = requests.Session()
        
        # Login
        response = session.post(
            f"{self.base_url}/api/auth/token",
            data={
                "username": username,
                "password": password,
                "grant_type": "password"
            }
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to login user: {response.text}")
            raise Exception(f"Failed to login user: {response.text}")
        
        token_data = response.json()
        
        # Set token in session headers
        session.headers.update({"Authorization": f"Bearer {token_data['access_token']}"})
        
        # Update user info with token
        for user in self.test_users:
            if user["username"] == username:
                user["token"] = token_data["access_token"]
                user["refresh_token"] = token_data["refresh_token"]
                break
        
        return token_data, session
    
    def create_document(self, session: requests.Session, title: str, content: str, is_public: bool = False, organization_id: Optional[str] = None) -> Dict:
        """Create a document"""
        doc_data = {
            "title": title,
            "content": content,
            "is_public": is_public
        }
        
        if organization_id:
            doc_data["organization_id"] = organization_id
        
        response = session.post(
            f"{self.base_url}/api/documents",
            json=doc_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to create document: {response.text}")
            raise Exception(f"Failed to create document: {response.text}")
        
        return response.json()
    
    def share_document(self, session: requests.Session, document_id: str, user_id: str, permission_level: str) -> Dict:
        """Share a document with another user"""
        share_data = {
            "document_id": document_id,
            "user_id": user_id,
            "permission_level": permission_level
        }
        
        response = session.post(
            f"{self.base_url}/api/documents/{document_id}/share",
            json=share_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to share document: {response.text}")
            raise Exception(f"Failed to share document: {response.text}")
        
        return response.json()
    
    def create_organization(self, session: requests.Session, name: str) -> Dict:
        """Create an organization"""
        org_data = {
            "name": name,
            "settings": {}
        }
        
        response = session.post(
            f"{self.base_url}/api/organizations",
            json=org_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to create organization: {response.text}")
            raise Exception(f"Failed to create organization: {response.text}")
        
        org_info = response.json()
        self.test_organizations.append(org_info)
        
        return org_info
    
    def add_user_to_organization(self, session: requests.Session, organization_id: str, user_id: str, role: str = "member") -> Dict:
        """Add a user to an organization"""
        member_data = {
            "organization_id": organization_id,
            "user_id": user_id,
            "role": role
        }
        
        response = session.post(
            f"{self.base_url}/api/organizations/{organization_id}/members",
            json=member_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to add user to organization: {response.text}")
            raise Exception(f"Failed to add user to organization: {response.text}")
        
        return response.json()
    
    def create_role(self, session: requests.Session, name: str, permissions: Dict) -> Dict:
        """Create a role"""
        role_data = {
            "name": name,
            "description": f"Test role: {name}",
            "permissions": permissions
        }
        
        response = session.post(
            f"{self.base_url}/api/roles",
            json=role_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to create role: {response.text}")
            raise Exception(f"Failed to create role: {response.text}")
        
        role_info = response.json()
        self.test_roles.append(role_info)
        
        return role_info
    
    def assign_role_to_user(self, session: requests.Session, user_id: str, role_id: str) -> Dict:
        """Assign a role to a user"""
        assignment_data = {
            "user_id": user_id,
            "role_id": role_id
        }
        
        response = session.post(
            f"{self.base_url}/api/roles/assign",
            json=assignment_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to assign role to user: {response.text}")
            raise Exception(f"Failed to assign role to user: {response.text}")
        
        return response.json()
    
    def query_rag(self, session: requests.Session, query: str) -> Dict:
        """Query the RAG system"""
        query_data = {
            "query": query,
            "max_results": 5
        }
        
        response = session.post(
            f"{self.base_url}/api/rag/query",
            json=query_data
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to query RAG: {response.text}")
            raise Exception(f"Failed to query RAG: {response.text}")
        
        return response.json()
    
    def test_document_sharing_scenario(self):
        """
        Test Scenario 1: Document sharing between users
        
        User A uploads doc, User B cannot see it.
        User A shares with User B (read). User B can query, User B cannot update.
        User A revokes access. User B cannot query.
        """
        logger.info("Starting document sharing scenario test")
        
        # Step 1: Create two users
        logger.info("Step 1: Creating two test users")
        user_a = self.create_test_user()
        user_b = self.create_test_user()
        logger.info(f"✓ Created User A: {user_a['username']}")
        logger.info(f"✓ Created User B: {user_b['username']}")
        
        # Step 2: Login as both users
        logger.info("Step 2: Logging in as both users")
        token_a, session_a = self.login_user(user_a["username"], user_a["password"])
        token_b, session_b = self.login_user(user_b["username"], user_b["password"])
        logger.info("✓ Both users logged in successfully")
        
        # Step 3: User A uploads a document
        logger.info("Step 3: User A uploads a document")
        doc_title = f"Test Document {uuid.uuid4().hex[:8]}"
        doc_content = "This document contains sensitive information about project X."
        doc = self.create_document(session_a, doc_title, doc_content, is_public=False)
        logger.info(f"✓ User A created document: {doc['title']}")
        
        # Step 4: User B tries to access the document (should fail)
        logger.info("Step 4: User B tries to access the document")
        response_b = session_b.get(f"{self.base_url}/api/documents/{doc['id']}")
        assert response_b.status_code == 404
        logger.info("✓ User B correctly denied access to document")
        
        # Step 5: User B tries to query the document content (should not find it)
        logger.info("Step 5: User B tries to query the document content")
        query_result = self.query_rag(session_b, "project X sensitive information")
        # Check that the document is not in the results
        found = False
        for result in query_result.get("results", []):
            if doc["id"] in result.get("document_id", ""):
                found = True
                break
        assert not found
        logger.info("✓ User B's query correctly did not return the document")
        
        # Step 6: User A shares the document with User B (read permission)
        logger.info("Step 6: User A shares the document with User B (read permission)")
        share_result = self.share_document(session_a, doc["id"], user_b["id"], "read")
        logger.info("✓ User A shared document with User B")
        
        # Step 7: User B accesses the document (should succeed)
        logger.info("Step 7: User B accesses the document")
        response_b = session_b.get(f"{self.base_url}/api/documents/{doc['id']}")
        assert response_b.status_code == 200
        doc_data = response_b.json()
        assert doc_data["id"] == doc["id"]
        logger.info("✓ User B successfully accessed the document")
        
        # Step 8: User B queries the document content (should find it now)
        logger.info("Step 8: User B queries the document content")
        query_result = self.query_rag(session_b, "project X sensitive information")
        # Check that the document is in the results
        found = False
        for result in query_result.get("results", []):
            if doc["id"] in result.get("document_id", ""):
                found = True
                break
        assert found
        logger.info("✓ User B's query successfully returned the document")
        
        # Step 9: User B tries to update the document (should fail with read permission)
        logger.info("Step 9: User B tries to update the document")
        update_data = {"title": f"Updated {doc_title}"}
        update_response = session_b.put(f"{self.base_url}/api/documents/{doc['id']}", json=update_data)
        assert update_response.status_code in [403, 404]  # Either forbidden or not found
        logger.info("✓ User B correctly denied update permission")
        
        # Step 10: User A revokes User B's access
        logger.info("Step 10: User A revokes User B's access")
        revoke_response = session_a.delete(f"{self.base_url}/api/documents/{doc['id']}/share/{user_b['id']}")
        assert revoke_response.status_code == 200
        logger.info("✓ User A revoked User B's access")
        
        # Step 11: User B tries to access the document again (should fail)
        logger.info("Step 11: User B tries to access the document again")
        response_b = session_b.get(f"{self.base_url}/api/documents/{doc['id']}")
        assert response_b.status_code == 404
        logger.info("✓ User B correctly denied access after revocation")
        
        # Step 12: User B queries the document content again (should not find it)
        logger.info("Step 12: User B queries the document content again")
        query_result = self.query_rag(session_b, "project X sensitive information")
        # Check that the document is not in the results
        found = False
        for result in query_result.get("results", []):
            if doc["id"] in result.get("document_id", ""):
                found = True
                break
        assert not found
        logger.info("✓ User B's query correctly did not return the document after revocation")
        
        logger.info("Document sharing scenario test passed successfully")
    
    def test_role_based_access_scenario(self):
        """
        Test Scenario 2: Role-based access control
        
        Admin user creates roles, assigns roles.
        Verify users with different roles have appropriate access levels.
        """
        logger.info("Starting role-based access control scenario test")
        
        # Step 1: Create admin user and regular users
        logger.info("Step 1: Creating admin and regular users")
        admin_user = self.create_test_user(is_admin=True)
        editor_user = self.create_test_user()
        viewer_user = self.create_test_user()
        logger.info(f"✓ Created admin user: {admin_user['username']}")
        logger.info(f"✓ Created editor user: {editor_user['username']}")
        logger.info(f"✓ Created viewer user: {viewer_user['username']}")
        
        # Step 2: Login as all users
        logger.info("Step 2: Logging in as all users")
        admin_token, admin_session = self.login_user(admin_user["username"], admin_user["password"])
        editor_token, editor_session = self.login_user(editor_user["username"], editor_user["password"])
        viewer_token, viewer_session = self.login_user(viewer_user["username"], viewer_user["password"])
        logger.info("✓ All users logged in successfully")
        
        # Step 3: Admin creates roles
        logger.info("Step 3: Admin creates roles")
        editor_role = self.create_role(admin_session, "Editor", {
            "documents": ["read", "write", "share"],
            "conversations": ["read", "write"]
        })
        viewer_role = self.create_role(admin_session, "Viewer", {
            "documents": ["read"],
            "conversations": ["read"]
        })
        logger.info(f"✓ Created editor role: {editor_role['name']}")
        logger.info(f"✓ Created viewer role: {viewer_role['name']}")
        
        # Step 4: Admin assigns roles to users
        logger.info("Step 4: Admin assigns roles to users")
        editor_assignment = self.assign_role_to_user(admin_session, editor_user["id"], editor_role["id"])
        viewer_assignment = self.assign_role_to_user(admin_session, viewer_user["id"], viewer_role["id"])
        logger.info("✓ Assigned roles to users")
        
        # Step 5: Admin creates a document
        logger.info("Step 5: Admin creates a document")
        doc_title = f"Role Test Document {uuid.uuid4().hex[:8]}"
        doc_content = "This document is for testing role-based access control."
        doc = self.create_document(admin_session, doc_title, doc_content, is_public=False)
        logger.info(f"✓ Admin created document: {doc['title']}")
        
        # Step 6: Admin shares the document with both users
        logger.info("Step 6: Admin shares the document with both users")
        editor_share = self.share_document(admin_session, doc["id"], editor_user["id"], "write")
        viewer_share = self.share_document(admin_session, doc["id"], viewer_user["id"], "read")
        logger.info("✓ Admin shared document with both users")
        
        # Step 7: Editor tries to update the document (should succeed)
        logger.info("Step 7: Editor tries to update the document")
        editor_update = {"title": f"Editor Updated {doc_title}"}
        editor_update_response = editor_session.put(f"{self.base_url}/api/documents/{doc['id']}", json=editor_update)
        assert editor_update_response.status_code == 200
        updated_doc = editor_update_response.json()
        assert updated_doc["title"] == editor_update["title"]
        logger.info("✓ Editor successfully updated the document")
        
        # Step 8: Viewer tries to update the document (should fail)
        logger.info("Step 8: Viewer tries to update the document")
        viewer_update = {"title": f"Viewer Updated {doc_title}"}
        viewer_update_response = viewer_session.put(f"{self.base_url}/api/documents/{doc['id']}", json=viewer_update)
        assert viewer_update_response.status_code in [403, 404]  # Either forbidden or not found
        logger.info("✓ Viewer correctly denied update permission")
        
        # Step 9: Editor tries to share the document (should succeed with Editor role)
        logger.info("Step 9: Editor tries to share the document")
        try:
            # Create a temporary user to share with
            temp_user = self.create_test_user()
            editor_share_response = self.share_document(editor_session, doc["id"], temp_user["id"], "read")
            logger.info("✓ Editor successfully shared the document")
            
            # Verify the share worked
            temp_token, temp_session = self.login_user(temp_user["username"], temp_user["password"])
            temp_access_response = temp_session.get(f"{self.base_url}/api/documents/{doc['id']}")
            assert temp_access_response.status_code == 200
            logger.info("✓ Temporary user successfully accessed the shared document")
        except Exception as e:
            logger.error(f"Editor share test failed: {e}")
            raise
        
        # Step 10: Viewer tries to share the document (should fail with Viewer role)
        logger.info("Step 10: Viewer tries to share the document")
        try:
            # Create another temporary user to share with
            temp_user2 = self.create_test_user()
            viewer_share_response = session_b = requests.Session()
            viewer_share_response.headers.update({"Authorization": f"Bearer {viewer_token['access_token']}"})
            viewer_share_data = {
                "document_id": doc["id"],
                "user_id": temp_user2["id"],
                "permission_level": "read"
            }
            viewer_share_result = viewer_share_response.post(
                f"{self.base_url}/api/documents/{doc['id']}/share",
                json=viewer_share_data
            )
            assert viewer_share_result.status_code in [403, 404]  # Either forbidden or not found
            logger.info("✓ Viewer correctly denied share permission")
        except Exception as e:
            logger.error(f"Viewer share test failed: {e}")
            # This is expected to fail, so continue
        
        logger.info("Role-based access control scenario test passed successfully")
    
    def test_organization_isolation_scenario(self):
        """
        Test Scenario 3: Organization isolation
        
        User in Org A uploads doc. User in Org B cannot access it.
        Admin shares across orgs. Verify access.
        """
        logger.info("Starting organization isolation scenario test")
        
        # Step 1: Create admin user and regular users
        logger.info("Step 1: Creating admin and regular users")
        admin_user = self.create_test_user(is_admin=True)
        org_a_user = self.create_test_user()
        org_b_user = self.create_test_user()
        logger.info(f"✓ Created admin user: {admin_user['username']}")
        logger.info(f"✓ Created Org A user: {org_a_user['username']}")
        logger.info(f"✓ Created Org B user: {org_b_user['username']}")
        
        # Step 2: Login as all users
        logger.info("Step 2: Logging in as all users")
        admin_token, admin_session = self.login_user(admin_user["username"], admin_user["password"])
        org_a_token, org_a_session = self.login_user(org_a_user["username"], org_a_user["password"])
        org_b_token, org_b_session = self.login_user(org_b_user["username"], org_b_user["password"])
        logger.info("✓ All users logged in successfully")
        
        # Step 3: Admin creates organizations
        logger.info("Step 3: Admin creates organizations")
        org_a = self.create_organization(admin_session, f"Organization A {uuid.uuid4().hex[:8]}")
        org_b = self.create_organization(admin_session, f"Organization B {uuid.uuid4().hex[:8]}")
        logger.info(f"✓ Created Organization A: {org_a['name']}")
        logger.info(f"✓ Created Organization B: {org_b['name']}")
        
        # Step 4: Admin adds users to organizations
        logger.info("Step 4: Admin adds users to organizations")
        org_a_member = self.add_user_to_organization(admin_session, org_a["id"], org_a_user["id"])
        org_b_member = self.add_user_to_organization(admin_session, org_b["id"], org_b_user["id"])
        logger.info("✓ Added users to organizations")
        
        # Step 5: Org A user creates a document in Org A
        logger.info("Step 5: Org A user creates a document in Org A")
        doc_title = f"Org A Document {uuid.uuid4().hex[:8]}"
        doc_content = "This document belongs to Organization A."
        doc = self.create_document(org_a_session, doc_title, doc_content, is_public=False, organization_id=org_a["id"])
        logger.info(f"✓ Org A user created document: {doc['title']}")
        
        # Step 6: Org B user tries to access the document (should fail)
        logger.info("Step 6: Org B user tries to access the document")
        org_b_response = org_b_session.get(f"{self.base_url}/api/documents/{doc['id']}")
        assert org_b_response.status_code == 404
        logger.info("✓ Org B user correctly denied access to document")
        
        # Step 7: Admin shares the document across organizations
        logger.info("Step 7: Admin shares the document across organizations")
        cross_org_share = self.share_document(admin_session, doc["id"], org_b_user["id"], "read")
        logger.info("✓ Admin shared document across organizations")
        
        # Step 8: Org B user accesses the document (should succeed)
        logger.info("Step 8: Org B user accesses the document")
        org_b_response = org_b_session.get(f"{self.base_url}/api/documents/{doc['id']}")
        assert org_b_response.status_code == 200
        doc_data = org_b_response.json()
        assert doc_data["id"] == doc["id"]
        logger.info("✓ Org B user successfully accessed the document")
        
        # Step 9: Org B user queries the document content (should find it)
        logger.info("Step 9: Org B user queries the document content")
        query_result = self.query_rag(org_b_session, "Organization A document")
        # Check that the document is in the results
        found = False
        for result in query_result.get("results", []):
            if doc["id"] in result.get("document_id", ""):
                found = True
                break
        assert found
        logger.info("✓ Org B user's query successfully returned the document")
        
        logger.info("Organization isolation scenario test passed successfully")


if __name__ == "__main__":
    # Run the tests
    test = TestPermissionScenarios()
    try:
        test.test_document_sharing_scenario()
        test.test_role_based_access_scenario()
        test.test_organization_isolation_scenario()
        print("All tests passed!")
    except Exception as e:
        print(f"Test failed: {e}")
    finally:
        test.teardown_method()

================
File: tests/integration/test_api.py
================
import pytest
from fastapi.testclient import TestClient
import os
import tempfile
from io import BytesIO
import uuid
from unittest.mock import AsyncMock

from app.main import app
from app.models.chat import ChatQuery
from app.models.document import DocumentProcessRequest
# Import necessary components for dependency injection and mocking
from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore

# --- Fixture for TestClient ---
@pytest.fixture
def client():
    """
    Create a TestClient instance for each test.
    This ensures that dependency overrides are correctly applied.
    """
    client = TestClient(app)
    yield client


# --- Helper function for dependency injection ---
async def get_langgraph_rag_agent():
    """Dependency for injecting the LangGraphRAGAgent"""
    # In a real application, you'd create the agent here with proper dependencies
    # For testing, we'll override this with a mocked version
    return LangGraphRAGAgent()


def test_health_check(client: TestClient):  # Use the fixture
    """
    Test the health check endpoint
    """
    response = client.get("/api/system/health")
    assert response.status_code == 200
    data = response.json()
    assert "status" in data
    # Note: This might fail in CI if Ollama is not available

def test_chat_query_without_context(client: TestClient): # Use the fixture
    """
    Test chat query without RAG context
    """
    # Arrange
    query = ChatQuery(
        message="Hello, world!",
        use_rag=False,
        stream=False
    )

    # Act
    response = client.post("/api/chat/query", json=query.dict())

    # Assert
    assert response.status_code == 200
    data = response.json()
    assert "message" in data
    assert "conversation_id" in data

@pytest.mark.asyncio
async def test_langgraph_rag_chat_endpoint(client: TestClient): # Use the fixture
    """
    Test the LangGraph RAG Agent query endpoint
    """
    # Arrange: Create a mock query
    query = ChatQuery(
        message="What are the key features of the Metis RAG system?",
        use_rag=True,
        stream=False
    )

    # Mock dependencies (OllamaClient and VectorStore)
    mock_ollama_client = AsyncMock(spec=OllamaClient)
    mock_vector_store = AsyncMock(spec=VectorStore)

    # Configure mock responses (adjust as needed for your test case)
    mock_ollama_client.generate.return_value = {"response": "This is a mock LLM response."}
    mock_vector_store.search.return_value = [
        {
            "chunk_id": "chunk1",
            "content": "Metis RAG features include document management...",
            "metadata": {"document_id": "doc1", "filename": "test.txt"},
            "distance": 0.1
        }
    ]

    # Create an instance of the LangGraphRAGAgent with mocked dependencies
    mock_rag_agent = LangGraphRAGAgent(
        ollama_client=mock_ollama_client,
        vector_store=mock_vector_store
    )

    # Patch the global langgraph_rag_agent in app/api/chat.py
    import app.api.chat
    original_agent = app.api.chat.langgraph_rag_agent
    app.api.chat.langgraph_rag_agent = mock_rag_agent

    try:
        # Act: Send the request to the correct endpoint
        response = client.post("/api/chat/langgraph_rag", json=query.dict())  # Using the endpoint defined in app/api/chat.py

        # Assert: Check the response
        assert response.status_code == 200
        data = response.json()
        assert "message" in data
        assert "conversation_id" in data
        assert "citations" in data

        # Assert that the mocked methods were called
        mock_ollama_client.generate.assert_called()
        mock_vector_store.search.assert_called()
    finally:
        # Restore the original agent (IMPORTANT for other tests)
        app.api.chat.langgraph_rag_agent = original_agent

# Ollama is running, so we can run this test
def test_document_upload_and_process(client: TestClient): # Use the fixture
    """
    Test document upload and processing
    """
    # Arrange
    test_content = b"This is a test document for RAG testing."
    test_file = BytesIO(test_content)
    test_file.name = "test.txt"

    # Act - Upload
    upload_response = client.post(
        "/api/documents/upload",
        files={"file": ("test.txt", test_file, "text/plain")}
    )

    # Assert - Upload
    assert upload_response.status_code == 200
    upload_data = upload_response.json()
    assert upload_data["success"] is True
    assert "document_id" in upload_data
    document_id = upload_data["document_id"]

    # Act - Process
    process_request = DocumentProcessRequest(document_ids=[document_id])
    process_response = client.post(
        "/api/documents/process",
        json=process_request.dict()
    )

    # Assert - Process
    assert process_response.status_code == 200
    process_data = process_response.json()
    assert process_data["success"] is True

    # Act - Get Document (to check chunk count)
    get_response = client.get(f"/api/documents/{document_id}")
    assert get_response.status_code == 200
    document = get_response.json()
    assert "chunks" in document
    assert len(document["chunks"]) > 0  # Check that chunks were created

    # Act - List Documents
    list_response = client.get("/api/documents/list")

    # Assert - List Documents
    assert list_response.status_code == 200
    documents = list_response.json()
    assert len(documents) >= 1
    found = any(doc["id"] == document_id for doc in documents)
    assert found is True

    # Act - Delete
    delete_response = client.delete(f"/api/documents/{document_id}")

    # Assert - Delete
    assert delete_response.status_code == 200
    delete_data = delete_response.json()
    assert delete_data["success"] is True

================
File: tests/integration/test_auth_endpoints.py
================
#!/usr/bin/env python3
"""
Integration tests for authentication endpoints in app/api/auth.py
"""

import pytest
import uuid
from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import AsyncSession
import asyncio
from datetime import datetime, timedelta

from app.main import app
from app.core.security import get_password_hash, create_access_token
from app.db.dependencies import get_db
from app.db.repositories.user_repository import UserRepository
from app.models.user import UserCreate, UserUpdate


@pytest.fixture
def client():
    """Create a TestClient instance for each test"""
    return TestClient(app)


@pytest.fixture
def test_user_data():
    """Create unique test user data for each test"""
    unique_id = uuid.uuid4().hex[:8]
    return {
        "username": f"testuser_{unique_id}",
        "email": f"testuser_{unique_id}@example.com",
        "password": "testpassword123",
        "full_name": "Test User",
        "is_active": True,
        "is_admin": False
    }


@pytest.fixture
async def test_user(test_user_data):
    """Create a test user in the database"""
    # Get database session
    db = await anext(get_db())
    
    try:
        # Create user repository
        user_repository = UserRepository(db)
        
        # Create user
        user_create = UserCreate(**test_user_data)
        user = await user_repository.create_user(user_create)
        
        # Return user and credentials
        yield user, test_user_data
        
        # Clean up - delete user
        await user_repository.delete_user(user.id)
    finally:
        await db.close()


@pytest.fixture
def event_loop():
    """Create an event loop for each test"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


class TestAuthEndpoints:
    """Tests for authentication endpoints"""
    
    @pytest.mark.asyncio
    async def test_login_success(self, client, test_user):
        """Test successful login"""
        user, user_data = await test_user
        
        # Login with valid credentials
        response = client.post(
            "/api/auth/token",
            data={
                "username": user_data["username"],
                "password": user_data["password"],
                "grant_type": "password"
            }
        )
        
        # Check response
        assert response.status_code == 200
        data = response.json()
        
        # Check token data
        assert "access_token" in data
        assert "refresh_token" in data
        assert data["token_type"] == "bearer"
        assert data["expires_in"] > 0
    
    @pytest.mark.asyncio
    async def test_login_invalid_credentials(self, client, test_user):
        """Test login with invalid credentials"""
        user, user_data = await test_user
        
        # Login with invalid password
        response = client.post(
            "/api/auth/token",
            data={
                "username": user_data["username"],
                "password": "wrongpassword",
                "grant_type": "password"
            }
        )
        
        # Check response
        assert response.status_code == 401
        data = response.json()
        assert "detail" in data
        assert "Incorrect username or password" in data["detail"]
        
        # Login with non-existent user
        response = client.post(
            "/api/auth/token",
            data={
                "username": "nonexistentuser",
                "password": user_data["password"],
                "grant_type": "password"
            }
        )
        
        # Check response
        assert response.status_code == 401
        data = response.json()
        assert "detail" in data
        assert "Incorrect username or password" in data["detail"]
    
    @pytest.mark.asyncio
    async def test_register_success(self, client):
        """Test successful user registration"""
        # Create unique user data
        unique_id = uuid.uuid4().hex[:8]
        user_data = {
            "username": f"newuser_{unique_id}",
            "email": f"newuser_{unique_id}@example.com",
            "password": "testpassword123",
            "full_name": "New Test User",
            "is_active": True,
            "is_admin": False
        }
        
        # Register new user
        response = client.post(
            "/api/auth/register",
            json=user_data
        )
        
        # Check response
        assert response.status_code == 200
        data = response.json()
        
        # Check user data
        assert data["username"] == user_data["username"]
        assert data["email"] == user_data["email"]
        assert data["full_name"] == user_data["full_name"]
        assert data["is_active"] == user_data["is_active"]
        assert data["is_admin"] == user_data["is_admin"]
        assert "id" in data
        assert "password" not in data  # Password should not be returned
        
        # Clean up - delete user
        db = await anext(get_db())
        try:
            user_repository = UserRepository(db)
            await user_repository.delete_user(data["id"])
        finally:
            await db.close()
    
    @pytest.mark.asyncio
    async def test_register_duplicate_username(self, client, test_user):
        """Test registration with duplicate username"""
        user, user_data = await test_user
        
        # Try to register with existing username
        duplicate_data = user_data.copy()
        duplicate_data["email"] = f"different_{uuid.uuid4().hex[:8]}@example.com"  # Different email
        
        response = client.post(
            "/api/auth/register",
            json=duplicate_data
        )
        
        # Check response
        assert response.status_code == 400
        data = response.json()
        assert "detail" in data
        assert "username already exists" in data["detail"].lower()
    
    @pytest.mark.asyncio
    async def test_register_duplicate_email(self, client, test_user):
        """Test registration with duplicate email"""
        user, user_data = await test_user
        
        # Try to register with existing email
        duplicate_data = user_data.copy()
        duplicate_data["username"] = f"different_{uuid.uuid4().hex[:8]}"  # Different username
        
        response = client.post(
            "/api/auth/register",
            json=duplicate_data
        )
        
        # Check response
        assert response.status_code == 400
        data = response.json()
        assert "detail" in data
        assert "email already exists" in data["detail"].lower()
    
    @pytest.mark.asyncio
    async def test_refresh_token(self, client, test_user):
        """Test refreshing an access token"""
        user, user_data = await test_user
        
        # Login to get tokens
        login_response = client.post(
            "/api/auth/token",
            data={
                "username": user_data["username"],
                "password": user_data["password"],
                "grant_type": "password"
            }
        )
        
        login_data = login_response.json()
        refresh_token = login_data["refresh_token"]
        
        # Refresh token
        refresh_response = client.post(
            "/api/auth/refresh",
            json={"refresh_token": refresh_token}
        )
        
        # Check response
        assert refresh_response.status_code == 200
        refresh_data = refresh_response.json()
        
        # Check new token data
        assert "access_token" in refresh_data
        assert refresh_data["access_token"] != login_data["access_token"]  # New token should be different
        assert refresh_data["token_type"] == "bearer"
        assert refresh_data["expires_in"] > 0
        assert refresh_data["refresh_token"] == refresh_token  # Same refresh token returned
    
    @pytest.mark.asyncio
    async def test_refresh_token_invalid(self, client):
        """Test refreshing with an invalid token"""
        # Try to refresh with invalid token
        response = client.post(
            "/api/auth/refresh",
            json={"refresh_token": "invalid.token.here"}
        )
        
        # Check response
        assert response.status_code == 401
        data = response.json()
        assert "detail" in data
        assert "Invalid refresh token" in data["detail"]
    
    @pytest.mark.asyncio
    async def test_get_current_user(self, client, test_user):
        """Test getting the current user"""
        user, user_data = await test_user
        
        # Login to get token
        login_response = client.post(
            "/api/auth/token",
            data={
                "username": user_data["username"],
                "password": user_data["password"],
                "grant_type": "password"
            }
        )
        
        token = login_response.json()["access_token"]
        
        # Get current user
        response = client.get(
            "/api/auth/me",
            headers={"Authorization": f"Bearer {token}"}
        )
        
        # Check response
        assert response.status_code == 200
        data = response.json()
        
        # Check user data
        assert data["username"] == user_data["username"]
        assert data["email"] == user_data["email"]
        assert data["full_name"] == user_data["full_name"]
        assert "id" in data
        assert "password" not in data  # Password should not be returned
    
    @pytest.mark.asyncio
    async def test_get_current_user_no_token(self, client):
        """Test getting the current user without a token"""
        # Try to get current user without token
        response = client.get("/api/auth/me")
        
        # Check response
        assert response.status_code == 401
        data = response.json()
        assert "detail" in data
        assert "Not authenticated" in data["detail"]
    
    @pytest.mark.asyncio
    async def test_update_current_user(self, client, test_user):
        """Test updating the current user"""
        user, user_data = await test_user
        
        # Login to get token
        login_response = client.post(
            "/api/auth/token",
            data={
                "username": user_data["username"],
                "password": user_data["password"],
                "grant_type": "password"
            }
        )
        
        token = login_response.json()["access_token"]
        
        # Update user data
        update_data = {
            "full_name": "Updated Test User",
            "email": f"updated_{uuid.uuid4().hex[:8]}@example.com"
        }
        
        response = client.put(
            "/api/auth/me",
            json=update_data,
            headers={"Authorization": f"Bearer {token}"}
        )
        
        # Check response
        assert response.status_code == 200
        data = response.json()
        
        # Check updated user data
        assert data["full_name"] == update_data["full_name"]
        assert data["email"] == update_data["email"]
        assert data["username"] == user_data["username"]  # Username should not change
    
    @pytest.mark.asyncio
    async def test_account_deactivation_reactivation(self, client, test_user):
        """Test account deactivation and reactivation (persistence test)"""
        user, user_data = await test_user
        
        # Login to get token
        login_response = client.post(
            "/api/auth/token",
            data={
                "username": user_data["username"],
                "password": user_data["password"],
                "grant_type": "password"
            }
        )
        
        token = login_response.json()["access_token"]
        
        # Deactivate account (requires admin access, so we'll do it directly in the DB)
        db = await anext(get_db())
        try:
            user_repository = UserRepository(db)
            await user_repository.update_user(user.id, {"is_active": False})
            
            # Try to access protected endpoint
            response = client.get(
                "/api/auth/me",
                headers={"Authorization": f"Bearer {token}"}
            )
            
            # Should fail with 403 Forbidden
            assert response.status_code == 403
            data = response.json()
            assert "detail" in data
            assert "Inactive user" in data["detail"]
            
            # Try to login
            login_response = client.post(
                "/api/auth/token",
                data={
                    "username": user_data["username"],
                    "password": user_data["password"],
                    "grant_type": "password"
                }
            )
            
            # Should fail with 401 Unauthorized
            assert login_response.status_code == 401
            
            # Reactivate account
            await user_repository.update_user(user.id, {"is_active": True})
            
            # Try to login again
            login_response = client.post(
                "/api/auth/token",
                data={
                    "username": user_data["username"],
                    "password": user_data["password"],
                    "grant_type": "password"
                }
            )
            
            # Should succeed
            assert login_response.status_code == 200
            new_token = login_response.json()["access_token"]
            
            # Access protected endpoint with new token
            response = client.get(
                "/api/auth/me",
                headers={"Authorization": f"Bearer {new_token}"}
            )
            
            # Should succeed
            assert response.status_code == 200
            
        finally:
            await db.close()
    
    @pytest.mark.asyncio
    async def test_password_reset_persistence(self, client, test_user):
        """Test password reset with persistence of user-document relationships"""
        user, user_data = await test_user
        
        # Login to get token
        login_response = client.post(
            "/api/auth/token",
            data={
                "username": user_data["username"],
                "password": user_data["password"],
                "grant_type": "password"
            }
        )
        
        original_token = login_response.json()["access_token"]
        
        # Change password directly in the DB (simulating password reset)
        new_password = "newpassword456"
        db = await anext(get_db())
        try:
            user_repository = UserRepository(db)
            new_password_hash = get_password_hash(new_password)
            await user_repository.update_user(user.id, {"password_hash": new_password_hash})
            
            # Try to login with old password
            login_response = client.post(
                "/api/auth/token",
                data={
                    "username": user_data["username"],
                    "password": user_data["password"],
                    "grant_type": "password"
                }
            )
            
            # Should fail
            assert login_response.status_code == 401
            
            # Login with new password
            login_response = client.post(
                "/api/auth/token",
                data={
                    "username": user_data["username"],
                    "password": new_password,
                    "grant_type": "password"
                }
            )
            
            # Should succeed
            assert login_response.status_code == 200
            new_token = login_response.json()["access_token"]
            
            # Access protected endpoint with new token
            response = client.get(
                "/api/auth/me",
                headers={"Authorization": f"Bearer {new_token}"}
            )
            
            # Should succeed
            assert response.status_code == 200
            user_data = response.json()
            
            # User ID should be the same (persistence)
            assert user_data["id"] == user.id
            
        finally:
            await db.close()
    
    @pytest.mark.asyncio
    async def test_token_expiry_refresh(self, client, test_user):
        """Test token expiry and refresh (persistence test)"""
        user, user_data = await test_user
        
        # Create a short-lived token (expires in 2 seconds)
        db = await anext(get_db())
        try:
            user_repository = UserRepository(db)
            user_db = await user_repository.get_by_username(user_data["username"])
            
            token_data = {
                "sub": user_db.username,
                "user_id": user_db.id,
                "aud": "metis-rag",
                "iss": "metis-rag-auth",
                "jti": str(uuid.uuid4())
            }
            
            # Create short-lived token
            short_lived_token = create_access_token(
                data=token_data,
                expires_delta=timedelta(seconds=2)
            )
            
            # Access protected endpoint with token
            response = client.get(
                "/api/auth/me",
                headers={"Authorization": f"Bearer {short_lived_token}"}
            )
            
            # Should succeed
            assert response.status_code == 200
            
            # Wait for token to expire
            await asyncio.sleep(3)
            
            # Try to access protected endpoint with expired token
            response = client.get(
                "/api/auth/me",
                headers={"Authorization": f"Bearer {short_lived_token}"}
            )
            
            # Should fail with 401 Unauthorized
            assert response.status_code == 401
            
            # Login to get new tokens
            login_response = client.post(
                "/api/auth/token",
                data={
                    "username": user_data["username"],
                    "password": user_data["password"],
                    "grant_type": "password"
                }
            )
            
            # Should succeed
            assert login_response.status_code == 200
            new_token = login_response.json()["access_token"]
            
            # Access protected endpoint with new token
            response = client.get(
                "/api/auth/me",
                headers={"Authorization": f"Bearer {new_token}"}
            )
            
            # Should succeed
            assert response.status_code == 200
            user_data_response = response.json()
            
            # User ID should be the same (persistence)
            assert user_data_response["id"] == user_db.id
            
        finally:
            await db.close()


if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/integration/test_chunking_judge_integration.py
================
"""
Integration tests for the Chunking Judge with DocumentProcessor
"""
import pytest
import os
from unittest.mock import AsyncMock, patch, MagicMock
import tempfile
import shutil

from app.rag.document_processor import DocumentProcessor
from app.rag.agents.chunking_judge import ChunkingJudge
from app.models.document import Document
from app.core.config import UPLOAD_DIR

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "strategy": "markdown",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is a structured markdown document with clear headers."
        }
        """
    }
    return client

@pytest.fixture
def test_document():
    """Create a test document for processing"""
    return Document(
        id="test-integration-id",
        filename="test_integration.md",
        content="""# Test Document

This is a test document for integration testing.

## Section 1

Content for section 1.

## Section 2

Content for section 2.
"""
    )

@pytest.fixture
def setup_document_directory(test_document):
    """Set up a temporary directory for the test document"""
    # Create document directory
    doc_dir = os.path.join(UPLOAD_DIR, test_document.id)
    os.makedirs(doc_dir, exist_ok=True)
    
    # Write document content to file
    doc_path = os.path.join(doc_dir, test_document.filename)
    with open(doc_path, 'w') as f:
        f.write(test_document.content)
    
    yield
    
    # Clean up
    if os.path.exists(doc_dir):
        shutil.rmtree(doc_dir)

@pytest.mark.asyncio
@patch('app.core.config.USE_CHUNKING_JUDGE', True)
async def test_document_processor_with_chunking_judge(mock_ollama_client, test_document, setup_document_directory):
    """Test that the DocumentProcessor correctly uses the Chunking Judge"""
    # Patch the ChunkingJudge to use our mock
    with patch('app.rag.agents.chunking_judge.ChunkingJudge', return_value=ChunkingJudge(ollama_client=mock_ollama_client)):
        # Create document processor
        processor = DocumentProcessor()
        
        # Process document
        processed_doc = await processor.process_document(test_document)
        
        # Verify chunking analysis was added to metadata
        assert "chunking_analysis" in processed_doc.metadata
        assert processed_doc.metadata["chunking_analysis"]["strategy"] == "markdown"
        assert processed_doc.metadata["chunking_analysis"]["parameters"]["chunk_size"] == 800
        assert processed_doc.metadata["chunking_analysis"]["parameters"]["chunk_overlap"] == 100
        
        # Verify chunks were created
        assert len(processed_doc.chunks) > 0

@pytest.mark.asyncio
@patch('app.core.config.USE_CHUNKING_JUDGE', False)
async def test_document_processor_without_chunking_judge(test_document, setup_document_directory):
    """Test that the DocumentProcessor works correctly when Chunking Judge is disabled"""
    # Create document processor
    processor = DocumentProcessor()
    
    # Process document
    processed_doc = await processor.process_document(test_document)
    
    # Verify chunking analysis was not added to metadata
    assert "chunking_analysis" not in processed_doc.metadata
    
    # Verify chunks were created
    assert len(processed_doc.chunks) > 0

================
File: tests/integration/test_enhanced_langgraph_rag_integration.py
================
"""
Integration test for the Enhanced LangGraph RAG Agent
"""
import pytest
import logging
import uuid
from typing import Dict, Any, List, Optional

from app.rag.agents.enhanced_langgraph_rag_agent import EnhancedLangGraphRAGAgent
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.query_analyzer import QueryAnalyzer
from app.rag.tools import ToolRegistry
from app.rag.process_logger import ProcessLogger

# Configure logging
logger = logging.getLogger("tests.integration.test_enhanced_langgraph_rag_integration")

class MockProcessLogger:
    """Mock ProcessLogger for testing"""
    def __init__(self):
        self.process_log = {}
        self.logger = logging.getLogger("tests.mock_process_logger")
    
    def log_step(self, query_id: str, step_name: str, step_data: Dict[str, Any]) -> None:
        """Log a step without requiring the query_id to be registered first"""
        if query_id not in self.process_log:
            self.process_log[query_id] = {
                "query": step_data.get("query", ""),
                "steps": [],
                "final_response": None
            }
        
        self.process_log[query_id]["steps"].append({
            "step_name": step_name,
            "step_data": step_data
        })
    
    def log_final_response(self, query_id: str, response: str, metadata: Dict[str, Any]) -> None:
        """Log the final response without requiring the query_id to be registered first"""
        if query_id not in self.process_log:
            self.process_log[query_id] = {
                "query": "",
                "steps": [],
                "final_response": None
            }
        
        self.process_log[query_id]["final_response"] = {
            "response": response,
            "metadata": metadata
        }
    
    def get_process_log(self, query_id: str) -> Optional[Dict[str, Any]]:
        """Get the process log for a query"""
        return self.process_log.get(query_id)

class MockOllamaClient:
    """Mock OllamaClient for testing"""
    async def generate(self, prompt, model=None, system_prompt=None, stream=False, parameters=None):
        # Check if this is an error query
        is_error = "Error:" in prompt
        
        if stream:
            # For streaming, return an async generator
            async def mock_stream():
                if is_error:
                    yield "Error: This is a mock error response."
                else:
                    for token in ["This", " is", " a", " mock", " response", "."]:
                        yield token
            return mock_stream()
        else:
            # For non-streaming, return a dict with the response
            if is_error:
                return {"response": "Error: This is a mock error response."}
            else:
                return {"response": "This is a mock response."}

class MockVectorStore:
    """Mock VectorStore for testing"""
    async def search(self, query, top_k=10, filter_criteria=None):
        # Return mock search results
        return [
            {
                "chunk_id": f"chunk_{i}",
                "content": f"This is mock content for chunk {i}.",
                "metadata": {
                    "document_id": f"doc_{i}",
                    "filename": f"test_file_{i}.txt",
                    "tags": ["test", "mock"],
                    "folder": "/test"
                },
                "distance": 0.1 + (i * 0.05)  # Increasing distance (lower relevance)
            }
            for i in range(3)
        ]
    
    def get_stats(self):
        return {"count": 10}

class MockQueryAnalyzer:
    """Mock QueryAnalyzer for testing"""
    async def analyze(self, query):
        # Return mock analysis based on query content
        if "complex" in query.lower() or "tool" in query.lower():
            return {
                "complexity": "complex",
                "requires_tools": ["calculator"] if "calculate" in query.lower() else [],
                "sub_queries": ["What is the meaning of life?"] if "meaning" in query.lower() else [],
                "parameters": {"k": 5, "threshold": 0.5, "reranking": True},
                "justification": "This is a complex query that requires multiple steps."
            }
        else:
            return {
                "complexity": "simple",
                "requires_tools": [],
                "sub_queries": [],
                "parameters": {"k": 3, "threshold": 0.4, "reranking": False},
                "justification": "This is a simple query that can be answered directly."
            }

class MockToolRegistry:
    """Mock ToolRegistry for testing"""
    def list_tools(self):
        return ["rag", "calculator", "database"]
    
    def get_tool(self, tool_name):
        if tool_name in ["rag", "calculator", "database"]:
            return MockTool(tool_name)
        return None

class MockTool:
    """Mock Tool for testing"""
    def __init__(self, name):
        self.name = name
    
    async def execute(self, input_data):
        if self.name == "calculator" and "expression" in input_data:
            return {"result": 42}
        elif self.name == "rag" and "query" in input_data:
            return {"chunks": [{"content": "This is a mock RAG result."}]}
        elif self.name == "database" and "query" in input_data:
            return {"results": [{"id": 1, "name": "Test"}]}
        else:
            return {"error": "Invalid input"}

@pytest.mark.asyncio
async def test_enhanced_langgraph_rag_agent_initialization():
    """Test that the Enhanced LangGraph RAG Agent can be initialized"""
    # Initialize components
    vector_store = MockVectorStore()
    ollama_client = MockOllamaClient()
    query_analyzer = MockQueryAnalyzer()
    tool_registry = MockToolRegistry()
    process_logger = MockProcessLogger()
    
    # Initialize Enhanced LangGraph RAG Agent
    agent = EnhancedLangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client,
        query_analyzer=query_analyzer,
        tool_registry=tool_registry,
        process_logger=process_logger
    )
    
    # Verify that the agent was initialized
    assert agent is not None
    assert agent.graph is not None
    assert agent.vector_store is not None
    assert agent.ollama_client is not None
    assert agent.query_analyzer is not None
    assert agent.query_planner is not None
    assert agent.plan_executor is not None
    assert agent.tool_registry is not None
    assert agent.process_logger is not None

@pytest.mark.asyncio
async def test_enhanced_langgraph_rag_agent_simple_query():
    """Test that the Enhanced LangGraph RAG Agent can process a simple query"""
    # Initialize components with mocks
    vector_store = MockVectorStore()
    ollama_client = MockOllamaClient()
    query_analyzer = MockQueryAnalyzer()
    tool_registry = MockToolRegistry()
    process_logger = MockProcessLogger()
    
    # Initialize Enhanced LangGraph RAG Agent
    agent = EnhancedLangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client,
        query_analyzer=query_analyzer,
        tool_registry=tool_registry,
        process_logger=process_logger
    )
    
    # Test simple query
    query = "What is artificial intelligence?"
    
    # Query the Enhanced LangGraph RAG Agent
    response = await agent.query(
        query=query,
        stream=False
    )
    
    # Verify the response structure
    assert response is not None
    assert "query" in response
    assert response["query"] == query
    assert "answer" in response
    assert response["answer"] == "This is a mock response."
    assert "sources" in response
    assert isinstance(response["sources"], list)
    assert "execution_trace" not in response or response["execution_trace"] is None

@pytest.mark.asyncio
async def test_enhanced_langgraph_rag_agent_complex_query():
    """Test that the Enhanced LangGraph RAG Agent can process a complex query"""
    # Initialize components with mocks
    vector_store = MockVectorStore()
    ollama_client = MockOllamaClient()
    query_analyzer = MockQueryAnalyzer()
    tool_registry = MockToolRegistry()
    process_logger = MockProcessLogger()
    
    # Initialize Enhanced LangGraph RAG Agent
    agent = EnhancedLangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client,
        query_analyzer=query_analyzer,
        tool_registry=tool_registry,
        process_logger=process_logger
    )
    
    # Test complex query that requires tools
    query = "Calculate the meaning of life and explain it in a complex way"
    
    # Query the Enhanced LangGraph RAG Agent
    response = await agent.query(
        query=query,
        stream=False
    )
    
    # Verify the response structure
    assert response is not None
    assert "query" in response
    assert response["query"] == query
    assert "answer" in response
    assert response["answer"] == "This is a mock response."
    assert "sources" in response
    assert isinstance(response["sources"], list)
    assert "execution_trace" in response
    assert isinstance(response["execution_trace"], list)

@pytest.mark.asyncio
async def test_enhanced_langgraph_rag_agent_streaming():
    """Test that the Enhanced LangGraph RAG Agent can stream responses"""
    # Initialize components with mocks
    vector_store = MockVectorStore()
    ollama_client = MockOllamaClient()
    query_analyzer = MockQueryAnalyzer()
    tool_registry = MockToolRegistry()
    process_logger = MockProcessLogger()
    
    # Initialize Enhanced LangGraph RAG Agent
    agent = EnhancedLangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client,
        query_analyzer=query_analyzer,
        tool_registry=tool_registry,
        process_logger=process_logger
    )
    
    # Test query with streaming
    query = "What is artificial intelligence?"
    
    # Query the Enhanced LangGraph RAG Agent with streaming
    response = await agent.query(
        query=query,
        stream=True
    )
    
    # Verify the response structure
    assert response is not None
    assert "query" in response
    assert response["query"] == query
    assert "stream" in response
    
    # Collect tokens from the stream
    tokens = []
    async for token in response["stream"]:
        tokens.append(token)
    
    # Verify the tokens
    assert len(tokens) > 0
    assert "".join(tokens) == "This is a mock response."
    
    # Verify sources are included
    assert "sources" in response
    assert isinstance(response["sources"], list)

@pytest.mark.asyncio
async def test_enhanced_langgraph_rag_agent_error_handling():
    """Test that the Enhanced LangGraph RAG Agent handles errors gracefully"""
    # Initialize components with mocks
    vector_store = MockVectorStore()
    ollama_client = MockOllamaClient()
    query_analyzer = MockQueryAnalyzer()
    tool_registry = MockToolRegistry()
    process_logger = MockProcessLogger()
    
    # Initialize Enhanced LangGraph RAG Agent
    agent = EnhancedLangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client,
        query_analyzer=query_analyzer,
        tool_registry=tool_registry,
        process_logger=process_logger
    )
    
    # Test query that triggers an error
    query = "Error: This should trigger an error response"
    
    try:
        # Query the Enhanced LangGraph RAG Agent
        response = await agent.query(
            query=query,
            stream=False
        )
        
        # If we get here, the agent handled the error
        assert response is not None
        assert "query" in response
        assert "answer" in response
        assert "Error" in response["answer"]
    except Exception as e:
        # The test fails if an unhandled exception is raised
        assert False, f"Enhanced LangGraph RAG Agent did not handle the error: {str(e)}"

================
File: tests/integration/test_langgraph_rag_integration.py
================
"""
Integration test for the LangGraph RAG Agent
"""
import pytest
import logging
from typing import Dict, Any, List

from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient

# Configure logging
logger = logging.getLogger("tests.integration.test_langgraph_rag_integration")

@pytest.mark.asyncio
async def test_langgraph_rag_agent_initialization():
    """Test that the LangGraph RAG Agent can be initialized"""
    # Initialize components
    vector_store = VectorStore()
    ollama_client = OllamaClient()
    
    # Initialize LangGraph RAG Agent
    langgraph_rag_agent = LangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client
    )
    
    # Verify that the agent was initialized
    assert langgraph_rag_agent is not None
    assert langgraph_rag_agent.graph is not None
    assert langgraph_rag_agent.vector_store is not None
    assert langgraph_rag_agent.ollama_client is not None
    assert langgraph_rag_agent.chunking_judge is not None
    assert langgraph_rag_agent.retrieval_judge is not None
    assert langgraph_rag_agent.semantic_chunker is not None

@pytest.mark.asyncio
async def test_langgraph_rag_agent_query():
    """Test that the LangGraph RAG Agent can process a query"""
    # Initialize components with mock ollama client to avoid actual LLM calls
    vector_store = VectorStore()
    
    # Create a mock OllamaClient that returns predefined responses
    class MockOllamaClient:
        async def generate(self, prompt, model=None, system_prompt=None, stream=False, parameters=None):
            if stream:
                # For streaming, return an async generator
                async def mock_stream():
                    for token in ["This", " is", " a", " mock", " response", "."] if not prompt.startswith("Error") else ["Error"]:
                        yield token
                return mock_stream()
            else:
                # For non-streaming, return a dict with the response
                return {"response": "This is a mock response." if not prompt.startswith("Error") else "Error"}
    
    ollama_client = MockOllamaClient()
    
    # Initialize LangGraph RAG Agent with mock client
    langgraph_rag_agent = LangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client
    )
    
    # Test query
    query = "What are the key features of the Metis RAG system?"
    
    # Query the LangGraph RAG Agent
    response = await langgraph_rag_agent.query(
        query=query,
        stream=False
    )
    
    # Verify the response structure
    assert response is not None
    assert "query" in response
    assert response["query"] == query
    assert "answer" in response or "stream" in response
    
    # If not streaming, verify the answer
    if "answer" in response:
        assert response["answer"] == "This is a mock response."
    
    # Verify sources are included (even if empty)
    assert "sources" in response

@pytest.mark.asyncio
async def test_langgraph_rag_agent_error_handling():
    """Test that the LangGraph RAG Agent handles errors gracefully"""
    # Initialize components with mock ollama client that raises exceptions
    vector_store = VectorStore()
    
    # Create a mock OllamaClient that raises exceptions
    class ErrorOllamaClient:
        async def generate(self, prompt, model=None, system_prompt=None, stream=False, parameters=None):
            if prompt.startswith("Error"):
                raise Exception("Mock error")
            
            if stream:
                # For streaming, return an async generator
                async def mock_stream():
                    for token in ["This", " is", " a", " mock", " response", "."]:
                        yield token
                return mock_stream()
            else:
                # For non-streaming, return a dict with the response
                return {"response": "This is a mock response."}
    
    ollama_client = ErrorOllamaClient()
    
    # Initialize LangGraph RAG Agent with error client
    langgraph_rag_agent = LangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client
    )
    
    # Test query that triggers an error
    query = "Error: This should trigger an exception"
    
    try:
        # Query the LangGraph RAG Agent
        response = await langgraph_rag_agent.query(
            query=query,
            stream=False
        )
        
        # If we get here, the agent handled the error
        assert response is not None
        assert "query" in response
        assert "error" in response or "answer" in response
    except Exception as e:
        # The test fails if an unhandled exception is raised
        assert False, f"LangGraph RAG Agent did not handle the error: {str(e)}"

================
File: tests/integration/test_permissions_db.py
================
#!/usr/bin/env python3
"""
Integration tests for database-level permissions (Row Level Security)
"""

import pytest
import uuid
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

from app.db.dependencies import get_db
from app.db.repositories.user_repository import UserRepository
from app.db.repositories.document_repository import DocumentRepository
from app.models.user import UserCreate
from app.models.document import DocumentCreate, DocumentPermissionCreate


@pytest.fixture
def event_loop():
    """Create an event loop for each test"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
async def test_users():
    """Create test users in the database"""
    # Get database session
    db = await anext(get_db())
    
    try:
        # Create user repository
        user_repository = UserRepository(db)
        
        # Create owner user
        owner_data = {
            "username": f"owner_{uuid.uuid4().hex[:8]}",
            "email": f"owner_{uuid.uuid4().hex[:8]}@example.com",
            "password": "testpassword123",
            "full_name": "Owner User",
            "is_active": True,
            "is_admin": False
        }
        owner_user = await user_repository.create_user(UserCreate(**owner_data))
        
        # Create collaborator user
        collaborator_data = {
            "username": f"collaborator_{uuid.uuid4().hex[:8]}",
            "email": f"collaborator_{uuid.uuid4().hex[:8]}@example.com",
            "password": "testpassword123",
            "full_name": "Collaborator User",
            "is_active": True,
            "is_admin": False
        }
        collaborator_user = await user_repository.create_user(UserCreate(**collaborator_data))
        
        # Create unrelated user
        unrelated_data = {
            "username": f"unrelated_{uuid.uuid4().hex[:8]}",
            "email": f"unrelated_{uuid.uuid4().hex[:8]}@example.com",
            "password": "testpassword123",
            "full_name": "Unrelated User",
            "is_active": True,
            "is_admin": False
        }
        unrelated_user = await user_repository.create_user(UserCreate(**unrelated_data))
        
        # Return users
        yield owner_user, collaborator_user, unrelated_user
        
        # Clean up - delete users
        await user_repository.delete_user(owner_user.id)
        await user_repository.delete_user(collaborator_user.id)
        await user_repository.delete_user(unrelated_user.id)
    finally:
        await db.close()


@pytest.fixture
async def test_documents(test_users):
    """Create test documents in the database"""
    owner_user, collaborator_user, unrelated_user = test_users
    
    # Get database session
    db = await anext(get_db())
    
    try:
        # Set database context to owner user
        await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
        
        # Create document repository
        document_repository = DocumentRepository(db)
        
        # Create private document
        private_doc_data = {
            "title": "Private Document",
            "content": "This is a private document",
            "user_id": owner_user.id,
            "is_public": False
        }
        private_doc = await document_repository.create_document(DocumentCreate(**private_doc_data))
        
        # Create public document
        public_doc_data = {
            "title": "Public Document",
            "content": "This is a public document",
            "user_id": owner_user.id,
            "is_public": True
        }
        public_doc = await document_repository.create_document(DocumentCreate(**public_doc_data))
        
        # Create shared document (read permission)
        shared_read_doc_data = {
            "title": "Shared Read Document",
            "content": "This is a document shared with read permission",
            "user_id": owner_user.id,
            "is_public": False
        }
        shared_read_doc = await document_repository.create_document(DocumentCreate(**shared_read_doc_data))
        
        # Create shared document (write permission)
        shared_write_doc_data = {
            "title": "Shared Write Document",
            "content": "This is a document shared with write permission",
            "user_id": owner_user.id,
            "is_public": False
        }
        shared_write_doc = await document_repository.create_document(DocumentCreate(**shared_write_doc_data))
        
        # Create shared document (admin permission)
        shared_admin_doc_data = {
            "title": "Shared Admin Document",
            "content": "This is a document shared with admin permission",
            "user_id": owner_user.id,
            "is_public": False
        }
        shared_admin_doc = await document_repository.create_document(DocumentCreate(**shared_admin_doc_data))
        
        # Share documents with collaborator
        read_permission = DocumentPermissionCreate(
            document_id=shared_read_doc.id,
            user_id=collaborator_user.id,
            permission_level="read"
        )
        await document_repository.create_document_permission(read_permission)
        
        write_permission = DocumentPermissionCreate(
            document_id=shared_write_doc.id,
            user_id=collaborator_user.id,
            permission_level="write"
        )
        await document_repository.create_document_permission(write_permission)
        
        admin_permission = DocumentPermissionCreate(
            document_id=shared_admin_doc.id,
            user_id=collaborator_user.id,
            permission_level="admin"
        )
        await document_repository.create_document_permission(admin_permission)
        
        # Return documents
        yield {
            "private_doc": private_doc,
            "public_doc": public_doc,
            "shared_read_doc": shared_read_doc,
            "shared_write_doc": shared_write_doc,
            "shared_admin_doc": shared_admin_doc
        }
        
        # Clean up - delete documents
        await document_repository.delete_document(private_doc.id)
        await document_repository.delete_document(public_doc.id)
        await document_repository.delete_document(shared_read_doc.id)
        await document_repository.delete_document(shared_write_doc.id)
        await document_repository.delete_document(shared_admin_doc.id)
    finally:
        # Reset database context
        await db.execute(text("SET app.current_user_id = NULL"))
        await db.close()


class TestDatabasePermissions:
    """Tests for database-level permissions (Row Level Security)"""
    
    @pytest.mark.asyncio
    async def test_owner_access(self, test_users, test_documents):
        """Test that the owner can access their own documents"""
        owner_user, _, _ = test_users
        
        # Get database session
        db = await anext(get_db())
        
        try:
            # Set database context to owner user
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            
            # Create document repository
            document_repository = DocumentRepository(db)
            
            # Get all documents
            documents = await document_repository.get_all_documents()
            
            # Owner should see all their documents
            assert len(documents) == 5
            
            # Check specific documents
            private_doc = await document_repository.get_document_by_id(test_documents["private_doc"].id)
            assert private_doc is not None
            assert private_doc.title == "Private Document"
            
            public_doc = await document_repository.get_document_by_id(test_documents["public_doc"].id)
            assert public_doc is not None
            assert public_doc.title == "Public Document"
            
            shared_read_doc = await document_repository.get_document_by_id(test_documents["shared_read_doc"].id)
            assert shared_read_doc is not None
            assert shared_read_doc.title == "Shared Read Document"
            
            # Test update access
            updated_title = "Updated Private Document"
            updated_doc = await document_repository.update_document(
                test_documents["private_doc"].id,
                {"title": updated_title}
            )
            assert updated_doc is not None
            assert updated_doc.title == updated_title
            
            # Test delete access
            temp_doc_data = {
                "title": "Temporary Document",
                "content": "This document will be deleted",
                "user_id": owner_user.id,
                "is_public": False
            }
            temp_doc = await document_repository.create_document(DocumentCreate(**temp_doc_data))
            
            # Delete the document
            delete_result = await document_repository.delete_document(temp_doc.id)
            assert delete_result is True
            
            # Verify it's deleted
            deleted_doc = await document_repository.get_document_by_id(temp_doc.id)
            assert deleted_doc is None
            
        finally:
            # Reset database context
            await db.execute(text("SET app.current_user_id = NULL"))
            await db.close()
    
    @pytest.mark.asyncio
    async def test_public_document_access(self, test_users, test_documents):
        """Test that any user can access public documents"""
        _, _, unrelated_user = test_users
        
        # Get database session
        db = await anext(get_db())
        
        try:
            # Set database context to unrelated user
            await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
            
            # Create document repository
            document_repository = DocumentRepository(db)
            
            # Get all documents
            documents = await document_repository.get_all_documents()
            
            # Unrelated user should only see public documents
            assert len(documents) == 1
            assert documents[0].title == "Public Document"
            
            # Check specific documents
            private_doc = await document_repository.get_document_by_id(test_documents["private_doc"].id)
            assert private_doc is None  # Should not be accessible
            
            public_doc = await document_repository.get_document_by_id(test_documents["public_doc"].id)
            assert public_doc is not None
            assert public_doc.title == "Public Document"
            
            shared_read_doc = await document_repository.get_document_by_id(test_documents["shared_read_doc"].id)
            assert shared_read_doc is None  # Should not be accessible
            
            # Test update access (should fail)
            try:
                updated_doc = await document_repository.update_document(
                    test_documents["public_doc"].id,
                    {"title": "Attempted Update"}
                )
                assert False, "Should not be able to update public document"
            except Exception:
                # Expected to fail
                pass
            
            # Verify document was not updated
            public_doc = await document_repository.get_document_by_id(test_documents["public_doc"].id)
            assert public_doc.title == "Public Document"
            
        finally:
            # Reset database context
            await db.execute(text("SET app.current_user_id = NULL"))
            await db.close()
    
    @pytest.mark.asyncio
    async def test_shared_document_access(self, test_users, test_documents):
        """Test access to documents shared with different permission levels"""
        _, collaborator_user, _ = test_users
        
        # Get database session
        db = await anext(get_db())
        
        try:
            # Set database context to collaborator user
            await db.execute(text(f"SET app.current_user_id = '{collaborator_user.id}'"))
            
            # Create document repository
            document_repository = DocumentRepository(db)
            
            # Get all documents
            documents = await document_repository.get_all_documents()
            
            # Collaborator should see public document and shared documents
            assert len(documents) == 4  # Public + 3 shared docs
            
            # Check specific documents
            private_doc = await document_repository.get_document_by_id(test_documents["private_doc"].id)
            assert private_doc is None  # Should not be accessible
            
            public_doc = await document_repository.get_document_by_id(test_documents["public_doc"].id)
            assert public_doc is not None
            assert public_doc.title == "Public Document"
            
            shared_read_doc = await document_repository.get_document_by_id(test_documents["shared_read_doc"].id)
            assert shared_read_doc is not None
            assert shared_read_doc.title == "Shared Read Document"
            
            shared_write_doc = await document_repository.get_document_by_id(test_documents["shared_write_doc"].id)
            assert shared_write_doc is not None
            assert shared_write_doc.title == "Shared Write Document"
            
            shared_admin_doc = await document_repository.get_document_by_id(test_documents["shared_admin_doc"].id)
            assert shared_admin_doc is not None
            assert shared_admin_doc.title == "Shared Admin Document"
            
            # Test update access on read-only document (should fail)
            try:
                updated_doc = await document_repository.update_document(
                    test_documents["shared_read_doc"].id,
                    {"title": "Attempted Update"}
                )
                assert False, "Should not be able to update read-only document"
            except Exception:
                # Expected to fail
                pass
            
            # Test update access on write-permission document (should succeed)
            updated_title = "Updated Shared Write Document"
            updated_doc = await document_repository.update_document(
                test_documents["shared_write_doc"].id,
                {"title": updated_title}
            )
            assert updated_doc is not None
            assert updated_doc.title == updated_title
            
            # Test update access on admin-permission document (should succeed)
            updated_title = "Updated Shared Admin Document"
            updated_doc = await document_repository.update_document(
                test_documents["shared_admin_doc"].id,
                {"title": updated_title}
            )
            assert updated_doc is not None
            assert updated_doc.title == updated_title
            
        finally:
            # Reset database context
            await db.execute(text("SET app.current_user_id = NULL"))
            await db.close()
    
    @pytest.mark.asyncio
    async def test_document_permission_management(self, test_users, test_documents):
        """Test creating, updating, and revoking document permissions"""
        owner_user, collaborator_user, unrelated_user = test_users
        
        # Get database session
        db = await anext(get_db())
        
        try:
            # Set database context to owner user
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            
            # Create document repository
            document_repository = DocumentRepository(db)
            
            # Create a new document for testing permissions
            test_doc_data = {
                "title": "Permission Test Document",
                "content": "This document is for testing permissions",
                "user_id": owner_user.id,
                "is_public": False
            }
            test_doc = await document_repository.create_document(DocumentCreate(**test_doc_data))
            
            # Initially, unrelated user should not be able to access the document
            await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
            unrelated_access = await document_repository.get_document_by_id(test_doc.id)
            assert unrelated_access is None
            
            # Grant read permission to unrelated user
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            read_permission = DocumentPermissionCreate(
                document_id=test_doc.id,
                user_id=unrelated_user.id,
                permission_level="read"
            )
            await document_repository.create_document_permission(read_permission)
            
            # Now unrelated user should be able to read the document
            await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
            unrelated_access = await document_repository.get_document_by_id(test_doc.id)
            assert unrelated_access is not None
            assert unrelated_access.title == "Permission Test Document"
            
            # But should not be able to update it
            try:
                updated_doc = await document_repository.update_document(
                    test_doc.id,
                    {"title": "Attempted Update"}
                )
                assert False, "Should not be able to update with read permission"
            except Exception:
                # Expected to fail
                pass
            
            # Upgrade to write permission
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            write_permission = DocumentPermissionCreate(
                document_id=test_doc.id,
                user_id=unrelated_user.id,
                permission_level="write"
            )
            # This will update the existing permission
            await document_repository.create_document_permission(write_permission)
            
            # Now unrelated user should be able to update the document
            await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
            updated_title = "Updated by Unrelated User"
            updated_doc = await document_repository.update_document(
                test_doc.id,
                {"title": updated_title}
            )
            assert updated_doc is not None
            assert updated_doc.title == updated_title
            
            # Revoke permission
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            await document_repository.delete_document_permission(test_doc.id, unrelated_user.id)
            
            # Now unrelated user should not be able to access the document again
            await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
            unrelated_access = await document_repository.get_document_by_id(test_doc.id)
            assert unrelated_access is None
            
            # Clean up - delete test document
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            await document_repository.delete_document(test_doc.id)
            
        finally:
            # Reset database context
            await db.execute(text("SET app.current_user_id = NULL"))
            await db.close()
    
    @pytest.mark.asyncio
    async def test_document_chunks_access(self, test_users, test_documents):
        """Test that chunk access follows document access permissions"""
        owner_user, collaborator_user, unrelated_user = test_users
        
        # Get database session
        db = await anext(get_db())
        
        try:
            # Set database context to owner user
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            
            # Create document repository
            document_repository = DocumentRepository(db)
            
            # Create a document with chunks
            test_doc_data = {
                "title": "Document with Chunks",
                "content": "This document has multiple chunks for testing",
                "user_id": owner_user.id,
                "is_public": False
            }
            test_doc = await document_repository.create_document(DocumentCreate(**test_doc_data))
            
            # Add chunks to the document (directly using SQL for simplicity)
            for i in range(3):
                await db.execute(text(
                    f"""
                    INSERT INTO chunks (id, document_id, content, chunk_index, metadata)
                    VALUES ('{uuid.uuid4()}', '{test_doc.id}', 'Chunk {i} content', {i}, '{{"index": {i}}}'::jsonb)
                    """
                ))
            await db.commit()
            
            # Owner should see all chunks
            chunks_query = text(f"SELECT * FROM chunks WHERE document_id = '{test_doc.id}'")
            result = await db.execute(chunks_query)
            owner_chunks = result.fetchall()
            assert len(owner_chunks) == 3
            
            # Unrelated user should not see any chunks
            await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
            result = await db.execute(chunks_query)
            unrelated_chunks = result.fetchall()
            assert len(unrelated_chunks) == 0
            
            # Share document with collaborator
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            read_permission = DocumentPermissionCreate(
                document_id=test_doc.id,
                user_id=collaborator_user.id,
                permission_level="read"
            )
            await document_repository.create_document_permission(read_permission)
            
            # Collaborator should now see all chunks
            await db.execute(text(f"SET app.current_user_id = '{collaborator_user.id}'"))
            result = await db.execute(chunks_query)
            collaborator_chunks = result.fetchall()
            assert len(collaborator_chunks) == 3
            
            # Make document public
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            await document_repository.update_document(test_doc.id, {"is_public": True})
            
            # Now unrelated user should see all chunks
            await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
            result = await db.execute(chunks_query)
            unrelated_chunks = result.fetchall()
            assert len(unrelated_chunks) == 3
            
            # Clean up - delete test document
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            await document_repository.delete_document(test_doc.id)
            
        finally:
            # Reset database context
            await db.execute(text("SET app.current_user_id = NULL"))
            await db.close()


if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/integration/test_permissions_vector.py
================
#!/usr/bin/env python3
"""
Integration tests for vector database security (metadata filtering)
"""

import pytest
import uuid
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
import json
import os
from pathlib import Path
import tempfile
import shutil

from app.db.dependencies import get_db
from app.db.repositories.user_repository import UserRepository
from app.db.repositories.document_repository import DocumentRepository
from app.models.user import UserCreate
from app.models.document import DocumentCreate, DocumentPermissionCreate
from app.rag.vector_store import VectorStore
from app.core.security import get_password_hash


@pytest.fixture
def event_loop():
    """Create an event loop for each test"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
async def test_users():
    """Create test users in the database"""
    # Get database session
    db = await anext(get_db())
    
    try:
        # Create user repository
        user_repository = UserRepository(db)
        
        # Create owner user
        owner_data = {
            "username": f"owner_{uuid.uuid4().hex[:8]}",
            "email": f"owner_{uuid.uuid4().hex[:8]}@example.com",
            "password": "testpassword123",
            "full_name": "Owner User",
            "is_active": True,
            "is_admin": False
        }
        owner_user = await user_repository.create_user(UserCreate(**owner_data))
        
        # Create collaborator user
        collaborator_data = {
            "username": f"collaborator_{uuid.uuid4().hex[:8]}",
            "email": f"collaborator_{uuid.uuid4().hex[:8]}@example.com",
            "password": "testpassword123",
            "full_name": "Collaborator User",
            "is_active": True,
            "is_admin": False
        }
        collaborator_user = await user_repository.create_user(UserCreate(**collaborator_data))
        
        # Create unrelated user
        unrelated_data = {
            "username": f"unrelated_{uuid.uuid4().hex[:8]}",
            "email": f"unrelated_{uuid.uuid4().hex[:8]}@example.com",
            "password": "testpassword123",
            "full_name": "Unrelated User",
            "is_active": True,
            "is_admin": False
        }
        unrelated_user = await user_repository.create_user(UserCreate(**unrelated_data))
        
        # Return users
        yield owner_user, collaborator_user, unrelated_user
        
        # Clean up - delete users
        await user_repository.delete_user(owner_user.id)
        await user_repository.delete_user(collaborator_user.id)
        await user_repository.delete_user(unrelated_user.id)
    finally:
        await db.close()


@pytest.fixture
async def test_vector_store():
    """Create a temporary vector store for testing"""
    # Create a temporary directory for the vector store
    temp_dir = tempfile.mkdtemp()
    vector_store_path = Path(temp_dir) / "test_vector_store"
    
    # Create vector store
    vector_store = VectorStore(str(vector_store_path))
    await vector_store.initialize()
    
    yield vector_store
    
    # Clean up
    await vector_store.close()
    shutil.rmtree(temp_dir)


@pytest.fixture
async def test_documents_with_vectors(test_users, test_vector_store):
    """Create test documents with vector embeddings"""
    owner_user, collaborator_user, unrelated_user = test_users
    
    # Get database session
    db = await anext(get_db())
    
    try:
        # Set database context to owner user
        await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
        
        # Create document repository
        document_repository = DocumentRepository(db)
        
        # Create documents
        documents = []
        
        # Private document
        private_doc_data = {
            "title": "Private Vector Document",
            "content": "This is a private document with vector embeddings",
            "user_id": owner_user.id,
            "is_public": False
        }
        private_doc = await document_repository.create_document(DocumentCreate(**private_doc_data))
        documents.append(private_doc)
        
        # Public document
        public_doc_data = {
            "title": "Public Vector Document",
            "content": "This is a public document with vector embeddings",
            "user_id": owner_user.id,
            "is_public": True
        }
        public_doc = await document_repository.create_document(DocumentCreate(**public_doc_data))
        documents.append(public_doc)
        
        # Shared document
        shared_doc_data = {
            "title": "Shared Vector Document",
            "content": "This is a document shared with collaborator with vector embeddings",
            "user_id": owner_user.id,
            "is_public": False
        }
        shared_doc = await document_repository.create_document(DocumentCreate(**shared_doc_data))
        documents.append(shared_doc)
        
        # Share document with collaborator
        share_permission = DocumentPermissionCreate(
            document_id=shared_doc.id,
            user_id=collaborator_user.id,
            permission_level="read"
        )
        await document_repository.create_document_permission(share_permission)
        
        # Collaborator's document
        collab_doc_data = {
            "title": "Collaborator Vector Document",
            "content": "This is a document owned by collaborator with vector embeddings",
            "user_id": collaborator_user.id,
            "is_public": False
        }
        # Set context to collaborator to create their document
        await db.execute(text(f"SET app.current_user_id = '{collaborator_user.id}'"))
        collab_doc = await document_repository.create_document(DocumentCreate(**collab_doc_data))
        documents.append(collab_doc)
        
        # Unrelated user's document
        unrelated_doc_data = {
            "title": "Unrelated Vector Document",
            "content": "This is a document owned by unrelated user with vector embeddings",
            "user_id": unrelated_user.id,
            "is_public": False
        }
        # Set context to unrelated user to create their document
        await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
        unrelated_doc = await document_repository.create_document(DocumentCreate(**unrelated_doc_data))
        documents.append(unrelated_doc)
        
        # Add vector embeddings for each document
        for doc in documents:
            # Create chunks with vector embeddings
            chunks = []
            for i in range(2):  # 2 chunks per document
                chunk_id = str(uuid.uuid4())
                chunk_content = f"Chunk {i} of document {doc.title}"
                chunk_embedding = [0.1] * 384  # Simple mock embedding
                
                # Create metadata with security information
                metadata = {
                    "document_id": doc.id,
                    "user_id": doc.user_id,
                    "is_public": doc.is_public,
                    "chunk_index": i,
                    "title": doc.title
                }
                
                # Add to vector store
                await test_vector_store.add_texts(
                    texts=[chunk_content],
                    metadatas=[metadata],
                    ids=[chunk_id]
                )
                
                chunks.append({
                    "id": chunk_id,
                    "content": chunk_content,
                    "metadata": metadata
                })
            
            # Add chunks to document
            doc.chunks = chunks
        
        # Return documents
        yield {
            "private_doc": private_doc,
            "public_doc": public_doc,
            "shared_doc": shared_doc,
            "collab_doc": collab_doc,
            "unrelated_doc": unrelated_doc
        }
        
        # Clean up - delete documents
        await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
        await document_repository.delete_document(private_doc.id)
        await document_repository.delete_document(public_doc.id)
        await document_repository.delete_document(shared_doc.id)
        
        await db.execute(text(f"SET app.current_user_id = '{collaborator_user.id}'"))
        await document_repository.delete_document(collab_doc.id)
        
        await db.execute(text(f"SET app.current_user_id = '{unrelated_user.id}'"))
        await document_repository.delete_document(unrelated_doc.id)
    finally:
        # Reset database context
        await db.execute(text("SET app.current_user_id = NULL"))
        await db.close()


class TestVectorDatabaseSecurity:
    """Tests for vector database security (metadata filtering)"""
    
    @pytest.mark.asyncio
    async def test_owner_vector_search(self, test_users, test_documents_with_vectors, test_vector_store):
        """Test that owner can search their own documents"""
        owner_user, _, _ = test_users
        docs = test_documents_with_vectors
        
        # Search as owner
        results = await test_vector_store.similarity_search(
            query="vector document",
            user_id=owner_user.id,
            k=10
        )
        
        # Owner should see their own documents (private, public, shared)
        assert len(results) >= 6  # At least 6 chunks (3 docs * 2 chunks)
        
        # Check document IDs in results
        result_doc_ids = {doc.metadata["document_id"] for doc in results}
        assert docs["private_doc"].id in result_doc_ids
        assert docs["public_doc"].id in result_doc_ids
        assert docs["shared_doc"].id in result_doc_ids
    
    @pytest.mark.asyncio
    async def test_collaborator_vector_search(self, test_users, test_documents_with_vectors, test_vector_store):
        """Test that collaborator can search their own and shared documents"""
        _, collaborator_user, _ = test_users
        docs = test_documents_with_vectors
        
        # Search as collaborator
        results = await test_vector_store.similarity_search(
            query="vector document",
            user_id=collaborator_user.id,
            k=10
        )
        
        # Collaborator should see their own documents, shared documents, and public documents
        assert len(results) >= 6  # At least 6 chunks (3 docs * 2 chunks)
        
        # Check document IDs in results
        result_doc_ids = {doc.metadata["document_id"] for doc in results}
        assert docs["collab_doc"].id in result_doc_ids  # Own document
        assert docs["shared_doc"].id in result_doc_ids  # Shared document
        assert docs["public_doc"].id in result_doc_ids  # Public document
        assert docs["private_doc"].id not in result_doc_ids  # Should not see owner's private document
        assert docs["unrelated_doc"].id not in result_doc_ids  # Should not see unrelated user's document
    
    @pytest.mark.asyncio
    async def test_unrelated_user_vector_search(self, test_users, test_documents_with_vectors, test_vector_store):
        """Test that unrelated user can only search public documents"""
        _, _, unrelated_user = test_users
        docs = test_documents_with_vectors
        
        # Search as unrelated user
        results = await test_vector_store.similarity_search(
            query="vector document",
            user_id=unrelated_user.id,
            k=10
        )
        
        # Unrelated user should see their own documents and public documents
        assert len(results) >= 4  # At least 4 chunks (2 docs * 2 chunks)
        
        # Check document IDs in results
        result_doc_ids = {doc.metadata["document_id"] for doc in results}
        assert docs["unrelated_doc"].id in result_doc_ids  # Own document
        assert docs["public_doc"].id in result_doc_ids  # Public document
        assert docs["private_doc"].id not in result_doc_ids  # Should not see owner's private document
        assert docs["shared_doc"].id not in result_doc_ids  # Should not see shared document
        assert docs["collab_doc"].id not in result_doc_ids  # Should not see collaborator's document
    
    @pytest.mark.asyncio
    async def test_post_retrieval_filtering(self, test_users, test_documents_with_vectors, test_vector_store):
        """Test post-retrieval filtering for additional security"""
        owner_user, collaborator_user, _ = test_users
        docs = test_documents_with_vectors
        
        # Simulate a scenario where metadata filtering might miss something
        # by directly adding a document with incorrect metadata
        
        # Create a document with incorrect metadata (missing user_id)
        bad_chunk_id = str(uuid.uuid4())
        bad_chunk_content = "This chunk has incorrect metadata"
        bad_metadata = {
            "document_id": docs["private_doc"].id,
            # Missing user_id field
            "is_public": False,
            "chunk_index": 99,
            "title": "Bad Metadata Document"
        }
        
        # Add to vector store
        await test_vector_store.add_texts(
            texts=[bad_chunk_content],
            metadatas=[bad_metadata],
            ids=[bad_chunk_id]
        )
        
        # Search as collaborator
        results = await test_vector_store.similarity_search(
            query="incorrect metadata",
            user_id=collaborator_user.id,
            k=10
        )
        
        # The bad chunk should be filtered out in post-retrieval filtering
        for doc in results:
            assert doc.page_content != bad_chunk_content
            
        # Double check by searching for the exact content
        exact_results = await test_vector_store.similarity_search(
            query="This chunk has incorrect metadata",
            user_id=collaborator_user.id,
            k=10
        )
        
        # Should not find the bad chunk
        for doc in exact_results:
            assert doc.page_content != bad_chunk_content
    
    @pytest.mark.asyncio
    async def test_password_reset_vector_persistence(self, test_users, test_documents_with_vectors, test_vector_store):
        """Test that vector metadata links persist after password reset"""
        owner_user, _, _ = test_users
        docs = test_documents_with_vectors
        
        # Initial search as owner
        initial_results = await test_vector_store.similarity_search(
            query="vector document",
            user_id=owner_user.id,
            k=10
        )
        
        # Get initial document IDs
        initial_doc_ids = {doc.metadata["document_id"] for doc in initial_results}
        
        # Simulate password reset by updating password hash
        db = await anext(get_db())
        try:
            user_repository = UserRepository(db)
            new_password = "newpassword456"
            new_password_hash = get_password_hash(new_password)
            await user_repository.update_user(owner_user.id, {"password_hash": new_password_hash})
            
            # Get updated user
            updated_user = await user_repository.get_by_id(owner_user.id)
            
            # Search again with updated user
            post_reset_results = await test_vector_store.similarity_search(
                query="vector document",
                user_id=updated_user.id,
                k=10
            )
            
            # Get post-reset document IDs
            post_reset_doc_ids = {doc.metadata["document_id"] for doc in post_reset_results}
            
            # Should see the same documents
            assert initial_doc_ids == post_reset_doc_ids
            
        finally:
            await db.close()
    
    @pytest.mark.asyncio
    async def test_cross_user_access_prevention(self, test_users, test_documents_with_vectors, test_vector_store):
        """Test prevention of cross-user access in vector search"""
        owner_user, _, unrelated_user = test_users
        docs = test_documents_with_vectors
        
        # Try to access owner's private document by directly searching for its content
        private_doc_query = "This is a private document with vector embeddings"
        
        # Search as unrelated user
        results = await test_vector_store.similarity_search(
            query=private_doc_query,
            user_id=unrelated_user.id,
            k=10
        )
        
        # Should not find owner's private document
        for doc in results:
            assert doc.metadata["document_id"] != docs["private_doc"].id
        
        # Now search as owner
        owner_results = await test_vector_store.similarity_search(
            query=private_doc_query,
            user_id=owner_user.id,
            k=10
        )
        
        # Should find owner's private document
        found_private = False
        for doc in owner_results:
            if doc.metadata["document_id"] == docs["private_doc"].id:
                found_private = True
                break
        
        assert found_private, "Owner should be able to find their private document"
    
    @pytest.mark.asyncio
    async def test_document_sharing_vector_access(self, test_users, test_documents_with_vectors, test_vector_store):
        """Test that document sharing affects vector search results"""
        owner_user, collaborator_user, unrelated_user = test_users
        docs = test_documents_with_vectors
        
        # Get database session
        db = await anext(get_db())
        
        try:
            # Set database context to owner user
            await db.execute(text(f"SET app.current_user_id = '{owner_user.id}'"))
            
            # Create document repository
            document_repository = DocumentRepository(db)
            
            # Initial search as unrelated user
            initial_results = await test_vector_store.similarity_search(
                query="private vector document",
                user_id=unrelated_user.id,
                k=10
            )
            
            # Should not find owner's private document
            initial_doc_ids = {doc.metadata["document_id"] for doc in initial_results}
            assert docs["private_doc"].id not in initial_doc_ids
            
            # Share private document with unrelated user
            share_permission = DocumentPermissionCreate(
                document_id=docs["private_doc"].id,
                user_id=unrelated_user.id,
                permission_level="read"
            )
            await document_repository.create_document_permission(share_permission)
            
            # Search again as unrelated user
            post_share_results = await test_vector_store.similarity_search(
                query="private vector document",
                user_id=unrelated_user.id,
                k=10
            )
            
            # Now should find owner's private document
            post_share_doc_ids = {doc.metadata["document_id"] for doc in post_share_results}
            assert docs["private_doc"].id in post_share_doc_ids
            
            # Revoke permission
            await document_repository.delete_document_permission(docs["private_doc"].id, unrelated_user.id)
            
            # Search again as unrelated user
            post_revoke_results = await test_vector_store.similarity_search(
                query="private vector document",
                user_id=unrelated_user.id,
                k=10
            )
            
            # Should not find owner's private document again
            post_revoke_doc_ids = {doc.metadata["document_id"] for doc in post_revoke_results}
            assert docs["private_doc"].id not in post_revoke_doc_ids
            
        finally:
            # Reset database context
            await db.execute(text("SET app.current_user_id = NULL"))
            await db.close()


if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/integration/test_response_quality_integration.py
================
"""
Integration tests for the response quality pipeline
"""
import pytest
import asyncio
import os
import json
from unittest.mock import MagicMock, AsyncMock, patch

from app.rag.response_quality_pipeline import ResponseQualityPipeline
from app.rag.process_logger import ProcessLogger
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient

class TestResponseQualityIntegration:
    """Integration tests for the response quality pipeline"""
    
    @pytest.fixture
    def process_logger(self):
        """Create a process logger"""
        log_dir = "tests/results/process_logs"
        os.makedirs(log_dir, exist_ok=True)
        return ProcessLogger(log_dir=log_dir)
    
    @pytest.fixture
    def ollama_client(self):
        """Create a mock Ollama client"""
        client = AsyncMock()
        
        # Mock the generate method to return different responses based on the prompt
        async def mock_generate(prompt, **kwargs):
            if "synthesize" in prompt.lower() or "context" in prompt.lower():
                return {
                    "response": "Paris is the capital of France. It is known for its art, culture, and the Eiffel Tower [1]."
                }
            elif "evaluate" in prompt.lower():
                evaluation = {
                    "factual_accuracy": 9,
                    "completeness": 7,
                    "relevance": 9,
                    "hallucination_detected": False,
                    "hallucination_details": "No hallucinations detected.",
                    "overall_score": 8,
                    "strengths": ["Accurate information", "Clear explanation", "Proper citation"],
                    "weaknesses": ["Could be more detailed"],
                    "improvement_suggestions": ["Add more context about Paris"]
                }
                return {"response": json.dumps(evaluation)}
            elif "refine" in prompt.lower():
                refinement = {
                    "refined_response": "Paris is the capital of France. It is known for its art, culture, and the Eiffel Tower [1]. As one of Europe's major centers for art, fashion, and culture, Paris is home to many famous landmarks including the Louvre Museum and Notre-Dame Cathedral.",
                    "improvement_summary": "Added more details about Paris as a cultural center and mentioned additional landmarks."
                }
                return {"response": json.dumps(refinement)}
            elif "analyze" in prompt.lower():
                analysis = {
                    "process_efficiency": {
                        "assessment": "The process was efficient.",
                        "issues_identified": [],
                        "recommendations": []
                    },
                    "retrieval_quality": {
                        "assessment": "Retrieval was effective.",
                        "issues_identified": [],
                        "recommendations": []
                    },
                    "response_quality": {
                        "assessment": "Response was accurate and well-structured.",
                        "issues_identified": [],
                        "recommendations": ["Could add more historical context"]
                    },
                    "overall_assessment": {
                        "strengths": ["Accurate information", "Good citations", "Clear structure"],
                        "weaknesses": ["Limited historical context"],
                        "recommendations": ["Add historical context in future responses"]
                    }
                }
                return {"response": json.dumps(analysis)}
            else:
                return {"response": "Default response"}
        
        client.generate = AsyncMock(side_effect=mock_generate)
        return client
    
    @pytest.fixture
    def vector_store(self):
        """Create a mock vector store"""
        store = AsyncMock()
        
        # Mock the search method to return sample chunks
        async def mock_search(query, **kwargs):
            return [
                {
                    "chunk_id": "chunk1",
                    "content": "Paris is the capital of France. It is known for the Eiffel Tower.",
                    "metadata": {
                        "document_id": "doc1",
                        "filename": "geography.txt",
                        "tags": ["geography", "europe"],
                        "folder": "/"
                    },
                    "distance": 0.1
                },
                {
                    "chunk_id": "chunk2",
                    "content": "France is a country in Western Europe. Its capital is Paris.",
                    "metadata": {
                        "document_id": "doc2",
                        "filename": "countries.txt",
                        "tags": ["geography", "europe"],
                        "folder": "/"
                    },
                    "distance": 0.2
                }
            ]
        
        store.search = AsyncMock(side_effect=mock_search)
        store.get_stats = MagicMock(return_value={"count": 100})
        return store
    
    @pytest.mark.asyncio
    async def test_response_quality_pipeline_standalone(self, ollama_client, process_logger):
        """Test the response quality pipeline as a standalone component"""
        # Create the pipeline
        pipeline = ResponseQualityPipeline(
            llm_provider=ollama_client,
            process_logger=process_logger,
            max_refinement_iterations=2,
            quality_threshold=8.0,
            enable_audit_reports=True
        )
        
        # Create test data
        query = "What is the capital of France?"
        query_id = "test-integration-id"
        context = "[1] Source: geography.txt, Tags: [geography, europe], Folder: /\n\nParis is the capital of France. It is known for the Eiffel Tower."
        sources = [
            {
                "document_id": "doc1",
                "chunk_id": "chunk1",
                "relevance_score": 0.9,
                "excerpt": "Paris is the capital of France. It is known for the Eiffel Tower.",
                "filename": "geography.txt",
                "tags": ["geography", "europe"],
                "folder": "/"
            }
        ]
        
        # Process the query
        result = await pipeline.process(
            query=query,
            context=context,
            sources=sources,
            query_id=query_id
        )
        
        # Check the result
        assert "response" in result
        assert "sources" in result
        assert "evaluation" in result
        assert "refinement_iterations" in result
        assert "execution_time" in result
        
        # Verify the response content
        assert "Paris" in result["response"]
        assert "France" in result["response"]
        
        # Verify the evaluation
        assert result["evaluation"]["overall_score"] >= 8.0
        assert not result["evaluation"]["hallucination_detected"]
        
        # Verify that refinement occurred
        assert "Louvre" in result["response"] or "Notre-Dame" in result["response"]
        
        # Check that the process logger was used
        log_file = os.path.join(process_logger.log_dir, f"query_{query_id}.json")
        assert os.path.exists(log_file)
    
    @pytest.mark.asyncio
    async def test_response_quality_with_rag_engine(self, ollama_client, vector_store, process_logger):
        """Test integrating the response quality pipeline with the RAG engine"""
        # Create the RAG engine
        rag_engine = RAGEngine(
            vector_store=vector_store,
            ollama_client=ollama_client
        )
        
        # Create the pipeline
        pipeline = ResponseQualityPipeline(
            llm_provider=ollama_client,
            process_logger=process_logger,
            max_refinement_iterations=1,
            quality_threshold=8.0,
            enable_audit_reports=True
        )
        
        # Mock the RAG engine's query method to use our pipeline
        original_query = rag_engine.query
        
        async def enhanced_query(query, **kwargs):
            # First, get the standard RAG result
            rag_result = await original_query(query, **kwargs)
            
            # Extract the necessary components
            response = rag_result.get("answer", "")
            sources = rag_result.get("sources", [])
            
            # Convert sources to the format expected by the pipeline
            formatted_sources = []
            for source in sources:
                formatted_sources.append({
                    "document_id": source.document_id,
                    "chunk_id": source.chunk_id,
                    "relevance_score": source.relevance_score,
                    "excerpt": source.excerpt,
                    "filename": source.filename,
                    "tags": source.tags,
                    "folder": source.folder
                })
            
            # Get the context from the RAG engine
            # In a real implementation, this would be extracted from the RAG engine's internal state
            context = "[1] Source: geography.txt, Tags: [geography, europe], Folder: /\n\nParis is the capital of France. It is known for the Eiffel Tower."
            
            # Process through the quality pipeline
            quality_result = await pipeline.process(
                query=query,
                context=context,
                sources=formatted_sources,
                conversation_context=kwargs.get("conversation_history")
            )
            
            # Return an enhanced result
            return {
                "query": query,
                "answer": quality_result["response"],
                "sources": sources,  # Keep the original source format for compatibility
                "evaluation": quality_result["evaluation"],
                "refinement_iterations": quality_result["refinement_iterations"],
                "quality_score": quality_result["evaluation"]["overall_score"]
            }
        
        # Replace the query method
        with patch.object(rag_engine, 'query', side_effect=enhanced_query):
            # Test the enhanced query
            result = await rag_engine.query(
                query="What is the capital of France?",
                model="llama3",
                use_rag=True
            )
            
            # Check the result
            assert "answer" in result
            assert "sources" in result
            assert "evaluation" in result
            assert "refinement_iterations" in result
            assert "quality_score" in result
            
            # Verify the response content
            assert "Paris" in result["answer"]
            assert "France" in result["answer"]
            
            # Verify the evaluation
            assert result["quality_score"] >= 8.0
            
            # Verify that the sources are included
            assert len(result["sources"]) > 0

if __name__ == "__main__":
    pytest.main(["-xvs", "test_response_quality_integration.py"])

================
File: tests/integration/test_retrieval_judge_integration.py
================
"""
Integration tests for the Retrieval Judge with RAG Engine
"""
import pytest
import os
import json
from unittest.mock import AsyncMock, MagicMock, patch

from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.models.document import Document, Chunk


@pytest.fixture
def mock_ollama_client():
    """Create a mock OllamaClient"""
    client = AsyncMock(spec=OllamaClient)
    
    # Mock generate method to return a valid response
    async def mock_generate(prompt, model=None, system_prompt=None, stream=False, parameters=None):
        if "analyze the query" in prompt.lower():
            return {
                "response": json.dumps({
                    "complexity": "moderate",
                    "parameters": {
                        "k": 5,
                        "threshold": 0.5,
                        "reranking": True
                    },
                    "justification": "This is a moderately complex query about machine learning."
                })
            }
        elif "evaluate the relevance" in prompt.lower():
            return {
                "response": json.dumps({
                    "relevance_scores": {
                        "1": 0.9,
                        "2": 0.7,
                        "3": 0.3
                    },
                    "needs_refinement": False,
                    "justification": "The first two chunks are highly relevant to the query."
                })
            }
        elif "refine the user's query" in prompt.lower():
            return {
                "response": "What are the key concepts and applications of machine learning?"
            }
        elif "optimize the assembly" in prompt.lower():
            return {
                "response": json.dumps({
                    "optimized_order": [1, 2],
                    "excluded_chunks": [3],
                    "justification": "Ordered chunks for logical flow and excluded less relevant chunk."
                })
            }
        else:
            return {
                "response": "This is a response from the mock LLM."
            }
    
    client.generate = mock_generate
    
    # Mock create_embedding method
    async def mock_create_embedding(text, model=None):
        # Return a simple mock embedding (10 dimensions)
        return [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    
    client.create_embedding = mock_create_embedding
    
    return client


@pytest.fixture
def mock_vector_store(mock_ollama_client):
    """Create a mock VectorStore"""
    store = AsyncMock(spec=VectorStore)
    
    # Mock get_stats method
    store.get_stats.return_value = {"count": 10, "embeddings_model": "test-model"}
    
    # Mock search method
    async def mock_search(query, top_k=5, filter_criteria=None):
        return [
            {
                "chunk_id": "chunk1",
                "content": "Machine learning is a subset of artificial intelligence that focuses on developing systems that learn from data.",
                "metadata": {
                    "document_id": "doc1",
                    "filename": "ml_basics.md",
                    "tags": "machine learning,ai",
                    "folder": "/tech"
                },
                "distance": 0.1
            },
            {
                "chunk_id": "chunk2",
                "content": "Common machine learning algorithms include linear regression, decision trees, and neural networks.",
                "metadata": {
                    "document_id": "doc1",
                    "filename": "ml_basics.md",
                    "tags": "machine learning,algorithms",
                    "folder": "/tech"
                },
                "distance": 0.2
            },
            {
                "chunk_id": "chunk3",
                "content": "Python is a popular programming language for data science and machine learning projects.",
                "metadata": {
                    "document_id": "doc2",
                    "filename": "programming.md",
                    "tags": "python,programming",
                    "folder": "/tech/programming"
                },
                "distance": 0.6
            }
        ]
    
    store.search = mock_search
    
    return store


@pytest.fixture
def retrieval_judge(mock_ollama_client):
    """Create a RetrievalJudge with a mock OllamaClient"""
    return RetrievalJudge(ollama_client=mock_ollama_client, model="test-model")


@pytest.fixture
def rag_engine(mock_vector_store, mock_ollama_client, retrieval_judge):
    """Create a RAGEngine with mock components"""
    with patch('app.rag.rag_engine.USE_RETRIEVAL_JUDGE', True):
        engine = RAGEngine(
            vector_store=mock_vector_store,
            ollama_client=mock_ollama_client,
            retrieval_judge=retrieval_judge
        )
        return engine


class TestRetrievalJudgeIntegration:
    """Integration tests for the RetrievalJudge with RAGEngine"""

    @pytest.mark.asyncio
    async def test_enhanced_retrieval(self, rag_engine):
        """Test the enhanced retrieval method"""
        # Call the enhanced retrieval method
        context, sources, document_ids = await rag_engine._enhanced_retrieval(
            query="What is machine learning?",
            top_k=5
        )
        
        # Verify the results
        assert context  # Context should not be empty
        assert "Machine learning is a subset of artificial intelligence" in context
        assert len(sources) > 0
        assert len(document_ids) > 0
        
        # Verify sources contain the expected information
        assert sources[0]["document_id"] == "doc1"
        assert sources[0]["chunk_id"] == "chunk1"
        assert sources[0]["filename"] == "ml_basics.md"
        
        # Verify document IDs were collected
        assert "doc1" in document_ids

    @pytest.mark.asyncio
    async def test_query_with_retrieval_judge(self, rag_engine):
        """Test the query method with Retrieval Judge enabled"""
        # Call the query method
        result = await rag_engine.query(
            query="What is machine learning?",
            model="test-model",
            use_rag=True,
            stream=False
        )
        
        # Verify the result
        assert "query" in result
        assert "answer" in result
        assert "sources" in result
        assert len(result["sources"]) > 0
        
        # Verify the query was processed
        assert result["query"] == "What is machine learning?"
        
        # Verify the answer is not empty
        assert result["answer"]
        
        # Verify sources are included
        assert result["sources"][0].document_id == "doc1"
        assert result["sources"][0].chunk_id == "chunk1"

    @pytest.mark.asyncio
    async def test_query_with_retrieval_judge_disabled(self, rag_engine):
        """Test the query method with Retrieval Judge disabled"""
        # Temporarily disable the Retrieval Judge
        rag_engine.retrieval_judge = None
        
        # Call the query method
        result = await rag_engine.query(
            query="What is machine learning?",
            model="test-model",
            use_rag=True,
            stream=False
        )
        
        # Verify the result
        assert "query" in result
        assert "answer" in result
        assert "sources" in result
        assert len(result["sources"]) > 0
        
        # Verify the query was processed
        assert result["query"] == "What is machine learning?"
        
        # Verify the answer is not empty
        assert result["answer"]
        
        # Verify sources are included
        assert result["sources"][0].document_id == "doc1"

    @pytest.mark.asyncio
    async def test_query_with_rag_disabled(self, rag_engine):
        """Test the query method with RAG disabled"""
        # Call the query method with use_rag=False
        result = await rag_engine.query(
            query="What is machine learning?",
            model="test-model",
            use_rag=False,
            stream=False
        )
        
        # Verify the result
        assert "query" in result
        assert "answer" in result
        assert "sources" in result
        
        # Verify no sources are included when RAG is disabled
        assert len(result["sources"]) == 0
        
        # Verify the query was processed
        assert result["query"] == "What is machine learning?"
        
        # Verify the answer is not empty
        assert result["answer"]

================
File: tests/integration/test_semantic_chunker_integration.py
================
"""
Integration tests for the Semantic Chunker
"""
import pytest
import os
from unittest.mock import AsyncMock, patch, MagicMock
import tempfile
import shutil

from app.rag.document_processor import DocumentProcessor
from app.rag.chunkers.semantic_chunker import SemanticChunker
from app.models.document import Document
from app.core.config import UPLOAD_DIR

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    # Mock for chunking judge
    client.generate.return_value = {
        "response": """
        {
            "strategy": "semantic",
            "parameters": {
                "chunk_size": 1000,
                "chunk_overlap": 150
            },
            "justification": "This document contains complex concepts that would benefit from semantic chunking to preserve meaning and context."
        }
        """
    }
    return client

@pytest.fixture
def mock_semantic_chunker():
    """Create a mock SemanticChunker for testing"""
    chunker = MagicMock(spec=SemanticChunker)
    
    # Mock the split_documents method
    def mock_split_documents(docs):
        # Create 3 chunks from each document
        result = []
        for doc in docs:
            content = doc.page_content
            chunk_size = len(content) // 3
            for i in range(3):
                start = i * chunk_size
                end = start + chunk_size if i < 2 else len(content)
                chunk_content = content[start:end]
                result.append(doc.__class__(
                    page_content=chunk_content,
                    metadata=doc.metadata.copy()
                ))
        return result
    
    chunker.split_documents.side_effect = mock_split_documents
    return chunker

@pytest.fixture
def sample_document():
    """Create a sample document for testing"""
    return Document(
        id="test-doc-id",
        filename="test_semantic.txt",
        content="""
        This is a complex document with multiple semantic sections.
        
        Section 1: Introduction
        This section introduces the main concepts and provides context.
        
        Section 2: Main Content
        This section contains the core information and detailed explanations.
        
        Section 3: Conclusion
        This section summarizes the key points and provides next steps.
        """
    )

@pytest.fixture
def temp_upload_dir():
    """Create a temporary upload directory for testing"""
    # Create a temporary directory
    temp_dir = tempfile.mkdtemp()
    
    # Create the document directory structure
    doc_dir = os.path.join(temp_dir, "test-doc-id")
    os.makedirs(doc_dir, exist_ok=True)
    
    # Yield the temp directory
    yield temp_dir
    
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.mark.asyncio
async def test_document_processor_with_semantic_chunking(mock_ollama_client, sample_document, temp_upload_dir):
    """Test that the DocumentProcessor correctly uses semantic chunking when recommended by the Chunking Judge"""
    # Create the document file
    doc_path = os.path.join(temp_upload_dir, sample_document.id, sample_document.filename)
    os.makedirs(os.path.dirname(doc_path), exist_ok=True)
    with open(doc_path, 'w') as f:
        f.write(sample_document.content)
    
    # Patch the UPLOAD_DIR config
    with patch('app.rag.document_processor.UPLOAD_DIR', temp_upload_dir):
        # Patch the USE_CHUNKING_JUDGE config to True
        with patch('app.rag.document_processor.USE_CHUNKING_JUDGE', True):
            # Patch the OllamaClient to use our mock
            with patch('app.rag.agents.chunking_judge.OllamaClient', return_value=mock_ollama_client):
                    # Create document processor
                    processor = DocumentProcessor()
                    
                    # Process the document
                    processed_doc = await processor.process_document(sample_document)
                    
                    # Verify the document was processed with semantic chunking
                    assert processed_doc.metadata["chunking_analysis"]["strategy"] == "semantic"
                    assert len(processed_doc.chunks) > 0
                    
                    # Verify the chunking parameters
                    assert processor.chunk_size == 1000
                    assert processor.chunk_overlap == 150

@pytest.mark.asyncio
async def test_semantic_chunker_with_document_processor(mock_ollama_client, mock_semantic_chunker, sample_document, temp_upload_dir):
    """Test that the SemanticChunker integrates correctly with DocumentProcessor"""
    # Create the document file
    doc_path = os.path.join(temp_upload_dir, sample_document.id, sample_document.filename)
    os.makedirs(os.path.dirname(doc_path), exist_ok=True)
    with open(doc_path, 'w') as f:
        f.write(sample_document.content)
    
    # Patch the UPLOAD_DIR config
    with patch('app.rag.document_processor.UPLOAD_DIR', temp_upload_dir):
        # Patch the USE_CHUNKING_JUDGE config to False to use direct strategy
        with patch('app.rag.document_processor.USE_CHUNKING_JUDGE', False):
                # Create document processor with semantic chunking strategy
                processor = DocumentProcessor(chunking_strategy="semantic")
                
                # Process the document
                processed_doc = await processor.process_document(sample_document)
                
                # Verify the document was processed
                assert len(processed_doc.chunks) > 0
                
                # Verify the chunking strategy
                assert processor.chunking_strategy == "semantic"

================
File: tests/results/chunking_judge_results/chunking_judge_real_results.json
================
[
  {
    "filename": "test_document.txt",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 1500,
      "chunk_overlap": 200
    },
    "justification": "The document is formatted with Markdown headers, indicating a clear hierarchical structure. Using the 'markdown' strategy will preserve this structure, ensuring that related sections and subsections are grouped together. A chunk size of 1500 characters allows for substantial content within each chunk while maintaining reasonable size. An overlap of 200 characters helps to avoid abrupt transitions between chunks and ensures context is carried over."
  },
  {
    "filename": "technical_documentation.md",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 1500,
      "chunk_overlap": 200
    },
    "justification": "The document is a markdown file with a well-defined structure using headers (e.g., Introduction, Architecture Overview, Deployment Options, Configuration, API Reference, Performance Considerations).  Using the 'markdown' strategy will allow us to split the document along these logical sections, preserving the context and relationships between different parts of the documentation. A chunk size of 1500 characters allows for a reasonable amount of content within each chunk while still allowing for meaningful sections. An overlap of 200 ensures that context isn't lost between chunks."
  },
  {
    "filename": "test_data.csv",
    "strategy": "recursive",
    "parameters": {
      "chunk_size": 1000,
      "chunk_overlap": 200
    },
    "justification": "The document is a CSV file, which doesn't inherently have a clear header structure suitable for markdown chunking. While it's structured data, the content within each cell (particularly the 'skills' column) can be quite variable in length.  Token-based chunking might be overly granular, as it could split skills lists mid-item. Recursive chunking provides a good balance. A chunk size of 1000 characters allows for capturing a reasonable amount of context from multiple rows, while an overlap of 200 helps maintain continuity between chunks, especially when a row's content spans across two chunks. This approach will allow the RAG system to understand the relationships between employees and their skills."
  }
]

================
File: tests/results/chunking_judge_results/chunking_judge_test_results.json
================
[
  {
    "filename": "test_document.txt",
    "strategy": "recursive",
    "parameters": {
      "chunk_size": 1000,
      "chunk_overlap": 150
    },
    "justification": "This is a plain text document with paragraphs that benefits from recursive chunking."
  },
  {
    "filename": "technical_documentation.md",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 800,
      "chunk_overlap": 100
    },
    "justification": "This is a structured markdown document with clear headers."
  },
  {
    "filename": "test_data.csv",
    "strategy": "token",
    "parameters": {
      "chunk_size": 600,
      "chunk_overlap": 75
    },
    "justification": "This is a CSV file with tabular data that benefits from token-based chunking."
  }
]

================
File: tests/results/quality_results/metis_rag_test_report.json
================
{
  "timestamp": "2025-03-07T10:58:40.732785",
  "test_suites": [
    {
      "name": "RAG Quality Tests",
      "description": "Tests for factual accuracy, relevance, and citation quality",
      "result": {
        "name": "RAG Quality Tests",
        "success": false,
        "duration_seconds": 0.18236899375915527,
        "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.10.16, pytest-8.3.5, pluggy-1.5.0 -- /Users/charleshoward/Metis_RAG/venv_py310/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/charleshoward/Metis_RAG\nconfigfile: pyproject.toml\nplugins: asyncio-0.25.3, anyio-3.7.1\nasyncio: mode=strict, asyncio_default_fixture_loop_scope=None\ncollecting ... collected 0 items\n\n============================ no tests ran in 0.00s =============================\n",
        "stderr": "/Users/charleshoward/Metis_RAG/venv_py310/lib/python3.10/site-packages/pytest_asyncio/plugin.py:207: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\nERROR: file or directory not found: tests.test_rag_quality\n\n",
        "return_code": 4
      },
      "reports": {
        "test_quality_results.json": [
          {
            "query": "What is the architecture of Metis RAG?",
            "answer": "Based on the provided context, I can provide a detailed overview of the architecture of Metis RAG.\n\nAccording to [1] Source: technical_documentation.md, Metis RAG follows a modular architecture with three main components:\n\n* Frontend Layer: Built with HTML, CSS, and JavaScript, this layer provides an intuitive interface for document management, chat interactions, system configuration, and analytics and monitoring.\n* API Layer: Implemented using FastAPI, this layer provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval.\n* RAG Engine: The core engine consists of three sub-components:\n\t+ Document Processing: Handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction.\n\t+ Vector Store: Responsible for storing document embeddings, efficient similarity search, and metadata filtering.\n\t+ LLM Integration: Connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization.\n\nThese components work together to provide advanced RAG capabilities, such as multi-document retrieval, enterprise security, and other features mentioned in the product specifications (Source: [2] product_specifications.csv).\n\nPlease note that this answer is based primarily on the provided context, specifically [1] technical_documentation.md. If you have any further questions or concerns, feel free to ask!",
            "expected_facts": [
              "modular architecture",
              "Frontend Layer",
              "API Layer",
              "RAG Engine",
              "HTML, CSS, and JavaScript",
              "FastAPI"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "technical_doc",
                "relevance_score": 0.6709409430541496
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4804580121776464
              },
              {
                "document_id": "business_report",
                "relevance_score": 0.2760642660368716
              }
            ]
          },
          {
            "query": "What was the revenue reported in Q1 2025?",
            "answer": "Based on the provided context, specifically [1] Source: quarterly_report.txt, I can find the answer to your question.\n\nAccording to the \"Financial Performance\" section of the report, the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year.\n\nSource: [1]\n\nTherefore, the reported revenue for Q1 2025 was $4.2 million.",
            "expected_facts": [
              "$4.2M",
              "15% increase",
              "year-over-year",
              "net profit of $1.4M"
            ],
            "facts_found": 3,
            "fact_percentage": 75.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "business_report",
                "relevance_score": 0.6630528548563427
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4826635659687333
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.418785820604604
              }
            ]
          },
          {
            "query": "What are the components of the RAG engine?",
            "answer": "Based on the provided context, I found the relevant information about the RAG engine's components.\n\nAccording to [1] Source: technical_documentation.md, Tags: technical,documentation,architecture, Folder: /test:\n\n\"The core RAG engine consists of:\n\n#### Document Processing\n\nThe document processing pipeline handles:\n- File validation and parsing\n- Text extraction\n- Chunking with configurable strategies\n- Metadata extraction\n\n#### Vector Store\n\nThe vector store is responsible for:\n- Storing document embeddings\n- Efficient similarity search\n- Metadata filtering\n\n#### LLM Integration\n\nThe LLM integration component:\n- Connects to Ollama for local LLM inference\n- Manages prompt templates\n- Handles context window optimization\"\n\nFrom this, we can conclude that the RAG engine has three main components: Document Processing, Vector Store, and LLM Integration.\n\nI did not find any additional information about the RAG engine's components in the other provided contexts ([2] Source: product_specifications.csv, Tags: product,specifications,pricing, Folder: /test) or [3] Source: quarterly_report.txt, Tags: business,report,quarterly, Folder: /test).\n\nTherefore, my answer is based primarily on the information found in [1].",
            "expected_facts": [
              "Document Processing",
              "Vector Store",
              "LLM Integration",
              "chunking",
              "embeddings",
              "Ollama"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "technical_doc",
                "relevance_score": 0.6109136250495526
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4640470886109769
              },
              {
                "document_id": "business_report",
                "relevance_score": 0.3472817564888965
              }
            ]
          },
          {
            "query": "What are the strategic initiatives for Q2?",
            "answer": "I'll do my best to provide an accurate and helpful response based on the provided context.\n\nAccording to the Quarterly Business Report [1], the strategic initiatives planned for Q2 2025 are:\n\n* International Expansion: Launch in European markets\n* Enterprise Solution: Develop and release enterprise-grade features\n* Strategic Partnerships: Form alliances with complementary service providers\n* Operational Efficiency: Implement automation to reduce operational costs\n\nThese initiatives aim to build upon the company's strong growth in key metrics, including revenue, customer acquisition, and product engagement.\n\nPlease note that I've based my response primarily on the provided context. If more information is needed or clarification is required, please let me know!",
            "expected_facts": [
              "International Expansion",
              "European markets",
              "Enterprise Solution",
              "Strategic Partnerships",
              "Operational Efficiency"
            ],
            "facts_found": 5,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "business_report",
                "relevance_score": 0.5109074115398808
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.38224892211823436
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.3782355963681827
              }
            ]
          },
          {
            "query": "What products are available and at what price points?",
            "answer": "I've reviewed the provided context and will do my best to provide an accurate answer based on the available information.\n\nAccording to the product specifications [1], Metis RAG offers several products with different price points:\n\n* MetisRAG Enterprise: $4999\n* MetisRAG Professional: $1999\n* MetisRAG Basic: $499\n* MetisRAG API: $0.10 per query (pay-per-use)\n* MetisRAG Mobile: $9.99 per month\n\nThe context does not contain any information about other products or pricing tiers beyond these five options.\n\nPlease note that this answer is based solely on the provided context and may not include all possible products or pricing information. If you have further questions, feel free to ask!",
            "expected_facts": [
              "MetisRAG Enterprise",
              "$4999",
              "MetisRAG Professional",
              "$1999",
              "MetisRAG Basic",
              "$499"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "product_specs",
                "relevance_score": 0.511074574924676
              },
              {
                "document_id": "business_report",
                "relevance_score": 0.45753814380850444
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.43169304353423765
              }
            ]
          },
          {
            "query": "What was the customer retention rate and NPS score?",
            "answer": "Based on the provided context [1], I found the customer retention rate and NPS score.\n\nAccording to the \"Customer Success\" section of the Quarterly Business Report [1], the customer retention rate remained strong at 94%. Additionally, the Net Promoter Score (NPS) improved from 42 to 48.\n\nSources: [1]",
            "expected_facts": [
              "94%",
              "Net Promoter Score",
              "improved from 42 to 48"
            ],
            "facts_found": 3,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "business_report",
                "relevance_score": 0.5571846440343674
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4651842907204182
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.40303926901945974
              }
            ]
          }
        ],
        "test_multi_doc_results.json": [
          {
            "query": "Compare the MetisRAG Enterprise product with the RAG Engine architecture",
            "answer": "I'll do my best to provide an accurate and informative response based on the provided context.\n\nTo compare the MetisRAG Enterprise product with the RAG Engine architecture, let's start by looking at the product specifications [2]. The MetisRAG Enterprise product is categorized as software, priced at $4999, and offers advanced RAG capabilities, multi-document retrieval, and enterprise security. This suggests that the product is a comprehensive solution that leverages the RAG Engine architecture.\n\nThe RAG Engine architecture [1] consists of several components, including the frontend layer, API layer, and RAG engine. The RAG engine itself includes document processing, vector store, and LLM integration. It appears that the RAG Engine architecture provides the underlying technology for the MetisRAG Enterprise product.\n\nOne key difference between the product and the architecture is the scope of features offered. The product specifications list advanced capabilities like multi-document retrieval and enterprise security, which are not explicitly mentioned in the RAG Engine architecture documentation. This suggests that the product may offer additional features beyond what's described in the architecture.\n\nHowever, it's important to note that the context doesn't provide direct information on how the MetisRAG Enterprise product specifically utilizes the RAG Engine architecture. For instance, we don't know which specific components of the architecture are used or how they contribute to the product's features and capabilities.\n\nTo better understand the relationship between the product and the architecture, I would recommend consulting additional resources or documentation that provides more detailed information on how the MetisRAG Enterprise product is built upon the RAG Engine architecture.",
            "expected_facts": [
              "MetisRAG Enterprise",
              "$4999",
              "Advanced RAG capabilities",
              "RAG Engine",
              "Document Processing",
              "Vector Store"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "multi_doc_retrieval": true,
            "correct_sources": true,
            "source_doc_ids": [
              "technical_doc",
              "product_specs",
              "business_report"
            ]
          },
          {
            "query": "What is the relationship between the Q1 financial performance and the product offerings?",
            "answer": "Based on the provided context, it appears that there is a relationship between the Q1 financial performance and the product offerings.\n\nAccording to [1], the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year. This strong financial performance can be attributed to the successful launch of three major features (Advanced Analytics Dashboard, Mobile Application Redesign, and API Integration Platform) by the product team.\n\nThe user engagement metrics show a 22% increase in daily active users following these releases, which likely contributed to the revenue growth. Additionally, the marketing team executed campaigns that generated 2,500 new leads, resulting in 300 new customers with a sales conversion rate of 12%. This suggests that the company's product offerings were effective in driving customer acquisition and revenue.\n\nFurthermore, the Customer Success section indicates that customer retention rate remained strong at 94%, which also likely had a positive impact on the financial performance. The overall improvement in key metrics, including revenue, customer acquisition, and product engagement, demonstrates a direct correlation between the Q1 financial performance and the company's product offerings.\n\nIn summary, based on the context, it can be inferred that the successful launch of new features and the resulting increase in user engagement and customer acquisition contributed to the strong financial performance in Q1 2025.",
            "expected_facts": [
              "$4.2M in revenue",
              "MetisRAG Enterprise",
              "MetisRAG Professional",
              "MetisRAG Basic"
            ],
            "facts_found": 1,
            "fact_percentage": 25.0,
            "multi_doc_retrieval": true,
            "correct_sources": true,
            "source_doc_ids": [
              "technical_doc",
              "product_specs",
              "business_report"
            ]
          }
        ],
        "test_citation_results.json": [
          {
            "query": "What is the architecture of Metis RAG?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What was the revenue reported in Q1 2025?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What are the components of the RAG engine?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What are the strategic initiatives for Q2?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What products are available and at what price points?",
            "has_citation_markers": true,
            "citation_count": 2,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What was the customer retention rate and NPS score?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          }
        ]
      }
    }
  ]
}

================
File: tests/results/quality_results/test_quality_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "answer": "Based on the provided context, I can provide a detailed overview of the architecture of Metis RAG.\n\nAccording to [1] Source: technical_documentation.md, Metis RAG follows a modular architecture with three main components:\n\n* Frontend Layer: Built with HTML, CSS, and JavaScript, this layer provides an intuitive interface for document management, chat interactions, system configuration, and analytics and monitoring.\n* API Layer: Implemented using FastAPI, this layer provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval.\n* RAG Engine: The core engine consists of three sub-components:\n\t+ Document Processing: Handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction.\n\t+ Vector Store: Responsible for storing document embeddings, efficient similarity search, and metadata filtering.\n\t+ LLM Integration: Connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization.\n\nThese components work together to provide advanced RAG capabilities, such as multi-document retrieval, enterprise security, and other features mentioned in the product specifications (Source: [2] product_specifications.csv).\n\nPlease note that this answer is based primarily on the provided context, specifically [1] technical_documentation.md. If you have any further questions or concerns, feel free to ask!",
    "expected_facts": [
      "modular architecture",
      "Frontend Layer",
      "API Layer",
      "RAG Engine",
      "HTML, CSS, and JavaScript",
      "FastAPI"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "technical_doc",
        "relevance_score": 0.6709409430541496
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4804580121776464
      },
      {
        "document_id": "business_report",
        "relevance_score": 0.2760642660368716
      }
    ]
  },
  {
    "query": "What was the revenue reported in Q1 2025?",
    "answer": "Based on the provided context, specifically [1] Source: quarterly_report.txt, I can find the answer to your question.\n\nAccording to the \"Financial Performance\" section of the report, the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year.\n\nSource: [1]\n\nTherefore, the reported revenue for Q1 2025 was $4.2 million.",
    "expected_facts": [
      "$4.2M",
      "15% increase",
      "year-over-year",
      "net profit of $1.4M"
    ],
    "facts_found": 3,
    "fact_percentage": 75.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "business_report",
        "relevance_score": 0.6630528548563427
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4826635659687333
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.418785820604604
      }
    ]
  },
  {
    "query": "What are the components of the RAG engine?",
    "answer": "Based on the provided context, I found the relevant information about the RAG engine's components.\n\nAccording to [1] Source: technical_documentation.md, Tags: technical,documentation,architecture, Folder: /test:\n\n\"The core RAG engine consists of:\n\n#### Document Processing\n\nThe document processing pipeline handles:\n- File validation and parsing\n- Text extraction\n- Chunking with configurable strategies\n- Metadata extraction\n\n#### Vector Store\n\nThe vector store is responsible for:\n- Storing document embeddings\n- Efficient similarity search\n- Metadata filtering\n\n#### LLM Integration\n\nThe LLM integration component:\n- Connects to Ollama for local LLM inference\n- Manages prompt templates\n- Handles context window optimization\"\n\nFrom this, we can conclude that the RAG engine has three main components: Document Processing, Vector Store, and LLM Integration.\n\nI did not find any additional information about the RAG engine's components in the other provided contexts ([2] Source: product_specifications.csv, Tags: product,specifications,pricing, Folder: /test) or [3] Source: quarterly_report.txt, Tags: business,report,quarterly, Folder: /test).\n\nTherefore, my answer is based primarily on the information found in [1].",
    "expected_facts": [
      "Document Processing",
      "Vector Store",
      "LLM Integration",
      "chunking",
      "embeddings",
      "Ollama"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "technical_doc",
        "relevance_score": 0.6109136250495526
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4640470886109769
      },
      {
        "document_id": "business_report",
        "relevance_score": 0.3472817564888965
      }
    ]
  },
  {
    "query": "What are the strategic initiatives for Q2?",
    "answer": "I'll do my best to provide an accurate and helpful response based on the provided context.\n\nAccording to the Quarterly Business Report [1], the strategic initiatives planned for Q2 2025 are:\n\n* International Expansion: Launch in European markets\n* Enterprise Solution: Develop and release enterprise-grade features\n* Strategic Partnerships: Form alliances with complementary service providers\n* Operational Efficiency: Implement automation to reduce operational costs\n\nThese initiatives aim to build upon the company's strong growth in key metrics, including revenue, customer acquisition, and product engagement.\n\nPlease note that I've based my response primarily on the provided context. If more information is needed or clarification is required, please let me know!",
    "expected_facts": [
      "International Expansion",
      "European markets",
      "Enterprise Solution",
      "Strategic Partnerships",
      "Operational Efficiency"
    ],
    "facts_found": 5,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "business_report",
        "relevance_score": 0.5109074115398808
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.38224892211823436
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.3782355963681827
      }
    ]
  },
  {
    "query": "What products are available and at what price points?",
    "answer": "I've reviewed the provided context and will do my best to provide an accurate answer based on the available information.\n\nAccording to the product specifications [1], Metis RAG offers several products with different price points:\n\n* MetisRAG Enterprise: $4999\n* MetisRAG Professional: $1999\n* MetisRAG Basic: $499\n* MetisRAG API: $0.10 per query (pay-per-use)\n* MetisRAG Mobile: $9.99 per month\n\nThe context does not contain any information about other products or pricing tiers beyond these five options.\n\nPlease note that this answer is based solely on the provided context and may not include all possible products or pricing information. If you have further questions, feel free to ask!",
    "expected_facts": [
      "MetisRAG Enterprise",
      "$4999",
      "MetisRAG Professional",
      "$1999",
      "MetisRAG Basic",
      "$499"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "product_specs",
        "relevance_score": 0.511074574924676
      },
      {
        "document_id": "business_report",
        "relevance_score": 0.45753814380850444
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.43169304353423765
      }
    ]
  },
  {
    "query": "What was the customer retention rate and NPS score?",
    "answer": "Based on the provided context [1], I found the customer retention rate and NPS score.\n\nAccording to the \"Customer Success\" section of the Quarterly Business Report [1], the customer retention rate remained strong at 94%. Additionally, the Net Promoter Score (NPS) improved from 42 to 48.\n\nSources: [1]",
    "expected_facts": [
      "94%",
      "Net Promoter Score",
      "improved from 42 to 48"
    ],
    "facts_found": 3,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "business_report",
        "relevance_score": 0.5571846440343674
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4651842907204182
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.40303926901945974
      }
    ]
  }
]

================
File: tests/results/query_basic_entity/report.md
================
# Query Test Report: basic_entity

## Query Information
- **Original Query**: Tell me about Stabilium and its applications in quantum computing.
- **Refined Query**: Tell me about Stabilium applications in quantum computing, specifically focusing on its use as a substrate for superconducting qubits, its ability to enhance quantum coherence times, and its role in facilitating quantum tunneling through non-Euclidean space-time manifolds.
- **Execution Time**: 60.84 seconds

## Entity Preservation Analysis
- **Entities Found**: Stabilium
- **Entities Preserved**: Yes

## Retrieval Statistics
- **Chunks Retrieved**: 15
- **Chunks Used in Context**: 6

## Sources Used

### Source 1
- **Document**: 
- **Relevance Score**: 0.95
- **Excerpt**: # Stabilium: A Revolutionary Material for Quantum Computing

## Overview

Stabilium is a synthetic m...

### Source 2
- **Document**: 
- **Relevance Score**: 0.90
- **Excerpt**: asurement Precision**: Stabilium-based quantum sensors can approach the theoretical limits imposed b...

### Source 3
- **Document**: 
- **Relevance Score**: 0.90
- **Excerpt**: um computing by enabling the development of more practical quantum processors:

1. **Qubit Stability...

### Source 4
- **Document**: 
- **Relevance Score**: 0.85
- **Excerpt**:  curved space-time regions where standard quantum mechanics breaks down.

## Applications in Advance...

### Source 5
- **Document**: 
- **Relevance Score**: 0.80
- **Excerpt**: g through non-Euclidean space-time manifolds due to several unique properties:

1. **Quantum Coheren...

### Source 6
- **Document**: 
- **Relevance Score**: 0.71
- **Excerpt**: # Stabilium Versions: Comparative Analysis

## Evolution of Stabilium Technology

Since its initial ...

## Generated Answer
```
Stabilium is a revolutionary material with significant applications in quantum computing. According to document [1], Stabilium maintains quantum coherence for up to 500 microseconds, operates effectively at temperatures up to 28K, and has resonance modulation properties built into its molecular structure. Furthermore, document [2] details its use in quantum teleportation enhancement and accelerating quantum computing by facilitating quantum tunneling through non-Euclidean space-time regions. Document [3] highlights its role in quantum memory and integration with Quantum Resonance Modulation (QRM) techniques. Finally, document [4] describes its application in quantum sensing in curved space-time regions. 
```

================
File: tests/results/query_basic_entity/results.json
================
{
  "query_id": "basic_entity",
  "query": "Tell me about Stabilium and its applications in quantum computing.",
  "execution_time": 60.8381609916687,
  "answer": "Stabilium is a revolutionary material with significant applications in quantum computing. According to document [1], Stabilium maintains quantum coherence for up to 500 microseconds, operates effectively at temperatures up to 28K, and has resonance modulation properties built into its molecular structure. Furthermore, document [2] details its use in quantum teleportation enhancement and accelerating quantum computing by facilitating quantum tunneling through non-Euclidean space-time regions. Document [3] highlights its role in quantum memory and integration with Quantum Resonance Modulation (QRM) techniques. Finally, document [4] describes its application in quantum sensing in curved space-time regions. ",
  "sources": [
    {
      "document_id": "stabilium_overview",
      "chunk_id": "stabilium_overview_chunk_0",
      "relevance_score": 0.95,
      "filename": "",
      "excerpt": "# Stabilium: A Revolutionary Material for Quantum Computing\n\n## Overview\n\nStabilium is a synthetic m..."
    },
    {
      "document_id": "quantum_physics",
      "chunk_id": "quantum_physics_chunk_2",
      "relevance_score": 0.9,
      "filename": "",
      "excerpt": "asurement Precision**: Stabilium-based quantum sensors can approach the theoretical limits imposed b..."
    },
    {
      "document_id": "stabilium_overview",
      "chunk_id": "stabilium_overview_chunk_1",
      "relevance_score": 0.9,
      "filename": "",
      "excerpt": "um computing by enabling the development of more practical quantum processors:\n\n1. **Qubit Stability..."
    },
    {
      "document_id": "advanced_quantum",
      "chunk_id": "advanced_quantum_chunk_3",
      "relevance_score": 0.85,
      "filename": "",
      "excerpt": " curved space-time regions where standard quantum mechanics breaks down.\n\n## Applications in Advance..."
    },
    {
      "document_id": "advanced_quantum",
      "chunk_id": "advanced_quantum_chunk_1",
      "relevance_score": 0.8,
      "filename": "",
      "excerpt": "g through non-Euclidean space-time manifolds due to several unique properties:\n\n1. **Quantum Coheren..."
    },
    {
      "document_id": "stabilium_versions",
      "chunk_id": "stabilium_versions_chunk_0",
      "relevance_score": 0.7107210560210238,
      "filename": "",
      "excerpt": "# Stabilium Versions: Comparative Analysis\n\n## Evolution of Stabilium Technology\n\nSince its initial ..."
    }
  ],
  "chunks_retrieved": 15,
  "chunks_used": 6,
  "refined_query": "Tell me about Stabilium applications in quantum computing, specifically focusing on its use as a substrate for superconducting qubits, its ability to enhance quantum coherence times, and its role in facilitating quantum tunneling through non-Euclidean space-time manifolds.",
  "entity_analysis": {
    "entities_found": [
      "Stabilium"
    ],
    "entities_preserved": true,
    "preserved_entities": [
      "Stabilium"
    ],
    "missing_entities": [],
    "analysis": "Found 1 entities, 1 preserved, 0 missing"
  }
}

================
File: tests/results/query_context_synthesis/report.md
================
# Query Test Report: context_synthesis

## Query Information
- **Original Query**: How does Stabilium interact with Heisenberg's Uncertainty Principle when used in quantum entanglement experiments?
- **Refined Query**: How does Stabilium extended coherence and topological protection, as demonstrated in experiments probing the uncertainty principle's boundaries and enabling multi-particle entangled tunneling through non-Euclidean space-time manifolds, interact with Heisenberg's Uncertainty Principle when used in quantum entanglement experiments?
- **Execution Time**: 49.76 seconds

## Entity Preservation Analysis
- **Entities Found**: Stabilium, Heisenberg, Uncertainty Principle
- **Entities Preserved**: Yes

## Retrieval Statistics
- **Chunks Retrieved**: 15
- **Chunks Used in Context**: 3

## Sources Used

### Source 1
- **Document**: 
- **Relevance Score**: 0.85
- **Excerpt**: : When entangled particles are measured, the uncertainty principle applies to both particles simulta...

### Source 2
- **Document**: 
- **Relevance Score**: 0.95
- **Excerpt**: asurement Precision**: Stabilium-based quantum sensors can approach the theoretical limits imposed b...

### Source 3
- **Document**: 
- **Relevance Score**: 0.90
- **Excerpt**: g through non-Euclidean space-time manifolds due to several unique properties:

1. **Quantum Coheren...

## Generated Answer
```
Stabilium interacts with Heisenberg’s Uncertainty Principle in several ways, as demonstrated in quantum entanglement experiments. Specifically, Stabilium’s ability to maintain quantum coherence for extended periods allows for more precise measurements within the bounds of the uncertainty principle [1]. Furthermore, Stabilium-based quantum sensors can approach the theoretical limits imposed by the uncertainty principle more closely than previous technologies [1]. Additionally, Stabilium can store entangled states as a quantum memory, preserving the entanglement properties while subject to the constraints of the uncertainty principle [1].

Sources: 
[1] Document ID: heisenberg_uncertainty_principle. md - "Document Title"
```

================
File: tests/results/query_context_synthesis/results.json
================
{
  "query_id": "context_synthesis",
  "query": "How does Stabilium interact with Heisenberg's Uncertainty Principle when used in quantum entanglement experiments?",
  "execution_time": 49.76212477684021,
  "answer": "Stabilium interacts with Heisenberg\u2019s Uncertainty Principle in several ways, as demonstrated in quantum entanglement experiments. Specifically, Stabilium\u2019s ability to maintain quantum coherence for extended periods allows for more precise measurements within the bounds of the uncertainty principle [1]. Furthermore, Stabilium-based quantum sensors can approach the theoretical limits imposed by the uncertainty principle more closely than previous technologies [1]. Additionally, Stabilium can store entangled states as a quantum memory, preserving the entanglement properties while subject to the constraints of the uncertainty principle [1].\n\nSources: \n[1] Document ID: heisenberg_uncertainty_principle. md - \"Document Title\"",
  "sources": [
    {
      "document_id": "quantum_physics",
      "chunk_id": "quantum_physics_chunk_1",
      "relevance_score": 0.85,
      "filename": "",
      "excerpt": ": When entangled particles are measured, the uncertainty principle applies to both particles simulta..."
    },
    {
      "document_id": "quantum_physics",
      "chunk_id": "quantum_physics_chunk_2",
      "relevance_score": 0.95,
      "filename": "",
      "excerpt": "asurement Precision**: Stabilium-based quantum sensors can approach the theoretical limits imposed b..."
    },
    {
      "document_id": "advanced_quantum",
      "chunk_id": "advanced_quantum_chunk_1",
      "relevance_score": 0.9,
      "filename": "",
      "excerpt": "g through non-Euclidean space-time manifolds due to several unique properties:\n\n1. **Quantum Coheren..."
    }
  ],
  "chunks_retrieved": 15,
  "chunks_used": 3,
  "refined_query": "How does Stabilium extended coherence and topological protection, as demonstrated in experiments probing the uncertainty principle's boundaries and enabling multi-particle entangled tunneling through non-Euclidean space-time manifolds, interact with Heisenberg's Uncertainty Principle when used in quantum entanglement experiments?",
  "entity_analysis": {
    "entities_found": [
      "Stabilium",
      "Heisenberg",
      "Uncertainty Principle"
    ],
    "entities_preserved": true,
    "preserved_entities": [
      "Stabilium",
      "Heisenberg",
      "Uncertainty Principle"
    ],
    "missing_entities": [],
    "analysis": "Found 3 entities, 3 preserved, 0 missing"
  }
}

================
File: tests/results/query_multi_entity/report.md
================
# Query Test Report: multi_entity

## Query Information
- **Original Query**: Compare the properties of Stabilium and Quantum Resonance Modulation in cold fusion experiments.
- **Refined Query**: 
- **Execution Time**: 31.57 seconds

## Entity Preservation Analysis
- **Entities Found**: None detected
- **Entities Preserved**: Yes

## Retrieval Statistics
- **Chunks Retrieved**: 15
- **Chunks Used in Context**: 4

## Sources Used

### Source 1
- **Document**: 
- **Relevance Score**: 0.95
- **Excerpt**: istribution, preventing hotspots
- **Reproducibility**: Significantly improves the reproducibility o...

### Source 2
- **Document**: 
- **Relevance Score**: 0.85
- **Excerpt**: # Quantum Resonance Modulation: Principles and Applications

## Fundamental Principles

Quantum Reso...

### Source 3
- **Document**: 
- **Relevance Score**: 0.80
- **Excerpt**: te
2. QRM systems create the conditions for quantum tunneling
3. The combined approach increases fus...

### Source 4
- **Document**: 
- **Relevance Score**: 0.75
- **Excerpt**: # Stabilium Versions: Comparative Analysis

## Evolution of Stabilium Technology

Since its initial ...

## Generated Answer
```
I've searched for information on this topic. While my knowledge is limited, However, the combined approach of QRM systems and Stabilium increases fusion efficiency by approximately 300% compared to traditional methods [3].
```

================
File: tests/results/query_multi_entity/results.json
================
{
  "query_id": "multi_entity",
  "query": "Compare the properties of Stabilium and Quantum Resonance Modulation in cold fusion experiments.",
  "execution_time": 31.57057499885559,
  "answer": "I've searched for information on this topic. While my knowledge is limited, However, the combined approach of QRM systems and Stabilium increases fusion efficiency by approximately 300% compared to traditional methods [3].",
  "sources": [
    {
      "document_id": "qrm_overview",
      "chunk_id": "qrm_overview_chunk_1",
      "relevance_score": 0.95,
      "filename": "",
      "excerpt": "istribution, preventing hotspots\n- **Reproducibility**: Significantly improves the reproducibility o..."
    },
    {
      "document_id": "qrm_overview",
      "chunk_id": "qrm_overview_chunk_0",
      "relevance_score": 0.85,
      "filename": "",
      "excerpt": "# Quantum Resonance Modulation: Principles and Applications\n\n## Fundamental Principles\n\nQuantum Reso..."
    },
    {
      "document_id": "qrm_overview",
      "chunk_id": "qrm_overview_chunk_2",
      "relevance_score": 0.8,
      "filename": "",
      "excerpt": "te\n2. QRM systems create the conditions for quantum tunneling\n3. The combined approach increases fus..."
    },
    {
      "document_id": "stabilium_versions",
      "chunk_id": "stabilium_versions_chunk_0",
      "relevance_score": 0.75,
      "filename": "",
      "excerpt": "# Stabilium Versions: Comparative Analysis\n\n## Evolution of Stabilium Technology\n\nSince its initial ..."
    }
  ],
  "chunks_retrieved": 15,
  "chunks_used": 4,
  "refined_query": "",
  "entity_analysis": {
    "entities_found": [],
    "entities_preserved": true,
    "analysis": "No query refinement was performed"
  }
}

================
File: tests/results/query_potential_ambiguity/report.md
================
# Query Test Report: potential_ambiguity

## Query Information
- **Original Query**: What are the differences between Stabilium QRM-12X and earlier versions?
- **Refined Query**: 
- **Execution Time**: 38.02 seconds

## Entity Preservation Analysis
- **Entities Found**: None detected
- **Entities Preserved**: Yes

## Retrieval Statistics
- **Chunks Retrieved**: 15
- **Chunks Used in Context**: 5

## Sources Used

### Source 1
- **Document**: 
- **Relevance Score**: 0.95
- **Excerpt**: # Stabilium Versions: Comparative Analysis

## Evolution of Stabilium Technology

Since its initial ...

### Source 2
- **Document**: 
- **Relevance Score**: 0.80
- **Excerpt**: um computing by enabling the development of more practical quantum processors:

1. **Qubit Stability...

### Source 3
- **Document**: 
- **Relevance Score**: 0.60
- **Excerpt**: te
2. QRM systems create the conditions for quantum tunneling
3. The combined approach increases fus...

### Source 4
- **Document**: 
- **Relevance Score**: 0.85
- **Excerpt**: -----|---------|
| Qubit Error Rate | 0.5% | 0.3% | 0.1% |
| Gate Operation Speed | 50ns | 35ns | 20...

### Source 5
- **Document**: 
- **Relevance Score**: 0.75
- **Excerpt**: ements in its composition, making it more cost-effective and environmentally sustainable.

5. **Impr...

## Generated Answer
```
Based on the provided documents, here’s a comparison of Stabilium QRM-12X with earlier versions: 

**Key Differences: **

* **Coherence Time: ** QRM-12X achieves coherence times exceeding 1 millisecond (currently in early development), significantly longer than the 300 microseconds of Standard Stabilium and QRM-8. 
* **Gate Operation Speed: ** QRM-12X has a gate operation speed of 20ns, faster than the 50ns of Standard Stabilium and 35ns of QRM-8. 
* **Power Consumption: ** QRM-12X consumes 45% of the baseline power, a reduction from the 100% of Standard Stabilium and 75% of QRM-8. 
* **Manufacturing Cost: ** The manufacturing cost of QRM-12X is 90% of the baseline, lower than the 100% of Standard Stabilium and 120% of QRM-8. 
* **Cooling Requirements: ** QRM-12X requires 60% of the baseline cooling requirements, less than the 100% of Standard Stabilium and 80% of QRM-8. 
* **Manufacturing Scale: ** QRM-12X can be manufactured in sheets up to 25cm², more than doubling the size capability of the original version. 

**Reference: ** [4], [5] and [5]
```

================
File: tests/results/query_potential_ambiguity/results.json
================
{
  "query_id": "potential_ambiguity",
  "query": "What are the differences between Stabilium QRM-12X and earlier versions?",
  "execution_time": 38.01968002319336,
  "answer": "Based on the provided documents, here\u2019s a comparison of Stabilium QRM-12X with earlier versions: \n\n**Key Differences: **\n\n* **Coherence Time: ** QRM-12X achieves coherence times exceeding 1 millisecond (currently in early development), significantly longer than the 300 microseconds of Standard Stabilium and QRM-8. \n* **Gate Operation Speed: ** QRM-12X has a gate operation speed of 20ns, faster than the 50ns of Standard Stabilium and 35ns of QRM-8. \n* **Power Consumption: ** QRM-12X consumes 45% of the baseline power, a reduction from the 100% of Standard Stabilium and 75% of QRM-8. \n* **Manufacturing Cost: ** The manufacturing cost of QRM-12X is 90% of the baseline, lower than the 100% of Standard Stabilium and 120% of QRM-8. \n* **Cooling Requirements: ** QRM-12X requires 60% of the baseline cooling requirements, less than the 100% of Standard Stabilium and 80% of QRM-8. \n* **Manufacturing Scale: ** QRM-12X can be manufactured in sheets up to 25cm\u00b2, more than doubling the size capability of the original version. \n\n**Reference: ** [4], [5] and [5]",
  "sources": [
    {
      "document_id": "stabilium_versions",
      "chunk_id": "stabilium_versions_chunk_0",
      "relevance_score": 0.95,
      "filename": "",
      "excerpt": "# Stabilium Versions: Comparative Analysis\n\n## Evolution of Stabilium Technology\n\nSince its initial ..."
    },
    {
      "document_id": "stabilium_overview",
      "chunk_id": "stabilium_overview_chunk_1",
      "relevance_score": 0.8,
      "filename": "",
      "excerpt": "um computing by enabling the development of more practical quantum processors:\n\n1. **Qubit Stability..."
    },
    {
      "document_id": "qrm_overview",
      "chunk_id": "qrm_overview_chunk_2",
      "relevance_score": 0.6,
      "filename": "",
      "excerpt": "te\n2. QRM systems create the conditions for quantum tunneling\n3. The combined approach increases fus..."
    },
    {
      "document_id": "stabilium_versions",
      "chunk_id": "stabilium_versions_chunk_2",
      "relevance_score": 0.85,
      "filename": "",
      "excerpt": "-----|---------|\n| Qubit Error Rate | 0.5% | 0.3% | 0.1% |\n| Gate Operation Speed | 50ns | 35ns | 20..."
    },
    {
      "document_id": "stabilium_versions",
      "chunk_id": "stabilium_versions_chunk_1",
      "relevance_score": 0.75,
      "filename": "",
      "excerpt": "ements in its composition, making it more cost-effective and environmentally sustainable.\n\n5. **Impr..."
    }
  ],
  "chunks_retrieved": 15,
  "chunks_used": 5,
  "refined_query": "",
  "entity_analysis": {
    "entities_found": [],
    "entities_preserved": true,
    "analysis": "No query refinement was performed"
  }
}

================
File: tests/results/query_specialized_terminology/report.md
================
# Query Test Report: specialized_terminology

## Query Information
- **Original Query**: Explain the role of Stabilium in facilitating quantum tunneling through non-Euclidean space-time manifolds.
- **Refined Query**: 
- **Execution Time**: 37.52 seconds

## Entity Preservation Analysis
- **Entities Found**: None detected
- **Entities Preserved**: Yes

## Retrieval Statistics
- **Chunks Retrieved**: 15
- **Chunks Used in Context**: 4

## Sources Used

### Source 1
- **Document**: 
- **Relevance Score**: 0.95
- **Excerpt**: g through non-Euclidean space-time manifolds due to several unique properties:

1. **Quantum Coheren...

### Source 2
- **Document**: 
- **Relevance Score**: 0.90
- **Excerpt**:  curved space-time regions where standard quantum mechanics breaks down.

## Applications in Advance...

### Source 3
- **Document**: 
- **Relevance Score**: 0.85
- **Excerpt**: # Quantum Tunneling Through Non-Euclidean Space-Time Manifolds

## Theoretical Framework

Quantum tu...

### Source 4
- **Document**: 
- **Relevance Score**: 0.75
- **Excerpt**: cy 300% higher than theoretical predictions for conventional materials.

2. **Information Preservati...

## Generated Answer
```
Stabilium plays a critical role in facilitating quantum tunneling through non-Euclidean space-time manifolds. According to document [1], Stabilium maintains quantum states even when subjected to extreme conditions near space-time curvature anomalies. Furthermore, it provides topological protection to quantum information, preventing decoherence during tunneling events [1]. The material’s unique response to space-time curvature creates “tunneling-friendly” conditions at its boundaries [1]. Document [4] elaborates on this, stating that Stabilium enables quantum information to maintain fidelity above 92% during tunneling, significantly higher than conventional materials [4]. Essentially, Stabilium acts as a conduit for quantum particles to traverse these complex geometries while preserving their quantum properties [1, 4].
```

================
File: tests/results/query_specialized_terminology/results.json
================
{
  "query_id": "specialized_terminology",
  "query": "Explain the role of Stabilium in facilitating quantum tunneling through non-Euclidean space-time manifolds.",
  "execution_time": 37.51750707626343,
  "answer": "Stabilium plays a critical role in facilitating quantum tunneling through non-Euclidean space-time manifolds. According to document [1], Stabilium maintains quantum states even when subjected to extreme conditions near space-time curvature anomalies. Furthermore, it provides topological protection to quantum information, preventing decoherence during tunneling events [1]. The material\u2019s unique response to space-time curvature creates \u201ctunneling-friendly\u201d conditions at its boundaries [1]. Document [4] elaborates on this, stating that Stabilium enables quantum information to maintain fidelity above 92% during tunneling, significantly higher than conventional materials [4]. Essentially, Stabilium acts as a conduit for quantum particles to traverse these complex geometries while preserving their quantum properties [1, 4].",
  "sources": [
    {
      "document_id": "advanced_quantum",
      "chunk_id": "advanced_quantum_chunk_1",
      "relevance_score": 0.95,
      "filename": "",
      "excerpt": "g through non-Euclidean space-time manifolds due to several unique properties:\n\n1. **Quantum Coheren..."
    },
    {
      "document_id": "advanced_quantum",
      "chunk_id": "advanced_quantum_chunk_3",
      "relevance_score": 0.9,
      "filename": "",
      "excerpt": " curved space-time regions where standard quantum mechanics breaks down.\n\n## Applications in Advance..."
    },
    {
      "document_id": "advanced_quantum",
      "chunk_id": "advanced_quantum_chunk_0",
      "relevance_score": 0.85,
      "filename": "",
      "excerpt": "# Quantum Tunneling Through Non-Euclidean Space-Time Manifolds\n\n## Theoretical Framework\n\nQuantum tu..."
    },
    {
      "document_id": "advanced_quantum",
      "chunk_id": "advanced_quantum_chunk_2",
      "relevance_score": 0.75,
      "filename": "",
      "excerpt": "cy 300% higher than theoretical predictions for conventional materials.\n\n2. **Information Preservati..."
    }
  ],
  "chunks_retrieved": 15,
  "chunks_used": 4,
  "refined_query": "",
  "entity_analysis": {
    "entities_found": [],
    "entities_preserved": true,
    "analysis": "No query refinement was performed"
  }
}

================
File: tests/results/retrieval_results/test_citation_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What was the revenue reported in Q1 2025?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What are the components of the RAG engine?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What are the strategic initiatives for Q2?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What products are available and at what price points?",
    "has_citation_markers": true,
    "citation_count": 2,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What was the customer retention rate and NPS score?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  }
]

================
File: tests/results/retrieval_results/test_multi_doc_results.json
================
[
  {
    "query": "Compare the MetisRAG Enterprise product with the RAG Engine architecture",
    "answer": "I'll do my best to provide an accurate and informative response based on the provided context.\n\nTo compare the MetisRAG Enterprise product with the RAG Engine architecture, let's start by looking at the product specifications [2]. The MetisRAG Enterprise product is categorized as software, priced at $4999, and offers advanced RAG capabilities, multi-document retrieval, and enterprise security. This suggests that the product is a comprehensive solution that leverages the RAG Engine architecture.\n\nThe RAG Engine architecture [1] consists of several components, including the frontend layer, API layer, and RAG engine. The RAG engine itself includes document processing, vector store, and LLM integration. It appears that the RAG Engine architecture provides the underlying technology for the MetisRAG Enterprise product.\n\nOne key difference between the product and the architecture is the scope of features offered. The product specifications list advanced capabilities like multi-document retrieval and enterprise security, which are not explicitly mentioned in the RAG Engine architecture documentation. This suggests that the product may offer additional features beyond what's described in the architecture.\n\nHowever, it's important to note that the context doesn't provide direct information on how the MetisRAG Enterprise product specifically utilizes the RAG Engine architecture. For instance, we don't know which specific components of the architecture are used or how they contribute to the product's features and capabilities.\n\nTo better understand the relationship between the product and the architecture, I would recommend consulting additional resources or documentation that provides more detailed information on how the MetisRAG Enterprise product is built upon the RAG Engine architecture.",
    "expected_facts": [
      "MetisRAG Enterprise",
      "$4999",
      "Advanced RAG capabilities",
      "RAG Engine",
      "Document Processing",
      "Vector Store"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "multi_doc_retrieval": true,
    "correct_sources": true,
    "source_doc_ids": [
      "technical_doc",
      "product_specs",
      "business_report"
    ]
  },
  {
    "query": "What is the relationship between the Q1 financial performance and the product offerings?",
    "answer": "Based on the provided context, it appears that there is a relationship between the Q1 financial performance and the product offerings.\n\nAccording to [1], the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year. This strong financial performance can be attributed to the successful launch of three major features (Advanced Analytics Dashboard, Mobile Application Redesign, and API Integration Platform) by the product team.\n\nThe user engagement metrics show a 22% increase in daily active users following these releases, which likely contributed to the revenue growth. Additionally, the marketing team executed campaigns that generated 2,500 new leads, resulting in 300 new customers with a sales conversion rate of 12%. This suggests that the company's product offerings were effective in driving customer acquisition and revenue.\n\nFurthermore, the Customer Success section indicates that customer retention rate remained strong at 94%, which also likely had a positive impact on the financial performance. The overall improvement in key metrics, including revenue, customer acquisition, and product engagement, demonstrates a direct correlation between the Q1 financial performance and the company's product offerings.\n\nIn summary, based on the context, it can be inferred that the successful launch of new features and the resulting increase in user engagement and customer acquisition contributed to the strong financial performance in Q1 2025.",
    "expected_facts": [
      "$4.2M in revenue",
      "MetisRAG Enterprise",
      "MetisRAG Professional",
      "MetisRAG Basic"
    ],
    "facts_found": 1,
    "fact_percentage": 25.0,
    "multi_doc_retrieval": true,
    "correct_sources": true,
    "source_doc_ids": [
      "technical_doc",
      "product_specs",
      "business_report"
    ]
  }
]

================
File: tests/results/retrieval_results/test_response_time_results.json
================
{
  "results": [
    {
      "query": "What is the architecture of Metis RAG?",
      "response_time_ms": 14820.42121887207,
      "answer_length": 1476,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0057449340820312,
      "memory_mb": 164.78125,
      "threads": 23,
      "system_cpu_percent": 11.5,
      "system_memory_percent": 81.5
    },
    {
      "query": "How does the document processing pipeline work?",
      "response_time_ms": 12753.172874450684,
      "answer_length": 1386,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0125160217285156,
      "memory_mb": 165.890625,
      "threads": 23,
      "system_cpu_percent": 9.7,
      "system_memory_percent": 81.5
    },
    {
      "query": "What is the role of the vector store?",
      "response_time_ms": 4216.8519496917725,
      "answer_length": 346,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0172843933105469,
      "memory_mb": 166.671875,
      "threads": 23,
      "system_cpu_percent": 9.4,
      "system_memory_percent": 81.6
    },
    {
      "query": "How does the frontend interface with the API layer?",
      "response_time_ms": 14805.053234100342,
      "answer_length": 1595,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0178565979003906,
      "memory_mb": 166.765625,
      "threads": 23,
      "system_cpu_percent": 8.4,
      "system_memory_percent": 80.6
    },
    {
      "query": "What is the purpose of the LLM integration component?",
      "response_time_ms": 10764.720916748047,
      "answer_length": 1051,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0232925415039062,
      "memory_mb": 167.65625,
      "threads": 23,
      "system_cpu_percent": 8.6,
      "system_memory_percent": 81.0
    },
    {
      "query": "Explain the chunking strategies used in document processing.",
      "response_time_ms": 12380.050897598267,
      "answer_length": 1321,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0245323181152344,
      "memory_mb": 167.859375,
      "threads": 23,
      "system_cpu_percent": 8.7,
      "system_memory_percent": 81.4
    },
    {
      "query": "How does the system handle metadata filtering?",
      "response_time_ms": 7647.094964981079,
      "answer_length": 786,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0296821594238281,
      "memory_mb": 168.703125,
      "threads": 23,
      "system_cpu_percent": 8.6,
      "system_memory_percent": 81.4
    },
    {
      "query": "What technologies are used in the frontend layer?",
      "response_time_ms": 4877.293109893799,
      "answer_length": 427,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0300636291503906,
      "memory_mb": 168.765625,
      "threads": 23,
      "system_cpu_percent": 9.6,
      "system_memory_percent": 81.0
    },
    {
      "query": "How does the RAG engine retrieve relevant information?",
      "response_time_ms": 6343.724012374878,
      "answer_length": 612,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0326385498046875,
      "memory_mb": 169.1875,
      "threads": 23,
      "system_cpu_percent": 13.0,
      "system_memory_percent": 81.2
    },
    {
      "query": "What is the overall purpose of the Metis RAG system?",
      "response_time_ms": 9755.89895248413,
      "answer_length": 1028,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0327339172363281,
      "memory_mb": 169.203125,
      "threads": 23,
      "system_cpu_percent": 8.9,
      "system_memory_percent": 81.5
    }
  ],
  "statistics": {
    "min_response_time_ms": 4216.8519496917725,
    "max_response_time_ms": 14820.42121887207,
    "avg_response_time_ms": 9836.428213119507,
    "median_response_time_ms": 10260.309934616089,
    "stddev_response_time_ms": 3923.403709708135,
    "total_queries": 10
  }
}

================
File: tests/results/chunking_judge_real_results.json
================
[
  {
    "filename": "test_document.txt",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 1500,
      "chunk_overlap": 200
    },
    "justification": "The document is formatted with Markdown headers, indicating a clear hierarchical structure. Using the 'markdown' strategy will preserve this structure, ensuring that related sections and subsections are grouped together. A chunk size of 1500 characters allows for substantial content within each chunk while maintaining reasonable size. An overlap of 200 characters helps to avoid abrupt transitions between chunks and ensures context is carried over."
  },
  {
    "filename": "technical_documentation.md",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 1500,
      "chunk_overlap": 200
    },
    "justification": "The document is a markdown file with a well-defined structure using headers (e.g., Introduction, Architecture Overview, Deployment Options, Configuration, API Reference, Performance Considerations).  Using the 'markdown' strategy will allow us to split the document along these logical sections, preserving the context and relationships between different parts of the documentation. A chunk size of 1500 characters allows for a reasonable amount of content within each chunk while still allowing for meaningful sections. An overlap of 200 ensures that context isn't lost between chunks."
  },
  {
    "filename": "test_data.csv",
    "strategy": "recursive",
    "parameters": {
      "chunk_size": 1000,
      "chunk_overlap": 200
    },
    "justification": "The document is a CSV file, which doesn't inherently have a clear header structure suitable for markdown chunking. While it's structured data, the content within each cell (particularly the 'skills' column) can be quite variable in length.  Token-based chunking might be overly granular, as it could split skills lists mid-item. Recursive chunking provides a good balance. A chunk size of 1000 characters allows for capturing a reasonable amount of context from multiple rows, while an overlap of 200 helps maintain continuity between chunks, especially when a row's content spans across two chunks. This approach will allow the RAG system to understand the relationships between employees and their skills."
  }
]

================
File: tests/results/db_benchmark_postgresql_20250320_110230.json
================
{
  "metadata": {
    "db_type": "postgresql",
    "timestamp": "2025-03-20T11:02:27.479278",
    "num_runs": 3
  },
  "document_operations": [
    {
      "size_name": "small",
      "size_kb": 5,
      "avg_create_time": 0.003162940343221029,
      "avg_retrieve_time": 0.0006759961446126302,
      "avg_update_time": 0.0019226868947347004,
      "avg_delete_time": 0.0008033116658528646,
      "total_avg_time": 0.006564935048421224,
      "num_runs": 3
    },
    {
      "size_name": "medium",
      "size_kb": 50,
      "avg_create_time": 0.0019379456837972004,
      "avg_retrieve_time": 0.0007118384043375651,
      "avg_update_time": 0.0018669764200846355,
      "avg_delete_time": 0.000572045644124349,
      "total_avg_time": 0.00508880615234375,
      "num_runs": 3
    },
    {
      "size_name": "large",
      "size_kb": 500,
      "avg_create_time": 0.00519100824991862,
      "avg_retrieve_time": 0.0008143583933512369,
      "avg_update_time": 0.0026547114054361978,
      "avg_delete_time": 0.0004947185516357422,
      "total_avg_time": 0.009154796600341797,
      "num_runs": 3
    },
    {
      "size_name": "xlarge",
      "size_kb": 2000,
      "avg_create_time": 0.015054543813069662,
      "avg_retrieve_time": 0.0019447008768717449,
      "avg_update_time": 0.006839354832967122,
      "avg_delete_time": 0.0008493264516194662,
      "total_avg_time": 0.024687925974527992,
      "num_runs": 3
    }
  ],
  "chunk_operations": [
    {
      "size_name": "small",
      "size_kb": 5,
      "chunking_strategy": "recursive",
      "chunk_count": 4,
      "chunk_insert_time": 0.019770145416259766,
      "chunk_retrieve_time": 0.0034301280975341797,
      "chunk_update_time": 0.011821985244750977,
      "per_chunk_insert_time": 0.004942536354064941,
      "per_chunk_retrieve_time": 0.0008575320243835449,
      "per_chunk_update_time": 0.002955496311187744
    },
    {
      "size_name": "small",
      "size_kb": 5,
      "chunking_strategy": "token",
      "chunk_count": 4,
      "chunk_insert_time": 0.0025451183319091797,
      "chunk_retrieve_time": 0.0006148815155029297,
      "chunk_update_time": 0.003389120101928711,
      "per_chunk_insert_time": 0.0006362795829772949,
      "per_chunk_retrieve_time": 0.00015372037887573242,
      "per_chunk_update_time": 0.0008472800254821777
    },
    {
      "size_name": "medium",
      "size_kb": 50,
      "chunking_strategy": "recursive",
      "chunk_count": 38,
      "chunk_insert_time": 0.005304813385009766,
      "chunk_retrieve_time": 0.0010678768157958984,
      "chunk_update_time": 0.029368162155151367,
      "per_chunk_insert_time": 0.00013960035223709909,
      "per_chunk_retrieve_time": 2.8102021468313116e-05,
      "per_chunk_update_time": 0.0007728463725039834
    },
    {
      "size_name": "medium",
      "size_kb": 50,
      "chunking_strategy": "token",
      "chunk_count": 38,
      "chunk_insert_time": 0.0049479007720947266,
      "chunk_retrieve_time": 0.0009081363677978516,
      "chunk_update_time": 0.02543783187866211,
      "per_chunk_insert_time": 0.0001302079150551244,
      "per_chunk_retrieve_time": 2.3898325468364516e-05,
      "per_chunk_update_time": 0.000669416628385845
    },
    {
      "size_name": "large",
      "size_kb": 500,
      "chunking_strategy": "recursive",
      "chunk_count": 381,
      "chunk_insert_time": 0.041272878646850586,
      "chunk_retrieve_time": 0.00441288948059082,
      "chunk_update_time": 0.23132109642028809,
      "per_chunk_insert_time": 0.00010832776547729812,
      "per_chunk_retrieve_time": 1.1582387088164883e-05,
      "per_chunk_update_time": 0.0006071419853550869
    },
    {
      "size_name": "large",
      "size_kb": 500,
      "chunking_strategy": "token",
      "chunk_count": 381,
      "chunk_insert_time": 0.030472993850708008,
      "chunk_retrieve_time": 0.005027055740356445,
      "chunk_update_time": 0.2580249309539795,
      "per_chunk_insert_time": 7.998161115671393e-05,
      "per_chunk_retrieve_time": 1.3194372021932927e-05,
      "per_chunk_update_time": 0.000677230789905458
    },
    {
      "size_name": "xlarge",
      "size_kb": 2000,
      "chunking_strategy": "recursive",
      "chunk_count": 1521,
      "chunk_insert_time": 0.1307377815246582,
      "chunk_retrieve_time": 0.016109228134155273,
      "chunk_update_time": 0.8459148406982422,
      "per_chunk_insert_time": 8.595514893139921e-05,
      "per_chunk_retrieve_time": 1.0591208503718128e-05,
      "per_chunk_update_time": 0.0005561570287299423
    },
    {
      "size_name": "xlarge",
      "size_kb": 2000,
      "chunking_strategy": "token",
      "chunk_count": 1521,
      "chunk_insert_time": 0.16907000541687012,
      "chunk_retrieve_time": 0.016005992889404297,
      "chunk_update_time": 0.8438138961791992,
      "per_chunk_insert_time": 0.0001111571370262131,
      "per_chunk_retrieve_time": 1.0523335233007428e-05,
      "per_chunk_update_time": 0.0005547757371329383
    }
  ],
  "query_performance": [
    {
      "query": "What is the main purpose of the RAG system?",
      "avg_query_time": 0.00044043858846028644,
      "avg_token_count": 9,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "How does the document analysis service work?",
      "avg_query_time": 0.00031264623006184894,
      "avg_token_count": 7,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "What are the key components of the LangGraph integration?",
      "avg_query_time": 0.00031264623006184894,
      "avg_token_count": 9,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "Explain the database schema for documents and chunks",
      "avg_query_time": 0.0002876122792561849,
      "avg_token_count": 8,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "How does the memory integration enhance the system?",
      "avg_query_time": 0.00026996930440266925,
      "avg_token_count": 8,
      "avg_source_count": 0,
      "num_runs": 3
    }
  ],
  "batch_processing": [
    {
      "batch_size": 1,
      "total_time": 0.005749940872192383,
      "per_document_time": 0.005749940872192383
    },
    {
      "batch_size": 5,
      "total_time": 0.0265960693359375,
      "per_document_time": 0.0053192138671875
    },
    {
      "batch_size": 10,
      "total_time": 0.051831960678100586,
      "per_document_time": 0.005183196067810059
    },
    {
      "batch_size": 20,
      "total_time": 0.11331009864807129,
      "per_document_time": 0.005665504932403564
    }
  ]
}

================
File: tests/results/db_benchmark_sqlite_20250320_110220.json
================
{
  "metadata": {
    "db_type": "sqlite",
    "timestamp": "2025-03-20T11:02:15.579199",
    "num_runs": 3
  },
  "document_operations": [
    {
      "size_name": "small",
      "size_kb": 5,
      "avg_create_time": 0.003882249196370443,
      "avg_retrieve_time": 0.0010714530944824219,
      "avg_update_time": 0.002068758010864258,
      "avg_delete_time": 0.0007795492808024088,
      "total_avg_time": 0.007802009582519532,
      "num_runs": 3
    },
    {
      "size_name": "medium",
      "size_kb": 50,
      "avg_create_time": 0.00255433718363444,
      "avg_retrieve_time": 0.000606695810953776,
      "avg_update_time": 0.0026903152465820312,
      "avg_delete_time": 0.0006040732065836588,
      "total_avg_time": 0.006455421447753906,
      "num_runs": 3
    },
    {
      "size_name": "large",
      "size_kb": 500,
      "avg_create_time": 0.006193637847900391,
      "avg_retrieve_time": 0.0012172857920328777,
      "avg_update_time": 0.003213644027709961,
      "avg_delete_time": 0.0006033579508463541,
      "total_avg_time": 0.011227925618489584,
      "num_runs": 3
    },
    {
      "size_name": "xlarge",
      "size_kb": 2000,
      "avg_create_time": 0.033665736516316734,
      "avg_retrieve_time": 0.002557039260864258,
      "avg_update_time": 0.007161299387613933,
      "avg_delete_time": 0.0006133715311686198,
      "total_avg_time": 0.04399744669596354,
      "num_runs": 3
    }
  ],
  "chunk_operations": [
    {
      "size_name": "small",
      "size_kb": 5,
      "chunking_strategy": "recursive",
      "chunk_count": 4,
      "chunk_insert_time": 0.0053060054779052734,
      "chunk_retrieve_time": 0.0007498264312744141,
      "chunk_update_time": 0.009877920150756836,
      "per_chunk_insert_time": 0.0013265013694763184,
      "per_chunk_retrieve_time": 0.00018745660781860352,
      "per_chunk_update_time": 0.002469480037689209
    },
    {
      "size_name": "small",
      "size_kb": 5,
      "chunking_strategy": "token",
      "chunk_count": 4,
      "chunk_insert_time": 0.006819963455200195,
      "chunk_retrieve_time": 0.0008149147033691406,
      "chunk_update_time": 0.0036919116973876953,
      "per_chunk_insert_time": 0.0017049908638000488,
      "per_chunk_retrieve_time": 0.00020372867584228516,
      "per_chunk_update_time": 0.0009229779243469238
    },
    {
      "size_name": "medium",
      "size_kb": 50,
      "chunking_strategy": "recursive",
      "chunk_count": 38,
      "chunk_insert_time": 0.0073659420013427734,
      "chunk_retrieve_time": 0.0010590553283691406,
      "chunk_update_time": 0.03266096115112305,
      "per_chunk_insert_time": 0.00019384057898270455,
      "per_chunk_retrieve_time": 2.7869877062345805e-05,
      "per_chunk_update_time": 0.0008594989776611328
    },
    {
      "size_name": "medium",
      "size_kb": 50,
      "chunking_strategy": "token",
      "chunk_count": 38,
      "chunk_insert_time": 0.005384922027587891,
      "chunk_retrieve_time": 0.0009839534759521484,
      "chunk_update_time": 0.03112506866455078,
      "per_chunk_insert_time": 0.00014170847441020764,
      "per_chunk_retrieve_time": 2.589351252505654e-05,
      "per_chunk_update_time": 0.0008190807543302837
    },
    {
      "size_name": "large",
      "size_kb": 500,
      "chunking_strategy": "recursive",
      "chunk_count": 381,
      "chunk_insert_time": 0.04839611053466797,
      "chunk_retrieve_time": 0.006017923355102539,
      "chunk_update_time": 0.3960578441619873,
      "per_chunk_insert_time": 0.00012702391216448287,
      "per_chunk_retrieve_time": 1.579507442284131e-05,
      "per_chunk_update_time": 0.0010395219006876307
    },
    {
      "size_name": "large",
      "size_kb": 500,
      "chunking_strategy": "token",
      "chunk_count": 381,
      "chunk_insert_time": 0.061131954193115234,
      "chunk_retrieve_time": 0.018208026885986328,
      "chunk_update_time": 0.2916529178619385,
      "per_chunk_insert_time": 0.0001604513233415098,
      "per_chunk_retrieve_time": 4.779009681361241e-05,
      "per_chunk_update_time": 0.0007654932227347467
    },
    {
      "size_name": "xlarge",
      "size_kb": 2000,
      "chunking_strategy": "recursive",
      "chunk_count": 1521,
      "chunk_insert_time": 0.20261001586914062,
      "chunk_retrieve_time": 0.01839900016784668,
      "chunk_update_time": 1.2032508850097656,
      "per_chunk_insert_time": 0.00013320842594946787,
      "per_chunk_retrieve_time": 1.2096647053153636e-05,
      "per_chunk_update_time": 0.0007910919691056973
    },
    {
      "size_name": "xlarge",
      "size_kb": 2000,
      "chunking_strategy": "token",
      "chunk_count": 1521,
      "chunk_insert_time": 0.15968799591064453,
      "chunk_retrieve_time": 0.01848292350769043,
      "chunk_update_time": 1.161231279373169,
      "per_chunk_insert_time": 0.00010498882045407267,
      "per_chunk_retrieve_time": 1.2151823476456562e-05,
      "per_chunk_update_time": 0.0007634656669120111
    }
  ],
  "query_performance": [
    {
      "query": "What is the main purpose of the RAG system?",
      "avg_query_time": 0.0011973381042480469,
      "avg_token_count": 9,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "How does the document analysis service work?",
      "avg_query_time": 0.0003578662872314453,
      "avg_token_count": 7,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "What are the key components of the LangGraph integration?",
      "avg_query_time": 0.0003227392832438151,
      "avg_token_count": 9,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "Explain the database schema for documents and chunks",
      "avg_query_time": 0.00031566619873046875,
      "avg_token_count": 8,
      "avg_source_count": 0,
      "num_runs": 3
    },
    {
      "query": "How does the memory integration enhance the system?",
      "avg_query_time": 0.0002849896748860677,
      "avg_token_count": 8,
      "avg_source_count": 0,
      "num_runs": 3
    }
  ],
  "batch_processing": [
    {
      "batch_size": 1,
      "total_time": 0.007443904876708984,
      "per_document_time": 0.007443904876708984
    },
    {
      "batch_size": 5,
      "total_time": 0.029572010040283203,
      "per_document_time": 0.0059144020080566405
    },
    {
      "batch_size": 10,
      "total_time": 0.16942405700683594,
      "per_document_time": 0.016942405700683595
    },
    {
      "batch_size": 20,
      "total_time": 0.14658093452453613,
      "per_document_time": 0.007329046726226807
    }
  ]
}

================
File: tests/results/db_comparison_report_20250320_110308.html
================
<!DOCTYPE html>
    <html>
    <head>
        <title>Database Performance Comparison: SQLite vs PostgreSQL</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            h1, h2, h3 { color: #333; }
            table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; }
            tr:nth-child(even) { background-color: #f9f9f9; }
            .chart-container { width: 100%; height: 400px; margin-bottom: 30px; }
            .winner { font-weight: bold; color: green; }
            .loser { color: #d9534f; }
            .summary { background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
        </style>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <body>
        <h1>Database Performance Comparison: SQLite vs PostgreSQL</h1>
        <p>Generated on: 2025-03-20 11:03:08</p>
        
        <div class="summary">
            <h2>Executive Summary</h2>
            <p>This report compares the performance of SQLite and PostgreSQL for the Metis RAG system across various operations:</p>
            <ul>
                <li>Document CRUD operations</li>
                <li>Chunk storage and retrieval</li>
                <li>Query performance</li>
                <li>Batch processing</li>
            </ul>
        </div>
        
        <h2>1. Document Operations</h2>
        <div class="chart-container">
            <canvas id="documentChart"></canvas>
        </div>
        
        <h3>Document Operations Comparison</h3>
        <table>
            <tr>
                <th>File Size</th>
                <th>Operation</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    
                <tr>
                    <td>small (5 KB)</td>
                    <td>Create</td>
                    <td class="loser">0.0039</td>
                    <td class="winner">0.0032</td>
                    <td>18.53%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>small (5 KB)</td>
                    <td>Retrieve</td>
                    <td class="loser">0.0011</td>
                    <td class="winner">0.0007</td>
                    <td>36.91%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>small (5 KB)</td>
                    <td>Update</td>
                    <td class="loser">0.0021</td>
                    <td class="winner">0.0019</td>
                    <td>7.06%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>small (5 KB)</td>
                    <td>Delete</td>
                    <td class="winner">0.0008</td>
                    <td class="loser">0.0008</td>
                    <td>3.05%</td>
                    <td>SQLite</td>
                </tr>
                
                <tr>
                    <td>small (5 KB)</td>
                    <td>Total</td>
                    <td class="loser">0.0078</td>
                    <td class="winner">0.0066</td>
                    <td>15.86%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>medium (50 KB)</td>
                    <td>Create</td>
                    <td class="loser">0.0026</td>
                    <td class="winner">0.0019</td>
                    <td>24.13%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>medium (50 KB)</td>
                    <td>Retrieve</td>
                    <td class="winner">0.0006</td>
                    <td class="loser">0.0007</td>
                    <td>17.33%</td>
                    <td>SQLite</td>
                </tr>
                
                <tr>
                    <td>medium (50 KB)</td>
                    <td>Update</td>
                    <td class="loser">0.0027</td>
                    <td class="winner">0.0019</td>
                    <td>30.60%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>medium (50 KB)</td>
                    <td>Delete</td>
                    <td class="loser">0.0006</td>
                    <td class="winner">0.0006</td>
                    <td>5.30%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>medium (50 KB)</td>
                    <td>Total</td>
                    <td class="loser">0.0065</td>
                    <td class="winner">0.0051</td>
                    <td>21.17%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>large (500 KB)</td>
                    <td>Create</td>
                    <td class="loser">0.0062</td>
                    <td class="winner">0.0052</td>
                    <td>16.19%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>large (500 KB)</td>
                    <td>Retrieve</td>
                    <td class="loser">0.0012</td>
                    <td class="winner">0.0008</td>
                    <td>33.10%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>large (500 KB)</td>
                    <td>Update</td>
                    <td class="loser">0.0032</td>
                    <td class="winner">0.0027</td>
                    <td>17.39%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>large (500 KB)</td>
                    <td>Delete</td>
                    <td class="loser">0.0006</td>
                    <td class="winner">0.0005</td>
                    <td>18.01%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>large (500 KB)</td>
                    <td>Total</td>
                    <td class="loser">0.0112</td>
                    <td class="winner">0.0092</td>
                    <td>18.46%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>xlarge (2000 KB)</td>
                    <td>Create</td>
                    <td class="loser">0.0337</td>
                    <td class="winner">0.0151</td>
                    <td>55.28%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>xlarge (2000 KB)</td>
                    <td>Retrieve</td>
                    <td class="loser">0.0026</td>
                    <td class="winner">0.0019</td>
                    <td>23.95%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>xlarge (2000 KB)</td>
                    <td>Update</td>
                    <td class="loser">0.0072</td>
                    <td class="winner">0.0068</td>
                    <td>4.50%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>xlarge (2000 KB)</td>
                    <td>Delete</td>
                    <td class="winner">0.0006</td>
                    <td class="loser">0.0008</td>
                    <td>38.47%</td>
                    <td>SQLite</td>
                </tr>
                
                <tr>
                    <td>xlarge (2000 KB)</td>
                    <td>Total</td>
                    <td class="loser">0.0440</td>
                    <td class="winner">0.0247</td>
                    <td>43.89%</td>
                    <td>PostgreSQL</td>
                </tr>
                
        </table>
        
        <h2>2. Chunk Operations</h2>
        <div class="chart-container">
            <canvas id="chunkChart"></canvas>
        </div>
        
        <h3>Chunk Operations Comparison</h3>
        <table>
            <tr>
                <th>File Size</th>
                <th>Strategy</th>
                <th>Operation</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>recursive</td>
                        <td>Insert</td>
                        <td class="winner">0.0053</td>
                        <td class="loser">0.0198</td>
                        <td>272.60%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>recursive</td>
                        <td>Retrieve</td>
                        <td class="winner">0.0007</td>
                        <td class="loser">0.0034</td>
                        <td>357.46%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>recursive</td>
                        <td>Update</td>
                        <td class="winner">0.0099</td>
                        <td class="loser">0.0118</td>
                        <td>19.68%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Insert</td>
                        <td class="winner">0.0013</td>
                        <td class="loser">0.0049</td>
                        <td>272.60%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="winner">0.0002</td>
                        <td class="loser">0.0009</td>
                        <td>357.46%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Update</td>
                        <td class="winner">0.0025</td>
                        <td class="loser">0.0030</td>
                        <td>19.68%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>token</td>
                        <td>Insert</td>
                        <td class="loser">0.0068</td>
                        <td class="winner">0.0025</td>
                        <td>62.68%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>token</td>
                        <td>Retrieve</td>
                        <td class="loser">0.0008</td>
                        <td class="winner">0.0006</td>
                        <td>24.55%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>token</td>
                        <td>Update</td>
                        <td class="loser">0.0037</td>
                        <td class="winner">0.0034</td>
                        <td>8.20%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Insert</td>
                        <td class="loser">0.0017</td>
                        <td class="winner">0.0006</td>
                        <td>62.68%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="loser">0.0002</td>
                        <td class="winner">0.0002</td>
                        <td>24.55%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>small (5 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Update</td>
                        <td class="loser">0.0009</td>
                        <td class="winner">0.0008</td>
                        <td>8.20%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>recursive</td>
                        <td>Insert</td>
                        <td class="loser">0.0074</td>
                        <td class="winner">0.0053</td>
                        <td>27.98%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>recursive</td>
                        <td>Retrieve</td>
                        <td class="winner">0.0011</td>
                        <td class="loser">0.0011</td>
                        <td>0.83%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>recursive</td>
                        <td>Update</td>
                        <td class="loser">0.0327</td>
                        <td class="winner">0.0294</td>
                        <td>10.08%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Insert</td>
                        <td class="loser">0.0002</td>
                        <td class="winner">0.0001</td>
                        <td>27.98%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="winner">0.0000</td>
                        <td class="loser">0.0000</td>
                        <td>0.83%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Update</td>
                        <td class="loser">0.0009</td>
                        <td class="winner">0.0008</td>
                        <td>10.08%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>token</td>
                        <td>Insert</td>
                        <td class="loser">0.0054</td>
                        <td class="winner">0.0049</td>
                        <td>8.12%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>token</td>
                        <td>Retrieve</td>
                        <td class="loser">0.0010</td>
                        <td class="winner">0.0009</td>
                        <td>7.71%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>token</td>
                        <td>Update</td>
                        <td class="loser">0.0311</td>
                        <td class="winner">0.0254</td>
                        <td>18.27%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Insert</td>
                        <td class="loser">0.0001</td>
                        <td class="winner">0.0001</td>
                        <td>8.12%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="loser">0.0000</td>
                        <td class="winner">0.0000</td>
                        <td>7.71%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>medium (50 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Update</td>
                        <td class="loser">0.0008</td>
                        <td class="winner">0.0007</td>
                        <td>18.27%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>recursive</td>
                        <td>Insert</td>
                        <td class="loser">0.0484</td>
                        <td class="winner">0.0413</td>
                        <td>14.72%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>recursive</td>
                        <td>Retrieve</td>
                        <td class="loser">0.0060</td>
                        <td class="winner">0.0044</td>
                        <td>26.67%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>recursive</td>
                        <td>Update</td>
                        <td class="loser">0.3961</td>
                        <td class="winner">0.2313</td>
                        <td>41.59%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Insert</td>
                        <td class="loser">0.0001</td>
                        <td class="winner">0.0001</td>
                        <td>14.72%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="loser">0.0000</td>
                        <td class="winner">0.0000</td>
                        <td>26.67%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Update</td>
                        <td class="loser">0.0010</td>
                        <td class="winner">0.0006</td>
                        <td>41.59%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>token</td>
                        <td>Insert</td>
                        <td class="loser">0.0611</td>
                        <td class="winner">0.0305</td>
                        <td>50.15%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>token</td>
                        <td>Retrieve</td>
                        <td class="loser">0.0182</td>
                        <td class="winner">0.0050</td>
                        <td>72.39%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>token</td>
                        <td>Update</td>
                        <td class="loser">0.2917</td>
                        <td class="winner">0.2580</td>
                        <td>11.53%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Insert</td>
                        <td class="loser">0.0002</td>
                        <td class="winner">0.0001</td>
                        <td>50.15%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="loser">0.0000</td>
                        <td class="winner">0.0000</td>
                        <td>72.39%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>large (500 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Update</td>
                        <td class="loser">0.0008</td>
                        <td class="winner">0.0007</td>
                        <td>11.53%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>recursive</td>
                        <td>Insert</td>
                        <td class="loser">0.2026</td>
                        <td class="winner">0.1307</td>
                        <td>35.47%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>recursive</td>
                        <td>Retrieve</td>
                        <td class="loser">0.0184</td>
                        <td class="winner">0.0161</td>
                        <td>12.45%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>recursive</td>
                        <td>Update</td>
                        <td class="loser">1.2033</td>
                        <td class="winner">0.8459</td>
                        <td>29.70%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Insert</td>
                        <td class="loser">0.0001</td>
                        <td class="winner">0.0001</td>
                        <td>35.47%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="loser">0.0000</td>
                        <td class="winner">0.0000</td>
                        <td>12.45%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>recursive</td>
                        <td>Per-Chunk Update</td>
                        <td class="loser">0.0008</td>
                        <td class="winner">0.0006</td>
                        <td>29.70%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>token</td>
                        <td>Insert</td>
                        <td class="winner">0.1597</td>
                        <td class="loser">0.1691</td>
                        <td>5.88%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>token</td>
                        <td>Retrieve</td>
                        <td class="loser">0.0185</td>
                        <td class="winner">0.0160</td>
                        <td>13.40%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>token</td>
                        <td>Update</td>
                        <td class="loser">1.1612</td>
                        <td class="winner">0.8438</td>
                        <td>27.33%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Insert</td>
                        <td class="winner">0.0001</td>
                        <td class="loser">0.0001</td>
                        <td>5.88%</td>
                        <td>SQLite</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Retrieve</td>
                        <td class="loser">0.0000</td>
                        <td class="winner">0.0000</td>
                        <td>13.40%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
                    <tr>
                        <td>xlarge (2000 KB)</td>
                        <td>token</td>
                        <td>Per-Chunk Update</td>
                        <td class="loser">0.0008</td>
                        <td class="winner">0.0006</td>
                        <td>27.33%</td>
                        <td>PostgreSQL</td>
                    </tr>
                    
        </table>
        
        <h2>3. Query Performance</h2>
        <div class="chart-container">
            <canvas id="queryChart"></canvas>
        </div>
        
        <h3>Query Performance Comparison</h3>
        <table>
            <tr>
                <th>Query</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    
            <tr>
                <td>What is the main purpose of the RAG system?</td>
                <td class="loser">0.0012</td>
                <td class="winner">0.0004</td>
                <td>63.22%</td>
                <td>PostgreSQL</td>
            </tr>
            
            <tr>
                <td>How does the document analysis service work?</td>
                <td class="loser">0.0004</td>
                <td class="winner">0.0003</td>
                <td>12.64%</td>
                <td>PostgreSQL</td>
            </tr>
            
            <tr>
                <td>What are the key components of the LangGraph integ...</td>
                <td class="loser">0.0003</td>
                <td class="winner">0.0003</td>
                <td>3.13%</td>
                <td>PostgreSQL</td>
            </tr>
            
            <tr>
                <td>Explain the database schema for documents and chun...</td>
                <td class="loser">0.0003</td>
                <td class="winner">0.0003</td>
                <td>8.89%</td>
                <td>PostgreSQL</td>
            </tr>
            
            <tr>
                <td>How does the memory integration enhance the system...</td>
                <td class="loser">0.0003</td>
                <td class="winner">0.0003</td>
                <td>5.27%</td>
                <td>PostgreSQL</td>
            </tr>
            
        </table>
        
        <h2>4. Batch Processing</h2>
        <div class="chart-container">
            <canvas id="batchChart"></canvas>
        </div>
        
        <h3>Batch Processing Comparison</h3>
        <table>
            <tr>
                <th>Batch Size</th>
                <th>Metric</th>
                <th>SQLite (s)</th>
                <th>PostgreSQL (s)</th>
                <th>Difference</th>
                <th>Faster DB</th>
            </tr>
    
                <tr>
                    <td>1</td>
                    <td>Total Time</td>
                    <td class="loser">0.0074</td>
                    <td class="winner">0.0057</td>
                    <td>22.76%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>1</td>
                    <td>Per-Document Time</td>
                    <td class="loser">0.0074</td>
                    <td class="winner">0.0057</td>
                    <td>22.76%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>5</td>
                    <td>Total Time</td>
                    <td class="loser">0.0296</td>
                    <td class="winner">0.0266</td>
                    <td>10.06%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>5</td>
                    <td>Per-Document Time</td>
                    <td class="loser">0.0059</td>
                    <td class="winner">0.0053</td>
                    <td>10.06%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>10</td>
                    <td>Total Time</td>
                    <td class="loser">0.1694</td>
                    <td class="winner">0.0518</td>
                    <td>69.41%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>10</td>
                    <td>Per-Document Time</td>
                    <td class="loser">0.0169</td>
                    <td class="winner">0.0052</td>
                    <td>69.41%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>20</td>
                    <td>Total Time</td>
                    <td class="loser">0.1466</td>
                    <td class="winner">0.1133</td>
                    <td>22.70%</td>
                    <td>PostgreSQL</td>
                </tr>
                
                <tr>
                    <td>20</td>
                    <td>Per-Document Time</td>
                    <td class="loser">0.0073</td>
                    <td class="winner">0.0057</td>
                    <td>22.70%</td>
                    <td>PostgreSQL</td>
                </tr>
                
        </table>
        
        <h2>5. Recommendations</h2>
        <div class="summary">
            <p>Based on the benchmark results, here are some recommendations:</p>
            <ul>
                <li><strong>Document Operations:</strong> PostgreSQL performs better for basic document operations.</li>
                <li><strong>Chunk Operations:</strong> PostgreSQL is more efficient for chunk storage and retrieval.</li>
                <li><strong>Query Performance:</strong> PostgreSQL provides faster query response times.</li>
                <li><strong>Batch Processing:</strong> PostgreSQL handles batch processing more efficiently.</li>
            </ul>
            <p><strong>Overall Recommendation:</strong> PostgreSQL appears to be the better choice for this workload based on overall performance.</p>
        </div>
        
        <script>
            // Document operations chart
            const docCtx = document.getElementById('documentChart').getContext('2d');
            const docChart = new Chart(docCtx, {
                type: 'bar',
                data: {
                    labels: ["small", "medium", "large", "xlarge"],
                    datasets: [
                        {
                            label: 'SQLite - Create',
                            data: [0.003882249196370443, 0.00255433718363444, 0.006193637847900391, 0.033665736516316734],
                            backgroundColor: 'rgba(54, 162, 235, 0.6)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'PostgreSQL - Create',
                            data: [0.003162940343221029, 0.0019379456837972004, 0.00519100824991862, 0.015054543813069662],
                            backgroundColor: 'rgba(255, 99, 132, 0.6)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'SQLite - Retrieve',
                            data: [0.0010714530944824219, 0.000606695810953776, 0.0012172857920328777, 0.002557039260864258],
                            backgroundColor: 'rgba(75, 192, 192, 0.6)',
                            borderColor: 'rgba(75, 192, 192, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'PostgreSQL - Retrieve',
                            data: [0.0006759961446126302, 0.0007118384043375651, 0.0008143583933512369, 0.0019447008768717449],
                            backgroundColor: 'rgba(255, 159, 64, 0.6)',
                            borderColor: 'rgba(255, 159, 64, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Document Operation Times by File Size'
                        },
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Time (seconds)'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'File Size'
                            }
                        }
                    }
                }
            });
            
            // Query performance chart
            const queryCtx = document.getElementById('queryChart').getContext('2d');
            const queryChart = new Chart(queryCtx, {
                type: 'bar',
                data: {
                    labels: ["Query 1", "Query 2", "Query 3", "Query 4", "Query 5"],
                    datasets: [
                        {
                            label: 'SQLite',
                            data: [0.0011973381042480469, 0.0003578662872314453, 0.0003227392832438151, 0.00031566619873046875, 0.0002849896748860677],
                            backgroundColor: 'rgba(54, 162, 235, 0.6)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'PostgreSQL',
                            data: [0.00044043858846028644, 0.00031264623006184894, 0.00031264623006184894, 0.0002876122792561849, 0.00026996930440266925],
                            backgroundColor: 'rgba(255, 99, 132, 0.6)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Query Response Times'
                        },
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Time (seconds)'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'Query'
                            }
                        }
                    }
                }
            });
            
            // Batch processing chart
            const batchCtx = document.getElementById('batchChart').getContext('2d');
            const batchChart = new Chart(batchCtx, {
                type: 'bar',
                data: {
                    labels: [1, 5, 10, 20],
                    datasets: [
                        {
                            label: 'SQLite',
                            data: [0.007443904876708984, 0.029572010040283203, 0.16942405700683594, 0.14658093452453613],
                            backgroundColor: 'rgba(54, 162, 235, 0.6)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'PostgreSQL',
                            data: [0.005749940872192383, 0.0265960693359375, 0.051831960678100586, 0.11331009864807129],
                            backgroundColor: 'rgba(255, 99, 132, 0.6)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Batch Processing Times'
                        },
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Time (seconds)'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'Batch Size'
                            }
                        }
                    }
                }
            });
        </script>
    </body>
    </html>

================
File: tests/results/metis_rag_test_report.html
================
<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Test Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1, h2, h3 { color: #333; }
        .section { margin-bottom: 30px; }
        .summary { display: flex; justify-content: space-between; margin-bottom: 20px; }
        .summary-box { background-color: #f5f5f5; padding: 15px; border-radius: 5px; width: 30%; text-align: center; }
        .success { color: green; }
        .failure { color: red; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
        .details { background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-top: 10px; }
        .details pre { white-space: pre-wrap; overflow-x: auto; }
        .toggle-btn { background-color: #4CAF50; color: white; padding: 5px 10px; border: none; border-radius: 3px; cursor: pointer; }
        .toggle-btn:hover { background-color: #45a049; }
    </style>
    <script>
        function toggleDetails(id) {
            var details = document.getElementById(id);
            if (details.style.display === "none") {
                details.style.display = "block";
            } else {
                details.style.display = "none";
            }
        }
    </script>
</head>
<body>
    <h1>Metis RAG Test Report</h1>
    <p>Generated on: 2025-03-07 10:58:40</p>
    
    <div class="summary">
        <div class="summary-box">
            <h3>Test Suites</h3>
            <p class="failure">0/1 (0.0%)</p>
        </div>
        <div class="summary-box">
            <h3>Total Duration</h3>
            <p>0.18 seconds</p>
        </div>
        <div class="summary-box">
            <h3>Timestamp</h3>
            <p>2025-03-07 10:58:40</p>
        </div>
    </div>
    
    <div class="section">
        <h2>Test Suite Results</h2>
        <table>
            <tr>
                <th>Test Suite</th>
                <th>Description</th>
                <th>Status</th>
                <th>Duration</th>
                <th>Details</th>
            </tr>
            
            <tr>
                <td>RAG Quality Tests</td>
                <td>Tests for factual accuracy, relevance, and citation quality</td>
                <td class="failure">Failure</td>
                <td>0.18 seconds</td>
                <td><button class="toggle-btn" onclick="toggleDetails('details-0')">Toggle Details</button></td>
            </tr>
            <tr>
                <td colspan="5">
                    <div id="details-0" class="details" style="display: none;">
                        <h4>Output:</h4>
                        <pre>============================= test session starts ==============================
platform darwin -- Python 3.10.16, pytest-8.3.5, pluggy-1.5.0 -- /Users/charleshoward/Metis_RAG/venv_py310/bin/python
cachedir: .pytest_cache
rootdir: /Users/charleshoward/Metis_RAG
configfile: pyproject.toml
plugins: asyncio-0.25.3, anyio-3.7.1
asyncio: mode=strict, asyncio_default_fixture_loop_scope=None
collecting ... collected 0 items

============================ no tests ran in 0.00s =============================
</pre>
                        <h4>Error Output:</h4>
                        <pre>/Users/charleshoward/Metis_RAG/venv_py310/lib/python3.10/site-packages/pytest_asyncio/plugin.py:207: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
ERROR: file or directory not found: tests.test_rag_quality

</pre>
                        <h4>Reports:</h4>
                        <ul>
                            <li><a href="test_quality_results.json">test_quality_results.json</a></li><li><a href="test_multi_doc_results.json">test_multi_doc_results.json</a></li><li><a href="test_citation_results.json">test_citation_results.json</a></li>
                        </ul>
                    </div>
                </td>
            </tr>
            
        </table>
    </div>
    
    <div class="section">
        <h2>Individual Test Reports</h2>
        <p>The following reports were generated by the test suites:</p>
        <ul>
            <li><a href="test_quality_results.json">test_quality_results.json</a></li><li><a href="test_multi_doc_results.json">test_multi_doc_results.json</a></li><li><a href="test_citation_results.json">test_citation_results.json</a></li>
        </ul>
    </div>
    
    <div class="section">
        <h2>Specific Report Links</h2>
        <h3>Performance Report</h3>
        <p><a href="performance_benchmark_report.html">Performance Benchmark Report</a></p>
        
        <h3>Edge Case Report</h3>
        <p><a href="edge_case_test_report.html">Edge Case Test Report</a></p>
    </div>
</body>
</html>

================
File: tests/results/summary_report.md
================
# Metis RAG Test Suite Summary Report

## Overview
This report summarizes the results of testing the Metis RAG system with a series of increasingly complex queries
to verify the fixes implemented for entity preservation, minimum context requirements, and citation handling.

## Test Results Summary

| Query ID | Entity Preservation | Chunks Retrieved | Chunks Used | Execution Time (s) |
|----------|---------------------|------------------|-------------|-------------------|
| basic_entity | ✅ | 15 | 6 | 60.84 |
| multi_entity | ✅ | 15 | 4 | 31.57 |
| potential_ambiguity | ✅ | 15 | 5 | 38.02 |
| context_synthesis | ✅ | 15 | 3 | 49.76 |
| specialized_terminology | ✅ | 15 | 4 | 37.52 |

## Detailed Analysis

### Entity Preservation

**Query basic_entity**: All entities preserved (Stabilium)

**Query multi_entity**: All entities preserved ()

**Query potential_ambiguity**: All entities preserved ()

**Query context_synthesis**: All entities preserved (Stabilium, Heisenberg, Uncertainty Principle)

**Query specialized_terminology**: All entities preserved ()


### Context Selection

**Query basic_entity**: Retrieved 15 chunks, used 6 in final context

**Query multi_entity**: Retrieved 15 chunks, used 4 in final context

**Query potential_ambiguity**: Retrieved 15 chunks, used 5 in final context

**Query context_synthesis**: Retrieved 15 chunks, used 3 in final context

**Query specialized_terminology**: Retrieved 15 chunks, used 4 in final context


### Source Relevance

**Query basic_entity**: Used 6 sources with relevance scores: 0.95, 0.90, 0.90, 0.85, 0.80, 0.71

**Query multi_entity**: Used 4 sources with relevance scores: 0.95, 0.85, 0.80, 0.75

**Query potential_ambiguity**: Used 5 sources with relevance scores: 0.95, 0.80, 0.60, 0.85, 0.75

**Query context_synthesis**: Used 3 sources with relevance scores: 0.85, 0.95, 0.90

**Query specialized_terminology**: Used 4 sources with relevance scores: 0.95, 0.90, 0.85, 0.75


## Conclusion

This test suite has verified the following aspects of the Metis RAG system:

1. **Entity Preservation**: The system's ability to preserve named entities during query refinement
2. **Minimum Context Requirements**: The system's ability to retrieve and use sufficient context
3. **Citation Handling**: The system's ability to track and cite sources properly

See individual test reports for detailed analysis of each query.

================
File: tests/retrieval_judge/data/performance_test_document.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/data/product_specifications.md
================
# Product Specifications

## System Requirements

The Metis RAG system requires the following minimum specifications:
- CPU: 4 cores, 2.5GHz or higher
- RAM: 16GB minimum, 32GB recommended
- Storage: 100GB SSD
- Operating System: Ubuntu 22.04 LTS, Windows Server 2019, or macOS 12+
- Network: 100Mbps internet connection

## API Reference

### Authentication

All API requests require authentication using JWT tokens. To obtain a token:

```
POST /api/auth/token
{
  "username": "your_username",
  "password": "your_password"
}
```

The response will include an access token valid for 24 hours.

### Document Management

#### Upload Document

```
POST /api/documents/upload
Content-Type: multipart/form-data
Authorization: Bearer <token>

Form fields:
- file: The document file
- tags: Comma-separated tags (optional)
- folder: Target folder path (optional)
```

#### List Documents

```
GET /api/documents/list
Authorization: Bearer <token>
```

Optional query parameters:
- folder: Filter by folder
- tags: Filter by tags (comma-separated)
- page: Page number (default: 1)
- limit: Items per page (default: 20)

### Chat API

#### Create Chat Session

```
POST /api/chat/sessions
Authorization: Bearer <token>
{
  "title": "Optional chat title"
}
```

#### Send Message

```
POST /api/chat/messages
Authorization: Bearer <token>
{
  "session_id": "chat_session_id",
  "content": "Your message here",
  "use_rag": true
}
```

## Performance Benchmarks

The system has been benchmarked with the following results:
- Document processing: 5 pages/second
- Vector search latency: <50ms for 10k documents
- End-to-end query response time: <2 seconds
- Maximum documents: 100,000
- Maximum vector store size: 10GB

================
File: tests/retrieval_judge/data/quarterly_report.txt
================
Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs

================
File: tests/retrieval_judge/data/technical_documentation.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/data/test_document.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/data/timing_test_document.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/results/chunk_evaluation_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.8,
        "chunk2": 0.5,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.9
      },
      "needs_refinement": true,
      "justification": "The query 'How does the system work?' is quite broad. While chunk 1 (technical_documentation.md) discusses the core components of the system \u2013 vector store, document processing pipeline, and API layer \u2013 it doesn't provide a high-level explanation of the system's overall functionality. Chunk 2 is tangentially related, focusing on document processing. Chunk 3 is irrelevant. Chunk 4 is irrelevant. Chunk 5, however, directly addresses the question by describing the Retrieval Judge's role in enhancing the RAG retrieval process, which is a key aspect of how the system works.  Because the initial chunks are too detailed and don't answer the question at a high level, and because chunk 5 provides a more relevant explanation, the query needs refinement to focus on the overall architecture and operation of the RAG system."
    },
    "time": 9.267931938171387
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.95,
        "chunk2": 0.3,
        "chunk3": 0.98,
        "chunk4": 0.1,
        "chunk5": 0.2
      },
      "needs_refinement": true,
      "justification": "The query asks 'Tell me about the API'. Chunk 1 and 3 directly address this, describing the API layer's implementation and endpoints. Chunk 5 discusses the Retrieval Judge, which is related to improving retrieval but doesn't directly answer the question. Chunk 2 is about the document processing pipeline, which is a component *related* to the API but not the API itself. Chunk 4 discusses LLM integration, which is also tangential.  Because chunks 1 and 3 are highly relevant and chunks 2, 4, and 5 are not directly addressing the user's request, the query likely needs refinement to focus on the API layer specifically.  A more targeted query, such as 'Describe the API endpoints' would likely yield better results."
    },
    "time": 8.807852029800415
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.8,
        "chunk2": 0.5,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.9
      },
      "needs_refinement": false,
      "justification": "Chunk 1 directly addresses the query by explaining the core functionality of a vector store \u2013 embeddings, similarity search, and metadata filtering. This is the most relevant chunk. Chunk 2 discusses the document processing pipeline, which is related but doesn't directly answer the question about how a vector store works. Chunk 3 is entirely irrelevant. Chunk 4 is also irrelevant. Chunk 5, while discussing a Retrieval Judge, provides a mechanism *for* improving retrieval, which is relevant to understanding how a vector store is used in a RAG system, and therefore deserves a high relevance score.  Because the most relevant chunk provides a solid explanation, and the other chunks are either irrelevant or provide supporting details, no query refinement is immediately needed."
    },
    "time": 8.454214334487915
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.8,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.4
      },
      "needs_refinement": true,
      "justification": "Chunks 1 and 2 directly address the query about the document processing pipeline, detailing its stages (validation, parsing, chunking, metadata extraction). Chunk 3 is entirely irrelevant. Chunk 4 is also irrelevant. Chunk 5 discusses the Retrieval Judge, which is a component *related* to the pipeline but doesn't describe the pipeline itself.  The retrieved chunks are somewhat broad and don't provide a concise overview of the document processing pipeline.  Therefore, the query needs refinement to focus on obtaining a specific definition or explanation of the pipeline stages."
    },
    "time": 7.412783145904541
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.95,
        "chunk2": 0.3,
        "chunk3": 0.1,
        "chunk4": 0.9,
        "chunk5": 0.95
      },
      "needs_refinement": false,
      "justification": "Chunks 1, 4, and 5 directly address the query about context window optimization. Chunk 1 describes the LLM integration component's role in managing context window optimization. Chunk 4 specifically mentions the LLM integration component and its support for context window optimization. Chunk 5 details the Retrieval Judge's function of optimizing context assembly, which is a key aspect of context window management. Chunk 2 is only tangentially related as it discusses the document processing pipeline, which is a broader component of the system. Therefore, no query refinement is needed based on these retrieved chunks."
    },
    "time": 7.632442951202393
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.8,
        "chunk2": 0.5,
        "chunk3": 0.1,
        "chunk4": 0.9,
        "chunk5": 0.7
      },
      "needs_refinement": false,
      "justification": "Chunk 1 and 4 directly address the query by mentioning supported embedding models (Llama 3, Gemma, Mistral). Chunk 4 is the most relevant as it explicitly lists the models. Chunk 1 provides context about the vector store and HNSW index, which is related but not a direct answer. Chunk 2 discusses document processing, which is a prerequisite but doesn't answer the question. Chunk 3 is irrelevant. Chunk 5 discusses the Retrieval Judge, which is related to improving retrieval but doesn't list the supported embedding models.  The retrieved chunks provide sufficient information to answer the query, so refinement is not immediately needed."
    },
    "time": 7.953837871551514
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.6,
        "chunk3": 0.1,
        "chunk4": 0.0,
        "chunk5": 0.8
      },
      "needs_refinement": true,
      "justification": "Chunks 1 and 5 directly address the query's focus on 'chunking strategies' and their impact on retrieval performance. Chunk 1 specifically discusses configurable strategies within the document processing pipeline. Chunk 5 describes the Retrieval Judge, which is a system designed to evaluate and refine retrieval based on chunk relevance \u2013 indicating a need to understand the strategies themselves. Chunk 2 is tangentially related as it discusses the document processing pipeline, but doesn't directly address the strategies. Chunk 3 and 4 are completely irrelevant to the query.  Because chunks 1 and 5 provide some information, but the overall discussion is still quite broad and doesn't fully explain the different chunking strategies, a refinement of the query is needed to narrow the focus and obtain more specific answers."
    },
    "time": 8.762250185012817
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.4,
        "chunk3": 0.9,
        "chunk4": 0.1,
        "chunk5": 0.6
      },
      "needs_refinement": true,
      "justification": "Several chunks directly address aspects of the query. Chunk 1 and 3 both discuss authentication and document management endpoints (specifically the API layer). Chunk 5 discusses the Retrieval Judge which is relevant to improving the retrieval process, but doesn't directly answer the user's question. Chunk 2 is only tangentially related as it discusses the document processing pipeline, which is a component *involved* in document management but doesn't detail the authentication or endpoints.  The retrieved chunks are somewhat broad and don't provide a consolidated answer.  To improve the response, the query could be refined to focus on specific authentication methods and the available API endpoints for document management."
    },
    "time": 8.091121912002563
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.4,
        "chunk3": 0.1,
        "chunk4": 0.2,
        "chunk5": 0.95
      },
      "needs_refinement": true,
      "justification": "The query 'RAG?' refers to Retrieval Augmented Generation. Chunk 1 ('vector store', 'HNSW index') and Chunk 5 ('Retrieval Judge') directly address the core concept of RAG, specifically the retrieval component. Chunk 5 is the most relevant as it explicitly defines the Retrieval Judge, which is a key element in RAG systems. Chunk 2 is tangentially related as it describes the document processing pipeline, but doesn't directly answer the question. Chunk 3 is irrelevant. Chunk 4 is also irrelevant.  Because several chunks are low relevance and the query is broad, it suggests the user may need a more specific question, such as 'How does a Retrieval Judge work in RAG?' or 'What are the components of a RAG system?'"
    },
    "time": 8.8508939743042
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.3,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.8
      },
      "needs_refinement": true,
      "justification": "The query 'Embeddings?' directly relates to the concept of vector embeddings, which are explicitly mentioned in chunk 1 and chunk 5. Chunk 1 provides a detailed explanation of the vector store's role in storing and utilizing embeddings. Chunk 5 discusses the Retrieval Judge's role in evaluating relevance, suggesting a need for more specific information about embeddings themselves. Chunks 2, 3, and 4 are tangentially related to document processing and API management, respectively, and do not directly address the user's question about embeddings.  Because chunks 2, 3, and 4 are low relevance and chunk 5 discusses evaluation of relevance, it suggests the query might need refinement to focus on the specific technical aspects of embeddings."
    },
    "time": 8.577693939208984
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.8,
        "chunk3": 0.1,
        "chunk4": 0.2,
        "chunk5": 0.95
      },
      "needs_refinement": false,
      "justification": "The retrieved chunks largely address the user's detailed query about the Metis RAG system. Chunk 1 and 2 directly address the document processing pipeline, chunking strategies, and vector store management \u2013 key aspects of the user's question. Chunk 5 provides a detailed explanation of the Retrieval Judge, which is also a crucial component of the system. Chunk 3 is tangentially related as it discusses the API layer, and Chunk 4 discusses LLM integration, which is not directly relevant to the core aspects of the user's question.  The relevance scores reflect this.  Because the chunks cover all the requested aspects in detail, no query refinement is needed."
    },
    "time": 8.356603860855103
  }
]

================
File: tests/retrieval_judge/results/context_optimization_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 13.782373189926147
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 9.767060041427612
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 12.937751054763794
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 12.568903923034668
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 14.531967878341675
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 8.959358930587769
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 11.965378761291504
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 12.734175205230713
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 11.795776128768921
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 7.629971981048584
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "original_chunk_count": 5,
    "optimized_chunk_count": 3,
    "optimized_chunk_ids": [
      "chunk1",
      "chunk3",
      "chunk2"
    ],
    "time": 14.76965880393982
  }
]

================
File: tests/retrieval_judge/results/judge_edge_case_analysis.json
================
{
  "by_type": {
    "long": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 0,
          "complex": 1
        },
        "avg_time": 11.576659202575684
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 0.0,
        "avg_time": 8.356603860855103
      },
      "query_refinement": {
        "avg_length_change_percent": 61.26582278481012,
        "avg_time": 4.841519117355347
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 40.0,
        "avg_time": 14.76965880393982
      }
    },
    "ambiguous": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 7.90027117729187
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 1.0,
        "avg_time": 9.0378919839859
      },
      "query_refinement": {
        "avg_length_change_percent": 650.5714285714286,
        "avg_time": 1.9932677745819092
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 11.77471661567688
      }
    },
    "domain-specific": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 8.493697881698608
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 0.0,
        "avg_time": 7.793140411376953
      },
      "query_refinement": {
        "avg_length_change_percent": 138.01948051948054,
        "avg_time": 1.816763997077942
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 11.745663404464722
      }
    },
    "short": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 1,
          "moderate": 1,
          "complex": 0
        },
        "avg_time": 8.945188164710999
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 1.0,
        "avg_time": 8.714293956756592
      },
      "query_refinement": {
        "avg_length_change_percent": 887.5,
        "avg_time": 1.51704740524292
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 9.712874054908752
      }
    },
    "multi-part": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 8.343578577041626
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 1.0,
        "avg_time": 8.42668604850769
      },
      "query_refinement": {
        "avg_length_change_percent": 131.19658119658118,
        "avg_time": 2.0405884981155396
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 12.349776983261108
      }
    },
    "typo": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 7.732823133468628
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 0.5,
        "avg_time": 7.933498740196228
      },
      "query_refinement": {
        "avg_length_change_percent": 70.64516129032256,
        "avg_time": 1.8848679065704346
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 12.753327488899231
      }
    }
  },
  "overall": {
    "query_analysis": {
      "complexity_distribution": {
        "moderate": 9,
        "simple": 1,
        "complex": 1
      },
      "avg_time": 8.58252518827265
    },
    "chunk_evaluation": {
      "needs_refinement_rate": 0.6363636363636364,
      "avg_time": 8.378875103863804
    },
    "query_refinement": {
      "avg_length_change_percent": 347.0119205400397,
      "avg_time": 2.122417298230258
    },
    "context_optimization": {
      "avg_chunk_reduction_percent": 76.36363636363637,
      "avg_time": 11.949306899851019
    }
  },
  "type_performance_ranking": {
    "by_refinement_rate": [
      {
        "type": "long",
        "rate": 0.0
      },
      {
        "type": "domain-specific",
        "rate": 0.0
      },
      {
        "type": "typo",
        "rate": 0.5
      },
      {
        "type": "ambiguous",
        "rate": 1.0
      },
      {
        "type": "short",
        "rate": 1.0
      },
      {
        "type": "multi-part",
        "rate": 1.0
      }
    ],
    "by_chunk_reduction": [
      {
        "type": "ambiguous",
        "reduction": 80.0
      },
      {
        "type": "domain-specific",
        "reduction": 80.0
      },
      {
        "type": "short",
        "reduction": 80.0
      },
      {
        "type": "multi-part",
        "reduction": 80.0
      },
      {
        "type": "typo",
        "reduction": 80.0
      },
      {
        "type": "long",
        "reduction": 40.0
      }
    ]
  }
}

================
File: tests/retrieval_judge/results/performance_analysis_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "standard": {
      "total_time": 176.16782879829407,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.814018726348877,
        "ollama_generate": 154.82823419570923,
        "rag_engine_enhanced_retrieval": 155.65416193008423
      }
    },
    "judge": {
      "total_time": 17.874395847320557,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 38.319051027297974,
        "judge_analyze_query": 11.228012084960938,
        "judge_evaluate_chunks": 6.622217893600464,
        "rag_engine_enhanced_retrieval": 17.857052087783813
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 1,
      "vector_store_cache_misses": 2,
      "ollama_cache_hits": 1,
      "ollama_cache_misses": 5
    }
  },
  {
    "query": "What are the components of the RAG engine?",
    "standard": {
      "total_time": 160.53554272651672,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.8336210250854492,
        "ollama_generate": 147.0875780582428,
        "rag_engine_enhanced_retrieval": 147.9596929550171
      }
    },
    "judge": {
      "total_time": 17.655393838882446,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 30.1685574054718,
        "judge_analyze_query": 11.909356117248535,
        "judge_evaluate_chunks": 5.723083019256592,
        "rag_engine_enhanced_retrieval": 17.63789200782776
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 2,
      "vector_store_cache_misses": 4,
      "ollama_cache_hits": 2,
      "ollama_cache_misses": 10
    }
  },
  {
    "query": "How does the vector store work?",
    "standard": {
      "total_time": 175.53075098991394,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.8636059761047363,
        "ollama_generate": 160.59179711341858,
        "rag_engine_enhanced_retrieval": 161.46342086791992
      }
    },
    "judge": {
      "total_time": 18.127063035964966,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 32.13697576522827,
        "judge_analyze_query": 11.290737867355347,
        "judge_evaluate_chunks": 6.814745187759399,
        "rag_engine_enhanced_retrieval": 18.110551118850708
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 3,
      "vector_store_cache_misses": 6,
      "ollama_cache_hits": 3,
      "ollama_cache_misses": 15
    }
  },
  {
    "query": "What is the role of the LLM integration component?",
    "standard": {
      "total_time": 172.4841389656067,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.8085780143737793,
        "ollama_generate": 158.92998576164246,
        "rag_engine_enhanced_retrieval": 159.7460000514984
      }
    },
    "judge": {
      "total_time": 17.969091653823853,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 30.641315460205078,
        "judge_analyze_query": 12.501478910446167,
        "judge_evaluate_chunks": 5.444411993026733,
        "rag_engine_enhanced_retrieval": 17.951611757278442
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 4,
      "vector_store_cache_misses": 8,
      "ollama_cache_hits": 4,
      "ollama_cache_misses": 20
    }
  },
  {
    "query": "How does the document processing pipeline work?",
    "standard": {
      "total_time": 172.63411116600037,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.873924970626831,
        "ollama_generate": 156.8555817604065,
        "rag_engine_enhanced_retrieval": 157.73775696754456
      }
    },
    "judge": {
      "total_time": 20.422394037246704,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 35.2523295879364,
        "judge_analyze_query": 14.513217687606812,
        "judge_evaluate_chunks": 5.88365912437439,
        "rag_engine_enhanced_retrieval": 20.402302980422974
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 5,
      "vector_store_cache_misses": 10,
      "ollama_cache_hits": 5,
      "ollama_cache_misses": 25
    }
  }
]

================
File: tests/retrieval_judge/results/performance_analysis.json
================
{
  "avg_standard_time": 171.47047452926637,
  "avg_judge_time": 18.409667682647704,
  "time_difference_percent": -89.26365152182187,
  "standard_component_averages": {
    "vector_store_search": 0.8387497425079345,
    "ollama_generate": 155.6586353778839,
    "rag_engine_enhanced_retrieval": 156.51220655441284
  },
  "judge_component_averages": {
    "vector_store_search": 0.0,
    "ollama_generate": 33.303645849227905,
    "judge_analyze_query": 12.28856053352356,
    "judge_evaluate_chunks": 6.097623443603515,
    "rag_engine_enhanced_retrieval": 18.39188199043274
  },
  "cache_statistics": {
    "vector_store_cache_hits": 15,
    "vector_store_cache_misses": 30,
    "vector_store_cache_hit_rate": 33.33333333333333,
    "ollama_cache_hits": 15,
    "ollama_cache_misses": 75,
    "ollama_cache_hit_rate": 16.666666666666664
  },
  "avg_standard_sources": 1.0,
  "avg_judge_sources": 1.0
}

================
File: tests/retrieval_judge/results/query_analysis_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the system work?\" is a moderate complexity question. It\u2019s not a simple factual question like \"What is the capital of France?\" but it doesn't require deep analytical reasoning. It necessitates understanding the overall architecture and functionality of the system, which likely involves multiple documents.  Therefore, retrieving a moderate number of chunks (k=8) is appropriate to capture a sufficient range of information. A relevance threshold of 0.6 suggests prioritizing documents that are strongly related to the system's operation, avoiding overly tangential results. Reranking is crucial here as initial retrieval might return documents discussing specific components without explicitly addressing the 'how it works' question. Reranking will help prioritize documents that provide a high-level explanation of the system's workflow."
    },
    "time": 7.263206243515015
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"Tell me about the API\" is moderately complex. It\u2019s not a simple factual question like \"What is the capital of France?\" but it\u2019s also not a complex analytical question. It clearly requires retrieval of information related to an API \u2013 likely documentation, tutorials, or explanations of its features.  The query is ambiguous as \u2018the API\u2019 could refer to many different APIs.  Therefore, a moderate retrieval approach is needed.  I recommend retrieving 8 chunks (k=8) to provide a good balance between breadth and precision. A relevance threshold of 0.6 is suggested to filter out less relevant results, prioritizing documents that discuss the API's functionality and purpose. Reranking is crucial here to further refine the results based on semantic similarity to the query, as different documents might use slightly different phrasing to describe the same API.  Without reranking, the initial retrieval might return documents that mention the API but don't directly address the user's request for information *about* it."
    },
    "time": 8.537336111068726
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the vektor store work?\" represents a moderate complexity question. It\u2019s not a simple factual recall but requires understanding the underlying mechanisms of a vektor store.  It necessitates accessing documents that explain the concepts of vector embeddings, similarity search, and potentially the architecture of vektor stores.  Because of this, a moderate number of chunks (k=8) is recommended to capture a range of explanations. A relevance threshold of 0.6 is suggested \u2013 this allows for some relevant but potentially tangential information to be retrieved, as the concept of a vektor store can be explained from various angles.  Reranking (true) is crucial here. Without reranking, the initial retrieval might return documents focused on specific implementations or use cases, rather than the core principles. Reranking will help prioritize documents that directly address the question of how a vektor store functions."
    },
    "time": 7.581515073776245
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"What is the document processing pipeline?\" is moderately complex. It\u2019s not a simple factual question like \"What is the capital of France?\" but it doesn't require deep analytical reasoning. It clearly indicates a need for information about a specific process \u2013 a document processing pipeline.  Therefore, we need to retrieve a sufficient number of relevant chunks (k=8) to cover different aspects of the pipeline. A relevance threshold of 0.6 suggests prioritizing chunks with a strong connection to the query, but allowing for some related information. Reranking is recommended to further refine the results by considering semantic similarity beyond just keyword matching.  Without more context about the domain (e.g., NLP, data science), assuming a general document processing pipeline is reasonable.  A lower k value (5-7) might miss crucial details, while a higher value (10-15) could introduce noise."
    },
    "time": 7.884131193161011
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the system handle context window optimization?\" is moderately complex. It\u2019s not a simple factual question, as it asks about a system\u2019s *handling* of a specific technical concept (context window optimization). This implies a need for understanding the system's architecture and processes, requiring retrieval of documents that explain the system's design and how it manages context.  It\u2019s not purely a general knowledge question; it demands specific knowledge about the RAG system\u2019s implementation. The query contains a single, focused question, but the \u2018how\u2019 implies a need to understand the mechanisms involved.  Therefore, a retrieval approach of 5-15 chunks is appropriate to capture the breadth of information needed. A relevance threshold of 0.6 is recommended to filter out less relevant results, focusing on documents that directly address context window optimization. Finally, reranking is recommended to further refine the retrieved chunks based on semantic similarity to the query, as simply retrieving the top k chunks might not perfectly align with the user's intent.  This will help prioritize documents that truly explain the system's approach."
    },
    "time": 9.091415882110596
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"What embedding models are supported for semantic search?\" is moderately complex. It\u2019s not a simple factual question like \"What is the capital of France?\" but it doesn't require deep analytical reasoning. It necessitates retrieving documents that discuss the various embedding models used in semantic search.  The intent is clear \u2013 the user wants a list of supported models.  Therefore, a moderate retrieval approach is appropriate.  I recommend k=8 because it balances retrieving enough relevant documents (to cover a reasonable range of models) without overwhelming the system with irrelevant results. A threshold of 0.6 is suggested to filter out documents with only a superficial connection to the query. Reranking is crucial here. Without it, the initial retrieval might return documents that mention embedding models in passing, rather than those specifically focused on their use in semantic search. Reranking will prioritize documents that directly address the query's focus."
    },
    "time": 7.895979881286621
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "This query is moderately complex as it combines a request for strategies (requiring specific knowledge) with an analytical component \u2013 how those strategies affect retrieval performance. It\u2019s not a simple factual question like \u2018What is chunking?\u2019 but it\u2019s also not a purely conceptual question.  The user wants to understand *how* chunking impacts retrieval, implying they're looking for evidence and comparisons.  Therefore, a moderate number of chunks (k=8) is appropriate to capture a range of relevant information. A relevance threshold of 0.6 suggests prioritizing results that are strongly related to both \u2018chunking strategies\u2019 and \u2018retrieval performance\u2019.  Finally, reranking is recommended (true) to further refine the results, as the initial retrieval might return documents that discuss chunking strategies but not necessarily their impact on retrieval.  The goal is to prioritize documents that directly address the query's analytical aspect."
    },
    "time": 7.855952024459839
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the authentication work and what endpoints are available for document management?\" is moderately complex. It contains two distinct sub-questions: authentication process and document management endpoints. This suggests a need for retrieving information from multiple document types and potentially different sections within those documents.  A retrieval of 8 chunks (k=8) provides a good balance between capturing relevant information and avoiding overwhelming the generation model with irrelevant results.  A threshold of 0.6 indicates a strong relevance requirement \u2013 we want to prioritize documents that are highly related to both authentication and document management. Reranking is recommended (reranking: true) to further refine the retrieved chunks, as the initial retrieval might produce a mix of documents with varying degrees of relevance to each sub-question.  Without reranking, the generation model might struggle to synthesize information effectively from a less-relevant set of chunks.  The query isn't purely factual; it requires understanding a process (authentication) and identifying specific resources (endpoints). Therefore, a more nuanced retrieval strategy is needed beyond simply finding documents containing the keywords."
    },
    "time": 8.831205129623413
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "analysis": {
      "complexity": "simple",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query 'RAG?' is extremely simple and represents a user seeking a basic understanding of the RAG (Retrieval Augmented Generation) concept. It\u2019s not a complex analytical question requiring deep understanding or nuanced interpretation.  Therefore, a moderate number of chunks (k=8) should be retrieved to capture a broad range of relevant information. A relevance threshold of 0.6 is recommended to filter out less relevant results, ensuring the generated response focuses on the core concept. Reranking is beneficial here to prioritize documents that explicitly define or explain RAG, even if they don't perfectly match the query terms.  Without further context, a broad retrieval is appropriate, but a slight filtering is needed to avoid irrelevant noise."
    },
    "time": 6.665657043457031
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query 'Embeddings?' is moderately complex. It\u2019s not a simple factual question like 'What is an embedding?' but it\u2019s also not a complex analytical question. The user is likely seeking a general understanding of embeddings \u2013 their definition, purpose, and perhaps different types.  Therefore, a retrieval approach that balances breadth and relevance is needed.  \n\n* **k (8):**  I've recommended 8 chunks. This provides a reasonable number of potentially relevant documents without overwhelming the generation model with too much information.  A lower value (5-7) might miss key concepts, while a higher value (10-15) could dilute the focus. \n* **threshold (0.6):** A threshold of 0.6 suggests prioritizing documents with a high degree of relevance. This allows the generation model to focus on the most pertinent information.  A lower threshold (0.4) would broaden the search, while a higher value (0.8) might exclude some useful context. \n* **reranking (true):** Reranking is crucial here.  Initial retrieval might return documents that use the word 'embeddings' in a tangential way. Reranking, using a model trained to understand semantic similarity, will help prioritize documents where 'embeddings' are central to the meaning, ensuring the generation model receives the most relevant context.  Without reranking, the generation model could be misled by less relevant results."
    },
    "time": 11.224719285964966
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "analysis": {
      "complexity": "complex",
      "parameters": {
        "k": 10,
        "threshold": 0.7,
        "reranking": true
      },
      "justification": "This query represents a complex analytical query rather than a simple factual one. The user isn't just asking for a single answer; they're seeking a deep understanding of the Metis RAG system's internal workings. The query explicitly asks about multiple aspects of the system \u2013 document processing, chunking strategies, vector store efficiency, and retrieval judge optimization. This necessitates retrieving a significant number of relevant chunks to cover all these areas.  \n\n* **k (Number of chunks to retrieve):**  I recommend 10. Given the multi-faceted nature of the query, retrieving 5-15 chunks would likely be insufficient to fully address all the user's questions.  A larger number allows for a more thorough exploration of the system's components.\n* **threshold (Relevance threshold):** I've set the threshold to 0.7. This suggests a strong emphasis on relevance. The user wants detailed explanations, so a higher threshold will prioritize chunks that are highly relevant to the core aspects of the Metis RAG system.\n* **reranking:**  `true`.  The user's query indicates a desire for optimized retrieval. The retrieval judge component of Metis RAG is specifically mentioned, implying a focus on improving relevance. Reranking will further refine the results, ensuring the most pertinent chunks are presented first.  Without reranking, the initial retrieval might return a mix of relevant and irrelevant chunks, requiring the user to sift through them."
    },
    "time": 11.576659202575684
  }
]

================
File: tests/retrieval_judge/results/query_refinement_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "refined_query": "How does the RAG system work, including document embedding storage, the document processing pipeline, and the LLM integration component?",
    "time": 1.9273748397827148
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "refined_query": "Tell me about the API layer, specifically its implementation using FastAPI for document upload and management, chat interactions, and analytics data retrieval, including authentication with JWT tokens.",
    "time": 2.0591607093811035
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "refined_query": "refinement process?",
    "time": 2.063575029373169
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "refined_query": "What is the document processing pipeline, including file validation, parsing, chunking, and metadata extraction?",
    "time": 1.7061607837677002
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "refined_query": "How does the LLM integration component handle context window optimization, specifically within the Retrieval Judge's query refinement process?",
    "time": 1.803995132446289
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "refined_query": "What embedding models are supported for semantic search, specifically considering models like Llama 3, Gemma, and Mistral?",
    "time": 1.8295328617095947
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "refined_query": "What are the different document chunking strategies and how do they affect retrieval performance, particularly within the context of document processing pipelines and retrieval judge optimization?",
    "time": 1.98307204246521
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "refined_query": "What authentication methods are used for the document management API endpoints (specifically JWT tokens), and what document management API endpoints are available (e.g., upload, management)?",
    "time": 2.098104953765869
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "refined_query": "RAG system components and functionality",
    "time": 1.2879359722137451
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "refined_query": "Explain vector store embeddings and their role in RAG systems, including HNSW indexing and metadata filtering.",
    "time": 1.7461588382720947
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "refined_query": "I want a detailed explanation of the Metis RAG system\u2019s architecture, focusing on its document processing pipeline, chunking strategies, vector store implementation, and the Retrieval Judge\u2019s optimization process. Specifically, I\u2019m interested in how the system handles different file formats (PDF, DOCX, TXT, Markdown), the configurable chunking strategies used for document splitting, the efficient metadata filtering within the vector store (using HNSW index and metadata like tags, folder, and document ID), and how the Retrieval Judge leverages LLM analysis to refine queries and optimize retrieval parameters for improved relevance.",
    "time": 4.841519117355347
  }
]

================
File: tests/retrieval_judge/results/single_query_result.json
================
{
  "query": "What are the components of the RAG engine and how do they work together?",
  "standard": {
    "answer": "Based on the provided documents [1] and [2], the components of the RAG engine are:\n\n* Document Processing: This pipeline handles file validation and parsing, text extraction, and Chu... (the text is cut off, but it seems to be related to processing documents)\n\nAccording to the context, these components work together as part of the RAG Engine within Metis RAG's modular architecture.\n\nNote that there might be more information about the Document Processing pipeline or other RAG engine components not included in the provided documents. However, based on the available context, this is all the information I can provide.",
    "sources_count": 2,
    "time": 184.06564712524414
  },
  "judge": {
    "answer": "Based on the provided documents [1], [2], and [3], the components of the RAG engine are:\n\n* Document Processing: The core RAG engine consists of a document processing pipeline that handles:\n\t+ File validation and parsing\n\t+ Text extraction\n\t[No further information is available in the context about how these components work together.]\n\nNote that there is no explicit information provided in the context about the specific interactions or processes between the different components of the RAG engine. However, based on the general architecture overview, it can be inferred that the RAG engine likely integrates with the frontend and API layers to facilitate document management, chat interactions, system configuration, and analytics data retrieval.\n\nSources:\n[1], [2], and [3]",
    "sources_count": 3,
    "time": 37.67006587982178
  }
}

================
File: tests/retrieval_judge/results/timing_analysis_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "standard": {
      "total_time": 171.3785719871521,
      "sources_count": 1
    },
    "judge": {
      "total_time": 36.81675601005554,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 12.479793787002563,
        "evaluate_chunks": 6.393211126327515,
        "enhanced_retrieval": 18.878051280975342
      }
    }
  },
  {
    "query": "What are the components of the RAG engine?",
    "standard": {
      "total_time": 164.1695818901062,
      "sources_count": 1
    },
    "judge": {
      "total_time": 31.121859073638916,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 12.357210874557495,
        "evaluate_chunks": 5.824731111526489,
        "enhanced_retrieval": 18.187943935394287
      }
    }
  },
  {
    "query": "How does the vector store work?",
    "standard": {
      "total_time": 167.41857886314392,
      "sources_count": 1
    },
    "judge": {
      "total_time": 31.442083835601807,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 11.610037803649902,
        "evaluate_chunks": 6.113225936889648,
        "enhanced_retrieval": 17.730960845947266
      }
    }
  },
  {
    "query": "What is the role of the LLM integration component?",
    "standard": {
      "total_time": 175.33833718299866,
      "sources_count": 1
    },
    "judge": {
      "total_time": 29.5294828414917,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 11.025973081588745,
        "evaluate_chunks": 5.471881866455078,
        "enhanced_retrieval": 16.504756927490234
      }
    }
  },
  {
    "query": "How does the document processing pipeline work?",
    "standard": {
      "total_time": 174.38789820671082,
      "sources_count": 1
    },
    "judge": {
      "total_time": 31.299037218093872,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 11.572484970092773,
        "evaluate_chunks": 5.757498025894165,
        "enhanced_retrieval": 17.335467100143433
      }
    }
  }
]

================
File: tests/retrieval_judge/results/timing_analysis.json
================
{
  "avg_standard_time": 170.53859362602233,
  "avg_judge_time": 32.041843795776366,
  "time_difference_percent": -81.21138264688543,
  "component_averages": {
    "analyze_query": 11.809100103378295,
    "evaluate_chunks": 5.912109613418579,
    "enhanced_retrieval": 17.72743601799011
  },
  "first_run": {
    "standard": 171.3785719871521,
    "judge": 36.81675601005554
  },
  "subsequent_runs": {
    "standard": 170.3285990357399,
    "judge": 30.848115742206573
  }
}

================
File: tests/retrieval_judge/analyze_retrieval_judge_results.py
================
#!/usr/bin/env python3
"""
Analysis script for Retrieval Judge comparison results.
This script:
1. Loads the results from the comparison test
2. Performs detailed analysis on the effectiveness of the Retrieval Judge
3. Generates visualizations to highlight key findings
4. Provides recommendations for improving the Retrieval Judge
"""

import os
import json
import logging
import argparse
from typing import Dict, List, Any, Optional
import matplotlib.pyplot as plt
import numpy as np
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("analyze_retrieval_judge_results")

def load_results(results_file: str) -> List[Dict[str, Any]]:
    """Load the comparison test results from file"""
    logger.info(f"Loading results from {results_file}")
    
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
        
        logger.info(f"Loaded {len(results)} test results")
        return results
    except Exception as e:
        logger.error(f"Error loading results: {str(e)}")
        return []

def load_metrics(metrics_file: str) -> Dict[str, Any]:
    """Load the analysis metrics from file"""
    logger.info(f"Loading metrics from {metrics_file}")
    
    try:
        with open(metrics_file, 'r') as f:
            metrics = json.load(f)
        
        logger.info(f"Loaded metrics for {metrics.get('total_queries', 0)} queries")
        return metrics
    except Exception as e:
        logger.error(f"Error loading metrics: {str(e)}")
        return {}

def analyze_source_relevance(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the relevance of sources retrieved by both methods"""
    logger.info("Analyzing source relevance...")
    
    relevance_analysis = {
        "avg_standard_relevance": 0,
        "avg_judge_relevance": 0,
        "relevance_improvement": 0,
        "by_complexity": {},
        "by_document": defaultdict(lambda: {"standard": 0, "judge": 0, "count": 0})
    }
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        relevance_analysis["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_relevance": 0,
            "avg_judge_relevance": 0,
            "improvement": 0
        }
    
    # Calculate average relevance scores
    total_standard_relevance = 0
    total_judge_relevance = 0
    total_sources = 0
    
    for result in results:
        complexity = result["complexity"]
        
        # Calculate average relevance for standard retrieval
        standard_sources = result["standard"]["sources"]
        standard_relevance = sum(s["relevance_score"] for s in standard_sources) if standard_sources else 0
        standard_avg_relevance = standard_relevance / len(standard_sources) if standard_sources else 0
        
        # Calculate average relevance for judge retrieval
        judge_sources = result["judge"]["sources"]
        judge_relevance = sum(s["relevance_score"] for s in judge_sources) if judge_sources else 0
        judge_avg_relevance = judge_relevance / len(judge_sources) if judge_sources else 0
        
        # Update totals
        total_standard_relevance += standard_avg_relevance
        total_judge_relevance += judge_avg_relevance
        total_sources += 1
        
        # Update complexity metrics
        if complexity in relevance_analysis["by_complexity"]:
            relevance_analysis["by_complexity"][complexity]["count"] += 1
            relevance_analysis["by_complexity"][complexity]["avg_standard_relevance"] += standard_avg_relevance
            relevance_analysis["by_complexity"][complexity]["avg_judge_relevance"] += judge_avg_relevance
        
        # Track relevance by document
        for source in standard_sources:
            doc_id = source["document_id"]
            relevance_analysis["by_document"][doc_id]["standard"] += source["relevance_score"]
            relevance_analysis["by_document"][doc_id]["count"] += 1
            
        for source in judge_sources:
            doc_id = source["document_id"]
            relevance_analysis["by_document"][doc_id]["judge"] += source["relevance_score"]
            # Don't increment count again as we're calculating averages
    
    # Calculate overall averages
    if total_sources > 0:
        relevance_analysis["avg_standard_relevance"] = total_standard_relevance / total_sources
        relevance_analysis["avg_judge_relevance"] = total_judge_relevance / total_sources
        
        # Calculate improvement percentage
        if relevance_analysis["avg_standard_relevance"] > 0:
            relevance_analysis["relevance_improvement"] = (
                (relevance_analysis["avg_judge_relevance"] - relevance_analysis["avg_standard_relevance"]) / 
                relevance_analysis["avg_standard_relevance"] * 100
            )
    
    # Calculate complexity averages and improvements
    for complexity, data in relevance_analysis["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_relevance"] /= data["count"]
            data["avg_judge_relevance"] /= data["count"]
            
            # Calculate improvement percentage
            if data["avg_standard_relevance"] > 0:
                data["improvement"] = (
                    (data["avg_judge_relevance"] - data["avg_standard_relevance"]) / 
                    data["avg_standard_relevance"] * 100
                )
    
    # Calculate document averages
    for doc_id, data in relevance_analysis["by_document"].items():
        if data["count"] > 0:
            data["avg_standard"] = data["standard"] / data["count"]
            data["avg_judge"] = data["judge"] / data["count"]
            
            # Calculate improvement percentage
            if data["avg_standard"] > 0:
                data["improvement"] = (
                    (data["avg_judge"] - data["avg_standard"]) / 
                    data["avg_standard"] * 100
                )
    
    # Convert defaultdict to regular dict for JSON serialization
    relevance_analysis["by_document"] = dict(relevance_analysis["by_document"])
    
    return relevance_analysis

def analyze_query_refinement(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the effectiveness of query refinement"""
    logger.info("Analyzing query refinement effectiveness...")
    
    # This is a bit tricky since we don't have direct access to the refined queries
    # We'll infer effectiveness by comparing source relevance for ambiguous queries
    
    refinement_analysis = {
        "ambiguous_queries": [],
        "multi_part_queries": [],
        "avg_improvement_ambiguous": 0,
        "avg_improvement_multi_part": 0
    }
    
    # Extract ambiguous and multi-part queries
    ambiguous_queries = [r for r in results if r["complexity"] == "ambiguous"]
    multi_part_queries = [r for r in results if r["complexity"] == "multi-part"]
    
    # Calculate average improvement for ambiguous queries
    total_improvement_ambiguous = 0
    for query in ambiguous_queries:
        standard_sources = query["standard"]["sources"]
        judge_sources = query["judge"]["sources"]
        
        standard_relevance = sum(s["relevance_score"] for s in standard_sources) / len(standard_sources) if standard_sources else 0
        judge_relevance = sum(s["relevance_score"] for s in judge_sources) / len(judge_sources) if judge_sources else 0
        
        improvement = ((judge_relevance - standard_relevance) / standard_relevance * 100) if standard_relevance > 0 else 0
        
        refinement_analysis["ambiguous_queries"].append({
            "query": query["query"],
            "standard_relevance": standard_relevance,
            "judge_relevance": judge_relevance,
            "improvement": improvement
        })
        
        total_improvement_ambiguous += improvement
    
    # Calculate average improvement for multi-part queries
    total_improvement_multi_part = 0
    for query in multi_part_queries:
        standard_sources = query["standard"]["sources"]
        judge_sources = query["judge"]["sources"]
        
        standard_relevance = sum(s["relevance_score"] for s in standard_sources) / len(standard_sources) if standard_sources else 0
        judge_relevance = sum(s["relevance_score"] for s in judge_sources) / len(judge_sources) if judge_sources else 0
        
        improvement = ((judge_relevance - standard_relevance) / standard_relevance * 100) if standard_relevance > 0 else 0
        
        refinement_analysis["multi_part_queries"].append({
            "query": query["query"],
            "standard_relevance": standard_relevance,
            "judge_relevance": judge_relevance,
            "improvement": improvement
        })
        
        total_improvement_multi_part += improvement
    
    # Calculate averages
    if ambiguous_queries:
        refinement_analysis["avg_improvement_ambiguous"] = total_improvement_ambiguous / len(ambiguous_queries)
    
    if multi_part_queries:
        refinement_analysis["avg_improvement_multi_part"] = total_improvement_multi_part / len(multi_part_queries)
    
    return refinement_analysis

def analyze_context_optimization(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the effectiveness of context optimization"""
    logger.info("Analyzing context optimization effectiveness...")
    
    optimization_analysis = {
        "avg_standard_sources": 0,
        "avg_judge_sources": 0,
        "source_count_difference": 0,
        "source_count_difference_percent": 0,
        "by_complexity": {}
    }
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        optimization_analysis["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_sources": 0,
            "avg_judge_sources": 0,
            "difference": 0,
            "difference_percent": 0
        }
    
    # Calculate source counts
    total_standard_sources = 0
    total_judge_sources = 0
    total_queries = len(results)
    
    for result in results:
        complexity = result["complexity"]
        standard_source_count = len(result["standard"]["sources"])
        judge_source_count = len(result["judge"]["sources"])
        
        # Update totals
        total_standard_sources += standard_source_count
        total_judge_sources += judge_source_count
        
        # Update complexity metrics
        if complexity in optimization_analysis["by_complexity"]:
            optimization_analysis["by_complexity"][complexity]["count"] += 1
            optimization_analysis["by_complexity"][complexity]["avg_standard_sources"] += standard_source_count
            optimization_analysis["by_complexity"][complexity]["avg_judge_sources"] += judge_source_count
    
    # Calculate overall averages
    if total_queries > 0:
        optimization_analysis["avg_standard_sources"] = total_standard_sources / total_queries
        optimization_analysis["avg_judge_sources"] = total_judge_sources / total_queries
        
        # Calculate difference
        optimization_analysis["source_count_difference"] = optimization_analysis["avg_judge_sources"] - optimization_analysis["avg_standard_sources"]
        
        # Calculate percentage difference
        if optimization_analysis["avg_standard_sources"] > 0:
            optimization_analysis["source_count_difference_percent"] = (
                optimization_analysis["source_count_difference"] / optimization_analysis["avg_standard_sources"] * 100
            )
    
    # Calculate complexity averages and differences
    for complexity, data in optimization_analysis["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_sources"] /= data["count"]
            data["avg_judge_sources"] /= data["count"]
            
            # Calculate difference
            data["difference"] = data["avg_judge_sources"] - data["avg_standard_sources"]
            
            # Calculate percentage difference
            if data["avg_standard_sources"] > 0:
                data["difference_percent"] = (
                    data["difference"] / data["avg_standard_sources"] * 100
                )
    
    return optimization_analysis

def analyze_performance_impact(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the performance impact of using the Retrieval Judge"""
    logger.info("Analyzing performance impact...")
    
    performance_analysis = {
        "avg_standard_time": 0,
        "avg_judge_time": 0,
        "time_difference": 0,
        "time_difference_percent": 0,
        "by_complexity": {}
    }
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        performance_analysis["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_time": 0,
            "avg_judge_time": 0,
            "difference": 0,
            "difference_percent": 0
        }
    
    # Calculate processing times
    total_standard_time = 0
    total_judge_time = 0
    total_queries = len(results)
    
    for result in results:
        complexity = result["complexity"]
        standard_time = result["standard"]["time"]
        judge_time = result["judge"]["time"]
        
        # Update totals
        total_standard_time += standard_time
        total_judge_time += judge_time
        
        # Update complexity metrics
        if complexity in performance_analysis["by_complexity"]:
            performance_analysis["by_complexity"][complexity]["count"] += 1
            performance_analysis["by_complexity"][complexity]["avg_standard_time"] += standard_time
            performance_analysis["by_complexity"][complexity]["avg_judge_time"] += judge_time
    
    # Calculate overall averages
    if total_queries > 0:
        performance_analysis["avg_standard_time"] = total_standard_time / total_queries
        performance_analysis["avg_judge_time"] = total_judge_time / total_queries
        
        # Calculate difference
        performance_analysis["time_difference"] = performance_analysis["avg_judge_time"] - performance_analysis["avg_standard_time"]
        
        # Calculate percentage difference
        if performance_analysis["avg_standard_time"] > 0:
            performance_analysis["time_difference_percent"] = (
                performance_analysis["time_difference"] / performance_analysis["avg_standard_time"] * 100
            )
    
    # Calculate complexity averages and differences
    for complexity, data in performance_analysis["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_time"] /= data["count"]
            data["avg_judge_time"] /= data["count"]
            
            # Calculate difference
            data["difference"] = data["avg_judge_time"] - data["avg_standard_time"]
            
            # Calculate percentage difference
            if data["avg_standard_time"] > 0:
                data["difference_percent"] = (
                    data["difference"] / data["avg_standard_time"] * 100
                )
    
    return performance_analysis

def generate_visualizations(
    results: List[Dict[str, Any]], 
    relevance_analysis: Dict[str, Any],
    refinement_analysis: Dict[str, Any],
    optimization_analysis: Dict[str, Any],
    performance_analysis: Dict[str, Any]
):
    """Generate visualizations to highlight key findings"""
    logger.info("Generating visualizations...")
    
    # Create output directory for visualizations
    vis_dir = os.path.join("tests", "retrieval_judge", "visualizations")
    os.makedirs(vis_dir, exist_ok=True)
    
    # 1. Relevance improvement by complexity
    plt.figure(figsize=(10, 6))
    complexities = list(relevance_analysis["by_complexity"].keys())
    improvements = [relevance_analysis["by_complexity"][c]["improvement"] for c in complexities]
    
    plt.bar(complexities, improvements, color='skyblue')
    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)
    plt.title('Relevance Improvement by Query Complexity')
    plt.xlabel('Query Complexity')
    plt.ylabel('Improvement (%)')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'relevance_by_complexity.png'))
    plt.close()
    
    # 2. Source count comparison
    plt.figure(figsize=(10, 6))
    complexities = list(optimization_analysis["by_complexity"].keys())
    standard_sources = [optimization_analysis["by_complexity"][c]["avg_standard_sources"] for c in complexities]
    judge_sources = [optimization_analysis["by_complexity"][c]["avg_judge_sources"] for c in complexities]
    
    x = np.arange(len(complexities))
    width = 0.35
    
    plt.bar(x - width/2, standard_sources, width, label='Standard Retrieval', color='lightcoral')
    plt.bar(x + width/2, judge_sources, width, label='Judge-Enhanced Retrieval', color='lightgreen')
    
    plt.title('Average Number of Sources by Query Complexity')
    plt.xlabel('Query Complexity')
    plt.ylabel('Average Number of Sources')
    plt.xticks(x, complexities)
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'source_count_comparison.png'))
    plt.close()
    
    # 3. Processing time comparison
    plt.figure(figsize=(10, 6))
    complexities = list(performance_analysis["by_complexity"].keys())
    standard_times = [performance_analysis["by_complexity"][c]["avg_standard_time"] for c in complexities]
    judge_times = [performance_analysis["by_complexity"][c]["avg_judge_time"] for c in complexities]
    
    x = np.arange(len(complexities))
    width = 0.35
    
    plt.bar(x - width/2, standard_times, width, label='Standard Retrieval', color='lightcoral')
    plt.bar(x + width/2, judge_times, width, label='Judge-Enhanced Retrieval', color='lightgreen')
    
    plt.title('Average Processing Time by Query Complexity')
    plt.xlabel('Query Complexity')
    plt.ylabel('Average Time (seconds)')
    plt.xticks(x, complexities)
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'processing_time_comparison.png'))
    plt.close()
    
    # 4. Query refinement effectiveness
    plt.figure(figsize=(10, 6))
    query_types = ['Ambiguous', 'Multi-part']
    improvements = [
        refinement_analysis["avg_improvement_ambiguous"],
        refinement_analysis["avg_improvement_multi_part"]
    ]
    
    plt.bar(query_types, improvements, color=['orange', 'purple'])
    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)
    plt.title('Query Refinement Effectiveness')
    plt.xlabel('Query Type')
    plt.ylabel('Average Relevance Improvement (%)')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'query_refinement_effectiveness.png'))
    plt.close()
    
    logger.info(f"Visualizations saved to {os.path.abspath(vis_dir)}")

def generate_improvement_recommendations(
    relevance_analysis: Dict[str, Any],
    refinement_analysis: Dict[str, Any],
    optimization_analysis: Dict[str, Any],
    performance_analysis: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """Generate recommendations for improving the Retrieval Judge"""
    logger.info("Generating improvement recommendations...")
    
    recommendations = []
    
    # 1. Check relevance improvement
    overall_relevance_improvement = relevance_analysis.get("relevance_improvement", 0)
    if overall_relevance_improvement < 10:
        recommendations.append({
            "area": "Relevance Scoring",
            "issue": "Limited overall relevance improvement",
            "recommendation": "Enhance the relevance evaluation prompt to better distinguish between highly relevant and tangentially relevant content. Consider using a more fine-grained scoring system or incorporating domain-specific knowledge."
        })
    
    # 2. Check performance impact
    time_difference_percent = performance_analysis.get("time_difference_percent", 0)
    if time_difference_percent > 50:
        recommendations.append({
            "area": "Performance",
            "issue": "Significant processing time increase",
            "recommendation": "Optimize the judge's processing pipeline by: 1) Using a smaller, faster model for initial query analysis, 2) Implementing caching for similar queries, 3) Reducing the context size in prompts, or 4) Implementing parallel processing for independent judge operations."
        })
    
    # 3. Check query refinement effectiveness
    avg_improvement_ambiguous = refinement_analysis.get("avg_improvement_ambiguous", 0)
    if avg_improvement_ambiguous < 15:
        recommendations.append({
            "area": "Query Refinement",
            "issue": "Limited effectiveness for ambiguous queries",
            "recommendation": "Improve the query refinement prompt to better handle ambiguity by: 1) Adding examples of successful disambiguations, 2) Incorporating domain-specific terminology, 3) Implementing a clarification step that generates multiple possible interpretations before selecting the most likely one."
        })
    
    # 4. Check context optimization
    source_count_difference = optimization_analysis.get("source_count_difference", 0)
    if source_count_difference < 0:
        recommendations.append({
            "area": "Context Optimization",
            "issue": "Judge retrieves fewer sources on average",
            "recommendation": "Revise the context optimization logic to: 1) Focus more on diversity of information rather than just relevance, 2) Implement a minimum source count based on query complexity, 3) Add a post-processing step to ensure critical information isn't excluded."
        })
    
    # 5. Check complex query handling
    complex_improvement = relevance_analysis.get("by_complexity", {}).get("complex", {}).get("improvement", 0)
    if complex_improvement < 20:
        recommendations.append({
            "area": "Complex Query Handling",
            "issue": "Limited improvement for complex analytical queries",
            "recommendation": "Enhance complex query processing by: 1) Breaking down complex queries into sub-queries, 2) Implementing a multi-step retrieval process that builds context incrementally, 3) Adding a synthesis step that combines information from multiple sources."
        })
    
    # 6. Check multi-part query handling
    multi_part_improvement = relevance_analysis.get("by_complexity", {}).get("multi-part", {}).get("improvement", 0)
    if multi_part_improvement < 15:
        recommendations.append({
            "area": "Multi-part Query Handling",
            "issue": "Limited improvement for multi-part queries",
            "recommendation": "Improve multi-part query handling by: 1) Implementing a query decomposition step that identifies distinct sub-questions, 2) Retrieving information for each sub-question separately, 3) Merging the results with appropriate weighting, 4) Adding a final relevance check to ensure all parts of the query are addressed."
        })
    
    # Always recommend monitoring and feedback loop
    recommendations.append({
        "area": "Continuous Improvement",
        "issue": "Need for ongoing optimization",
        "recommendation": "Implement a feedback loop by: 1) Tracking user satisfaction with responses, 2) Logging cases where the judge significantly improves or degrades results, 3) Periodically retraining or fine-tuning the judge with examples of successful and unsuccessful retrievals."
    })
    
    return recommendations

def main():
    """Main analysis function"""
    parser = argparse.ArgumentParser(description="Analyze Retrieval Judge comparison results")
    parser.add_argument("--results", default=os.path.join("tests", "retrieval_judge", "results", "retrieval_judge_comparison_results.json"), 
                        help="Path to results JSON file")
    parser.add_argument("--metrics", default=os.path.join("tests", "retrieval_judge", "results", "retrieval_judge_metrics.json"), 
                        help="Path to metrics JSON file")
    parser.add_argument("--output", default=os.path.join("tests", "retrieval_judge", "results", "retrieval_judge_analysis_report.json"), 
                        help="Path to output analysis report")
    args = parser.parse_args()
    
    logger.info("Starting Retrieval Judge results analysis...")
    
    try:
        # Load results and metrics
        results = load_results(args.results)
        metrics = load_metrics(args.metrics)
        
        if not results:
            logger.error("No results to analyze. Please run the comparison test first.")
            return
        
        # Perform detailed analysis
        relevance_analysis = analyze_source_relevance(results)
        refinement_analysis = analyze_query_refinement(results)
        optimization_analysis = analyze_context_optimization(results)
        performance_analysis = analyze_performance_impact(results)
        
        # Generate visualizations
        generate_visualizations(
            results,
            relevance_analysis,
            refinement_analysis,
            optimization_analysis,
            performance_analysis
        )
        
        # Generate improvement recommendations
        recommendations = generate_improvement_recommendations(
            relevance_analysis,
            refinement_analysis,
            optimization_analysis,
            performance_analysis
        )
        
        # Compile analysis report
        analysis_report = {
            "summary": {
                "total_queries": len(results),
                "overall_relevance_improvement": relevance_analysis["relevance_improvement"],
                "performance_impact": performance_analysis["time_difference_percent"],
                "recommendation_count": len(recommendations)
            },
            "detailed_analysis": {
                "relevance": relevance_analysis,
                "query_refinement": refinement_analysis,
                "context_optimization": optimization_analysis,
                "performance": performance_analysis
            },
            "recommendations": recommendations
        }
        
        # Save analysis report
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        with open(args.output, "w") as f:
            json.dump(analysis_report, f, indent=2)
        
        # Print summary
        logger.info("\n=== RETRIEVAL JUDGE ANALYSIS SUMMARY ===")
        logger.info(f"Total queries analyzed: {len(results)}")
        logger.info(f"Overall relevance improvement: {relevance_analysis['relevance_improvement']:.2f}%")
        logger.info(f"Performance impact: {performance_analysis['time_difference_percent']:.2f}% increase in processing time")
        logger.info(f"Generated {len(recommendations)} improvement recommendations")
        logger.info(f"Analysis report saved to {os.path.abspath(args.output)}")
        logger.info(f"Visualizations saved to {os.path.abspath(os.path.join('tests', 'retrieval_judge', 'visualizations'))}")
        
        # Print top recommendations
        logger.info("\nTop improvement recommendations:")
        for i, rec in enumerate(recommendations[:3], 1):
            logger.info(f"{i}. {rec['area']}: {rec['recommendation']}")
        
        logger.info("\nRetrieval Judge analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Error during analysis: {str(e)}")
        raise

if __name__ == "__main__":
    main()

================
File: tests/retrieval_judge/IMPLEMENTATION_NOTES.md
================
# Retrieval Judge Implementation Notes

## Overview

The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by analyzing queries, evaluating retrieved chunks, refining queries when needed, and optimizing context assembly. This document provides technical details about the implementation and testing framework.

## Implementation Details

The Retrieval Judge is implemented in `app/rag/agents/retrieval_judge.py` and consists of four main components:

1. **Query Analysis**: Analyzes the complexity of a query and recommends optimal retrieval parameters (k, threshold, reranking).
2. **Chunk Evaluation**: Evaluates the relevance of retrieved chunks to the query and determines if query refinement is needed.
3. **Query Refinement**: Refines ambiguous or complex queries to improve retrieval precision.
4. **Context Optimization**: Reorders and filters chunks to create an optimal context for the LLM.

The judge is integrated with the RAG engine in `app/rag/rag_engine.py` through the `_enhanced_retrieval` method, which is called when the judge is enabled.

## Testing Framework

The testing framework consists of:

1. **Comparison Tests** (`test_retrieval_judge_comparison.py`): Compares standard retrieval vs. retrieval with the judge enabled using a variety of test queries.
2. **Analysis Tools** (`analyze_retrieval_judge_results.py`): Analyzes the test results and generates visualizations and recommendations.
3. **Run Script** (`run_tests.sh`): Shell script to run both the comparison tests and analysis in sequence.

### Test Methodology

The tests use a controlled environment with predefined test documents and queries of varying complexity. For each query, the test:

1. Runs the query with standard retrieval
2. Runs the query with judge-enhanced retrieval
3. Records the results, including:
   - Retrieved sources
   - Relevance scores
   - Processing time
   - Generated answers

The analysis then compares these results to evaluate the effectiveness of the judge.

### Test Queries

The test queries are designed to cover a range of complexity levels and query types:

- **Simple factual queries**: Direct questions with clear answers
- **Moderate complexity queries**: Questions requiring synthesis of information
- **Complex analytical queries**: Questions requiring deeper understanding and inference
- **Ambiguous queries**: Questions with unclear intent or multiple interpretations
- **Multi-part queries**: Questions that combine multiple distinct information needs

### Analysis Metrics

The analysis evaluates the Retrieval Judge on several dimensions:

1. **Source relevance**: How relevant are the retrieved chunks to the query?
2. **Query refinement effectiveness**: How well does the judge improve ambiguous or complex queries?
3. **Context optimization**: How effectively does the judge select and order chunks?
4. **Performance impact**: What is the processing time overhead of using the judge?

## Expected Results

Based on the implementation, we expect the Retrieval Judge to show:

1. **Improved relevance** for complex, ambiguous, and multi-part queries
2. **More focused retrieval** with fewer but more relevant chunks
3. **Better context organization** for improved response generation
4. **Some performance overhead** due to the additional LLM calls

The analysis will quantify these improvements and identify areas for further optimization.

## Future Improvements

Potential areas for improvement in the Retrieval Judge include:

1. **Performance optimization**: Reducing the overhead of LLM calls
2. **Caching**: Implementing caching for similar queries
3. **Parallel processing**: Running independent judge operations in parallel
4. **Feedback loop**: Incorporating user feedback to improve the judge over time
5. **Domain adaptation**: Fine-tuning the judge for specific domains or document types

## Running the Tests

See the `README.md` file for detailed instructions on running the tests.

## Interpreting Results

The analysis generates visualizations and recommendations that can be used to:

1. Evaluate the overall effectiveness of the Retrieval Judge
2. Identify specific query types where the judge performs well or poorly
3. Quantify the performance impact of using the judge
4. Guide future improvements to the judge implementation

The recommendations are based on the test results and suggest specific ways to improve the judge's effectiveness.

================
File: tests/retrieval_judge/README.md
================
# Retrieval Judge Testing

This directory contains tests and analysis tools for evaluating the effectiveness of the Retrieval Judge component in the Metis RAG system.

## Overview

The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by:

1. Analyzing queries to determine optimal retrieval parameters
2. Evaluating retrieved chunks for relevance
3. Refining queries when needed to improve retrieval precision
4. Optimizing context assembly for better response generation

These tests compare standard retrieval against retrieval with the judge enabled to measure the impact and effectiveness of the judge.

## Test Files

- `test_retrieval_judge_comparison.py`: Main test script that compares standard retrieval vs. retrieval with the judge enabled
- `analyze_retrieval_judge_results.py`: Analysis script that processes test results and generates visualizations and recommendations
- `run_tests.sh`: Shell script to run both the comparison test and analysis in sequence
- `run_tests.py`: Python script to run both the comparison test and analysis in sequence
- `IMPLEMENTATION_NOTES.md`: Technical details about the implementation and testing framework

## Directory Structure

- `data/`: Test documents used for retrieval testing
- `results/`: JSON output files from test runs
- `visualizations/`: Generated charts and graphs showing test results

## Running the Tests

### Prerequisites

Make sure you have the required dependencies installed:

```bash
pip install matplotlib numpy
```

### Option 1: Run the Complete Test Suite

The easiest way to run all tests is to use one of the provided runner scripts:

#### Using the Shell Script:

```bash
cd /path/to/Metis_RAG
./tests/retrieval_judge/run_tests.sh
```

#### Using the Python Script:

```bash
cd /path/to/Metis_RAG
python -m tests.retrieval_judge.run_tests
```

Both scripts will run the comparison tests and the analysis in sequence.

### Option 2: Run Tests Individually

#### Step 1: Run the Comparison Test

```bash
cd /path/to/Metis_RAG
python -m tests.retrieval_judge.test_retrieval_judge_comparison
```

This will:
- Create test documents in the `tests/retrieval_judge/data/` directory
- Process these documents and add them to a test vector store
- Run test queries with both standard retrieval and judge-enhanced retrieval
- Save the results to `tests/retrieval_judge/results/retrieval_judge_comparison_results.json`
- Save metrics to `tests/retrieval_judge/results/retrieval_judge_metrics.json`

#### Step 2: Analyze the Results

```bash
cd /path/to/Metis_RAG
python -m tests.retrieval_judge.analyze_retrieval_judge_results
```

This will:
- Load the test results and metrics
- Perform detailed analysis on the effectiveness of the Retrieval Judge
- Generate visualizations in the `tests/retrieval_judge/visualizations/` directory
- Generate recommendations for improving the Retrieval Judge
- Save the analysis report to `tests/retrieval_judge/results/retrieval_judge_analysis_report.json`

## Test Queries

The test includes queries of varying complexity:

1. **Simple factual queries**: Direct questions with clear answers in the documents
2. **Moderate complexity queries**: Questions requiring synthesis of information
3. **Complex analytical queries**: Questions requiring deeper understanding and inference
4. **Ambiguous queries**: Questions with unclear intent or multiple interpretations
5. **Multi-part queries**: Questions that combine multiple distinct information needs

## Analysis Metrics

The analysis evaluates the Retrieval Judge on several dimensions:

1. **Source relevance**: How relevant are the retrieved chunks to the query?
2. **Query refinement effectiveness**: How well does the judge improve ambiguous or complex queries?
3. **Context optimization**: How effectively does the judge select and order chunks?
4. **Performance impact**: What is the processing time overhead of using the judge?

## Customizing Tests

You can modify the test queries or add new ones by editing the `TEST_QUERIES` list in `test_retrieval_judge_comparison.py`.

To test with different documents, you can modify the document content variables (`MARKDOWN_CONTENT`, `PDF_CONTENT`, `TECHNICAL_SPECS_CONTENT`) or add new test documents.

## Interpreting Results

The analysis generates several visualizations to help interpret the results:

1. **Relevance by complexity**: Shows how the judge improves relevance across query types
2. **Source count comparison**: Compares the number of sources retrieved by each method
3. **Processing time comparison**: Shows the performance impact of using the judge
4. **Query refinement effectiveness**: Shows how well the judge handles ambiguous queries

The analysis also generates specific recommendations for improving the Retrieval Judge based on the test results.

## Implementation Details

For more information about the implementation and testing methodology, see the `IMPLEMENTATION_NOTES.md` file in this directory.

================
File: tests/retrieval_judge/run_tests.py
================
#!/usr/bin/env python3
"""
Python script to run the Retrieval Judge tests and analysis
"""

import os
import sys
import subprocess
import time
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("run_tests")

def run_command(command, description):
    """Run a command and log the output"""
    logger.info(f"Running: {description}")
    start_time = time.time()
    
    try:
        result = subprocess.run(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=True
        )
        
        logger.info(f"Command completed in {time.time() - start_time:.2f} seconds")
        logger.info(result.stdout)
        
        if result.stderr:
            logger.warning(result.stderr)
            
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Command failed with exit code {e.returncode}")
        logger.error(e.stdout)
        logger.error(e.stderr)
        return False

def main():
    """Main function to run the tests"""
    logger.info("===== Metis RAG Retrieval Judge Test Suite =====")
    
    # Get the base directory
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    os.chdir(base_dir)
    
    # Run the comparison tests
    logger.info("Step 1: Running comparison tests...")
    if not run_command(
        [sys.executable, "-m", "tests.retrieval_judge.test_retrieval_judge_comparison"],
        "Comparison tests"
    ):
        logger.error("Comparison tests failed. Exiting.")
        return 1
    
    # Run the analysis
    logger.info("Step 2: Running analysis...")
    if not run_command(
        [sys.executable, "-m", "tests.retrieval_judge.analyze_retrieval_judge_results"],
        "Analysis"
    ):
        logger.error("Analysis failed. Exiting.")
        return 1
    
    logger.info("===== Test Suite Completed Successfully =====")
    logger.info("Results are available in: tests/retrieval_judge/results/")
    logger.info("Visualizations are available in: tests/retrieval_judge/visualizations/")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/retrieval_judge/run_tests.sh
================
#!/bin/bash
# Script to run the Retrieval Judge tests and analysis

echo "===== Metis RAG Retrieval Judge Test Suite ====="
echo ""

# Set the base directory
BASE_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$BASE_DIR/../.."

echo "Step 1: Running comparison tests..."
python -m tests.retrieval_judge.test_retrieval_judge_comparison
if [ $? -ne 0 ]; then
    echo "Error: Comparison tests failed."
    exit 1
fi

echo ""
echo "Step 2: Running analysis..."
python -m tests.retrieval_judge.analyze_retrieval_judge_results
if [ $? -ne 0 ]; then
    echo "Error: Analysis failed."
    exit 1
fi

echo ""
echo "===== Test Suite Completed Successfully ====="
echo "Results are available in: tests/retrieval_judge/results/"
echo "Visualizations are available in: tests/retrieval_judge/visualizations/"

================
File: tests/retrieval_judge/test_judge_edge_cases.py
================
#!/usr/bin/env python3
"""
Test script to evaluate the Retrieval Judge's handling of edge cases.
This script:
1. Tests the judge with ambiguous queries
2. Tests the judge with queries containing typos
3. Tests the judge with domain-specific queries
4. Tests the judge with multi-part queries
5. Tests the judge with very short and very long queries
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_judge_edge_cases")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test queries for different edge cases
EDGE_CASE_QUERIES = [
    # Ambiguous queries
    {
        "query": "How does the system work?",
        "type": "ambiguous",
        "description": "Very general query without specific focus"
    },
    {
        "query": "Tell me about the API",
        "type": "ambiguous",
        "description": "Ambiguous query about the API without specifying which aspect"
    },
    
    # Queries with typos
    {
        "query": "How does the vektor store work?",
        "type": "typo",
        "description": "Typo in 'vector'"
    },
    {
        "query": "What is the documnet procesing pipeline?",
        "type": "typo",
        "description": "Multiple typos in 'document processing'"
    },
    
    # Domain-specific queries
    {
        "query": "How does the system handle context window optimization?",
        "type": "domain-specific",
        "description": "Domain-specific query about LLM context windows"
    },
    {
        "query": "What embedding models are supported for semantic search?",
        "type": "domain-specific",
        "description": "Domain-specific query about embedding models"
    },
    
    # Multi-part queries
    {
        "query": "What are the chunking strategies and how do they affect retrieval performance?",
        "type": "multi-part",
        "description": "Multi-part query about chunking strategies and their impact"
    },
    {
        "query": "How does the authentication work and what endpoints are available for document management?",
        "type": "multi-part",
        "description": "Multi-part query about authentication and document management endpoints"
    },
    
    # Very short queries
    {
        "query": "RAG?",
        "type": "short",
        "description": "Very short query with just an acronym"
    },
    {
        "query": "Embeddings?",
        "type": "short",
        "description": "Very short query with just a technical term"
    },
    
    # Very long queries
    {
        "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
        "type": "long",
        "description": "Very long query with multiple questions"
    }
]

# Sample chunks for testing
SAMPLE_CHUNKS = [
    {
        "chunk_id": "chunk1",
        "content": "The vector store is responsible for storing document embeddings, efficient similarity search, and metadata filtering. It uses a HNSW index for approximate nearest neighbor search and supports filtering by metadata such as tags, folder, and document ID.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "vector store,embeddings,search",
            "folder": "/docs"
        },
        "distance": 0.2
    },
    {
        "chunk_id": "chunk2",
        "content": "The document processing pipeline handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction. It supports various file formats including PDF, DOCX, TXT, and Markdown.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "document processing,chunking,extraction",
            "folder": "/docs"
        },
        "distance": 0.3
    },
    {
        "chunk_id": "chunk3",
        "content": "The API layer is implemented using FastAPI and provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval. All API requests require authentication using JWT tokens.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "api,fastapi,endpoints",
            "folder": "/docs"
        },
        "distance": 0.4
    },
    {
        "chunk_id": "chunk4",
        "content": "The LLM integration component connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization. It supports various models including Llama 3, Gemma, and Mistral.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "llm,inference,context window",
            "folder": "/docs"
        },
        "distance": 0.5
    },
    {
        "chunk_id": "chunk5",
        "content": "The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by analyzing queries to determine optimal retrieval parameters, evaluating retrieved chunks for relevance, refining queries when needed to improve retrieval precision, and optimizing context assembly for better response generation.",
        "metadata": {
            "document_id": "doc2",
            "filename": "advanced_features.md",
            "tags": "retrieval judge,llm,optimization",
            "folder": "/docs/advanced"
        },
        "distance": 0.6
    }
]

async def test_query_analysis(retrieval_judge):
    """Test the judge's query analysis capabilities"""
    logger.info("Testing query analysis...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Analyze query
        start_time = time.time()
        analysis = await retrieval_judge.analyze_query(query)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Query complexity: {analysis.get('complexity', 'unknown')}")
        logger.info(f"Recommended parameters: {analysis.get('parameters', {})}")
        logger.info(f"Analysis time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "analysis": analysis,
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "query_analysis_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Query analysis results saved to {os.path.abspath(results_path)}")
    
    return results

async def test_chunk_evaluation(retrieval_judge):
    """Test the judge's chunk evaluation capabilities"""
    logger.info("Testing chunk evaluation...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Evaluate chunks
        start_time = time.time()
        evaluation = await retrieval_judge.evaluate_chunks(query, SAMPLE_CHUNKS)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Needs refinement: {evaluation.get('needs_refinement', False)}")
        logger.info(f"Relevance scores: {evaluation.get('relevance_scores', {})}")
        logger.info(f"Evaluation time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "evaluation": evaluation,
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "chunk_evaluation_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Chunk evaluation results saved to {os.path.abspath(results_path)}")
    
    return results

async def test_query_refinement(retrieval_judge):
    """Test the judge's query refinement capabilities"""
    logger.info("Testing query refinement...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Refine query
        start_time = time.time()
        refined_query = await retrieval_judge.refine_query(query, SAMPLE_CHUNKS)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Original query: {query}")
        logger.info(f"Refined query: {refined_query}")
        logger.info(f"Refinement time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "refined_query": refined_query,
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "query_refinement_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Query refinement results saved to {os.path.abspath(results_path)}")
    
    return results

async def test_context_optimization(retrieval_judge):
    """Test the judge's context optimization capabilities"""
    logger.info("Testing context optimization...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Optimize context
        start_time = time.time()
        optimized_chunks = await retrieval_judge.optimize_context(query, SAMPLE_CHUNKS)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Original chunks: {len(SAMPLE_CHUNKS)}")
        logger.info(f"Optimized chunks: {len(optimized_chunks)}")
        logger.info(f"Optimization time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "original_chunk_count": len(SAMPLE_CHUNKS),
            "optimized_chunk_count": len(optimized_chunks),
            "optimized_chunk_ids": [chunk["chunk_id"] for chunk in optimized_chunks],
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "context_optimization_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Context optimization results saved to {os.path.abspath(results_path)}")
    
    return results

async def analyze_results(query_analysis_results, chunk_evaluation_results, query_refinement_results, context_optimization_results):
    """Analyze the test results"""
    logger.info("\n=== ANALYSIS OF RETRIEVAL JUDGE EDGE CASE HANDLING ===")
    
    # Group results by query type
    query_types = set(r["type"] for r in query_analysis_results)
    
    # Analyze each query type
    for query_type in query_types:
        logger.info(f"\n--- {query_type.upper()} QUERIES ---")
        
        # Filter results for this query type
        qa_results = [r for r in query_analysis_results if r["type"] == query_type]
        ce_results = [r for r in chunk_evaluation_results if r["type"] == query_type]
        qr_results = [r for r in query_refinement_results if r["type"] == query_type]
        co_results = [r for r in context_optimization_results if r["type"] == query_type]
        
        # Query analysis metrics
        complexities = [r["analysis"].get("complexity", "unknown") for r in qa_results]
        complexity_counts = {}
        for complexity in complexities:
            complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
        
        avg_qa_time = sum(r["time"] for r in qa_results) / len(qa_results) if qa_results else 0
        
        logger.info("Query Analysis:")
        logger.info(f"  Complexity distribution: {complexity_counts}")
        logger.info(f"  Average analysis time: {avg_qa_time:.2f}s")
        
        # Chunk evaluation metrics
        needs_refinement_count = sum(1 for r in ce_results if r["evaluation"].get("needs_refinement", False))
        avg_ce_time = sum(r["time"] for r in ce_results) / len(ce_results) if ce_results else 0
        
        logger.info("Chunk Evaluation:")
        logger.info(f"  Queries needing refinement: {needs_refinement_count}/{len(ce_results)}")
        logger.info(f"  Average evaluation time: {avg_ce_time:.2f}s")
        
        # Query refinement metrics
        avg_qr_time = sum(r["time"] for r in qr_results) / len(qr_results) if qr_results else 0
        
        # Calculate average change in query length
        original_lengths = [len(r["query"]) for r in qr_results]
        refined_lengths = [len(r["refined_query"]) for r in qr_results]
        avg_length_change = sum(refined - original for original, refined in zip(original_lengths, refined_lengths)) / len(qr_results) if qr_results else 0
        avg_length_change_percent = (sum(refined / original for original, refined in zip(original_lengths, refined_lengths) if original > 0) / len(qr_results) - 1) * 100 if qr_results else 0
        
        logger.info("Query Refinement:")
        logger.info(f"  Average refinement time: {avg_qr_time:.2f}s")
        logger.info(f"  Average change in query length: {avg_length_change:.2f} characters ({avg_length_change_percent:.2f}%)")
        
        # Context optimization metrics
        avg_co_time = sum(r["time"] for r in co_results) / len(co_results) if co_results else 0
        avg_chunk_reduction = sum(r["original_chunk_count"] - r["optimized_chunk_count"] for r in co_results) / len(co_results) if co_results else 0
        avg_chunk_reduction_percent = (1 - sum(r["optimized_chunk_count"] for r in co_results) / sum(r["original_chunk_count"] for r in co_results)) * 100 if sum(r["original_chunk_count"] for r in co_results) > 0 else 0
        
        logger.info("Context Optimization:")
        logger.info(f"  Average optimization time: {avg_co_time:.2f}s")
        logger.info(f"  Average chunk reduction: {avg_chunk_reduction:.2f} chunks ({avg_chunk_reduction_percent:.2f}%)")
    
    # Overall analysis
    logger.info("\n--- OVERALL ANALYSIS ---")
    
    # Query analysis metrics
    all_complexities = [r["analysis"].get("complexity", "unknown") for r in query_analysis_results]
    all_complexity_counts = {}
    for complexity in all_complexities:
        all_complexity_counts[complexity] = all_complexity_counts.get(complexity, 0) + 1
    
    avg_all_qa_time = sum(r["time"] for r in query_analysis_results) / len(query_analysis_results) if query_analysis_results else 0
    
    logger.info("Query Analysis:")
    logger.info(f"  Overall complexity distribution: {all_complexity_counts}")
    logger.info(f"  Overall average analysis time: {avg_all_qa_time:.2f}s")
    
    # Chunk evaluation metrics
    all_needs_refinement_count = sum(1 for r in chunk_evaluation_results if r["evaluation"].get("needs_refinement", False))
    avg_all_ce_time = sum(r["time"] for r in chunk_evaluation_results) / len(chunk_evaluation_results) if chunk_evaluation_results else 0
    
    logger.info("Chunk Evaluation:")
    logger.info(f"  Overall queries needing refinement: {all_needs_refinement_count}/{len(chunk_evaluation_results)}")
    logger.info(f"  Overall average evaluation time: {avg_all_ce_time:.2f}s")
    
    # Query refinement metrics
    avg_all_qr_time = sum(r["time"] for r in query_refinement_results) / len(query_refinement_results) if query_refinement_results else 0
    
    # Calculate overall average change in query length
    all_original_lengths = [len(r["query"]) for r in query_refinement_results]
    all_refined_lengths = [len(r["refined_query"]) for r in query_refinement_results]
    avg_all_length_change = sum(refined - original for original, refined in zip(all_original_lengths, all_refined_lengths)) / len(query_refinement_results) if query_refinement_results else 0
    avg_all_length_change_percent = (sum(refined / original for original, refined in zip(all_original_lengths, all_refined_lengths) if original > 0) / len(query_refinement_results) - 1) * 100 if query_refinement_results else 0
    
    logger.info("Query Refinement:")
    logger.info(f"  Overall average refinement time: {avg_all_qr_time:.2f}s")
    logger.info(f"  Overall average change in query length: {avg_all_length_change:.2f} characters ({avg_all_length_change_percent:.2f}%)")
    
    # Context optimization metrics
    avg_all_co_time = sum(r["time"] for r in context_optimization_results) / len(context_optimization_results) if context_optimization_results else 0
    avg_all_chunk_reduction = sum(r["original_chunk_count"] - r["optimized_chunk_count"] for r in context_optimization_results) / len(context_optimization_results) if context_optimization_results else 0
    avg_all_chunk_reduction_percent = (1 - sum(r["optimized_chunk_count"] for r in context_optimization_results) / sum(r["original_chunk_count"] for r in context_optimization_results)) * 100 if sum(r["original_chunk_count"] for r in context_optimization_results) > 0 else 0
    
    logger.info("Context Optimization:")
    logger.info(f"  Overall average optimization time: {avg_all_co_time:.2f}s")
    logger.info(f"  Overall average chunk reduction: {avg_all_chunk_reduction:.2f} chunks ({avg_all_chunk_reduction_percent:.2f}%)")
    
    # Identify query types where judge performs best/worst
    type_performance = {}
    for query_type in query_types:
        # Filter results for this query type
        qa_results = [r for r in query_analysis_results if r["type"] == query_type]
        ce_results = [r for r in chunk_evaluation_results if r["type"] == query_type]
        qr_results = [r for r in query_refinement_results if r["type"] == query_type]
        co_results = [r for r in context_optimization_results if r["type"] == query_type]
        
        # Calculate performance metrics
        needs_refinement_rate = sum(1 for r in ce_results if r["evaluation"].get("needs_refinement", False)) / len(ce_results) if ce_results else 0
        avg_length_change_percent = (sum(len(r["refined_query"]) / len(r["query"]) for r in qr_results if len(r["query"]) > 0) / len(qr_results) - 1) * 100 if qr_results else 0
        avg_chunk_reduction_percent = (1 - sum(r["optimized_chunk_count"] for r in co_results) / sum(r["original_chunk_count"] for r in co_results)) * 100 if sum(r["original_chunk_count"] for r in co_results) > 0 else 0
        
        type_performance[query_type] = {
            "needs_refinement_rate": needs_refinement_rate,
            "avg_length_change_percent": avg_length_change_percent,
            "avg_chunk_reduction_percent": avg_chunk_reduction_percent
        }
    
    # Sort by refinement rate (lower is better)
    refinement_sorted = sorted(type_performance.items(), key=lambda x: x[1]["needs_refinement_rate"])
    
    logger.info("\nQuery types by refinement rate (lower is better):")
    for query_type, metrics in refinement_sorted:
        logger.info(f"  {query_type}: {metrics['needs_refinement_rate']*100:.2f}% need refinement")
    
    # Sort by chunk reduction (higher is better)
    chunk_reduction_sorted = sorted(type_performance.items(), key=lambda x: x[1]["avg_chunk_reduction_percent"], reverse=True)
    
    logger.info("\nQuery types by chunk reduction (higher is better):")
    for query_type, metrics in chunk_reduction_sorted:
        logger.info(f"  {query_type}: {metrics['avg_chunk_reduction_percent']:.2f}% chunk reduction")
    
    # Save analysis to file
    analysis = {
        "by_type": {
            query_type: {
                "query_analysis": {
                    "complexity_distribution": {complexity: sum(1 for r in query_analysis_results if r["type"] == query_type and r["analysis"].get("complexity") == complexity) for complexity in set(all_complexities)},
                    "avg_time": sum(r["time"] for r in query_analysis_results if r["type"] == query_type) / len([r for r in query_analysis_results if r["type"] == query_type]) if [r for r in query_analysis_results if r["type"] == query_type] else 0
                },
                "chunk_evaluation": {
                    "needs_refinement_rate": sum(1 for r in chunk_evaluation_results if r["type"] == query_type and r["evaluation"].get("needs_refinement", False)) / len([r for r in chunk_evaluation_results if r["type"] == query_type]) if [r for r in chunk_evaluation_results if r["type"] == query_type] else 0,
                    "avg_time": sum(r["time"] for r in chunk_evaluation_results if r["type"] == query_type) / len([r for r in chunk_evaluation_results if r["type"] == query_type]) if [r for r in chunk_evaluation_results if r["type"] == query_type] else 0
                },
                "query_refinement": {
                    "avg_length_change_percent": (sum(len(r["refined_query"]) / len(r["query"]) for r in query_refinement_results if r["type"] == query_type and len(r["query"]) > 0) / len([r for r in query_refinement_results if r["type"] == query_type]) - 1) * 100 if [r for r in query_refinement_results if r["type"] == query_type] else 0,
                    "avg_time": sum(r["time"] for r in query_refinement_results if r["type"] == query_type) / len([r for r in query_refinement_results if r["type"] == query_type]) if [r for r in query_refinement_results if r["type"] == query_type] else 0
                },
                "context_optimization": {
                    "avg_chunk_reduction_percent": (1 - sum(r["optimized_chunk_count"] for r in context_optimization_results if r["type"] == query_type) / sum(r["original_chunk_count"] for r in context_optimization_results if r["type"] == query_type)) * 100 if sum(r["original_chunk_count"] for r in context_optimization_results if r["type"] == query_type) > 0 else 0,
                    "avg_time": sum(r["time"] for r in context_optimization_results if r["type"] == query_type) / len([r for r in context_optimization_results if r["type"] == query_type]) if [r for r in context_optimization_results if r["type"] == query_type] else 0
                }
            }
            for query_type in query_types
        },
        "overall": {
            "query_analysis": {
                "complexity_distribution": all_complexity_counts,
                "avg_time": avg_all_qa_time
            },
            "chunk_evaluation": {
                "needs_refinement_rate": all_needs_refinement_count / len(chunk_evaluation_results) if chunk_evaluation_results else 0,
                "avg_time": avg_all_ce_time
            },
            "query_refinement": {
                "avg_length_change_percent": avg_all_length_change_percent,
                "avg_time": avg_all_qr_time
            },
            "context_optimization": {
                "avg_chunk_reduction_percent": avg_all_chunk_reduction_percent,
                "avg_time": avg_all_co_time
            }
        },
        "type_performance_ranking": {
            "by_refinement_rate": [{"type": t, "rate": m["needs_refinement_rate"]} for t, m in refinement_sorted],
            "by_chunk_reduction": [{"type": t, "reduction": m["avg_chunk_reduction_percent"]} for t, m in chunk_reduction_sorted]
        }
    }
    
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    analysis_path = os.path.join(results_dir, "judge_edge_case_analysis.json")
    
    with open(analysis_path, "w") as f:
        json.dump(analysis, f, indent=2)
    
    logger.info(f"Edge case analysis saved to {os.path.abspath(analysis_path)}")
    
    return analysis

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge edge case tests...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create OllamaClient
        ollama_client = OllamaClient()
        
        # Create Retrieval Judge
        retrieval_judge = RetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
        
        # Test query analysis
        query_analysis_results = await test_query_analysis(retrieval_judge)
        
        # Test chunk evaluation
        chunk_evaluation_results = await test_chunk_evaluation(retrieval_judge)
        
        # Test query refinement
        query_refinement_results = await test_query_refinement(retrieval_judge)
        
        # Test context optimization
        context_optimization_results = await test_context_optimization(retrieval_judge)
        
        # Analyze results
        analysis = await analyze_results(
            query_analysis_results,
            chunk_evaluation_results,
            query_refinement_results,
            context_optimization_results
        )
        
        logger.info("Retrieval Judge edge case tests completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge edge case tests: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_performance_analysis.py
================
#!/usr/bin/env python3
"""
Test script to analyze why the judge-enhanced retrieval is faster than standard retrieval.
This script:
1. Instruments both retrieval methods with detailed timing
2. Analyzes the time spent in each component
3. Investigates caching effects and other optimizations
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_performance_analysis")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test queries
TEST_QUERIES = [
    "What is the architecture of Metis RAG?",
    "What are the components of the RAG engine?",
    "How does the vector store work?",
    "What is the role of the LLM integration component?",
    "How does the document processing pipeline work?"
]

class InstrumentedVectorStore(VectorStore):
    """Instrumented version of VectorStore that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.query_cache = {}
    
    async def search(self, query: str, top_k: int = 5, filter_criteria: Optional[Dict[str, Any]] = None):
        """Instrumented version of search"""
        # Check cache
        cache_key = f"{query}_{top_k}_{json.dumps(filter_criteria) if filter_criteria else 'none'}"
        if cache_key in self.query_cache:
            self.cache_hits += 1
            logger.info(f"Vector store cache hit for query: {query[:30]}...")
            return self.query_cache[cache_key]
        
        self.cache_misses += 1
        logger.info(f"Vector store cache miss for query: {query[:30]}...")
        
        # Measure search time
        start_time = time.time()
        result = await super().search(query, top_k, filter_criteria)
        elapsed = time.time() - start_time
        
        # Store timing
        if "search" not in self.timings:
            self.timings["search"] = []
        self.timings["search"].append(elapsed)
        
        # Cache result
        self.query_cache[cache_key] = result
        
        logger.info(f"Vector store search took {elapsed:.2f}s")
        return result

class InstrumentedOllamaClient(OllamaClient):
    """Instrumented version of OllamaClient that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.generate_cache = {}
        self.embedding_cache = {}
    
    async def generate(self, prompt: str, model: Optional[str] = None, system_prompt: Optional[str] = None, 
                      stream: bool = False, parameters: Optional[Dict[str, Any]] = None):
        """Instrumented version of generate"""
        # Check cache for non-streaming requests
        if not stream:
            cache_key = f"{prompt}_{model}_{system_prompt}_{json.dumps(parameters) if parameters else 'none'}"
            if cache_key in self.generate_cache:
                self.cache_hits += 1
                logger.info(f"LLM generate cache hit for prompt: {prompt[:30]}...")
                return self.generate_cache[cache_key]
            
            self.cache_misses += 1
            logger.info(f"LLM generate cache miss for prompt: {prompt[:30]}...")
        
        # Measure generate time
        start_time = time.time()
        result = await super().generate(prompt, model, system_prompt, stream, parameters)
        elapsed = time.time() - start_time
        
        # Store timing
        if "generate" not in self.timings:
            self.timings["generate"] = []
        self.timings["generate"].append(elapsed)
        
        # Cache result for non-streaming requests
        if not stream:
            self.generate_cache[cache_key] = result
        
        logger.info(f"LLM generate took {elapsed:.2f}s")
        return result
    
    async def create_embedding(self, text: str, model: Optional[str] = None):
        """Instrumented version of create_embedding"""
        # Check cache
        cache_key = f"{text}_{model}"
        if cache_key in self.embedding_cache:
            self.cache_hits += 1
            logger.info(f"Embedding cache hit for text: {text[:30]}...")
            return self.embedding_cache[cache_key]
        
        self.cache_misses += 1
        logger.info(f"Embedding cache miss for text: {text[:30]}...")
        
        # Measure embedding time
        start_time = time.time()
        result = await super().create_embedding(text, model)
        elapsed = time.time() - start_time
        
        # Store timing
        if "embedding" not in self.timings:
            self.timings["embedding"] = []
        self.timings["embedding"].append(elapsed)
        
        # Cache result
        self.embedding_cache[cache_key] = result
        
        logger.info(f"Embedding creation took {elapsed:.2f}s")
        return result

class InstrumentedRetrievalJudge(RetrievalJudge):
    """Instrumented version of RetrievalJudge that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """Instrumented version of analyze_query"""
        start_time = time.time()
        result = await super().analyze_query(query)
        elapsed = time.time() - start_time
        
        if "analyze_query" not in self.timings:
            self.timings["analyze_query"] = []
        self.timings["analyze_query"].append(elapsed)
        
        logger.info(f"analyze_query took {elapsed:.2f}s")
        return result
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Instrumented version of evaluate_chunks"""
        start_time = time.time()
        result = await super().evaluate_chunks(query, chunks)
        elapsed = time.time() - start_time
        
        if "evaluate_chunks" not in self.timings:
            self.timings["evaluate_chunks"] = []
        self.timings["evaluate_chunks"].append(elapsed)
        
        logger.info(f"evaluate_chunks took {elapsed:.2f}s")
        return result
    
    async def refine_query(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Instrumented version of refine_query"""
        start_time = time.time()
        result = await super().refine_query(query, chunks)
        elapsed = time.time() - start_time
        
        if "refine_query" not in self.timings:
            self.timings["refine_query"] = []
        self.timings["refine_query"].append(elapsed)
        
        logger.info(f"refine_query took {elapsed:.2f}s")
        return result
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Instrumented version of optimize_context"""
        start_time = time.time()
        result = await super().optimize_context(query, chunks)
        elapsed = time.time() - start_time
        
        if "optimize_context" not in self.timings:
            self.timings["optimize_context"] = []
        self.timings["optimize_context"].append(elapsed)
        
        logger.info(f"optimize_context took {elapsed:.2f}s")
        return result

class InstrumentedRAGEngine(RAGEngine):
    """Instrumented version of RAGEngine that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def _enhanced_retrieval(self, *args, **kwargs):
        """Instrumented version of _enhanced_retrieval"""
        start_time = time.time()
        result = await super()._enhanced_retrieval(*args, **kwargs)
        elapsed = time.time() - start_time
        
        if "enhanced_retrieval" not in self.timings:
            self.timings["enhanced_retrieval"] = []
        self.timings["enhanced_retrieval"].append(elapsed)
        
        logger.info(f"enhanced_retrieval took {elapsed:.2f}s")
        return result

async def create_test_document():
    """Create a test document for RAG testing"""
    logger.info("Creating test document...")
    
    # Create directory for test document
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create document file
    doc_path = os.path.join(test_dir, "performance_test_document.md")
    with open(doc_path, "w") as f:
        f.write(TEST_DOCUMENT)
    
    logger.info(f"Created test document at {os.path.abspath(doc_path)}")
    return doc_path

async def process_document(doc_path):
    """Process the test document and add it to the vector store"""
    logger.info("Processing test document...")
    
    # Read file content
    with open(doc_path, "r") as f:
        doc_content = f.read()
    
    # Create Document object
    document = Document(
        id=str(uuid.uuid4()),
        filename="performance_test_document.md",
        content=doc_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    # Create a vector store
    test_persist_dir = os.path.join("tests", "retrieval_judge", "performance_test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = InstrumentedVectorStore(persist_directory=test_persist_dir)
    
    # Create chunks
    document.chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=doc_content,
            metadata={
                "index": 0,
                "source": doc_path,
                "document_id": document.id
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(document)
    
    logger.info(f"Added document {document.id} to vector store")
    
    return document, vector_store

async def run_performance_tests(vector_store):
    """Run performance tests for standard and judge-enhanced retrieval"""
    logger.info("Running performance tests...")
    
    # Create instrumented OllamaClient
    ollama_client = InstrumentedOllamaClient()
    
    # Create instrumented Retrieval Judge
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = InstrumentedRetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Create engines
    rag_engine_with_judge = InstrumentedRAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    rag_engine_standard = InstrumentedRAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    results = []
    
    # Run each test query twice - first with standard retrieval, then with judge
    for query in TEST_QUERIES:
        logger.info(f"\n=== Testing query: {query} ===")
        
        # Test with standard retrieval
        logger.info("Running with standard retrieval...")
        standard_start_time = time.time()
        standard_response = await rag_engine_standard.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        standard_total_time = time.time() - standard_start_time
        
        # Test with judge-enhanced retrieval
        logger.info("Running with judge-enhanced retrieval...")
        judge_start_time = time.time()
        judge_response = await rag_engine_with_judge.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        judge_total_time = time.time() - judge_start_time
        
        # Extract results
        standard_answer = standard_response.get("answer", "")
        standard_sources = standard_response.get("sources", [])
        
        judge_answer = judge_response.get("answer", "")
        judge_sources = judge_response.get("sources", [])
        
        # Log results
        logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_total_time:.2f}s")
        logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_total_time:.2f}s")
        
        # Collect component timings
        standard_component_timings = {}
        judge_component_timings = {}
        
        # Vector store timings
        if hasattr(vector_store, 'timings'):
            for component, times in vector_store.timings.items():
                standard_component_timings[f"vector_store_{component}"] = times[0] if len(times) > 0 else 0
                judge_component_timings[f"vector_store_{component}"] = times[1] if len(times) > 1 else 0
        
        # Ollama client timings
        if hasattr(ollama_client, 'timings'):
            for component, times in ollama_client.timings.items():
                # First half of times are from standard retrieval, second half from judge
                standard_times = [t for i, t in enumerate(times) if i < len(times)//2]
                judge_times = [t for i, t in enumerate(times) if i >= len(times)//2]
                
                standard_component_timings[f"ollama_{component}"] = sum(standard_times)
                judge_component_timings[f"ollama_{component}"] = sum(judge_times)
        
        # Retrieval judge timings
        if hasattr(retrieval_judge, 'timings'):
            for component, times in retrieval_judge.timings.items():
                judge_component_timings[f"judge_{component}"] = sum(times)
        
        # RAG engine timings
        if hasattr(rag_engine_with_judge, 'timings'):
            for component, times in rag_engine_with_judge.timings.items():
                judge_component_timings[f"rag_engine_{component}"] = sum(times)
        
        if hasattr(rag_engine_standard, 'timings'):
            for component, times in rag_engine_standard.timings.items():
                standard_component_timings[f"rag_engine_{component}"] = sum(times)
        
        # Cache statistics
        cache_stats = {
            "vector_store_cache_hits": vector_store.cache_hits,
            "vector_store_cache_misses": vector_store.cache_misses,
            "ollama_cache_hits": ollama_client.cache_hits,
            "ollama_cache_misses": ollama_client.cache_misses
        }
        
        # Store results
        results.append({
            "query": query,
            "standard": {
                "total_time": standard_total_time,
                "sources_count": len(standard_sources),
                "component_timings": standard_component_timings
            },
            "judge": {
                "total_time": judge_total_time,
                "sources_count": len(judge_sources),
                "component_timings": judge_component_timings
            },
            "cache_stats": cache_stats
        })
        
        # Reset timings for next run
        vector_store.timings = {}
        ollama_client.timings = {}
        retrieval_judge.timings = {}
        rag_engine_with_judge.timings = {}
        rag_engine_standard.timings = {}
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "performance_analysis_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Performance analysis results saved to {os.path.abspath(results_path)}")
    
    return results

async def analyze_performance_results(results):
    """Analyze the performance test results"""
    logger.info("\n=== PERFORMANCE ANALYSIS ===")
    
    # Calculate averages
    avg_standard_time = sum(r["standard"]["total_time"] for r in results) / len(results)
    avg_judge_time = sum(r["judge"]["total_time"] for r in results) / len(results)
    
    logger.info(f"Average standard retrieval time: {avg_standard_time:.2f}s")
    logger.info(f"Average judge-enhanced retrieval time: {avg_judge_time:.2f}s")
    logger.info(f"Time difference: {((avg_judge_time - avg_standard_time) / avg_standard_time * 100):.2f}%")
    
    # Analyze component timings
    standard_component_times = {}
    judge_component_times = {}
    
    for r in results:
        for component, time_value in r["standard"]["component_timings"].items():
            if component not in standard_component_times:
                standard_component_times[component] = []
            standard_component_times[component].append(time_value)
        
        for component, time_value in r["judge"]["component_timings"].items():
            if component not in judge_component_times:
                judge_component_times[component] = []
            judge_component_times[component].append(time_value)
    
    logger.info("\nStandard retrieval component timing averages:")
    for component, times in standard_component_times.items():
        avg_time = sum(times) / len(times)
        logger.info(f"  {component}: {avg_time:.2f}s")
    
    logger.info("\nJudge-enhanced retrieval component timing averages:")
    for component, times in judge_component_times.items():
        avg_time = sum(times) / len(times)
        logger.info(f"  {component}: {avg_time:.2f}s")
    
    # Analyze cache statistics
    total_vector_store_hits = sum(r["cache_stats"]["vector_store_cache_hits"] for r in results)
    total_vector_store_misses = sum(r["cache_stats"]["vector_store_cache_misses"] for r in results)
    total_ollama_hits = sum(r["cache_stats"]["ollama_cache_hits"] for r in results)
    total_ollama_misses = sum(r["cache_stats"]["ollama_cache_misses"] for r in results)
    
    logger.info("\nCache statistics:")
    logger.info(f"  Vector store cache hits: {total_vector_store_hits}")
    logger.info(f"  Vector store cache misses: {total_vector_store_misses}")
    logger.info(f"  Vector store cache hit rate: {total_vector_store_hits/(total_vector_store_hits+total_vector_store_misses)*100:.2f}%")
    logger.info(f"  Ollama cache hits: {total_ollama_hits}")
    logger.info(f"  Ollama cache misses: {total_ollama_misses}")
    logger.info(f"  Ollama cache hit rate: {total_ollama_hits/(total_ollama_hits+total_ollama_misses)*100:.2f}%")
    
    # Analyze potential reasons for performance difference
    logger.info("\nPotential reasons for performance difference:")
    
    # Check if judge retrieves fewer chunks
    avg_standard_sources = sum(r["standard"]["sources_count"] for r in results) / len(results)
    avg_judge_sources = sum(r["judge"]["sources_count"] for r in results) / len(results)
    
    if avg_judge_sources < avg_standard_sources:
        logger.info(f"  - Judge retrieves fewer sources ({avg_judge_sources:.2f} vs {avg_standard_sources:.2f})")
    
    # Check if judge has better cache utilization
    if total_ollama_hits > 0:
        logger.info(f"  - Judge benefits from LLM caching ({total_ollama_hits} cache hits)")
    
    if total_vector_store_hits > 0:
        logger.info(f"  - Judge benefits from vector store caching ({total_vector_store_hits} cache hits)")
    
    # Check if judge optimizes query parameters
    if "judge_analyze_query" in judge_component_times:
        logger.info(f"  - Judge optimizes query parameters (analyze_query: {sum(judge_component_times['judge_analyze_query'])/len(judge_component_times['judge_analyze_query']):.2f}s)")
    
    # Save analysis to file
    analysis = {
        "avg_standard_time": avg_standard_time,
        "avg_judge_time": avg_judge_time,
        "time_difference_percent": ((avg_judge_time - avg_standard_time) / avg_standard_time * 100),
        "standard_component_averages": {component: sum(times) / len(times) for component, times in standard_component_times.items()},
        "judge_component_averages": {component: sum(times) / len(times) for component, times in judge_component_times.items()},
        "cache_statistics": {
            "vector_store_cache_hits": total_vector_store_hits,
            "vector_store_cache_misses": total_vector_store_misses,
            "vector_store_cache_hit_rate": total_vector_store_hits/(total_vector_store_hits+total_vector_store_misses)*100 if (total_vector_store_hits+total_vector_store_misses) > 0 else 0,
            "ollama_cache_hits": total_ollama_hits,
            "ollama_cache_misses": total_ollama_misses,
            "ollama_cache_hit_rate": total_ollama_hits/(total_ollama_hits+total_ollama_misses)*100 if (total_ollama_hits+total_ollama_misses) > 0 else 0
        },
        "avg_standard_sources": avg_standard_sources,
        "avg_judge_sources": avg_judge_sources
    }
    
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    analysis_path = os.path.join(results_dir, "performance_analysis.json")
    
    with open(analysis_path, "w") as f:
        json.dump(analysis, f, indent=2)
    
    logger.info(f"Performance analysis saved to {os.path.abspath(analysis_path)}")
    
    return analysis

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge performance analysis...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test document
        doc_path = await create_test_document()
        
        # Process document
        document, vector_store = await process_document(doc_path)
        
        # Run performance tests
        results = await run_performance_tests(vector_store)
        
        # Analyze results
        analysis = await analyze_performance_results(results)
        
        logger.info("Retrieval Judge performance analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge performance analysis: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_retrieval_judge_comparison.py
================
#!/usr/bin/env python3
"""
Test script to compare standard retrieval vs. retrieval with the Retrieval Judge enabled.
This script:
1. Creates test documents and processes them
2. Runs test queries with both standard retrieval and judge-enhanced retrieval
3. Compares the results to evaluate the judge's effectiveness
4. Analyzes areas for improvement
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_retrieval_judge_comparison")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content - using the same content as test_rag_retrieval.py
MARKDOWN_CONTENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

PDF_CONTENT = """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
"""

# Additional test document with more technical content
TECHNICAL_SPECS_CONTENT = """# Product Specifications

## System Requirements

The Metis RAG system requires the following minimum specifications:
- CPU: 4 cores, 2.5GHz or higher
- RAM: 16GB minimum, 32GB recommended
- Storage: 100GB SSD
- Operating System: Ubuntu 22.04 LTS, Windows Server 2019, or macOS 12+
- Network: 100Mbps internet connection

## API Reference

### Authentication

All API requests require authentication using JWT tokens. To obtain a token:

```
POST /api/auth/token
{
  "username": "your_username",
  "password": "your_password"
}
```

The response will include an access token valid for 24 hours.

### Document Management

#### Upload Document

```
POST /api/documents/upload
Content-Type: multipart/form-data
Authorization: Bearer <token>

Form fields:
- file: The document file
- tags: Comma-separated tags (optional)
- folder: Target folder path (optional)
```

#### List Documents

```
GET /api/documents/list
Authorization: Bearer <token>
```

Optional query parameters:
- folder: Filter by folder
- tags: Filter by tags (comma-separated)
- page: Page number (default: 1)
- limit: Items per page (default: 20)

### Chat API

#### Create Chat Session

```
POST /api/chat/sessions
Authorization: Bearer <token>
{
  "title": "Optional chat title"
}
```

#### Send Message

```
POST /api/chat/messages
Authorization: Bearer <token>
{
  "session_id": "chat_session_id",
  "content": "Your message here",
  "use_rag": true
}
```

## Performance Benchmarks

The system has been benchmarked with the following results:
- Document processing: 5 pages/second
- Vector search latency: <50ms for 10k documents
- End-to-end query response time: <2 seconds
- Maximum documents: 100,000
- Maximum vector store size: 10GB
"""

# Test queries of varying complexity
TEST_QUERIES = [
    # Simple factual queries
    {
        "query": "What is the architecture of Metis RAG?",
        "complexity": "simple",
        "description": "Simple factual query about architecture"
    },
    {
        "query": "What was the revenue reported in Q1 2025?",
        "complexity": "simple",
        "description": "Simple factual query about revenue"
    },
    
    # Moderate complexity queries
    {
        "query": "What are the components of the RAG engine and how do they work together?",
        "complexity": "moderate",
        "description": "Moderate complexity query requiring synthesis of multiple components"
    },
    {
        "query": "Compare the financial performance metrics from the quarterly report.",
        "complexity": "moderate",
        "description": "Moderate complexity query requiring comparison and analysis"
    },
    
    # Complex analytical queries
    {
        "query": "How does the document processing pipeline handle different file types and what are the implications for retrieval quality?",
        "complexity": "complex",
        "description": "Complex query requiring deep technical understanding and inference"
    },
    {
        "query": "Based on the quarterly report, what strategic initiatives might have the highest ROI and why?",
        "complexity": "complex",
        "description": "Complex query requiring business analysis and inference"
    },
    
    # Ambiguous queries
    {
        "query": "What are the system requirements?",
        "complexity": "ambiguous",
        "description": "Ambiguous query that could refer to different aspects"
    },
    {
        "query": "How does the API work?",
        "complexity": "ambiguous",
        "description": "Ambiguous query with broad scope"
    },
    
    # Multi-part queries
    {
        "query": "What is the vector store responsible for and what are the minimum RAM requirements for the system?",
        "complexity": "multi-part",
        "description": "Multi-part query combining two different topics"
    },
    {
        "query": "Explain the authentication process for the API and list the strategic initiatives for Q2.",
        "complexity": "multi-part",
        "description": "Multi-part query requiring information from different documents"
    }
]

async def create_test_documents():
    """Create test documents for RAG testing"""
    logger.info("Creating test documents...")
    
    # Create directories for test documents
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create Markdown document
    markdown_path = os.path.join(test_dir, "technical_documentation.md")
    with open(markdown_path, "w") as f:
        f.write(MARKDOWN_CONTENT)
    
    # Create PDF-like text document
    pdf_path = os.path.join(test_dir, "quarterly_report.txt")
    with open(pdf_path, "w") as f:
        f.write(PDF_CONTENT)
    
    # Create technical specs document
    specs_path = os.path.join(test_dir, "product_specifications.md")
    with open(specs_path, "w") as f:
        f.write(TECHNICAL_SPECS_CONTENT)
    
    logger.info(f"Created test documents in {os.path.abspath(test_dir)}")
    return markdown_path, pdf_path, specs_path

async def process_documents(vector_store, markdown_path, pdf_path, specs_path):
    """Process the test documents and add them to the vector store"""
    logger.info("Processing test documents...")
    
    # Read file contents
    with open(markdown_path, "r") as f:
        markdown_content = f.read()
    
    with open(pdf_path, "r") as f:
        pdf_content = f.read()
    
    with open(specs_path, "r") as f:
        specs_content = f.read()
    
    # Create Document objects
    markdown_doc = Document(
        id=str(uuid.uuid4()),
        filename="technical_documentation.md",
        content=markdown_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    pdf_doc = Document(
        id=str(uuid.uuid4()),
        filename="quarterly_report.txt",
        content=pdf_content,
        tags=["business", "report", "quarterly"],
        folder="/test"
    )
    
    specs_doc = Document(
        id=str(uuid.uuid4()),
        filename="product_specifications.md",
        content=specs_content,
        tags=["technical", "specifications", "api"],
        folder="/test"
    )
    
    # Create a custom vector store to handle tags properly
    class CustomVectorStore(VectorStore):
        async def add_document(self, document: Document) -> None:
            """Override to fix tags handling"""
            try:
                logger.info(f"Adding document {document.id} to vector store")
                
                # Make sure we have an Ollama client
                if self.ollama_client is None:
                    self.ollama_client = OllamaClient()
                
                # Prepare chunks for batch processing
                chunks_to_embed = [chunk for chunk in document.chunks if not chunk.embedding]
                chunk_contents = [chunk.content for chunk in chunks_to_embed]
                
                # Create embeddings in batch if possible
                if chunk_contents:
                    try:
                        # Batch embedding
                        embeddings = await self._batch_create_embeddings(chunk_contents)
                        
                        # Assign embeddings to chunks
                        for i, chunk in enumerate(chunks_to_embed):
                            chunk.embedding = embeddings[i]
                    except Exception as batch_error:
                        logger.warning(f"Batch embedding failed: {str(batch_error)}. Falling back to sequential embedding.")
                        # Fall back to sequential embedding
                        for chunk in chunks_to_embed:
                            chunk.embedding = await self.ollama_client.create_embedding(
                                text=chunk.content,
                                model=self.embedding_model
                            )
                
                # Add chunks to the collection
                for chunk in document.chunks:
                    if not chunk.embedding:
                        logger.warning(f"Chunk {chunk.id} has no embedding, skipping")
                        continue
                        
                    # Convert tags to string to avoid ChromaDB error
                    tags_str = ",".join(document.tags) if document.tags else ""
                    
                    self.collection.add(
                        ids=[chunk.id],
                        embeddings=[chunk.embedding],
                        documents=[chunk.content],
                        metadatas=[{
                            "document_id": document.id,
                            "chunk_index": chunk.metadata.get("index", 0),
                            "filename": document.filename,
                            "tags": tags_str,  # Use string instead of list
                            "folder": document.folder,
                            **chunk.metadata
                        }]
                    )
                
                logger.info(f"Added {len(document.chunks)} chunks to vector store for document {document.id}")
            except Exception as e:
                logger.error(f"Error adding document {document.id} to vector store: {str(e)}")
                raise
    
    # Use our custom vector store with a unique persist directory for this test
    test_persist_dir = os.path.join("tests", "retrieval_judge", "test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = CustomVectorStore(persist_directory=test_persist_dir)
    
    # Create chunks for documents
    # For a more realistic test, we'll split the documents into smaller chunks
    
    # Helper function to create chunks
    def create_chunks(content, doc_id, source_path, chunk_size=500, overlap=100):
        chunks = []
        # Simple chunking by splitting the content
        words = content.split()
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            if len(chunk_words) < 50:  # Skip very small chunks
                continue
                
            chunk_content = " ".join(chunk_words)
            chunks.append(
                Chunk(
                    id=str(uuid.uuid4()),
                    content=chunk_content,
                    metadata={
                        "index": len(chunks),
                        "source": source_path,
                        "document_id": doc_id
                    }
                )
            )
        
        return chunks
    
    # Create chunks for each document
    markdown_doc.chunks = create_chunks(markdown_content, markdown_doc.id, markdown_path)
    pdf_doc.chunks = create_chunks(pdf_content, pdf_doc.id, pdf_path)
    specs_doc.chunks = create_chunks(specs_content, specs_doc.id, specs_path)
    
    # Add documents to vector store
    await vector_store.add_document(markdown_doc)
    await vector_store.add_document(pdf_doc)
    await vector_store.add_document(specs_doc)
    
    logger.info(f"Added documents to vector store: {markdown_doc.id}, {pdf_doc.id}, {specs_doc.id}")
    logger.info(f"Total chunks: {len(markdown_doc.chunks) + len(pdf_doc.chunks) + len(specs_doc.chunks)}")
    
    return markdown_doc, pdf_doc, specs_doc, vector_store

async def run_comparison_tests(vector_store):
    """Run comparison tests between standard retrieval and judge-enhanced retrieval"""
    logger.info("Running comparison tests...")
    
    # Create two RAG engines - one with Retrieval Judge and one without
    ollama_client = OllamaClient()
    
    # Create Retrieval Judge with the specified model
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = RetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Engine with Retrieval Judge
    rag_engine_with_judge = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    # Engine without Retrieval Judge
    rag_engine_standard = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    results = []
    
    # Run each test query with both engines
    for query_info in TEST_QUERIES:
        query = query_info["query"]
        complexity = query_info["complexity"]
        description = query_info["description"]
        
        logger.info(f"Testing query: {query}")
        logger.info(f"Complexity: {complexity}")
        
        # Test with standard retrieval
        logger.info("Running with standard retrieval...")
        start_time_standard = time.time()
        standard_response = await rag_engine_standard.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        standard_time = time.time() - start_time_standard
        
        # Test with judge-enhanced retrieval
        logger.info("Running with judge-enhanced retrieval...")
        start_time_judge = time.time()
        judge_response = await rag_engine_with_judge.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        judge_time = time.time() - start_time_judge
        
        # Extract results
        standard_answer = standard_response.get("answer", "")
        standard_sources = standard_response.get("sources", [])
        
        judge_answer = judge_response.get("answer", "")
        judge_sources = judge_response.get("sources", [])
        
        # Log basic results
        logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_time:.2f}s")
        logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_time:.2f}s")
        
        # Store results for analysis
        results.append({
            "query": query,
            "complexity": complexity,
            "description": description,
            "standard": {
                "answer": standard_answer,
                "sources": [
                    {
                        "document_id": s.document_id,
                        "chunk_id": s.chunk_id,
                        "relevance_score": s.relevance_score,
                        "excerpt": s.excerpt[:100] + "..." if len(s.excerpt) > 100 else s.excerpt
                    }
                    for s in standard_sources
                ] if standard_sources else [],
                "time": standard_time
            },
            "judge": {
                "answer": judge_answer,
                "sources": [
                    {
                        "document_id": s.document_id,
                        "chunk_id": s.chunk_id,
                        "relevance_score": s.relevance_score,
                        "excerpt": s.excerpt[:100] + "..." if len(s.excerpt) > 100 else s.excerpt
                    }
                    for s in judge_sources
                ] if judge_sources else [],
                "time": judge_time
            }
        })
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "retrieval_judge_comparison_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Test results saved to {os.path.abspath(results_path)}")
    return results

async def analyze_results(results):
    """Analyze the comparison test results"""
    logger.info("Analyzing comparison test results...")
    
    # Initialize metrics
    metrics = {
        "total_queries": len(results),
        "avg_standard_time": 0,
        "avg_judge_time": 0,
        "avg_standard_sources": 0,
        "avg_judge_sources": 0,
        "queries_with_more_judge_sources": 0,
        "queries_with_more_standard_sources": 0,
        "queries_with_equal_sources": 0,
        "by_complexity": {},
        "improvement_areas": []
    }
    
    # Calculate metrics
    total_standard_time = 0
    total_judge_time = 0
    total_standard_sources = 0
    total_judge_sources = 0
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        metrics["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_time": 0,
            "avg_judge_time": 0,
            "avg_standard_sources": 0,
            "avg_judge_sources": 0,
            "improvement_percentage": 0
        }
    
    # Process each result
    for result in results:
        complexity = result["complexity"]
        standard_sources = len(result["standard"]["sources"])
        judge_sources = len(result["judge"]["sources"])
        standard_time = result["standard"]["time"]
        judge_time = result["judge"]["time"]
        
        # Update totals
        total_standard_time += standard_time
        total_judge_time += judge_time
        total_standard_sources += standard_sources
        total_judge_sources += judge_sources
        
        # Update source comparison counts
        if judge_sources > standard_sources:
            metrics["queries_with_more_judge_sources"] += 1
        elif standard_sources > judge_sources:
            metrics["queries_with_more_standard_sources"] += 1
        else:
            metrics["queries_with_equal_sources"] += 1
        
        # Update complexity metrics
        if complexity in metrics["by_complexity"]:
            metrics["by_complexity"][complexity]["count"] += 1
            metrics["by_complexity"][complexity]["avg_standard_time"] += standard_time
            metrics["by_complexity"][complexity]["avg_judge_time"] += judge_time
            metrics["by_complexity"][complexity]["avg_standard_sources"] += standard_sources
            metrics["by_complexity"][complexity]["avg_judge_sources"] += judge_sources
    
    # Calculate averages
    metrics["avg_standard_time"] = total_standard_time / len(results)
    metrics["avg_judge_time"] = total_judge_time / len(results)
    metrics["avg_standard_sources"] = total_standard_sources / len(results)
    metrics["avg_judge_sources"] = total_judge_sources / len(results)
    
    # Calculate complexity averages and improvement percentages
    for complexity, data in metrics["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_time"] /= data["count"]
            data["avg_judge_time"] /= data["count"]
            data["avg_standard_sources"] /= data["count"]
            data["avg_judge_sources"] /= data["count"]
            
            # Calculate improvement percentage in source relevance
            if data["avg_standard_sources"] > 0:
                data["improvement_percentage"] = ((data["avg_judge_sources"] - data["avg_standard_sources"]) / 
                                                data["avg_standard_sources"]) * 100
            else:
                data["improvement_percentage"] = 0
    
    # Identify areas for improvement
    # 1. Check for queries where judge performed worse
    for result in results:
        standard_sources = len(result["standard"]["sources"])
        judge_sources = len(result["judge"]["sources"])
        
        if standard_sources > judge_sources:
            metrics["improvement_areas"].append({
                "query": result["query"],
                "complexity": result["complexity"],
                "issue": "Judge retrieved fewer sources than standard retrieval",
                "standard_sources": standard_sources,
                "judge_sources": judge_sources
            })
        
        # 2. Check for excessive processing time
        if result["judge"]["time"] > result["standard"]["time"] * 2:
            metrics["improvement_areas"].append({
                "query": result["query"],
                "complexity": result["complexity"],
                "issue": "Judge processing time significantly higher",
                "standard_time": result["standard"]["time"],
                "judge_time": result["judge"]["time"]
            })
    
    # Log summary metrics
    logger.info(f"Total queries: {metrics['total_queries']}")
    logger.info(f"Average standard retrieval time: {metrics['avg_standard_time']:.2f}s")
    logger.info(f"Average judge-enhanced retrieval time: {metrics['avg_judge_time']:.2f}s")
    logger.info(f"Average standard sources: {metrics['avg_standard_sources']:.2f}")
    logger.info(f"Average judge-enhanced sources: {metrics['avg_judge_sources']:.2f}")
    logger.info(f"Queries with more judge sources: {metrics['queries_with_more_judge_sources']}")
    logger.info(f"Queries with more standard sources: {metrics['queries_with_more_standard_sources']}")
    logger.info(f"Queries with equal sources: {metrics['queries_with_equal_sources']}")
    
    # Log complexity metrics
    for complexity, data in metrics["by_complexity"].items():
        if data["count"] > 0:
            logger.info(f"\nComplexity: {complexity} ({data['count']} queries)")
            logger.info(f"  Avg standard time: {data['avg_standard_time']:.2f}s")
            logger.info(f"  Avg judge time: {data['avg_judge_time']:.2f}s")
            logger.info(f"  Avg standard sources: {data['avg_standard_sources']:.2f}")
            logger.info(f"  Avg judge sources: {data['avg_judge_sources']:.2f}")
            logger.info(f"  Improvement percentage: {data['improvement_percentage']:.2f}%")
    
    # Log improvement areas
    if metrics["improvement_areas"]:
        logger.info("\nAreas for improvement:")
        for area in metrics["improvement_areas"]:
            logger.info(f"  Query: {area['query']}")
            logger.info(f"  Issue: {area['issue']}")
    
    # Save metrics to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    metrics_path = os.path.join(results_dir, "retrieval_judge_metrics.json")
    
    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=2)
    
    logger.info(f"Analysis metrics saved to {os.path.abspath(metrics_path)}")
    return metrics

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge comparison test...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test documents
        markdown_path, pdf_path, specs_path = await create_test_documents()
        
        # Process documents
        markdown_doc, pdf_doc, specs_doc, vector_store = await process_documents(
            None, markdown_path, pdf_path, specs_path
        )
        
        # Run comparison tests
        results = await run_comparison_tests(vector_store)
        
        # Analyze results
        metrics = await analyze_results(results)
        
        # Print summary
        logger.info("\n=== RETRIEVAL JUDGE COMPARISON SUMMARY ===")
        logger.info(f"Total queries tested: {metrics['total_queries']}")
        
        # Calculate overall improvement
        source_improvement = ((metrics['avg_judge_sources'] - metrics['avg_standard_sources']) / 
                             metrics['avg_standard_sources'] * 100) if metrics['avg_standard_sources'] > 0 else 0
        
        time_difference = ((metrics['avg_judge_time'] - metrics['avg_standard_time']) / 
                          metrics['avg_standard_time'] * 100) if metrics['avg_standard_time'] > 0 else 0
        
        logger.info(f"Overall source relevance improvement: {source_improvement:.2f}%")
        logger.info(f"Processing time difference: {time_difference:.2f}%")
        
        # Effectiveness by query complexity
        logger.info("\nEffectiveness by query complexity:")
        for complexity, data in metrics["by_complexity"].items():
            if data["count"] > 0:
                logger.info(f"  {complexity.capitalize()}: {data['improvement_percentage']:.2f}% improvement")
        
        # Improvement areas summary
        if metrics["improvement_areas"]:
            logger.info(f"\nIdentified {len(metrics['improvement_areas'])} areas for improvement")
        else:
            logger.info("\nNo specific areas for improvement identified")
        
        logger.info("Retrieval Judge comparison test completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge comparison test: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_single_query.py
================
#!/usr/bin/env python3
"""
Simplified test script to demonstrate the Retrieval Judge with a single query.
This script:
1. Creates a test document
2. Processes the document
3. Runs a single query with both standard retrieval and judge-enhanced retrieval
4. Compares the results
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_single_query")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test query
TEST_QUERY = "What are the components of the RAG engine and how do they work together?"

async def create_test_document():
    """Create a test document for RAG testing"""
    logger.info("Creating test document...")
    
    # Create directory for test document
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create document file
    doc_path = os.path.join(test_dir, "test_document.md")
    with open(doc_path, "w") as f:
        f.write(TEST_DOCUMENT)
    
    logger.info(f"Created test document at {os.path.abspath(doc_path)}")
    return doc_path

async def process_document(doc_path):
    """Process the test document and add it to the vector store"""
    logger.info("Processing test document...")
    
    # Read file content
    with open(doc_path, "r") as f:
        doc_content = f.read()
    
    # Create Document object
    document = Document(
        id=str(uuid.uuid4()),
        filename="test_document.md",
        content=doc_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    # Create a vector store
    test_persist_dir = os.path.join("tests", "retrieval_judge", "test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = VectorStore(persist_directory=test_persist_dir)
    
    # Create chunks
    document.chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=doc_content,
            metadata={
                "index": 0,
                "source": doc_path,
                "document_id": document.id
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(document)
    
    logger.info(f"Added document {document.id} to vector store")
    
    return document, vector_store

async def run_comparison_test(vector_store):
    """Run a comparison test between standard retrieval and judge-enhanced retrieval"""
    logger.info("Running comparison test...")
    
    # Create OllamaClient
    ollama_client = OllamaClient()
    
    # Create Retrieval Judge with the specified model
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = RetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Engine with Retrieval Judge
    rag_engine_with_judge = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    # Engine without Retrieval Judge
    rag_engine_standard = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    # Test with standard retrieval
    logger.info("Running with standard retrieval...")
    start_time_standard = time.time()
    standard_response = await rag_engine_standard.query(
        query=TEST_QUERY,
        use_rag=True,
        top_k=5,
        stream=False
    )
    standard_time = time.time() - start_time_standard
    
    # Test with judge-enhanced retrieval
    logger.info("Running with judge-enhanced retrieval...")
    start_time_judge = time.time()
    judge_response = await rag_engine_with_judge.query(
        query=TEST_QUERY,
        use_rag=True,
        top_k=5,
        stream=False
    )
    judge_time = time.time() - start_time_judge
    
    # Extract results
    standard_answer = standard_response.get("answer", "")
    standard_sources = standard_response.get("sources", [])
    
    judge_answer = judge_response.get("answer", "")
    judge_sources = judge_response.get("sources", [])
    
    # Log results
    logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_time:.2f}s")
    logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_time:.2f}s")
    
    # Format results for saving
    result = {
        "query": TEST_QUERY,
        "standard": {
            "answer": standard_answer,
            "sources_count": len(standard_sources),
            "time": standard_time
        },
        "judge": {
            "answer": judge_answer,
            "sources_count": len(judge_sources),
            "time": judge_time
        }
    }
    
    # Save result to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    result_path = os.path.join(results_dir, "single_query_result.json")
    
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2)
    
    logger.info(f"Test result saved to {os.path.abspath(result_path)}")
    
    # Print comparison
    logger.info("\n=== RETRIEVAL JUDGE COMPARISON ===")
    logger.info(f"Query: {TEST_QUERY}")
    logger.info(f"Standard retrieval time: {standard_time:.2f}s")
    logger.info(f"Judge-enhanced retrieval time: {judge_time:.2f}s")
    logger.info(f"Time difference: {((judge_time - standard_time) / standard_time * 100):.2f}%")
    logger.info(f"Standard sources: {len(standard_sources)}")
    logger.info(f"Judge sources: {len(judge_sources)}")
    
    # Print answers
    logger.info("\nStandard Answer:")
    logger.info(standard_answer[:500] + "..." if len(standard_answer) > 500 else standard_answer)
    
    logger.info("\nJudge-Enhanced Answer:")
    logger.info(judge_answer[:500] + "..." if len(judge_answer) > 500 else judge_answer)
    
    return result

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge single query test...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test document
        doc_path = await create_test_document()
        
        # Process document
        document, vector_store = await process_document(doc_path)
        
        # Run comparison test
        result = await run_comparison_test(vector_store)
        
        logger.info("Retrieval Judge single query test completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge single query test: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_timing_analysis.py
================
#!/usr/bin/env python3
"""
Test script to analyze the timing differences between standard retrieval and judge-enhanced retrieval.
This script:
1. Creates a test document
2. Runs multiple queries with detailed timing measurements
3. Analyzes the timing differences between components
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_timing_analysis")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test queries
TEST_QUERIES = [
    "What is the architecture of Metis RAG?",
    "What are the components of the RAG engine?",
    "How does the vector store work?",
    "What is the role of the LLM integration component?",
    "How does the document processing pipeline work?"
]

class TimingTracker:
    """Helper class to track timing of operations"""
    def __init__(self):
        self.timings = {}
        self.current_operation = None
        self.start_time = None
    
    def start(self, operation):
        """Start timing an operation"""
        self.current_operation = operation
        self.start_time = time.time()
        logger.info(f"Starting operation: {operation}")
    
    def stop(self):
        """Stop timing the current operation"""
        if self.current_operation and self.start_time:
            elapsed = time.time() - self.start_time
            self.timings[self.current_operation] = elapsed
            logger.info(f"Completed operation: {self.current_operation} in {elapsed:.2f}s")
            self.current_operation = None
            self.start_time = None
    
    def get_summary(self):
        """Get a summary of all timings"""
        return self.timings

async def create_test_document():
    """Create a test document for RAG testing"""
    logger.info("Creating test document...")
    
    # Create directory for test document
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create document file
    doc_path = os.path.join(test_dir, "timing_test_document.md")
    with open(doc_path, "w") as f:
        f.write(TEST_DOCUMENT)
    
    logger.info(f"Created test document at {os.path.abspath(doc_path)}")
    return doc_path

async def process_document(doc_path):
    """Process the test document and add it to the vector store"""
    logger.info("Processing test document...")
    
    # Read file content
    with open(doc_path, "r") as f:
        doc_content = f.read()
    
    # Create Document object
    document = Document(
        id=str(uuid.uuid4()),
        filename="timing_test_document.md",
        content=doc_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    # Create a vector store
    test_persist_dir = os.path.join("tests", "retrieval_judge", "timing_test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = VectorStore(persist_directory=test_persist_dir)
    
    # Create chunks
    document.chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=doc_content,
            metadata={
                "index": 0,
                "source": doc_path,
                "document_id": document.id
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(document)
    
    logger.info(f"Added document {document.id} to vector store")
    
    return document, vector_store

class InstrumentedRetrievalJudge(RetrievalJudge):
    """Instrumented version of RetrievalJudge that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """Instrumented version of analyze_query"""
        start_time = time.time()
        result = await super().analyze_query(query)
        elapsed = time.time() - start_time
        self.timings["analyze_query"] = elapsed
        logger.info(f"analyze_query took {elapsed:.2f}s")
        return result
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Instrumented version of evaluate_chunks"""
        start_time = time.time()
        result = await super().evaluate_chunks(query, chunks)
        elapsed = time.time() - start_time
        self.timings["evaluate_chunks"] = elapsed
        logger.info(f"evaluate_chunks took {elapsed:.2f}s")
        return result
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Instrumented version of optimize_context"""
        start_time = time.time()
        result = await super().optimize_context(query, chunks)
        elapsed = time.time() - start_time
        self.timings["optimize_context"] = elapsed
        logger.info(f"optimize_context took {elapsed:.2f}s")
        return result

class InstrumentedRAGEngine(RAGEngine):
    """Instrumented version of RAGEngine that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def _enhanced_retrieval(self, *args, **kwargs):
        """Instrumented version of _enhanced_retrieval"""
        start_time = time.time()
        result = await super()._enhanced_retrieval(*args, **kwargs)
        elapsed = time.time() - start_time
        self.timings["enhanced_retrieval"] = elapsed
        logger.info(f"enhanced_retrieval took {elapsed:.2f}s")
        return result

async def run_timing_tests(vector_store):
    """Run timing tests for standard and judge-enhanced retrieval"""
    logger.info("Running timing tests...")
    
    # Create OllamaClient
    ollama_client = OllamaClient()
    
    # Create instrumented Retrieval Judge
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = InstrumentedRetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Create engines
    rag_engine_with_judge = InstrumentedRAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    rag_engine_standard = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    results = []
    
    # Run each test query twice - first with standard retrieval, then with judge
    for query in TEST_QUERIES:
        logger.info(f"\n=== Testing query: {query} ===")
        
        # Test with standard retrieval
        logger.info("Running with standard retrieval...")
        standard_tracker = TimingTracker()
        
        standard_tracker.start("standard_total")
        standard_response = await rag_engine_standard.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        standard_tracker.stop()
        
        # Test with judge-enhanced retrieval
        logger.info("Running with judge-enhanced retrieval...")
        judge_tracker = TimingTracker()
        
        judge_tracker.start("judge_total")
        judge_response = await rag_engine_with_judge.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        judge_tracker.stop()
        
        # Extract results
        standard_answer = standard_response.get("answer", "")
        standard_sources = standard_response.get("sources", [])
        
        judge_answer = judge_response.get("answer", "")
        judge_sources = judge_response.get("sources", [])
        
        # Log results
        logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_tracker.timings['standard_total']:.2f}s")
        logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_tracker.timings['judge_total']:.2f}s")
        
        # Collect judge component timings
        judge_component_timings = {}
        if hasattr(retrieval_judge, 'timings'):
            judge_component_timings = retrieval_judge.timings.copy()
        
        if hasattr(rag_engine_with_judge, 'timings'):
            judge_component_timings.update(rag_engine_with_judge.timings)
        
        # Clear timings for next run
        retrieval_judge.timings = {}
        rag_engine_with_judge.timings = {}
        
        # Store results
        results.append({
            "query": query,
            "standard": {
                "total_time": standard_tracker.timings['standard_total'],
                "sources_count": len(standard_sources)
            },
            "judge": {
                "total_time": judge_tracker.timings['judge_total'],
                "sources_count": len(judge_sources),
                "component_timings": judge_component_timings
            }
        })
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "timing_analysis_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Timing analysis results saved to {os.path.abspath(results_path)}")
    
    return results

async def analyze_timing_results(results):
    """Analyze the timing test results"""
    logger.info("\n=== TIMING ANALYSIS ===")
    
    # Calculate averages
    avg_standard_time = sum(r["standard"]["total_time"] for r in results) / len(results)
    avg_judge_time = sum(r["judge"]["total_time"] for r in results) / len(results)
    
    logger.info(f"Average standard retrieval time: {avg_standard_time:.2f}s")
    logger.info(f"Average judge-enhanced retrieval time: {avg_judge_time:.2f}s")
    logger.info(f"Time difference: {((avg_judge_time - avg_standard_time) / avg_standard_time * 100):.2f}%")
    
    # Analyze component timings
    component_times = {}
    for r in results:
        for component, time_value in r["judge"].get("component_timings", {}).items():
            if component not in component_times:
                component_times[component] = []
            component_times[component].append(time_value)
    
    logger.info("\nJudge component timing averages:")
    for component, times in component_times.items():
        avg_time = sum(times) / len(times)
        logger.info(f"  {component}: {avg_time:.2f}s")
    
    # Analyze first run vs. subsequent runs
    first_standard_time = results[0]["standard"]["total_time"]
    first_judge_time = results[0]["judge"]["total_time"]
    
    rest_standard_time = sum(r["standard"]["total_time"] for r in results[1:]) / (len(results) - 1) if len(results) > 1 else 0
    rest_judge_time = sum(r["judge"]["total_time"] for r in results[1:]) / (len(results) - 1) if len(results) > 1 else 0
    
    logger.info("\nFirst run vs. subsequent runs:")
    logger.info(f"  First standard run: {first_standard_time:.2f}s")
    logger.info(f"  Average of subsequent standard runs: {rest_standard_time:.2f}s")
    logger.info(f"  First judge run: {first_judge_time:.2f}s")
    logger.info(f"  Average of subsequent judge runs: {rest_judge_time:.2f}s")
    
    # Check for caching effects
    logger.info("\nPossible explanations for timing differences:")
    
    if first_standard_time > rest_standard_time and first_judge_time > rest_judge_time:
        logger.info("  - Warm-up effect: Both methods show faster performance after the first run")
    
    if avg_judge_time < avg_standard_time:
        logger.info("  - Caching effect: Judge-enhanced retrieval might benefit from caching in the vector store")
        logger.info("  - Model loading: The LLM might be loaded into memory during the first run")
        logger.info("  - Query optimization: The judge might be selecting more efficient retrieval parameters")
    else:
        logger.info("  - Additional processing: Judge operations add overhead as expected")
    
    # Save analysis to file
    analysis = {
        "avg_standard_time": avg_standard_time,
        "avg_judge_time": avg_judge_time,
        "time_difference_percent": ((avg_judge_time - avg_standard_time) / avg_standard_time * 100),
        "component_averages": {component: sum(times) / len(times) for component, times in component_times.items()},
        "first_run": {
            "standard": first_standard_time,
            "judge": first_judge_time
        },
        "subsequent_runs": {
            "standard": rest_standard_time,
            "judge": rest_judge_time
        }
    }
    
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    analysis_path = os.path.join(results_dir, "timing_analysis.json")
    
    with open(analysis_path, "w") as f:
        json.dump(analysis, f, indent=2)
    
    logger.info(f"Timing analysis saved to {os.path.abspath(analysis_path)}")
    
    return analysis

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge timing analysis...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test document
        doc_path = await create_test_document()
        
        # Process document
        document, vector_store = await process_document(doc_path)
        
        # Run timing tests
        results = await run_timing_tests(vector_store)
        
        # Analyze results
        analysis = await analyze_timing_results(results)
        
        logger.info("Retrieval Judge timing analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge timing analysis: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/unit/test_cache.py
================
import os
import time
import unittest
import shutil
from typing import Dict, Any, List

from app.cache.base import Cache
from app.cache.vector_search_cache import VectorSearchCache
from app.cache.document_cache import DocumentCache
from app.cache.llm_response_cache import LLMResponseCache
from app.cache.cache_manager import CacheManager

class TestCache(unittest.TestCase):
    """Test the base Cache class"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_cache_dir = "data/test_cache"
        os.makedirs(self.test_cache_dir, exist_ok=True)
        self.cache = Cache[str](
            name="test_cache",
            ttl=1,  # 1 second TTL for faster testing
            max_size=5,
            persist=True,
            persist_dir=self.test_cache_dir
        )
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_cache_dir):
            shutil.rmtree(self.test_cache_dir)
    
    def test_set_get(self):
        """Test setting and getting values"""
        # Set a value
        self.cache.set("key1", "value1")
        
        # Get the value
        value = self.cache.get("key1")
        self.assertEqual(value, "value1")
        
        # Get a non-existent key
        value = self.cache.get("non_existent")
        self.assertIsNone(value)
    
    def test_ttl_expiration(self):
        """Test TTL expiration"""
        # Set a value
        self.cache.set("key1", "value1")
        
        # Wait for TTL to expire
        time.sleep(1.1)
        
        # Get the value (should be None)
        value = self.cache.get("key1")
        self.assertIsNone(value)
    
    def test_max_size_pruning(self):
        """Test max size pruning"""
        # Set values up to max_size + 1
        for i in range(6):
            self.cache.set(f"key{i}", f"value{i}")
        
        # Check that the cache size is at most max_size
        self.assertLessEqual(len(self.cache.cache), self.cache.max_size)
        
        # Check that the oldest entries were removed
        self.assertIsNone(self.cache.get("key0"))
        self.assertIsNone(self.cache.get("key1"))
        
        # Check that the newest entries are still there
        self.assertEqual(self.cache.get("key5"), "value5")
    
    def test_clear(self):
        """Test clearing the cache"""
        # Set some values
        self.cache.set("key1", "value1")
        self.cache.set("key2", "value2")
        
        # Clear the cache
        self.cache.clear()
        
        # Check that the cache is empty
        self.assertEqual(len(self.cache.cache), 0)
        self.assertIsNone(self.cache.get("key1"))
        self.assertIsNone(self.cache.get("key2"))
    
    def test_delete(self):
        """Test deleting a value"""
        # Set some values
        self.cache.set("key1", "value1")
        self.cache.set("key2", "value2")
        
        # Delete a value
        result = self.cache.delete("key1")
        self.assertTrue(result)
        
        # Check that the value is gone
        self.assertIsNone(self.cache.get("key1"))
        self.assertEqual(self.cache.get("key2"), "value2")
        
        # Try to delete a non-existent key
        result = self.cache.delete("non_existent")
        self.assertFalse(result)
    
    def test_get_stats(self):
        """Test getting cache statistics"""
        # Set some values
        self.cache.set("key1", "value1")
        
        # Get a value (hit)
        self.cache.get("key1")
        
        # Get a non-existent value (miss)
        self.cache.get("non_existent")
        
        # Get stats
        stats = self.cache.get_stats()
        
        # Check stats
        self.assertEqual(stats["name"], "test_cache")
        self.assertEqual(stats["size"], 1)
        self.assertEqual(stats["max_size"], 5)
        self.assertEqual(stats["hits"], 1)
        self.assertEqual(stats["misses"], 1)
        self.assertEqual(stats["hit_ratio"], 0.5)
        self.assertEqual(stats["ttl_seconds"], 1)
        self.assertTrue(stats["persist"])
    
    def test_persistence(self):
        """Test cache persistence"""
        # Set a value
        self.cache.set("key1", "value1")
        
        # Create a new cache instance (should load from disk)
        new_cache = Cache[str](
            name="test_cache",
            ttl=1,
            max_size=5,
            persist=True,
            persist_dir=self.test_cache_dir
        )
        
        # Check that the value was loaded
        self.assertEqual(new_cache.get("key1"), "value1")


class TestVectorSearchCache(unittest.TestCase):
    """Test the VectorSearchCache class"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_cache_dir = "data/test_cache"
        os.makedirs(self.test_cache_dir, exist_ok=True)
        self.cache = VectorSearchCache(
            ttl=1,  # 1 second TTL for faster testing
            max_size=5,
            persist=True,
            persist_dir=self.test_cache_dir
        )
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_cache_dir):
            shutil.rmtree(self.test_cache_dir)
    
    def test_set_get_results(self):
        """Test setting and getting search results"""
        # Create test results
        results = [
            {
                "chunk_id": "chunk1",
                "content": "Test content 1",
                "metadata": {"document_id": "doc1"},
                "distance": 0.1
            },
            {
                "chunk_id": "chunk2",
                "content": "Test content 2",
                "metadata": {"document_id": "doc2"},
                "distance": 0.2
            }
        ]
        
        # Set results
        self.cache.set_results("test query", 2, results)
        
        # Get results
        cached_results = self.cache.get_results("test query", 2)
        self.assertEqual(cached_results, results)
        
        # Get results with different parameters
        cached_results = self.cache.get_results("test query", 3)
        self.assertIsNone(cached_results)
    
    def test_invalidate_by_document_id(self):
        """Test invalidating cache entries by document ID"""
        # Create test results
        results1 = [
            {
                "chunk_id": "chunk1",
                "content": "Test content 1",
                "metadata": {"document_id": "doc1"},
                "distance": 0.1
            },
            {
                "chunk_id": "chunk2",
                "content": "Test content 2",
                "metadata": {"document_id": "doc2"},
                "distance": 0.2
            }
        ]
        
        results2 = [
            {
                "chunk_id": "chunk3",
                "content": "Test content 3",
                "metadata": {"document_id": "doc1"},
                "distance": 0.3
            },
            {
                "chunk_id": "chunk4",
                "content": "Test content 4",
                "metadata": {"document_id": "doc3"},
                "distance": 0.4
            }
        ]
        
        # Set results
        self.cache.set_results("query1", 2, results1)
        self.cache.set_results("query2", 2, results2)
        
        # Invalidate by document ID
        count = self.cache.invalidate_by_document_id("doc1")
        self.assertEqual(count, 2)
        
        # Check that the entries were invalidated
        self.assertIsNone(self.cache.get_results("query1", 2))
        self.assertIsNone(self.cache.get_results("query2", 2))


class TestDocumentCache(unittest.TestCase):
    """Test the DocumentCache class"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_cache_dir = "data/test_cache"
        os.makedirs(self.test_cache_dir, exist_ok=True)
        self.cache = DocumentCache(
            ttl=1,  # 1 second TTL for faster testing
            max_size=5,
            persist=True,
            persist_dir=self.test_cache_dir
        )
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_cache_dir):
            shutil.rmtree(self.test_cache_dir)
    
    def test_set_get_document(self):
        """Test setting and getting documents"""
        # Create test document
        document = {
            "id": "doc1",
            "content": "Test document content",
            "metadata": {
                "filename": "test.txt",
                "tags": ["test", "document"],
                "folder": "/test"
            }
        }
        
        # Set document
        self.cache.set_document("doc1", document)
        
        # Get document
        cached_document = self.cache.get_document("doc1")
        self.assertEqual(cached_document, document)
        
        # Get document content
        content = self.cache.get_document_content("doc1")
        self.assertEqual(content, "Test document content")
        
        # Get document metadata
        metadata = self.cache.get_document_metadata("doc1")
        self.assertEqual(metadata, document["metadata"])
    
    def test_invalidate_document(self):
        """Test invalidating a document"""
        # Create test document
        document = {
            "id": "doc1",
            "content": "Test document content",
            "metadata": {
                "filename": "test.txt",
                "tags": ["test", "document"],
                "folder": "/test"
            }
        }
        
        # Set document
        self.cache.set_document("doc1", document)
        
        # Invalidate document
        result = self.cache.invalidate_document("doc1")
        self.assertTrue(result)
        
        # Check that the document is gone
        self.assertIsNone(self.cache.get_document("doc1"))
    
    def test_get_documents_by_tag(self):
        """Test getting documents by tag"""
        # Create test documents
        document1 = {
            "id": "doc1",
            "content": "Test document 1",
            "metadata": {
                "filename": "test1.txt",
                "tags": ["test", "document"],
                "folder": "/test"
            }
        }
        
        document2 = {
            "id": "doc2",
            "content": "Test document 2",
            "metadata": {
                "filename": "test2.txt",
                "tags": ["test", "important"],
                "folder": "/test"
            }
        }
        
        # Set documents
        self.cache.set_document("doc1", document1)
        self.cache.set_document("doc2", document2)
        
        # Get documents by tag
        documents = self.cache.get_documents_by_tag("test")
        self.assertEqual(len(documents), 2)
        
        documents = self.cache.get_documents_by_tag("important")
        self.assertEqual(len(documents), 1)
        self.assertEqual(documents[0]["id"], "doc2")
    
    def test_get_documents_by_folder(self):
        """Test getting documents by folder"""
        # Create test documents
        document1 = {
            "id": "doc1",
            "content": "Test document 1",
            "metadata": {
                "filename": "test1.txt",
                "tags": ["test"],
                "folder": "/test1"
            }
        }
        
        document2 = {
            "id": "doc2",
            "content": "Test document 2",
            "metadata": {
                "filename": "test2.txt",
                "tags": ["test"],
                "folder": "/test2"
            }
        }
        
        # Set documents
        self.cache.set_document("doc1", document1)
        self.cache.set_document("doc2", document2)
        
        # Get documents by folder
        documents = self.cache.get_documents_by_folder("/test1")
        self.assertEqual(len(documents), 1)
        self.assertEqual(documents[0]["id"], "doc1")
        
        documents = self.cache.get_documents_by_folder("/test2")
        self.assertEqual(len(documents), 1)
        self.assertEqual(documents[0]["id"], "doc2")


class TestLLMResponseCache(unittest.TestCase):
    """Test the LLMResponseCache class"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_cache_dir = "data/test_cache"
        os.makedirs(self.test_cache_dir, exist_ok=True)
        self.cache = LLMResponseCache(
            ttl=1,  # 1 second TTL for faster testing
            max_size=5,
            persist=True,
            persist_dir=self.test_cache_dir
        )
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_cache_dir):
            shutil.rmtree(self.test_cache_dir)
    
    def test_set_get_response(self):
        """Test setting and getting LLM responses"""
        # Create test response
        response = {
            "response": "This is a test response",
            "model": "test-model",
            "prompt": "Test prompt",
            "tokens": 10,
            "finish_reason": "stop"
        }
        
        # Set response
        self.cache.set_response(
            prompt="Test prompt",
            model="test-model",
            response=response,
            temperature=0.0
        )
        
        # Get response
        cached_response = self.cache.get_response(
            prompt="Test prompt",
            model="test-model",
            temperature=0.0
        )
        self.assertEqual(cached_response, response)
        
        # Get response with different parameters
        cached_response = self.cache.get_response(
            prompt="Test prompt",
            model="test-model",
            temperature=0.5
        )
        self.assertIsNone(cached_response)
    
    def test_invalidate_by_model(self):
        """Test invalidating cache entries by model"""
        # Create test responses
        response1 = {
            "response": "Response 1",
            "model": "model1",
            "prompt": "Prompt 1"
        }
        
        response2 = {
            "response": "Response 2",
            "model": "model2",
            "prompt": "Prompt 2"
        }
        
        response3 = {
            "response": "Response 3",
            "model": "model1",
            "prompt": "Prompt 3"
        }
        
        # Set responses
        self.cache.set_response("Prompt 1", "model1", response1)
        self.cache.set_response("Prompt 2", "model2", response2)
        self.cache.set_response("Prompt 3", "model1", response3)
        
        # Invalidate by model
        count = self.cache.invalidate_by_model("model1")
        self.assertEqual(count, 2)
        
        # Check that the entries were invalidated
        self.assertIsNone(self.cache.get_response("Prompt 1", "model1"))
        self.assertIsNone(self.cache.get_response("Prompt 3", "model1"))
        self.assertIsNotNone(self.cache.get_response("Prompt 2", "model2"))
    
    def test_should_cache_response(self):
        """Test should_cache_response method"""
        # Test with high temperature (should not cache)
        result = self.cache.should_cache_response(
            prompt="Test prompt",
            model="test-model",
            temperature=0.7,
            response={"response": "Test response"}
        )
        self.assertFalse(result)
        
        # Test with low temperature (should cache)
        result = self.cache.should_cache_response(
            prompt="Test prompt",
            model="test-model",
            temperature=0.2,
            response={"response": "Test response"}
        )
        self.assertTrue(result)
        
        # Test with short response (should not cache)
        result = self.cache.should_cache_response(
            prompt="Test prompt",
            model="test-model",
            temperature=0.2,
            response={"response": "Short"}
        )
        self.assertFalse(result)
        
        # Test with error response (should not cache)
        result = self.cache.should_cache_response(
            prompt="Test prompt",
            model="test-model",
            temperature=0.2,
            response={"response": "Test response", "error": "Some error"}
        )
        self.assertFalse(result)


class TestCacheManager(unittest.TestCase):
    """Test the CacheManager class"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_cache_dir = "data/test_cache"
        os.makedirs(self.test_cache_dir, exist_ok=True)
        self.cache_manager = CacheManager(
            cache_dir=self.test_cache_dir,
            enable_caching=True
        )
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_cache_dir):
            shutil.rmtree(self.test_cache_dir)
    
    def test_initialization(self):
        """Test cache manager initialization"""
        self.assertIsInstance(self.cache_manager.vector_search_cache, VectorSearchCache)
        self.assertIsInstance(self.cache_manager.document_cache, DocumentCache)
        self.assertIsInstance(self.cache_manager.llm_response_cache, LLMResponseCache)
    
    def test_clear_all_caches(self):
        """Test clearing all caches"""
        # Set some values in each cache
        self.cache_manager.vector_search_cache.set_results("query", 2, [{"chunk_id": "chunk1"}])
        self.cache_manager.document_cache.set_document("doc1", {"id": "doc1"})
        self.cache_manager.llm_response_cache.set_response("prompt", "model", {"response": "test"})
        
        # Clear all caches
        self.cache_manager.clear_all_caches()
        
        # Check that all caches are empty
        self.assertIsNone(self.cache_manager.vector_search_cache.get_results("query", 2))
        self.assertIsNone(self.cache_manager.document_cache.get_document("doc1"))
        self.assertIsNone(self.cache_manager.llm_response_cache.get_response("prompt", "model"))
    
    def test_get_all_cache_stats(self):
        """Test getting all cache stats"""
        # Get stats
        stats = self.cache_manager.get_all_cache_stats()
        
        # Check stats
        self.assertTrue(stats["caching_enabled"])
        self.assertIn("vector_search_cache", stats)
        self.assertIn("document_cache", stats)
        self.assertIn("llm_response_cache", stats)
    
    def test_invalidate_document(self):
        """Test invalidating a document in all caches"""
        # Set some values in each cache
        self.cache_manager.vector_search_cache.set_results(
            "query", 2, [{"chunk_id": "chunk1", "metadata": {"document_id": "doc1"}}]
        )
        self.cache_manager.document_cache.set_document("doc1", {"id": "doc1"})
        
        # Invalidate document
        self.cache_manager.invalidate_document("doc1")
        
        # Check that the document is invalidated in all caches
        self.assertIsNone(self.cache_manager.vector_search_cache.get_results("query", 2))
        self.assertIsNone(self.cache_manager.document_cache.get_document("doc1"))
    
    def test_disabled_caching(self):
        """Test with caching disabled"""
        # Create a cache manager with caching disabled
        disabled_manager = CacheManager(
            cache_dir=self.test_cache_dir,
            enable_caching=False
        )
        
        # Try to set and get values
        disabled_manager.vector_search_cache.set_results("query", 2, [{"chunk_id": "chunk1"}])
        result = disabled_manager.vector_search_cache.get_results("query", 2)
        
        # Check that caching is effectively disabled
        self.assertIsNone(result)
        
        # Check stats
        stats = disabled_manager.get_all_cache_stats()
        self.assertFalse(stats["caching_enabled"])


if __name__ == "__main__":
    unittest.main()

================
File: tests/unit/test_chunking_judge.py
================
"""
Unit tests for the Chunking Judge
"""
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
import json

from app.rag.agents.chunking_judge import ChunkingJudge
from app.models.document import Document

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "strategy": "markdown",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is a structured markdown document with clear headers."
        }
        """
    }
    return client

@pytest.fixture
def markdown_document():
    """Create a test markdown document"""
    return Document(
        id="test-id",
        filename="test.md",
        content="""# Header 1
This is content under header 1.

## Header 2
This is content under header 2.

### Header 3
This is content under header 3.
"""
    )

@pytest.fixture
def text_document():
    """Create a test text document"""
    return Document(
        id="test-id-2",
        filename="test.txt",
        content="""This is a plain text document.
It has multiple paragraphs but no clear structure.

This is the second paragraph.
It continues for a few lines.
"""
    )

@pytest.mark.asyncio
async def test_chunking_judge_markdown_analysis(mock_ollama_client, markdown_document):
    """Test that the Chunking Judge correctly analyzes a markdown document"""
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(markdown_document)
    
    # Verify result
    assert result["strategy"] == "markdown"
    assert result["parameters"]["chunk_size"] == 800
    assert result["parameters"]["chunk_overlap"] == 100
    assert "justification" in result
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "document analysis expert" in call_args["prompt"].lower()
    assert "test.md" in call_args["prompt"]
    assert "Header 1" in call_args["prompt"]
    assert "Header 2" in call_args["prompt"]

@pytest.mark.asyncio
async def test_chunking_judge_text_analysis(mock_ollama_client, text_document):
    """Test that the Chunking Judge correctly analyzes a text document"""
    # Override the mock response for text document
    mock_ollama_client.generate.return_value = {
        "response": """
        {
            "strategy": "recursive",
            "parameters": {
                "chunk_size": 1000,
                "chunk_overlap": 150
            },
            "justification": "This is a plain text document with paragraphs."
        }
        """
    }
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(text_document)
    
    # Verify result
    assert result["strategy"] == "recursive"
    assert result["parameters"]["chunk_size"] == 1000
    assert result["parameters"]["chunk_overlap"] == 150
    assert "justification" in result
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "document analysis expert" in call_args["prompt"].lower()
    assert "test.txt" in call_args["prompt"]
    assert "plain text document" in call_args["prompt"]

@pytest.mark.asyncio
async def test_chunking_judge_error_handling(mock_ollama_client, markdown_document):
    """Test that the Chunking Judge handles errors gracefully"""
    # Make the mock client return an invalid JSON response
    mock_ollama_client.generate.return_value = {
        "response": "This is not valid JSON"
    }
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(markdown_document)
    
    # Verify fallback to default values
    assert result["strategy"] == "recursive"
    assert result["parameters"]["chunk_size"] == 500
    assert result["parameters"]["chunk_overlap"] == 50
    assert "Failed to parse" in result["justification"]

@pytest.mark.asyncio
async def test_chunking_judge_invalid_strategy(mock_ollama_client, markdown_document):
    """Test that the Chunking Judge handles invalid strategies gracefully"""
    # Make the mock client return an invalid strategy
    mock_ollama_client.generate.return_value = {
        "response": """
        {
            "strategy": "invalid_strategy",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is an invalid strategy."
        }
        """
    }
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(markdown_document)
    
    # Verify fallback to recursive strategy
    assert result["strategy"] == "recursive"
    assert result["parameters"]["chunk_size"] == 800
    assert result["parameters"]["chunk_overlap"] == 100

@pytest.mark.asyncio
async def test_extract_representative_sample_markdown():
    """Test the _extract_representative_sample method with markdown content"""
    judge = ChunkingJudge(ollama_client=AsyncMock())
    
    # Create a long markdown document
    long_content = "# Header 1\n\n" + "Content under header 1.\n" * 100
    long_content += "\n## Header 2\n\n" + "Content under header 2.\n" * 100
    long_content += "\n### Header 3\n\n" + "Content under header 3.\n" * 100
    
    # Extract sample
    sample = judge._extract_representative_sample(long_content, "test.md", max_length=1000)
    
    # Verify sample contains headers and some content
    assert "# Header 1" in sample
    assert "## Header 2" in sample
    assert "### Header 3" in sample
    assert "Content under header" in sample
    assert len(sample) <= 1000 + 100  # Allow for some buffer

@pytest.mark.asyncio
async def test_extract_representative_sample_text():
    """Test the _extract_representative_sample method with text content"""
    judge = ChunkingJudge(ollama_client=AsyncMock())
    
    # Create a long text document
    long_content = "This is the beginning of the document.\n" * 50
    long_content += "This is the middle of the document.\n" * 50
    long_content += "This is the end of the document.\n" * 50
    
    # Extract sample
    sample = judge._extract_representative_sample(long_content, "test.txt", max_length=1000)
    
    # Verify sample contains beginning, middle, and end
    assert "beginning of the document" in sample
    assert "middle of the document" in sample
    assert "end of the document" in sample
    assert len(sample) <= 1000 + 100  # Allow for some buffer

================
File: tests/unit/test_connection_manager.py
================
"""
Unit tests for the database connection manager
"""
import os
import pytest
import asyncio
import tempfile
from pathlib import Path

from app.db.connection_manager import connection_manager

# Skip PostgreSQL tests if no connection string is provided
POSTGRES_TEST_URL = os.environ.get("TEST_POSTGRES_URL")
SKIP_POSTGRES = POSTGRES_TEST_URL is None

@pytest.fixture
async def sqlite_temp_db():
    """Create a temporary SQLite database for testing"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.db')
    os.close(fd)
    
    # Register the connection
    conn_id = connection_manager.register_connection(path)
    
    # Return the connection ID and path
    yield conn_id, path
    
    # Clean up
    await connection_manager.close(conn_id)
    if os.path.exists(path):
        os.unlink(path)

@pytest.mark.asyncio
async def test_sqlite_connection_registration():
    """Test SQLite connection registration"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.db')
    os.close(fd)
    
    try:
        # Register the connection
        conn_id = connection_manager.register_connection(path)
        
        # Verify connection type
        assert connection_manager.get_connection_type(conn_id) == 'sqlite'
        
        # Verify connection string
        assert connection_manager.get_connection_string(conn_id) == path
    finally:
        # Clean up
        await connection_manager.close(conn_id)
        if os.path.exists(path):
            os.unlink(path)

@pytest.mark.asyncio
async def test_sqlite_connection_acquisition(sqlite_temp_db):
    """Test SQLite connection acquisition"""
    conn_id, path = sqlite_temp_db
    
    # Get a connection
    conn = await connection_manager.get_sqlite_connection(conn_id)
    
    # Verify it's a valid connection
    assert conn is not None
    
    # Create a table and insert data
    await conn.execute("CREATE TABLE test (id INTEGER PRIMARY KEY, name TEXT)")
    await conn.execute("INSERT INTO test (name) VALUES (?)", ("test_value",))
    await conn.commit()
    
    # Query the data
    cursor = await conn.execute("SELECT name FROM test WHERE id = 1")
    row = await cursor.fetchone()
    await cursor.close()
    
    # Verify the data
    assert row is not None
    assert row["name"] == "test_value"

@pytest.mark.asyncio
async def test_sqlite_connection_reuse(sqlite_temp_db):
    """Test SQLite connection reuse"""
    conn_id, path = sqlite_temp_db
    
    # Get a connection
    conn1 = await connection_manager.get_sqlite_connection(conn_id)
    
    # Get another connection with the same ID
    conn2 = await connection_manager.get_sqlite_connection(conn_id)
    
    # Verify they're the same connection
    assert conn1 is conn2

@pytest.mark.asyncio
async def test_connection_manager_close(sqlite_temp_db):
    """Test connection manager close method"""
    conn_id, path = sqlite_temp_db
    
    # Get a connection
    conn = await connection_manager.get_sqlite_connection(conn_id)
    
    # Close the connection
    await connection_manager.close(conn_id)
    
    # Verify the connection is closed
    assert conn_id not in connection_manager._pools

@pytest.mark.asyncio
@pytest.mark.skipif(SKIP_POSTGRES, reason="PostgreSQL connection string not provided")
async def test_postgres_connection_registration():
    """Test PostgreSQL connection registration"""
    # Skip if no PostgreSQL connection string
    if SKIP_POSTGRES:
        return
    
    # Register the connection
    conn_id = connection_manager.register_connection(POSTGRES_TEST_URL)
    
    try:
        # Verify connection type
        assert connection_manager.get_connection_type(conn_id) == 'postgres'
        
        # Verify connection string
        assert connection_manager.get_connection_string(conn_id) == POSTGRES_TEST_URL
    finally:
        # Clean up
        await connection_manager.close(conn_id)

@pytest.mark.asyncio
@pytest.mark.skipif(SKIP_POSTGRES, reason="PostgreSQL connection string not provided")
async def test_postgres_connection_acquisition():
    """Test PostgreSQL connection acquisition"""
    # Skip if no PostgreSQL connection string
    if SKIP_POSTGRES:
        return
    
    # Register the connection
    conn_id = connection_manager.register_connection(POSTGRES_TEST_URL)
    
    try:
        # Get a connection
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        # Verify it's a valid connection
        assert conn is not None
        
        # Execute a simple query
        result = await conn.fetchval("SELECT 1")
        
        # Verify the result
        assert result == 1
    finally:
        # Clean up
        await connection_manager.close(conn_id)

@pytest.mark.asyncio
@pytest.mark.skipif(SKIP_POSTGRES, reason="PostgreSQL connection string not provided")
async def test_postgres_connection_release():
    """Test PostgreSQL connection release"""
    # Skip if no PostgreSQL connection string
    if SKIP_POSTGRES:
        return
    
    # Register the connection
    conn_id = connection_manager.register_connection(POSTGRES_TEST_URL)
    
    try:
        # Get a connection
        conn = await connection_manager.get_postgres_connection(conn_id)
        
        # Release the connection
        await connection_manager.release_postgres_connection(conn_id, conn)
        
        # Verify we can get another connection
        conn2 = await connection_manager.get_postgres_connection(conn_id)
        assert conn2 is not None
    finally:
        # Clean up
        await connection_manager.close(conn_id)

@pytest.mark.asyncio
async def test_generic_get_connection(sqlite_temp_db):
    """Test generic get_connection method"""
    conn_id, path = sqlite_temp_db
    
    # Get a connection using the generic method
    conn = await connection_manager.get_connection(conn_id)
    
    # Verify it's a valid connection
    assert conn is not None
    
    # Create a table and insert data
    await conn.execute("CREATE TABLE test_generic (id INTEGER PRIMARY KEY, name TEXT)")
    await conn.execute("INSERT INTO test_generic (name) VALUES (?)", ("generic_test",))
    await conn.commit()
    
    # Query the data
    cursor = await conn.execute("SELECT name FROM test_generic WHERE id = 1")
    row = await cursor.fetchone()
    await cursor.close()
    
    # Verify the data
    assert row is not None
    assert row["name"] == "generic_test"

================
File: tests/unit/test_csv_json_handler.py
================
"""
Unit tests for the CSV and JSON handlers
"""
import os
import pytest
import pytest_asyncio
import asyncio
import tempfile
import json
import csv
from pathlib import Path

from app.rag.tools.csv_json_handler import AsyncCSVHandler, AsyncJSONHandler

class TestCSVJSONHandler:
    """Test class for CSV and JSON handlers"""
    
    @pytest_asyncio.fixture
    async def temp_csv_file(self):
        """Create a temporary CSV file for testing"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix='.csv')
        os.close(fd)
        
        # Create test data
        data = [
            ['id', 'name', 'age', 'email'],
            ['1', 'John Doe', '35', 'john@example.com'],
            ['2', 'Jane Smith', '28', 'jane@example.com'],
            ['3', 'Bob Johnson', '42', 'bob@example.com'],
            ['4', 'Alice Brown', '31', 'alice@example.com'],
            ['5', 'Charlie Davis', '45', 'charlie@example.com']
        ]
        
        # Write to CSV
        with open(path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(data)
        
        # Return the path
        yield path
        
        # Clean up
        if os.path.exists(path):
            os.unlink(path)
    
    @pytest_asyncio.fixture
    async def temp_json_file(self):
        """Create a temporary JSON file for testing"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix='.json')
        os.close(fd)
        
        # Create test data
        data = [
            {'id': 1, 'name': 'John Doe', 'age': 35, 'email': 'john@example.com'},
            {'id': 2, 'name': 'Jane Smith', 'age': 28, 'email': 'jane@example.com'},
            {'id': 3, 'name': 'Bob Johnson', 'age': 42, 'email': 'bob@example.com'},
            {'id': 4, 'name': 'Alice Brown', 'age': 31, 'email': 'alice@example.com'},
            {'id': 5, 'name': 'Charlie Davis', 'age': 45, 'email': 'charlie@example.com'}
        ]
        
        # Write to JSON
        with open(path, 'w') as f:
            json.dump(data, f)
        
        # Return the path
        yield path
        
        # Clean up
        if os.path.exists(path):
            os.unlink(path)
    
    @pytest_asyncio.fixture
    async def temp_nested_json_file(self):
        """Create a temporary nested JSON file for testing"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix='.json')
        os.close(fd)
        
        # Create test data with nested structure
        data = {
            'users': [
                {'id': 1, 'name': 'John Doe', 'age': 35, 'email': 'john@example.com'},
                {'id': 2, 'name': 'Jane Smith', 'age': 28, 'email': 'jane@example.com'},
                {'id': 3, 'name': 'Bob Johnson', 'age': 42, 'email': 'bob@example.com'}
            ],
            'metadata': {
                'version': '1.0',
                'generated_at': '2025-03-31T12:00:00Z',
                'record_count': 3
            }
        }
        
        # Write to JSON
        with open(path, 'w') as f:
            json.dump(data, f)
        
        # Return the path
        yield path
        
        # Clean up
        if os.path.exists(path):
            os.unlink(path)
    
    @pytest.mark.asyncio
    async def test_csv_read(self, temp_csv_file):
        """Test reading a CSV file"""
        # Read the CSV file
        data, headers = await AsyncCSVHandler.read_csv(temp_csv_file)
        
        # Check headers
        assert headers == ['id', 'name', 'age', 'email']
        
        # Check data
        assert len(data) == 5
        assert data[0]['id'] == '1'
        assert data[0]['name'] == 'John Doe'
        assert data[0]['age'] == '35'
        assert data[0]['email'] == 'john@example.com'
    
    @pytest.mark.asyncio
    async def test_csv_to_sqlite(self, temp_csv_file):
        """Test creating a SQLite table from a CSV file"""
        import aiosqlite
        
        # Create an in-memory SQLite database
        async with aiosqlite.connect(':memory:') as conn:
            # Create a table from the CSV file
            table_name = 'test_csv'
            headers, rows_inserted = await AsyncCSVHandler.create_table_from_csv(conn, table_name, temp_csv_file)
            
            # Check results
            assert headers == ['id', 'name', 'age', 'email']
            assert rows_inserted == 5
            
            # Query the table
            cursor = await conn.execute(f'SELECT * FROM {table_name}')
            rows = await cursor.fetchall()
            
            # Check results
            assert len(rows) == 5
            
            # Check column types
            cursor = await conn.execute(f'PRAGMA table_info({table_name})')
            columns = await cursor.fetchall()
            
            # Check that numeric columns are detected correctly
            column_types = {col[1]: col[2] for col in columns}
            assert column_types['id'] in ('INTEGER', 'TEXT')  # Could be either depending on detection
            assert column_types['name'] == 'TEXT'
            assert column_types['age'] in ('INTEGER', 'TEXT')  # Could be either depending on detection
            assert column_types['email'] == 'TEXT'
    
    @pytest.mark.asyncio
    async def test_json_read(self, temp_json_file):
        """Test reading a JSON file"""
        # Read the JSON file
        data = await AsyncJSONHandler.read_json(temp_json_file)
        
        # Check data
        assert len(data) == 5
        assert data[0]['id'] == 1
        assert data[0]['name'] == 'John Doe'
        assert data[0]['age'] == 35
        assert data[0]['email'] == 'john@example.com'
    
    @pytest.mark.asyncio
    async def test_json_to_sqlite(self, temp_json_file):
        """Test creating a SQLite table from a JSON file"""
        import aiosqlite
        
        # Create an in-memory SQLite database
        async with aiosqlite.connect(':memory:') as conn:
            # Create a table from the JSON file
            table_name = 'test_json'
            headers, rows_inserted = await AsyncJSONHandler.create_table_from_json(conn, table_name, temp_json_file)
            
            # Check results
            assert set(headers) == {'id', 'name', 'age', 'email'}
            assert rows_inserted == 5
            
            # Query the table
            cursor = await conn.execute(f'SELECT * FROM {table_name}')
            rows = await cursor.fetchall()
            
            # Check results
            assert len(rows) == 5
            
            # Check column types
            cursor = await conn.execute(f'PRAGMA table_info({table_name})')
            columns = await cursor.fetchall()
            
            # Check that numeric columns are detected correctly
            column_types = {col[1]: col[2] for col in columns}
            assert column_types['id'] == 'INTEGER'
            assert column_types['name'] == 'TEXT'
            assert column_types['age'] == 'INTEGER'
            assert column_types['email'] == 'TEXT'
    
    @pytest.mark.asyncio
    async def test_nested_json_to_sqlite(self, temp_nested_json_file):
        """Test creating a SQLite table from a nested JSON file"""
        import aiosqlite
        
        # Create an in-memory SQLite database
        async with aiosqlite.connect(':memory:') as conn:
            # Create a table from the JSON file
            table_name = 'test_nested_json'
            headers, rows_inserted = await AsyncJSONHandler.create_table_from_json(conn, table_name, temp_nested_json_file)
            
            # Check results - should have flattened the nested structure
            assert rows_inserted > 0
            
            # Query the table
            cursor = await conn.execute(f'SELECT * FROM {table_name}')
            rows = await cursor.fetchall()
            
            # Check that we have some rows
            assert len(rows) > 0

================
File: tests/unit/test_database_tool_async_concurrent.py
================
"""
Unit tests for the async DatabaseTool under concurrent load
"""
import os
import pytest
import pytest_asyncio
import asyncio
import tempfile
import json
import time
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import uuid
import random

# Import the connection manager directly
from app.db.connection_manager import connection_manager
from app.rag.tools.database_tool_async import DatabaseTool

class TestDatabaseToolAsyncConcurrent:
    """Test class for async DatabaseTool under concurrent load"""
    
    @pytest_asyncio.fixture
    async def sqlite_temp_db(self):
        """Create a temporary SQLite database for testing"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix='.db')
        os.close(fd)
        
        # Create a test table and insert data
        import aiosqlite
        async with aiosqlite.connect(path) as conn:
            await conn.execute("""
                CREATE TABLE test_users (
                    id INTEGER PRIMARY KEY,
                    name TEXT,
                    age INTEGER,
                    email TEXT
                )
            """)
            
            # Insert 1000 test records
            for i in range(1000):
                await conn.execute(
                    "INSERT INTO test_users (name, age, email) VALUES (?, ?, ?)",
                    (f"User {i}", random.randint(18, 80), f"user{i}@example.com")
                )
            
            await conn.commit()
        
        # Return the path
        yield path
        
        # Clean up
        if os.path.exists(path):
            os.unlink(path)
    
    @pytest_asyncio.fixture
    async def temp_csv_file(self):
        """Create a temporary CSV file for testing"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix='.csv')
        os.close(fd)
        
        # Create test data with 1000 records
        data = {
            'product_id': list(range(1, 1001)),
            'product_name': [f"Product {i}" for i in range(1, 1001)],
            'price': [round(random.uniform(10, 1000), 2) for _ in range(1000)],
            'category': random.choices(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], k=1000)
        }
        df = pd.DataFrame(data)
        
        # Write to CSV
        df.to_csv(path, index=False)
        
        # Return the path
        yield path
        
        # Clean up
        if os.path.exists(path):
            os.unlink(path)
    
    @pytest_asyncio.fixture
    async def temp_json_file(self):
        """Create a temporary JSON file for testing"""
        # Create a temporary file
        fd, path = tempfile.mkstemp(suffix='.json')
        os.close(fd)
        
        # Create test data with 1000 records
        data = []
        for i in range(1000):
            data.append({
                'id': i,
                'name': f"Item {i}",
                'value': random.randint(1, 1000),
                'tags': random.sample(['tag1', 'tag2', 'tag3', 'tag4', 'tag5'], k=random.randint(1, 3))
            })
        
        # Write to JSON
        with open(path, 'w') as f:
            json.dump(data, f)
        
        # Return the path
        yield path
        
        # Clean up
        if os.path.exists(path):
            os.unlink(path)
    
    @pytest_asyncio.fixture
    async def database_tool(self):
        """Create a DatabaseTool instance for testing"""
        tool = DatabaseTool()
        yield tool
        # Ensure all connections are closed
        for conn_id in list(tool.connection_manager._pools.keys()):
            try:
                await tool.connection_manager.close(conn_id)
            except Exception:
                pass
    
    async def run_concurrent_queries(self, database_tool, source, queries, num_concurrent=10):
        """Run multiple queries concurrently and measure performance"""
        start_time = time.time()
        
        async def run_query(query, params=None):
            try:
                result = await database_tool.execute({
                    'query': query,
                    'source': source,
                    'params': params
                })
                return result
            except Exception as e:
                print(f"Error executing query: {str(e)}")
                return {"error": f"Error executing query: {str(e)}"}
        
        # Create tasks for all queries
        tasks = [run_query(query['query'], query.get('params')) for query in queries]
        
        # Run queries in batches of num_concurrent to avoid overwhelming the connection pool
        results = []
        for i in range(0, len(tasks), num_concurrent):
            batch = tasks[i:i+num_concurrent]
            batch_results = await asyncio.gather(*batch, return_exceptions=False)
            results.extend(batch_results)
            
            # Add a small delay between batches to allow connections to be properly released
            if i + num_concurrent < len(tasks):
                await asyncio.sleep(0.1)
        
        end_time = time.time()
        
        return {
            'results': results,
            'total_time': end_time - start_time,
            'avg_time_per_query': (end_time - start_time) / len(queries),
            'queries_per_second': len(queries) / (end_time - start_time)
        }
    
    @pytest.mark.asyncio
    async def test_sqlite_concurrent_queries(self, database_tool, sqlite_temp_db):
        """Test concurrent queries on SQLite database"""
        print(f"\nTesting concurrent SQLite queries...")
        
        # Create a list of 100 different queries
        queries = []
        for i in range(100):
            min_age = random.randint(18, 60)
            queries.append({
                'query': f"SELECT * FROM test_users WHERE age > :min_age LIMIT 10",
                'params': {'min_age': min_age}
            })
        
        # Test with different concurrency levels
        concurrency_levels = [1, 5, 10, 20, 50]
        results = {}
        
        for concurrency in concurrency_levels:
            print(f"  Testing with concurrency level: {concurrency}")
            result = await self.run_concurrent_queries(
                database_tool, 
                sqlite_temp_db, 
                queries, 
                concurrency
            )
            results[concurrency] = result
            print(f"    Completed {len(queries)} queries in {result['total_time']:.2f}s")
            print(f"    Average time per query: {result['avg_time_per_query'] * 1000:.2f}ms")
            print(f"    Queries per second: {result['queries_per_second']:.2f}")
        
        # Verify results
        for concurrency, result in results.items():
            # Check that all queries returned results
            assert len(result['results']) == len(queries)
            
            # Check that most results have data (allow some errors)
            successful_results = [r for r in result['results'] if 'error' not in r]
            assert len(successful_results) >= len(result['results']) * 0.9, "Too many failed queries"
            
            for query_result in successful_results:
                assert 'results' in query_result
                assert 'row_count' in query_result
                assert query_result['row_count'] > 0
        
        # Check that higher concurrency levels generally improve throughput
        # (up to a point, as SQLite has limitations)
        throughputs = [results[c]['queries_per_second'] for c in concurrency_levels]
        print(f"\n  Throughput by concurrency level:")
        for i, c in enumerate(concurrency_levels):
            print(f"    Concurrency {c}: {throughputs[i]:.2f} queries/second")
        
        # The throughput should not collapse under load
        # Note: SQLite may not scale linearly due to its design
        assert throughputs[-1] >= throughputs[0] * 0.5, "Throughput collapsed under load"
    
    @pytest.mark.asyncio
    async def test_csv_concurrent_queries(self, database_tool, temp_csv_file):
        """Test concurrent queries on CSV file"""
        print(f"\nTesting concurrent CSV queries...")
        
        # Create a list of 50 different queries
        queries = []
        for i in range(50):
            min_price = random.randint(100, 800)
            queries.append({
                'query': f"SELECT * FROM data WHERE price > {min_price} LIMIT 10",
            })
        
        # Test with different concurrency levels
        concurrency_levels = [1, 5, 10, 20]
        results = {}
        
        for concurrency in concurrency_levels:
            print(f"  Testing with concurrency level: {concurrency}")
            result = await self.run_concurrent_queries(
                database_tool, 
                temp_csv_file, 
                queries, 
                concurrency
            )
            results[concurrency] = result
            print(f"    Completed {len(queries)} queries in {result['total_time']:.2f}s")
            print(f"    Average time per query: {result['avg_time_per_query'] * 1000:.2f}ms")
            print(f"    Queries per second: {result['queries_per_second']:.2f}")
        
        # Verify results
        for concurrency, result in results.items():
            # Check that all queries returned results
            assert len(result['results']) == len(queries)
            
            # Check that most results have data (allow some errors)
            successful_results = [r for r in result['results'] if 'error' not in r]
            assert len(successful_results) >= len(result['results']) * 0.8, "Too many failed queries"
            
            for query_result in successful_results:
                assert 'results' in query_result
                assert 'row_count' in query_result
    
    @pytest.mark.asyncio
    async def test_json_concurrent_queries(self, database_tool, temp_json_file):
        """Test concurrent queries on JSON file"""
        print(f"\nTesting concurrent JSON queries...")
        
        # Create a list of 50 different queries
        queries = []
        for i in range(50):
            min_value = random.randint(100, 800)
            queries.append({
                'query': f"SELECT * FROM data WHERE value > {min_value} LIMIT 10",
            })
        
        # Test with different concurrency levels
        concurrency_levels = [1, 5, 10, 20]
        results = {}
        
        for concurrency in concurrency_levels:
            print(f"  Testing with concurrency level: {concurrency}")
            result = await self.run_concurrent_queries(
                database_tool, 
                temp_json_file, 
                queries, 
                concurrency
            )
            results[concurrency] = result
            print(f"    Completed {len(queries)} queries in {result['total_time']:.2f}s")
            print(f"    Average time per query: {result['avg_time_per_query'] * 1000:.2f}ms")
            print(f"    Queries per second: {result['queries_per_second']:.2f}")
        
        # Verify results
        for concurrency, result in results.items():
            # Check that all queries returned results
            assert len(result['results']) == len(queries)
            
            # Check that most results have data (allow some errors)
            successful_results = [r for r in result['results'] if 'error' not in r]
            assert len(successful_results) >= len(result['results']) * 0.8, "Too many failed queries"
            
            for query_result in successful_results:
                assert 'results' in query_result
                assert 'row_count' in query_result
    
    @pytest.mark.asyncio
    async def test_mixed_source_concurrent_queries(self, database_tool, sqlite_temp_db, temp_csv_file, temp_json_file):
        """Test concurrent queries on mixed data sources"""
        print(f"\nTesting concurrent queries on mixed data sources...")
        
        # Create a list of queries for different data sources
        queries = []
        
        # SQLite queries
        for i in range(20):
            min_age = random.randint(18, 60)
            queries.append({
                'source': sqlite_temp_db,
                'query': f"SELECT * FROM test_users WHERE age > :min_age LIMIT 5",
                'params': {'min_age': min_age}
            })
        
        # CSV queries
        for i in range(15):
            min_price = random.randint(100, 800)
            queries.append({
                'source': temp_csv_file,
                'query': f"SELECT * FROM data WHERE price > {min_price} LIMIT 5",
            })
        
        # JSON queries
        for i in range(15):
            min_value = random.randint(100, 800)
            queries.append({
                'source': temp_json_file,
                'query': f"SELECT * FROM data WHERE value > {min_value} LIMIT 5",
            })
        
        # Shuffle the queries to mix data sources
        random.shuffle(queries)
        
        # Run mixed queries concurrently
        concurrency = 20
        print(f"  Testing with concurrency level: {concurrency}")
        
        async def run_query(query):
            result = await database_tool.execute({
                'query': query['query'],
                'source': query['source'],
                'params': query.get('params')
            })
            return result
        
        # Create tasks for all queries
        tasks = [run_query(query) for query in queries]
        
        # Run queries concurrently
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time = total_time / len(queries)
        qps = len(queries) / total_time
        
        print(f"    Completed {len(queries)} mixed queries in {total_time:.2f}s")
        print(f"    Average time per query: {avg_time * 1000:.2f}ms")
        print(f"    Queries per second: {qps:.2f}")
        
        # Verify results
        assert len(results) == len(queries)
        
        # Check that most results have data (allow some errors)
        successful_results = [r for r in results if 'error' not in r]
        assert len(successful_results) >= len(results) * 0.8, "Too many failed queries"
        
        for result in successful_results:
            assert 'results' in result
            assert 'row_count' in result
    
    @pytest.mark.asyncio
    async def test_connection_pool_under_load(self, database_tool, sqlite_temp_db):
        """Test connection pool behavior under load"""
        print(f"\nTesting connection pool behavior under load...")
        
        # Register the connection with the connection manager
        conn_id = connection_manager.register_connection(sqlite_temp_db)
        
        # Function to get and release a connection
        async def get_and_release_connection():
            conn = await connection_manager.get_sqlite_connection(conn_id)
            # Simulate some work
            cursor = await conn.execute("SELECT COUNT(*) FROM test_users")
            row = await cursor.fetchone()
            await cursor.close()
            # No explicit release for SQLite connections
            return row[0]
        
        # Run many connection operations concurrently
        concurrency = 50
        tasks = [get_and_release_connection() for _ in range(concurrency)]
        
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        total_time = end_time - start_time
        
        print(f"    Completed {concurrency} concurrent connection operations in {total_time:.2f}s")
        print(f"    All operations returned correct count: {all(r == 1000 for r in results)}")
        
        # Verify all operations returned the correct count
        assert all(r == 1000 for r in results)
        
        # Clean up
        await connection_manager.close(conn_id)

================
File: tests/unit/test_database_tool_async.py
================
"""
Unit tests for the async DatabaseTool
"""
import os
import pytest
import asyncio
import tempfile
import json
import pandas as pd
from pathlib import Path

from app.rag.tools.database_tool_async import DatabaseTool

# Skip PostgreSQL tests if no connection string is provided
POSTGRES_TEST_URL = os.environ.get("TEST_POSTGRES_URL")
SKIP_POSTGRES = POSTGRES_TEST_URL is None

@pytest.fixture
def database_tool():
    """Create a DatabaseTool instance for testing"""
    return DatabaseTool()

@pytest.fixture
async def sqlite_temp_db():
    """Create a temporary SQLite database for testing"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.db')
    os.close(fd)
    
    # Create a test table and insert data
    import aiosqlite
    async with aiosqlite.connect(path) as conn:
        await conn.execute("""
            CREATE TABLE test_users (
                id INTEGER PRIMARY KEY,
                name TEXT,
                age INTEGER,
                email TEXT
            )
        """)
        await conn.execute("""
            INSERT INTO test_users (name, age, email) VALUES
            ('John Doe', 35, 'john@example.com'),
            ('Jane Smith', 28, 'jane@example.com'),
            ('Bob Johnson', 42, 'bob@example.com')
        """)
        await conn.commit()
    
    # Return the path
    yield path
    
    # Clean up
    if os.path.exists(path):
        os.unlink(path)

@pytest.fixture
async def temp_csv_file():
    """Create a temporary CSV file for testing"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.csv')
    os.close(fd)
    
    # Create test data
    data = {
        'product_id': [1, 2, 3, 4],
        'product_name': ['Smartphone', 'Laptop', 'Headphones', 'Tablet'],
        'price': [699.99, 1299.99, 149.99, 499.99],
        'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics']
    }
    df = pd.DataFrame(data)
    
    # Write to CSV
    df.to_csv(path, index=False)
    
    # Return the path
    yield path
    
    # Clean up
    if os.path.exists(path):
        os.unlink(path)

@pytest.fixture
async def temp_json_file():
    """Create a temporary JSON file for testing"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.json')
    os.close(fd)
    
    # Create test data
    data = [
        {'city': 'New York', 'population': 8336817, 'country': 'USA'},
        {'city': 'Los Angeles', 'population': 3979576, 'country': 'USA'},
        {'city': 'Chicago', 'population': 2693976, 'country': 'USA'},
        {'city': 'Houston', 'population': 2320268, 'country': 'USA'},
        {'city': 'Phoenix', 'population': 1680992, 'country': 'USA'}
    ]
    
    # Write to JSON
    with open(path, 'w') as f:
        json.dump(data, f)
    
    # Return the path
    yield path
    
    # Clean up
    if os.path.exists(path):
        os.unlink(path)

@pytest.mark.asyncio
async def test_sqlite_query(database_tool, sqlite_temp_db):
    """Test querying a SQLite database"""
    # Execute a query
    result = await database_tool.execute({
        'query': 'SELECT * FROM test_users WHERE age > 30',
        'source': sqlite_temp_db
    })
    
    # Check the result
    assert 'error' not in result
    assert 'results' in result
    assert 'columns' in result
    assert 'row_count' in result
    assert 'execution_time' in result
    
    # Check the data
    assert result['row_count'] == 2
    assert len(result['results']) == 2
    assert result['columns'] == ['id', 'name', 'age', 'email']
    
    # Check specific values
    assert any(row['name'] == 'John Doe' for row in result['results'])
    assert any(row['name'] == 'Bob Johnson' for row in result['results'])

@pytest.mark.asyncio
async def test_sqlite_query_with_params(database_tool, sqlite_temp_db):
    """Test querying a SQLite database with parameters"""
    # Execute a query with parameters
    result = await database_tool.execute({
        'query': 'SELECT * FROM test_users WHERE age > :min_age',
        'source': sqlite_temp_db,
        'params': {'min_age': 30}
    })
    
    # Check the result
    assert 'error' not in result
    assert result['row_count'] == 2
    
    # Check specific values
    assert any(row['name'] == 'John Doe' for row in result['results'])
    assert any(row['name'] == 'Bob Johnson' for row in result['results'])

@pytest.mark.asyncio
async def test_sqlite_query_with_limit(database_tool, sqlite_temp_db):
    """Test querying a SQLite database with a limit"""
    # Execute a query with a limit
    result = await database_tool.execute({
        'query': 'SELECT * FROM test_users',
        'source': sqlite_temp_db,
        'limit': 2
    })
    
    # Check the result
    assert 'error' not in result
    assert result['row_count'] == 2
    assert len(result['results']) == 2

@pytest.mark.asyncio
async def test_csv_query(database_tool, temp_csv_file):
    """Test querying a CSV file"""
    # Execute a query
    result = await database_tool.execute({
        'query': 'SELECT * FROM data WHERE price > 500',
        'source': temp_csv_file
    })
    
    # Check the result
    assert 'error' not in result
    assert 'results' in result
    assert 'columns' in result
    assert 'row_count' in result
    assert 'execution_time' in result
    
    # Check the data
    assert result['row_count'] == 3
    assert len(result['results']) == 3
    
    # Check specific values
    product_names = [row['product_name'] for row in result['results']]
    assert 'Smartphone' in product_names
    assert 'Laptop' in product_names
    assert 'Tablet' in product_names

@pytest.mark.asyncio
async def test_json_query(database_tool, temp_json_file):
    """Test querying a JSON file"""
    # Execute a query
    result = await database_tool.execute({
        'query': 'SELECT * FROM data WHERE population > 3000000',
        'source': temp_json_file
    })
    
    # Check the result
    assert 'error' not in result
    assert 'results' in result
    assert 'columns' in result
    assert 'row_count' in result
    assert 'execution_time' in result
    
    # Check the data
    assert result['row_count'] == 2
    assert len(result['results']) == 2
    
    # Check specific values
    cities = [row['city'] for row in result['results']]
    assert 'New York' in cities
    assert 'Los Angeles' in cities

@pytest.mark.asyncio
@pytest.mark.skipif(SKIP_POSTGRES, reason="PostgreSQL connection string not provided")
async def test_postgres_query():
    """Test querying a PostgreSQL database"""
    # Skip if no PostgreSQL connection string
    if SKIP_POSTGRES:
        return
    
    # Create a database tool
    database_tool = DatabaseTool()
    
    # Execute a query
    result = await database_tool.execute({
        'query': 'SELECT 1 as test',
        'source': POSTGRES_TEST_URL
    })
    
    # Check the result
    assert 'error' not in result
    assert 'results' in result
    assert 'columns' in result
    assert 'row_count' in result
    assert 'execution_time' in result
    
    # Check the data
    assert result['row_count'] == 1
    assert len(result['results']) == 1
    assert result['results'][0]['test'] == 1

@pytest.mark.asyncio
async def test_error_handling(database_tool):
    """Test error handling"""
    # Execute a query with an invalid source
    result = await database_tool.execute({
        'query': 'SELECT * FROM test',
        'source': 'nonexistent.db'
    })
    
    # Check the result
    assert 'error' in result
    assert 'results' not in result

@pytest.mark.asyncio
async def test_input_validation(database_tool):
    """Test input validation"""
    # Missing query
    result = await database_tool.execute({
        'source': 'test.db'
    })
    
    # Check the result
    assert 'error' in result
    assert result['error'] == 'Query is required'
    
    # Missing source
    result = await database_tool.execute({
        'query': 'SELECT * FROM test'
    })
    
    # Check the result
    assert 'error' in result
    assert result['error'] == 'Data source is required'

================
File: tests/unit/test_database_tool_simple.py
================
"""
Simplified unit tests for the async DatabaseTool
"""
import os
import pytest
import pytest_asyncio
import asyncio
import tempfile
import json
import pandas as pd
from pathlib import Path

# Import the connection manager directly
from app.db.connection_manager import connection_manager

# Create a simplified version of the Tool base class for testing
class MockTool:
    """Mock Tool class for testing"""
    def __init__(self, name, description):
        self.name = name
        self.description = description
        self.logger = type('MockLogger', (), {
            'info': lambda *args, **kwargs: None,
            'error': lambda *args, **kwargs: None,
            'debug': lambda *args, **kwargs: None,
            'warning': lambda *args, **kwargs: None
        })()

# Import the DatabaseTool class directly from the file
import sys
import inspect
from pathlib import Path

# Get the content of the database_tool_async.py file
database_tool_path = Path(__file__).parent.parent.parent / "app" / "rag" / "tools" / "database_tool_async.py"
with open(database_tool_path, "r") as f:
    code = f.read()

# Replace the Tool import with our MockTool
code = code.replace("from app.rag.tools.base import Tool", "from tests.unit.test_database_tool_simple import MockTool as Tool")

# Create a namespace and execute the code
namespace = {'__file__': str(database_tool_path), 'connection_manager': connection_manager}
exec(code, namespace)

# Get the DatabaseTool class from the namespace
DatabaseTool = namespace['DatabaseTool']

@pytest_asyncio.fixture
async def sqlite_temp_db():
    """Create a temporary SQLite database for testing"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.db')
    os.close(fd)
    
    # Create a test table and insert data
    import aiosqlite
    async with aiosqlite.connect(path) as conn:
        await conn.execute("""
            CREATE TABLE test_users (
                id INTEGER PRIMARY KEY,
                name TEXT,
                age INTEGER,
                email TEXT
            )
        """)
        await conn.execute("""
            INSERT INTO test_users (name, age, email) VALUES
            ('John Doe', 35, 'john@example.com'),
            ('Jane Smith', 28, 'jane@example.com'),
            ('Bob Johnson', 42, 'bob@example.com')
        """)
        await conn.commit()
    
    # Return the path
    yield path
    
    # Clean up
    if os.path.exists(path):
        os.unlink(path)

@pytest_asyncio.fixture
async def temp_csv_file():
    """Create a temporary CSV file for testing"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.csv')
    os.close(fd)
    
    # Create test data
    data = {
        'product_id': [1, 2, 3, 4],
        'product_name': ['Smartphone', 'Laptop', 'Headphones', 'Tablet'],
        'price': [699.99, 1299.99, 149.99, 499.99],
        'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics']
    }
    df = pd.DataFrame(data)
    
    # Write to CSV
    df.to_csv(path, index=False)
    
    # Return the path
    yield path
    
    # Clean up
    if os.path.exists(path):
        os.unlink(path)

@pytest_asyncio.fixture
async def temp_json_file():
    """Create a temporary JSON file for testing"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix='.json')
    os.close(fd)
    
    # Create test data
    data = [
        {'city': 'New York', 'population': 8336817, 'country': 'USA'},
        {'city': 'Los Angeles', 'population': 3979576, 'country': 'USA'},
        {'city': 'Chicago', 'population': 2693976, 'country': 'USA'},
        {'city': 'Houston', 'population': 2320268, 'country': 'USA'},
        {'city': 'Phoenix', 'population': 1680992, 'country': 'USA'}
    ]
    
    # Write to JSON
    with open(path, 'w') as f:
        json.dump(data, f)
    
    # Return the path
    yield path
    
    # Clean up
    if os.path.exists(path):
        os.unlink(path)

@pytest.fixture
def database_tool():
    """Create a DatabaseTool instance for testing"""
    return DatabaseTool()

@pytest.mark.asyncio
async def test_sqlite_query(database_tool, sqlite_temp_db):
    """Test querying a SQLite database"""
    # Execute a query
    result = await database_tool.execute({
        'query': 'SELECT * FROM test_users WHERE age > 30',
        'source': sqlite_temp_db
    })
    
    # Check the result
    assert 'error' not in result
    assert 'results' in result
    assert 'columns' in result
    assert 'row_count' in result
    assert 'execution_time' in result
    
    # Check the data
    assert result['row_count'] == 2
    assert len(result['results']) == 2
    assert result['columns'] == ['id', 'name', 'age', 'email']
    
    # Check specific values
    assert any(row['name'] == 'John Doe' for row in result['results'])
    assert any(row['name'] == 'Bob Johnson' for row in result['results'])

@pytest.mark.asyncio
async def test_sqlite_query_with_params(database_tool, sqlite_temp_db):
    """Test querying a SQLite database with parameters"""
    # Execute a query with parameters
    result = await database_tool.execute({
        'query': 'SELECT * FROM test_users WHERE age > :min_age',
        'source': sqlite_temp_db,
        'params': {'min_age': 30}
    })
    
    # Check the result
    assert 'error' not in result
    assert result['row_count'] == 2
    
    # Check specific values
    assert any(row['name'] == 'John Doe' for row in result['results'])
    assert any(row['name'] == 'Bob Johnson' for row in result['results'])

@pytest.mark.asyncio
async def test_csv_query(database_tool, temp_csv_file):
    """Test querying a CSV file"""
    # Execute a query
    result = await database_tool.execute({
        'query': 'SELECT * FROM data WHERE price > 500',
        'source': temp_csv_file
    })
    
    # Check the result
    assert 'error' not in result
    assert 'results' in result
    assert 'columns' in result
    assert 'row_count' in result
    assert 'execution_time' in result
    
    # Check the data
    assert result['row_count'] == 3
    assert len(result['results']) == 3
    
    # Check specific values
    product_names = [row['product_name'] for row in result['results']]
    assert 'Smartphone' in product_names
    assert 'Laptop' in product_names
    assert 'Tablet' in product_names

@pytest.mark.asyncio
async def test_json_query(database_tool, temp_json_file):
    """Test querying a JSON file"""
    # Execute a query
    result = await database_tool.execute({
        'query': 'SELECT * FROM data WHERE population > 3000000',
        'source': temp_json_file
    })
    
    # Check the result
    assert 'error' not in result
    assert 'results' in result
    assert 'columns' in result
    assert 'row_count' in result
    assert 'execution_time' in result
    
    # Check the data
    assert result['row_count'] == 2
    assert len(result['results']) == 2
    
    # Check specific values
    cities = [row['city'] for row in result['results']]
    assert 'New York' in cities
    assert 'Los Angeles' in cities

================
File: tests/unit/test_document_analysis_service.py
================
"""
Unit tests for DocumentAnalysisService
"""
import pytest
import uuid
from unittest.mock import MagicMock, patch

from app.models.document import Document
from app.rag.document_analysis_service import DocumentAnalysisService

@pytest.fixture
def mock_llm_provider():
    """Mock LLM provider"""
    provider = MagicMock()
    
    async def mock_generate(**kwargs):
        return {"response": """
        {
            "strategy": "recursive",
            "parameters": {
                "chunk_size": 1500,
                "chunk_overlap": 150
            },
            "justification": "Test justification"
        }
        """}
    
    provider.generate = mock_generate
    return provider

@pytest.fixture
def document_analysis_service(mock_llm_provider):
    """Document analysis service with mock LLM provider"""
    return DocumentAnalysisService(llm_provider=mock_llm_provider)

@pytest.fixture
def sample_document():
    """Sample document for testing"""
    return Document(
        id=str(uuid.uuid4()),
        filename="test_document.txt",
        content="This is a test document with some content for testing the document analysis service.",
        metadata={"file_type": "txt"}
    )

@pytest.mark.asyncio
async def test_analyze_document(document_analysis_service, sample_document):
    """Test analyzing a document"""
    # Analyze document
    result = await document_analysis_service.analyze_document(sample_document)
    
    # Check result
    assert result is not None
    assert "strategy" in result
    assert result["strategy"] == "recursive"
    assert "parameters" in result
    assert "chunk_size" in result["parameters"]
    assert result["parameters"]["chunk_size"] == 1500
    assert "chunk_overlap" in result["parameters"]
    assert result["parameters"]["chunk_overlap"] == 150
    assert "justification" in result
    assert result["justification"] == "Test justification"

@pytest.mark.asyncio
async def test_rule_based_strategy(document_analysis_service, sample_document):
    """Test rule-based strategy when LLM is not available"""
    # Remove LLM provider
    document_analysis_service.llm_provider = None
    
    # Analyze document
    result = await document_analysis_service.analyze_document(sample_document)
    
    # Check result
    assert result is not None
    assert "strategy" in result
    assert "parameters" in result
    assert "chunk_size" in result["parameters"]
    assert "chunk_overlap" in result["parameters"]
    assert "justification" in result

@pytest.mark.asyncio
async def test_analyze_document_batch(document_analysis_service, sample_document):
    """Test analyzing a batch of documents"""
    # Create batch of documents
    documents = [sample_document, sample_document]
    
    # Analyze batch
    result = await document_analysis_service.analyze_document_batch(documents)
    
    # Check result
    assert result is not None
    assert "strategy" in result
    assert result["strategy"] == "recursive"
    assert "parameters" in result
    assert "chunk_size" in result["parameters"]
    assert result["parameters"]["chunk_size"] == 1500
    assert "chunk_overlap" in result["parameters"]
    assert result["parameters"]["chunk_overlap"] == 150
    assert "justification" in result
    assert result["justification"] == "Test justification"

@pytest.mark.asyncio
async def test_extract_sample_content(document_analysis_service, sample_document, monkeypatch):
    """Test extracting sample content from a document"""
    # Mock file operations
    def mock_open(*args, **kwargs):
        return MagicMock(__enter__=MagicMock(return_value=MagicMock(read=MagicMock(return_value=sample_document.content))))
    
    def mock_path_exists(*args, **kwargs):
        return True
    
    def mock_path_getsize(*args, **kwargs):
        return len(sample_document.content)
    
    monkeypatch.setattr("builtins.open", mock_open)
    monkeypatch.setattr("os.path.exists", mock_path_exists)
    monkeypatch.setattr("os.path.getsize", mock_path_getsize)
    
    # Extract sample content
    content = await document_analysis_service._extract_sample_content(sample_document)
    
    # Check content
    assert content is not None
    assert len(content) > 0

================
File: tests/unit/test_mem0_client.py
================
"""
Unit tests for Mem0 client
"""
import pytest
from unittest.mock import patch, MagicMock
from app.rag.mem0_client import get_mem0_client, store_message, get_conversation_history

@pytest.fixture
def mock_mem0_client():
    """
    Mock Mem0 client
    """
    with patch("app.rag.mem0_client.EnhancedMem0Client") as mock_client:
        mock_instance = MagicMock()
        mock_client.return_value = mock_instance
        yield mock_instance

@pytest.fixture
def mock_settings():
    """
    Mock settings
    """
    with patch("app.rag.mem0_client.SETTINGS") as mock_settings:
        mock_settings.use_mem0 = True
        mock_settings.mem0_endpoint = "http://localhost:8050"
        mock_settings.mem0_api_key = "test_key"
        yield mock_settings

def test_get_mem0_client(mock_mem0_client, mock_settings):
    """
    Test get_mem0_client
    """
    # Reset singleton
    import app.rag.mem0_client
    app.rag.mem0_client._mem0_client = None
    
    # Mock MEM0_AVAILABLE
    with patch("app.rag.mem0_client.MEM0_AVAILABLE", True):
        client = get_mem0_client()
        assert client is not None
        assert client == mock_mem0_client

def test_store_message(mock_mem0_client, mock_settings):
    """
    Test store_message
    """
    # Reset singleton
    import app.rag.mem0_client
    app.rag.mem0_client._mem0_client = None
    
    # Mock MEM0_AVAILABLE and get_or_create_human
    with patch("app.rag.mem0_client.MEM0_AVAILABLE", True):
        with patch("app.rag.mem0_client.get_or_create_human", return_value=True):
            result = store_message("user123", "user", "Hello")
            assert result is True
            mock_mem0_client.append_message.assert_called_once()

def test_get_conversation_history(mock_mem0_client, mock_settings):
    """
    Test get_conversation_history
    """
    # Reset singleton
    import app.rag.mem0_client
    app.rag.mem0_client._mem0_client = None
    
    # Mock MEM0_AVAILABLE
    with patch("app.rag.mem0_client.MEM0_AVAILABLE", True):
        mock_mem0_client.get_recall_memory.return_value = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there"}
        ]
        
        history = get_conversation_history("user123")
        assert len(history) == 2
        assert history[0]["role"] == "user"
        assert history[1]["role"] == "assistant"
        mock_mem0_client.get_recall_memory.assert_called_once()

================
File: tests/unit/test_plan_executor.py
================
"""
Unit tests for the PlanExecutor
"""
import pytest
import asyncio
from unittest.mock import MagicMock, AsyncMock

from app.rag.plan_executor import PlanExecutor
from app.rag.query_planner import QueryPlan
from app.rag.tools import ToolRegistry, Tool
from app.rag.process_logger import ProcessLogger

class MockTool(Tool):
    """Mock tool for testing"""
    
    def __init__(self, name="mock_tool", description="Mock tool"):
        super().__init__(name=name, description=description)
    
    async def execute(self, input_data):
        return {"result": f"Executed {self.name} with input: {input_data}"}
    
    def get_input_schema(self):
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Query string"
                }
            }
        }
    
    def get_output_schema(self):
        return {
            "type": "object",
            "properties": {
                "result": {
                    "type": "string",
                    "description": "Result string"
                }
            }
        }
    
    def get_examples(self):
        return [
            {
                "input": {"query": "test query"},
                "output": {"result": "test result"}
            }
        ]


class TestPlanExecutor:
    """Tests for the PlanExecutor class"""
    
    @pytest.mark.asyncio
    async def test_execute_simple_plan(self):
        """Test executing a simple plan"""
        # Create mock tool registry
        mock_registry = MagicMock()
        mock_registry.get_tool.return_value = MockTool(name="rag")
        
        # Create mock process logger
        mock_logger = MagicMock()
        
        # Create plan executor
        executor = PlanExecutor(
            tool_registry=mock_registry,
            process_logger=mock_logger
        )
        
        # Create a simple plan
        plan = QueryPlan(
            query_id="test_id",
            query="What is the capital of France?",
            steps=[
                {
                    "type": "tool",
                    "tool": "rag",
                    "input": {"query": "What is the capital of France?"},
                    "description": "Retrieve information using RAG"
                }
            ]
        )
        
        # Execute the plan
        result = await executor.execute_plan(plan)
        
        # Check result
        assert result["query_id"] == "test_id"
        assert "response" in result
        assert "steps" in result
        assert len(result["steps"]) == 1
        assert "execution_time" in result
        
        # Check that the tool was called
        mock_registry.get_tool.assert_called_once_with("rag")
        
        # Check that the process logger was called
        assert mock_logger.log_step.call_count >= 2  # At least start and complete
        assert mock_logger.log_final_response.call_count == 1
    
    @pytest.mark.asyncio
    async def test_execute_complex_plan(self):
        """Test executing a complex plan"""
        # Create mock tools
        mock_rag_tool = AsyncMock()
        mock_rag_tool.execute.return_value = {
            "chunks": [
                {
                    "content": "Paris is the capital of France.",
                    "metadata": {"document_id": "doc1"},
                    "score": 0.95
                }
            ],
            "sources": ["doc1"]
        }
        
        mock_calc_tool = AsyncMock()
        mock_calc_tool.execute.return_value = {
            "result": 42
        }
        
        # Create mock tool registry
        mock_registry = MagicMock()
        mock_registry.get_tool.side_effect = lambda name: {
            "rag": mock_rag_tool,
            "calculator": mock_calc_tool
        }.get(name)
        
        # Create mock process logger
        mock_logger = MagicMock()
        
        # Create plan executor
        executor = PlanExecutor(
            tool_registry=mock_registry,
            process_logger=mock_logger
        )
        
        # Create a complex plan
        plan = QueryPlan(
            query_id="test_id",
            query="What is the capital of France and what is 6 * 7?",
            steps=[
                {
                    "type": "tool",
                    "tool": "rag",
                    "input": {"query": "What is the capital of France?"},
                    "description": "Retrieve information about France's capital"
                },
                {
                    "type": "tool",
                    "tool": "calculator",
                    "input": {"expression": "6 * 7"},
                    "description": "Calculate 6 * 7"
                },
                {
                    "type": "synthesize",
                    "description": "Synthesize results"
                }
            ]
        )
        
        # Execute the plan
        result = await executor.execute_plan(plan)
        
        # Check result
        assert result["query_id"] == "test_id"
        assert "response" in result
        assert "steps" in result
        assert len(result["steps"]) == 3  # All steps should be executed
        assert "execution_time" in result
        
        # Check that the tools were called
        mock_registry.get_tool.assert_any_call("rag")
        mock_registry.get_tool.assert_any_call("calculator")
        mock_rag_tool.execute.assert_called_once()
        mock_calc_tool.execute.assert_called_once()
        
        # Check that the process logger was called
        assert mock_logger.log_step.call_count >= 6  # At least start and complete for each step
        assert mock_logger.log_final_response.call_count == 1
    
    @pytest.mark.asyncio
    async def test_execute_tool(self):
        """Test executing a tool"""
        # Create mock tool
        mock_tool = AsyncMock()
        mock_tool.execute.return_value = {"result": "test result"}
        
        # Create mock tool registry
        mock_registry = MagicMock()
        mock_registry.get_tool.return_value = mock_tool
        
        # Create plan executor
        executor = PlanExecutor(
            tool_registry=mock_registry
        )
        
        # Execute the tool
        result = await executor._execute_tool("test_tool", {"query": "test query"})
        
        # Check result
        assert result["result"] == "test result"
        
        # Check that the tool was called correctly
        mock_registry.get_tool.assert_called_once_with("test_tool")
        mock_tool.execute.assert_called_once_with({"query": "test query"})
    
    @pytest.mark.asyncio
    async def test_execute_nonexistent_tool(self):
        """Test executing a nonexistent tool"""
        # Create mock tool registry that returns None
        mock_registry = MagicMock()
        mock_registry.get_tool.return_value = None
        
        # Create plan executor
        executor = PlanExecutor(
            tool_registry=mock_registry
        )
        
        # Execute the nonexistent tool
        result = await executor._execute_tool("nonexistent_tool", {"query": "test query"})
        
        # Check result
        assert "error" in result
        assert "Tool not found" in result["error"]
        
        # Check that the tool registry was called correctly
        mock_registry.get_tool.assert_called_once_with("nonexistent_tool")
    
    @pytest.mark.asyncio
    async def test_execute_tool_with_error(self):
        """Test executing a tool that raises an error"""
        # Create mock tool that raises an exception
        mock_tool = AsyncMock()
        mock_tool.execute.side_effect = Exception("Test error")
        
        # Create mock tool registry
        mock_registry = MagicMock()
        mock_registry.get_tool.return_value = mock_tool
        
        # Create plan executor
        executor = PlanExecutor(
            tool_registry=mock_registry
        )
        
        # Execute the tool
        result = await executor._execute_tool("test_tool", {"query": "test query"})
        
        # Check result
        assert "error" in result
        assert "Test error" in result["error"]
        
        # Check that the tool was called correctly
        mock_registry.get_tool.assert_called_once_with("test_tool")
        mock_tool.execute.assert_called_once_with({"query": "test query"})
    
    @pytest.mark.asyncio
    async def test_generate_response_with_llm(self):
        """Test generating a response with an LLM"""
        # Create mock LLM provider
        mock_llm = AsyncMock()
        mock_llm.generate.return_value = {"response": "This is a test response"}
        
        # Create plan executor with LLM
        executor = PlanExecutor(
            tool_registry=MagicMock(),
            llm_provider=mock_llm
        )
        
        # Create a plan with results
        plan = QueryPlan(
            query_id="test_id",
            query="test query",
            steps=[
                {"type": "tool", "tool": "rag", "description": "RAG step"}
            ]
        )
        plan.record_step_result({"chunks": [{"content": "Test content"}]})
        
        # Generate response
        response = await executor._generate_response(plan)
        
        # Check response
        assert response == "This is a test response"
        
        # Check that the LLM was called
        mock_llm.generate.assert_called_once()
        
    @pytest.mark.asyncio
    async def test_generate_simple_response(self):
        """Test generating a simple response without an LLM"""
        # Create plan executor without LLM
        executor = PlanExecutor(
            tool_registry=MagicMock()
        )
        
        # Create a plan with RAG results
        rag_plan = QueryPlan(
            query_id="test_id",
            query="What is the capital of France?",
            steps=[{"type": "tool", "tool": "rag"}]
        )
        rag_plan.record_step_result({
            "chunks": [{"content": "Paris is the capital of France."}]
        })
        
        # Generate response for RAG plan
        rag_response = executor._generate_simple_response(rag_plan)
        assert "Paris is the capital of France" in rag_response
        
        # Create a plan with calculator results
        calc_plan = QueryPlan(
            query_id="test_id",
            query="What is 2 + 2?",
            steps=[{"type": "tool", "tool": "calculator"}]
        )
        calc_plan.record_step_result({"result": 4})
        
        # Generate response for calculator plan
        calc_response = executor._generate_simple_response(calc_plan)
        assert "4" in calc_response
        
        # Create a plan with database results
        db_plan = QueryPlan(
            query_id="test_id",
            query="Query the database",
            steps=[{"type": "tool", "tool": "database"}]
        )
        db_plan.record_step_result({"results": [{"id": 1}, {"id": 2}]})
        
        # Generate response for database plan
        db_response = executor._generate_simple_response(db_plan)
        assert "2 records" in db_response
        
        # Create a plan with an error
        error_plan = QueryPlan(
            query_id="test_id",
            query="Error query",
            steps=[{"type": "tool", "tool": "rag"}]
        )
        error_plan.record_step_result({"error": "Test error"})
        
        # Generate response for error plan
        error_response = executor._generate_simple_response(error_plan)
        assert "error" in error_response.lower()
        assert "Test error" in error_response

================
File: tests/unit/test_process_logger.py
================
"""
Unit tests for the ProcessLogger
"""
import pytest
import os
import json
import tempfile
from datetime import datetime
from unittest.mock import MagicMock

from app.rag.process_logger import ProcessLogger


class TestProcessLogger:
    """Tests for the ProcessLogger class"""
    
    def test_start_process(self):
        """Test starting a new process"""
        # Create process logger
        process_logger = ProcessLogger()
        
        # Start process
        process_logger.start_process("query123", "What is the capital of France?")
        
        # Check process log
        log = process_logger.get_process_log("query123")
        assert log is not None
        assert log["query"] == "What is the capital of France?"
        assert "timestamp" in log
        assert log["steps"] == []
        assert log["final_response"] is None
        assert log["audit_report"] is None
    
    def test_log_step(self):
        """Test logging a step"""
        # Create process logger
        process_logger = ProcessLogger()
        
        # Start process
        process_logger.start_process("query123", "What is the capital of France?")
        
        # Log step
        process_logger.log_step("query123", "query_analysis", {
            "complexity": "simple",
            "requires_tools": ["rag"]
        })
        
        # Check process log
        log = process_logger.get_process_log("query123")
        assert len(log["steps"]) == 1
        assert log["steps"][0]["step_name"] == "query_analysis"
        assert log["steps"][0]["data"]["complexity"] == "simple"
        assert log["steps"][0]["data"]["requires_tools"] == ["rag"]
        assert "timestamp" in log["steps"][0]
    
    def test_log_tool_usage(self):
        """Test logging tool usage"""
        # Create process logger
        process_logger = ProcessLogger()
        
        # Start process
        process_logger.start_process("query123", "What is the capital of France?")
        
        # Log tool usage
        process_logger.log_tool_usage(
            query_id="query123",
            tool_name="rag",
            input_data={"query": "What is the capital of France?", "top_k": 3},
            output_data={"chunks": [{"content": "Paris is the capital of France."}]}
        )
        
        # Check process log
        log = process_logger.get_process_log("query123")
        assert len(log["steps"]) == 1
        assert log["steps"][0]["step_name"] == "tool_rag"
        assert log["steps"][0]["data"]["tool"] == "rag"
        assert log["steps"][0]["data"]["input"]["query"] == "What is the capital of France?"
        assert log["steps"][0]["data"]["output"]["chunks"][0]["content"] == "Paris is the capital of France."
    
    def test_log_final_response(self):
        """Test logging the final response"""
        # Create process logger
        process_logger = ProcessLogger()
        
        # Start process
        process_logger.start_process("query123", "What is the capital of France?")
        
        # Log final response
        process_logger.log_final_response(
            query_id="query123",
            response="The capital of France is Paris.",
            metadata={"confidence": 0.95}
        )
        
        # Check process log
        log = process_logger.get_process_log("query123")
        assert log["final_response"] is not None
        assert log["final_response"]["text"] == "The capital of France is Paris."
        assert log["final_response"]["metadata"]["confidence"] == 0.95
        assert "timestamp" in log["final_response"]
    
    def test_get_step_data(self):
        """Test getting step data"""
        # Create process logger
        process_logger = ProcessLogger()
        
        # Start process
        process_logger.start_process("query123", "What is the capital of France?")
        
        # Log steps
        process_logger.log_step("query123", "query_analysis", {"complexity": "simple"})
        process_logger.log_step("query123", "retrieval", {"chunks": [{"content": "Paris is the capital of France."}]})
        process_logger.log_step("query123", "retrieval", {"chunks": [{"content": "France's capital city is Paris."}]})
        
        # Get step data
        retrieval_steps = process_logger.get_step_data("query123", "retrieval")
        
        # Check step data
        assert len(retrieval_steps) == 2
        assert retrieval_steps[0]["chunks"][0]["content"] == "Paris is the capital of France."
        assert retrieval_steps[1]["chunks"][0]["content"] == "France's capital city is Paris."
    
    def test_get_tool_usage(self):
        """Test getting tool usage data"""
        # Create process logger
        process_logger = ProcessLogger()
        
        # Start process
        process_logger.start_process("query123", "What is the capital of France?")
        
        # Log tool usage
        process_logger.log_tool_usage(
            query_id="query123",
            tool_name="rag",
            input_data={"query": "What is the capital of France?"},
            output_data={"chunks": [{"content": "Paris is the capital of France."}]}
        )
        
        process_logger.log_tool_usage(
            query_id="query123",
            tool_name="rag",
            input_data={"query": "Where is Paris located?"},
            output_data={"chunks": [{"content": "Paris is located in northern France."}]}
        )
        
        # Get tool usage
        rag_usage = process_logger.get_tool_usage("query123", "rag")
        
        # Check tool usage
        assert len(rag_usage) == 2
        assert rag_usage[0]["input"]["query"] == "What is the capital of France?"
        assert rag_usage[1]["input"]["query"] == "Where is Paris located?"
    
    def test_clear_log(self):
        """Test clearing the process log"""
        # Create process logger
        process_logger = ProcessLogger()
        
        # Start processes
        process_logger.start_process("query123", "What is the capital of France?")
        process_logger.start_process("query456", "What is the population of Germany?")
        
        # Clear specific log
        process_logger.clear_log("query123")
        
        # Check logs
        assert process_logger.get_process_log("query123") is None
        assert process_logger.get_process_log("query456") is not None
        
        # Clear all logs
        process_logger.clear_log()
        
        # Check logs
        assert process_logger.get_process_log("query456") is None
    
    def test_save_to_file(self):
        """Test saving the process log to a file"""
        # Create temporary directory
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create process logger with log directory
            process_logger = ProcessLogger(log_dir=temp_dir)
            
            # Start process
            process_logger.start_process("query123", "What is the capital of France?")
            
            # Log step
            process_logger.log_step("query123", "query_analysis", {"complexity": "simple"})
            
            # Log final response
            process_logger.log_final_response("query123", "The capital of France is Paris.")
            
            # Check that file was created
            log_file = os.path.join(temp_dir, "query_query123.json")
            assert os.path.exists(log_file)
            
            # Check file contents
            with open(log_file, 'r') as f:
                log_data = json.load(f)
                assert log_data["query"] == "What is the capital of France?"
                assert len(log_data["steps"]) == 1
                assert log_data["final_response"]["text"] == "The capital of France is Paris."

================
File: tests/unit/test_processing_job.py
================
"""
Unit tests for ProcessingJob and WorkerPool
"""
import pytest
import pytest_asyncio
import asyncio
import uuid
from unittest.mock import MagicMock, patch

from app.models.document import Document
from app.rag.processing_job import ProcessingJob, WorkerPool, DocumentProcessingService

@pytest.fixture
def sample_document_ids():
    """Sample document IDs for testing"""
    return [str(uuid.uuid4()) for _ in range(3)]

@pytest.fixture
def processing_job(sample_document_ids):
    """Processing job for testing"""
    return ProcessingJob(document_ids=sample_document_ids)

@pytest_asyncio.fixture
async def worker_pool():
    """Worker pool for testing"""
    pool = WorkerPool(max_workers=2)
    await pool.start()
    yield pool
    await pool.stop()

@pytest.fixture
def mock_document_processor():
    """Mock document processor"""
    processor = MagicMock()
    processor.process_document = MagicMock(return_value=None)
    return processor

@pytest_asyncio.fixture
async def document_processing_service(mock_document_processor):
    """Document processing service for testing"""
    service = DocumentProcessingService(document_processor=mock_document_processor)
    await service.start()
    yield service
    await service.stop()

def test_processing_job_init(processing_job, sample_document_ids):
    """Test ProcessingJob initialization"""
    assert processing_job.id is not None
    assert processing_job.document_ids == sample_document_ids
    assert processing_job.status == "pending"
    assert processing_job.document_count == len(sample_document_ids)
    assert processing_job.processed_count == 0
    assert processing_job.progress_percentage == 0

def test_processing_job_update_progress(processing_job):
    """Test updating job progress"""
    processing_job.update_progress(2)
    assert processing_job.processed_count == 2
    assert processing_job.progress_percentage == (2 / processing_job.document_count) * 100

def test_processing_job_complete(processing_job):
    """Test marking job as completed"""
    processing_job.complete()
    assert processing_job.status == "completed"
    assert processing_job.completed_at is not None
    assert processing_job.processed_count == processing_job.document_count
    assert processing_job.progress_percentage == 100

def test_processing_job_fail(processing_job):
    """Test marking job as failed"""
    error_message = "Test error"
    processing_job.fail(error_message)
    assert processing_job.status == "failed"
    assert processing_job.completed_at is not None
    assert processing_job.error_message == error_message

def test_processing_job_to_dict(processing_job):
    """Test converting job to dictionary"""
    job_dict = processing_job.to_dict()
    assert job_dict["id"] == processing_job.id
    assert job_dict["document_ids"] == processing_job.document_ids
    assert job_dict["status"] == processing_job.status
    assert job_dict["document_count"] == processing_job.document_count
    assert job_dict["processed_count"] == processing_job.processed_count
    assert job_dict["progress_percentage"] == processing_job.progress_percentage

@pytest.mark.asyncio
async def test_worker_pool_start_stop(worker_pool):
    """Test starting and stopping worker pool"""
    assert worker_pool.running is True
    
    await worker_pool.stop()
    assert worker_pool.running is False

@pytest.mark.asyncio
async def test_worker_pool_add_job(worker_pool):
    """Test adding a job to the worker pool"""
    # Create mock job function
    job_executed = False
    
    async def mock_job():
        nonlocal job_executed
        job_executed = True
    
    # Add job to pool
    await worker_pool.add_job(mock_job)
    
    # Wait for job to execute
    await asyncio.sleep(0.1)
    
    # Check if job was executed
    assert job_executed is True

@pytest.mark.asyncio
async def test_document_processing_service_create_job(document_processing_service, sample_document_ids):
    """Test creating a processing job"""
    # Create job
    job = await document_processing_service.create_job(document_ids=sample_document_ids)
    
    # Check job
    assert job.id is not None
    assert job.document_ids == sample_document_ids
    assert job.status == "pending"
    
    # Wait for job to be processed
    await asyncio.sleep(0.1)

@pytest.mark.asyncio
async def test_document_processing_service_get_job(document_processing_service, sample_document_ids):
    """Test getting a job by ID"""
    # Create job
    job = await document_processing_service.create_job(document_ids=sample_document_ids)
    
    # Get job
    retrieved_job = await document_processing_service.get_job(job.id)
    
    # Check job
    assert retrieved_job is not None
    assert retrieved_job.id == job.id

@pytest.mark.asyncio
async def test_document_processing_service_list_jobs(document_processing_service, sample_document_ids):
    """Test listing jobs"""
    # Create jobs
    job1 = await document_processing_service.create_job(document_ids=sample_document_ids)
    job2 = await document_processing_service.create_job(document_ids=sample_document_ids)
    
    # List jobs
    jobs = await document_processing_service.list_jobs()
    
    # Check jobs
    assert len(jobs) == 2
    assert any(job.id == job1.id for job in jobs)
    assert any(job.id == job2.id for job in jobs)

@pytest.mark.asyncio
async def test_document_processing_service_cancel_job(document_processing_service, sample_document_ids):
    """Test cancelling a job"""
    # Create job
    job = await document_processing_service.create_job(document_ids=sample_document_ids)
    
    # Cancel job
    success = await document_processing_service.cancel_job(job.id)
    
    # Check result
    assert success is True
    
    # Get job
    cancelled_job = await document_processing_service.get_job(job.id)
    
    # Check job
    assert cancelled_job is not None
    assert cancelled_job.status == "cancelled"

================
File: tests/unit/test_prompt_manager.py
================
"""
Tests for the PromptManager class
"""
import pytest
from app.rag.prompt_manager import PromptManager

class TestPromptManager:
    """Test cases for the PromptManager class"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.prompt_manager = PromptManager()
    
    def test_initialization(self):
        """Test that the PromptManager initializes correctly"""
        assert self.prompt_manager is not None
        assert self.prompt_manager.templates is not None
        assert "with_documents" in self.prompt_manager.templates
        assert "no_documents" in self.prompt_manager.templates
        assert "low_relevance" in self.prompt_manager.templates
        assert "error" in self.prompt_manager.templates
    
    def test_create_prompt_with_documents(self):
        """Test creating a prompt when documents are available"""
        query = "What is the capital of France?"
        context = "[1] Source: geography.txt, Tags: ['geography', 'europe'], Folder: /\n\nParis is the capital of France."
        
        system_prompt, user_prompt = self.prompt_manager.create_prompt(
            query=query,
            retrieval_state="success",
            context=context
        )
        
        # Check system prompt
        assert "ALWAYS prioritize information from the provided documents" in system_prompt
        assert "Use citations [1] ONLY when referring to specific documents" in system_prompt
        
        # Check user prompt
        assert "Context:" in user_prompt
        assert "Paris is the capital of France" in user_prompt
        assert "User question: What is the capital of France?" in user_prompt
        assert "ALWAYS reference your sources with the number in square brackets" in user_prompt
    
    def test_create_prompt_no_documents(self):
        """Test creating a prompt when no documents are available"""
        query = "What is the capital of France?"
        
        system_prompt, user_prompt = self.prompt_manager.create_prompt(
            query=query,
            retrieval_state="no_documents",
            context=""
        )
        
        # Check system prompt
        assert "Be honest about limitations when no relevant documents are available" in system_prompt
        assert "DO NOT use citations [1] as there are no documents to cite" in system_prompt
        
        # Check user prompt
        assert "Context:" not in user_prompt
        assert "User question: What is the capital of France?" in user_prompt
        assert "No relevant documents were found for this query" in user_prompt
        assert "DO NOT use citations [1] as there are no documents to cite" in user_prompt
    
    def test_create_prompt_low_relevance(self):
        """Test creating a prompt when documents have low relevance"""
        query = "What is the capital of France?"
        context = "[1] Source: history.txt, Tags: ['history', 'europe'], Folder: /\n\nFrance has a rich history dating back to the Roman Empire."
        
        system_prompt, user_prompt = self.prompt_manager.create_prompt(
            query=query,
            retrieval_state="low_relevance",
            context=context
        )
        
        # Check system prompt
        assert "Be honest when available documents have low relevance to the query" in system_prompt
        
        # Check user prompt
        assert "Context (Low Relevance):" in user_prompt
        assert "France has a rich history" in user_prompt
        assert "The retrieved documents have low relevance to this query" in user_prompt
    
    def test_create_prompt_with_conversation_history(self):
        """Test creating a prompt with conversation history"""
        query = "What is its population?"
        context = "[1] Source: geography.txt, Tags: ['geography', 'europe'], Folder: /\n\nParis has a population of approximately 2.2 million people."
        conversation_history = [
            {"role": "user", "content": "Tell me about Paris."},
            {"role": "assistant", "content": "Paris is the capital of France and is known for the Eiffel Tower."}
        ]
        
        system_prompt, user_prompt = self.prompt_manager.create_prompt(
            query=query,
            retrieval_state="success",
            context=context,
            conversation_history=conversation_history
        )
        
        # Check that conversation history is included
        assert "Previous conversation:" in user_prompt
        assert "User: Tell me about Paris." in user_prompt
        assert "Assistant: Paris is the capital of France and is known for the Eiffel Tower." in user_prompt
    
    def test_get_retrieval_state(self):
        """Test determining retrieval state from search results"""
        # Test with no results
        assert self.prompt_manager.get_retrieval_state([]) == "no_documents"
        
        # Test with results below threshold
        low_relevance_results = [
            {"chunk_id": "1", "distance": 0.7, "content": "Some content"}
        ]
        assert self.prompt_manager.get_retrieval_state(low_relevance_results) == "low_relevance"
        
        # Test with relevant results
        relevant_results = [
            {"chunk_id": "1", "distance": 0.3, "content": "Some relevant content"}
        ]
        assert self.prompt_manager.get_retrieval_state(relevant_results) == "success"

================
File: tests/unit/test_query_analyzer.py
================
"""
Unit tests for the QueryAnalyzer
"""
import pytest
from unittest.mock import AsyncMock, MagicMock

from app.rag.query_analyzer import QueryAnalyzer


class TestQueryAnalyzer:
    """Tests for the QueryAnalyzer class"""
    
    @pytest.mark.asyncio
    async def test_analyze_simple_query(self):
        """Test analyzing a simple query"""
        # Create mock LLM provider
        mock_llm_provider = AsyncMock()
        mock_llm_provider.generate.return_value = {
            "response": """
            {
              "complexity": "simple",
              "requires_tools": ["rag"],
              "sub_queries": [],
              "reasoning": "This is a simple factual query that can be answered with a single RAG lookup."
            }
            """
        }
        
        # Create query analyzer
        query_analyzer = QueryAnalyzer(llm_provider=mock_llm_provider)
        
        # Analyze query
        result = await query_analyzer.analyze("What is the capital of France?")
        
        # Check result
        assert result["complexity"] == "simple"
        assert result["requires_tools"] == ["rag"]
        assert result["sub_queries"] == []
        assert "reasoning" in result
        
        # Check LLM provider was called
        mock_llm_provider.generate.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_analyze_complex_query(self):
        """Test analyzing a complex query"""
        # Create mock LLM provider
        mock_llm_provider = AsyncMock()
        mock_llm_provider.generate.return_value = {
            "response": """
            {
              "complexity": "complex",
              "requires_tools": ["rag", "calculator"],
              "sub_queries": [
                "What is the population of France?",
                "What is the population of Germany?",
                "Calculate the population difference between France and Germany"
              ],
              "reasoning": "This query requires retrieving population data and performing a calculation."
            }
            """
        }
        
        # Create query analyzer
        query_analyzer = QueryAnalyzer(llm_provider=mock_llm_provider)
        
        # Analyze query
        result = await query_analyzer.analyze("What is the population difference between France and Germany?")
        
        # Check result
        assert result["complexity"] == "complex"
        assert "rag" in result["requires_tools"]
        assert "calculator" in result["requires_tools"]
        assert len(result["sub_queries"]) == 3
        assert "reasoning" in result
        
        # Check LLM provider was called
        mock_llm_provider.generate.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_analyze_with_json_parsing_failure(self):
        """Test analyzing a query with JSON parsing failure"""
        # Create mock LLM provider
        mock_llm_provider = AsyncMock()
        mock_llm_provider.generate.return_value = {
            "response": """
            The query "What is the capital of France?" is a simple factual query.
            It requires the rag tool to retrieve information.
            complexity: simple
            requires_tools: rag
            """
        }
        
        # Create query analyzer
        query_analyzer = QueryAnalyzer(llm_provider=mock_llm_provider)
        
        # Analyze query
        result = await query_analyzer.analyze("What is the capital of France?")
        
        # Check result - should use fallback parsing
        assert "complexity" in result
        assert "requires_tools" in result
        assert "sub_queries" in result
        assert "reasoning" in result
        
        # Check LLM provider was called
        mock_llm_provider.generate.assert_called_once()

================
File: tests/unit/test_query_planner.py
================
"""
Unit tests for the QueryPlanner
"""
import pytest
import asyncio
from unittest.mock import MagicMock, AsyncMock

from app.rag.query_planner import QueryPlanner, QueryPlan
from app.rag.query_analyzer import QueryAnalyzer
from app.rag.tools import ToolRegistry, Tool

class MockTool(Tool):
    """Mock tool for testing"""
    
    def __init__(self, name="mock_tool", description="Mock tool"):
        super().__init__(name=name, description=description)
    
    async def execute(self, input_data):
        return {"result": f"Executed {self.name} with input: {input_data}"}
    
    def get_input_schema(self):
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Query string"
                }
            }
        }
    
    def get_output_schema(self):
        return {
            "type": "object",
            "properties": {
                "result": {
                    "type": "string",
                    "description": "Result string"
                }
            }
        }
    
    def get_examples(self):
        return [
            {
                "input": {"query": "test query"},
                "output": {"result": "test result"}
            }
        ]


class TestQueryPlan:
    """Tests for the QueryPlan class"""
    
    def test_query_plan_initialization(self):
        """Test initializing a query plan"""
        plan = QueryPlan(
            query_id="test_id",
            query="test query",
            steps=[
                {"type": "tool", "tool": "rag", "input": {"query": "test query"}}
            ]
        )
        
        assert plan.query_id == "test_id"
        assert plan.query == "test query"
        assert len(plan.steps) == 1
        assert plan.current_step == 0
        assert not plan.is_completed()
    
    def test_get_next_step(self):
        """Test getting the next step in a plan"""
        plan = QueryPlan(
            query_id="test_id",
            query="test query",
            steps=[
                {"type": "tool", "tool": "rag", "input": {"query": "test query"}},
                {"type": "tool", "tool": "calculator", "input": {"expression": "1 + 1"}}
            ]
        )
        
        # Get first step
        step = plan.get_next_step()
        assert step["type"] == "tool"
        assert step["tool"] == "rag"
        
        # Record result and get next step
        plan.record_step_result({"chunks": []})
        step = plan.get_next_step()
        assert step["type"] == "tool"
        assert step["tool"] == "calculator"
        
        # Record result and check completion
        plan.record_step_result({"result": 2})
        assert plan.is_completed()
        assert plan.get_next_step() is None
    
    def test_to_dict_and_from_dict(self):
        """Test converting a plan to and from a dictionary"""
        original_plan = QueryPlan(
            query_id="test_id",
            query="test query",
            steps=[
                {"type": "tool", "tool": "rag", "input": {"query": "test query"}}
            ]
        )
        
        # Convert to dictionary
        plan_dict = original_plan.to_dict()
        
        # Create new plan from dictionary
        new_plan = QueryPlan.from_dict(plan_dict)
        
        # Check that the plans are equivalent
        assert new_plan.query_id == original_plan.query_id
        assert new_plan.query == original_plan.query
        assert new_plan.steps == original_plan.steps
        assert new_plan.current_step == original_plan.current_step
        assert new_plan.results == original_plan.results
        assert new_plan.completed == original_plan.completed


class TestQueryPlanner:
    """Tests for the QueryPlanner class"""
    
    @pytest.mark.asyncio
    async def test_create_simple_plan(self):
        """Test creating a simple plan"""
        # Create mock query analyzer
        mock_analyzer = AsyncMock()
        mock_analyzer.analyze.return_value = {
            "complexity": "simple",
            "requires_tools": ["rag"],
            "sub_queries": [],
            "reasoning": "This is a simple factual query"
        }
        
        # Create mock tool registry
        mock_registry = MagicMock()
        mock_registry.get_tool.return_value = MockTool(name="rag")
        
        # Create query planner
        planner = QueryPlanner(
            query_analyzer=mock_analyzer,
            tool_registry=mock_registry
        )
        
        # Create plan
        plan = await planner.create_plan(
            query_id="test_id",
            query="What is the capital of France?"
        )
        
        # Check plan
        assert plan.query_id == "test_id"
        assert plan.query == "What is the capital of France?"
        assert len(plan.steps) == 1
        assert plan.steps[0]["type"] == "tool"
        assert plan.steps[0]["tool"] == "rag"
        
        # Check that the analyzer was called correctly
        mock_analyzer.analyze.assert_called_once_with("What is the capital of France?")
    
    @pytest.mark.asyncio
    async def test_create_complex_plan(self):
        """Test creating a complex plan"""
        # Create mock query analyzer
        mock_analyzer = AsyncMock()
        mock_analyzer.analyze.return_value = {
            "complexity": "complex",
            "requires_tools": ["calculator", "rag"],
            "sub_queries": ["What is the population of France?", "What is the population of Germany?"],
            "reasoning": "This query requires calculation and retrieval"
        }
        
        # Create mock tool registry
        mock_registry = MagicMock()
        mock_registry.get_tool.side_effect = lambda name: MockTool(name=name)
        
        # Create query planner
        planner = QueryPlanner(
            query_analyzer=mock_analyzer,
            tool_registry=mock_registry
        )
        
        # Create plan
        plan = await planner.create_plan(
            query_id="test_id",
            query="What is the combined population of France and Germany?"
        )
        
        # Check plan
        assert plan.query_id == "test_id"
        assert plan.query == "What is the combined population of France and Germany?"
        
        # Should have 4 steps: calculator, rag, 2 sub-queries, and synthesize
        assert len(plan.steps) == 5
        
        # Check that the steps include the required tools
        tools = [step["tool"] for step in plan.steps if step["type"] == "tool"]
        assert "calculator" in tools
        assert "rag" in tools
        
        # Check that there's a synthesize step
        assert any(step["type"] == "synthesize" for step in plan.steps)
        
        # Check that the analyzer was called correctly
        mock_analyzer.analyze.assert_called_once_with("What is the combined population of France and Germany?")
    
    def test_create_tool_input(self):
        """Test creating tool input"""
        # Create mock query analyzer and tool registry
        mock_analyzer = MagicMock()
        mock_registry = MagicMock()
        
        # Create query planner
        planner = QueryPlanner(
            query_analyzer=mock_analyzer,
            tool_registry=mock_registry
        )
        
        # Test creating input for RAG tool
        rag_input = planner._create_tool_input("rag", "What is the capital of France?")
        assert rag_input["query"] == "What is the capital of France?"
        assert rag_input["top_k"] == 5
        
        # Test creating input for calculator tool
        calc_input = planner._create_tool_input("calculator", "Calculate 2 + 2")
        assert "expression" in calc_input
        
        # Test creating input for database tool
        db_input = planner._create_tool_input("database", "Query the database")
        assert "query" in db_input
        assert "source" in db_input

================
File: tests/unit/test_rag_engine.py
================
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
import asyncio

from app.rag.rag_engine import RAGEngine
from app.models.chat import Citation

@pytest.fixture
def mock_vector_store():
    """
    Create a mock vector store
    """
    mock = AsyncMock()
    mock.search.return_value = [
        {
            "chunk_id": "chunk1",
            "content": "This is a test chunk",
            "metadata": {"document_id": "doc1"},
            "distance": 0.1
        },
        {
            "chunk_id": "chunk2",
            "content": "This is another test chunk",
            "metadata": {"document_id": "doc1"},
            "distance": 0.2
        }
    ]
    return mock

@pytest.fixture
def mock_ollama_client():
    """
    Create a mock Ollama client
    """
    mock = AsyncMock()
    mock.generate.return_value = {"response": "This is a test response"}
    return mock

@pytest.mark.asyncio
async def test_rag_engine_query_with_rag(mock_vector_store, mock_ollama_client):
    """
    Test RAG engine query with RAG enabled
    """
    # Arrange
    engine = RAGEngine(
        vector_store=mock_vector_store,
        ollama_client=mock_ollama_client
    )
    
    # Act
    result = await engine.query(
        query="test query",
        model="test-model",
        use_rag=True,
        stream=False
    )
    
    # Assert
    assert mock_vector_store.search.called
    assert mock_ollama_client.generate.called
    assert result["query"] == "test query"
    assert result["answer"] == "This is a test response"
    assert len(result["sources"]) == 2
    assert isinstance(result["sources"][0], Citation)

@pytest.mark.asyncio
async def test_rag_engine_query_without_rag(mock_vector_store, mock_ollama_client):
    """
    Test RAG engine query with RAG disabled
    """
    # Arrange
    engine = RAGEngine(
        vector_store=mock_vector_store,
        ollama_client=mock_ollama_client
    )
    
    # Act
    result = await engine.query(
        query="test query",
        model="test-model",
        use_rag=False,
        stream=False
    )
    
    # Assert
    assert not mock_vector_store.search.called
    assert mock_ollama_client.generate.called
    assert result["query"] == "test query"
    assert result["answer"] == "This is a test response"
    assert result["sources"] is None

================
File: tests/unit/test_response_quality.py
================
"""
Tests for the response quality components
"""
import pytest
import asyncio
import json
from unittest.mock import MagicMock, AsyncMock, patch

from app.rag.response_synthesizer import ResponseSynthesizer
from app.rag.response_evaluator import ResponseEvaluator
from app.rag.response_refiner import ResponseRefiner
from app.rag.audit_report_generator import AuditReportGenerator
from app.rag.process_logger import ProcessLogger

class TestResponseSynthesizer:
    """Tests for the ResponseSynthesizer class"""
    
    @pytest.fixture
    def llm_provider(self):
        """Create a mock LLM provider"""
        provider = AsyncMock()
        provider.generate = AsyncMock(return_value={"response": "This is a synthesized response with citation [1]."})
        return provider
    
    @pytest.fixture
    def process_logger(self):
        """Create a mock process logger"""
        logger = MagicMock(spec=ProcessLogger)
        logger.log_step = MagicMock()
        return logger
    
    @pytest.mark.asyncio
    async def test_synthesize(self, llm_provider, process_logger):
        """Test the synthesize method"""
        # Create a response synthesizer
        synthesizer = ResponseSynthesizer(
            llm_provider=llm_provider,
            process_logger=process_logger
        )
        
        # Create test data
        query = "What is the capital of France?"
        query_id = "test-query-id"
        context = "[1] Source: geography.txt, Tags: [geography, europe], Folder: /\n\nParis is the capital of France."
        sources = [
            {
                "document_id": "doc1",
                "chunk_id": "chunk1",
                "relevance_score": 0.95,
                "excerpt": "Paris is the capital of France.",
                "filename": "geography.txt",
                "tags": ["geography", "europe"],
                "folder": "/"
            }
        ]
        
        # Call the synthesize method
        result = await synthesizer.synthesize(
            query=query,
            query_id=query_id,
            context=context,
            sources=sources
        )
        
        # Check the result
        assert "response" in result
        assert "sources" in result
        assert "execution_time" in result
        assert result["response"] == "This is a synthesized response with citation [1]."
        assert len(result["sources"]) == 1
        assert result["sources"][0]["document_id"] == "doc1"
        
        # Check that the LLM provider was called
        llm_provider.generate.assert_called_once()
        
        # Check that the process logger was called
        process_logger.log_step.assert_called()

class TestResponseEvaluator:
    """Tests for the ResponseEvaluator class"""
    
    @pytest.fixture
    def llm_provider(self):
        """Create a mock LLM provider"""
        provider = AsyncMock()
        evaluation_json = {
            "factual_accuracy": 8,
            "completeness": 7,
            "relevance": 9,
            "hallucination_detected": False,
            "hallucination_details": "No hallucinations detected.",
            "overall_score": 8,
            "strengths": ["Accurate information", "Clear explanation"],
            "weaknesses": ["Could be more detailed"],
            "improvement_suggestions": ["Add more context about France"]
        }
        provider.generate = AsyncMock(return_value={"response": json.dumps(evaluation_json)})
        return provider
    
    @pytest.fixture
    def process_logger(self):
        """Create a mock process logger"""
        logger = MagicMock(spec=ProcessLogger)
        logger.log_step = MagicMock()
        return logger
    
    @pytest.mark.asyncio
    async def test_evaluate(self, llm_provider, process_logger):
        """Test the evaluate method"""
        # Create a response evaluator
        evaluator = ResponseEvaluator(
            llm_provider=llm_provider,
            process_logger=process_logger
        )
        
        # Create test data
        query = "What is the capital of France?"
        query_id = "test-query-id"
        response = "The capital of France is Paris."
        context = "[1] Source: geography.txt, Tags: [geography, europe], Folder: /\n\nParis is the capital of France."
        sources = [
            {
                "document_id": "doc1",
                "chunk_id": "chunk1",
                "relevance_score": 0.95,
                "excerpt": "Paris is the capital of France.",
                "filename": "geography.txt",
                "tags": ["geography", "europe"],
                "folder": "/"
            }
        ]
        
        # Call the evaluate method
        result = await evaluator.evaluate(
            query=query,
            query_id=query_id,
            response=response,
            context=context,
            sources=sources
        )
        
        # Check the result
        assert "factual_accuracy" in result
        assert "completeness" in result
        assert "relevance" in result
        assert "hallucination_detected" in result
        assert "overall_score" in result
        assert result["factual_accuracy"] == 8
        assert result["completeness"] == 7
        assert result["relevance"] == 9
        assert result["hallucination_detected"] is False
        assert result["overall_score"] == 8
        
        # Check that the LLM provider was called
        llm_provider.generate.assert_called_once()
        
        # Check that the process logger was called
        process_logger.log_step.assert_called()

class TestResponseRefiner:
    """Tests for the ResponseRefiner class"""
    
    @pytest.fixture
    def llm_provider(self):
        """Create a mock LLM provider"""
        provider = AsyncMock()
        refinement_json = {
            "refined_response": "The capital of France is Paris, a city known for its cultural heritage.",
            "improvement_summary": "Added more context about Paris."
        }
        provider.generate = AsyncMock(return_value={"response": json.dumps(refinement_json)})
        return provider
    
    @pytest.fixture
    def process_logger(self):
        """Create a mock process logger"""
        logger = MagicMock(spec=ProcessLogger)
        logger.log_step = MagicMock()
        return logger
    
    @pytest.mark.asyncio
    async def test_refine(self, llm_provider, process_logger):
        """Test the refine method"""
        # Create a response refiner
        refiner = ResponseRefiner(
            llm_provider=llm_provider,
            process_logger=process_logger
        )
        
        # Create test data
        query = "What is the capital of France?"
        query_id = "test-query-id"
        response = "The capital of France is Paris."
        evaluation = {
            "factual_accuracy": 8,
            "completeness": 7,
            "relevance": 9,
            "hallucination_detected": False,
            "hallucination_details": "No hallucinations detected.",
            "overall_score": 8,
            "strengths": ["Accurate information", "Clear explanation"],
            "weaknesses": ["Could be more detailed"],
            "improvement_suggestions": ["Add more context about France"]
        }
        context = "[1] Source: geography.txt, Tags: [geography, europe], Folder: /\n\nParis is the capital of France."
        sources = [
            {
                "document_id": "doc1",
                "chunk_id": "chunk1",
                "relevance_score": 0.95,
                "excerpt": "Paris is the capital of France.",
                "filename": "geography.txt",
                "tags": ["geography", "europe"],
                "folder": "/"
            }
        ]
        
        # Call the refine method
        result = await refiner.refine(
            query=query,
            query_id=query_id,
            response=response,
            evaluation=evaluation,
            context=context,
            sources=sources
        )
        
        # Check the result
        assert "refined_response" in result
        assert "improvement_summary" in result
        assert "execution_time" in result
        assert "iteration" in result
        assert result["refined_response"] == "The capital of France is Paris, a city known for its cultural heritage."
        assert result["improvement_summary"] == "Added more context about Paris."
        assert result["iteration"] == 1
        
        # Check that the LLM provider was called
        llm_provider.generate.assert_called_once()
        
        # Check that the process logger was called
        process_logger.log_step.assert_called()

class TestAuditReportGenerator:
    """Tests for the AuditReportGenerator class"""
    
    @pytest.fixture
    def process_logger(self):
        """Create a mock process logger"""
        logger = MagicMock(spec=ProcessLogger)
        logger.get_process_log = MagicMock(return_value={
            "query": "What is the capital of France?",
            "timestamp": "2025-03-18T18:00:00.000Z",
            "steps": [
                {
                    "step_name": "query_analysis",
                    "timestamp": "2025-03-18T18:00:01.000Z",
                    "data": {
                        "analysis": {
                            "complexity": "simple",
                            "requires_tools": [],
                            "sub_queries": []
                        }
                    }
                },
                {
                    "step_name": "retrieve_chunks",
                    "timestamp": "2025-03-18T18:00:02.000Z",
                    "data": {
                        "chunks": [
                            {
                                "content": "Paris is the capital of France.",
                                "metadata": {
                                    "document_id": "doc1",
                                    "chunk_id": "chunk1",
                                    "filename": "geography.txt",
                                    "tags": ["geography", "europe"],
                                    "folder": "/"
                                }
                            }
                        ]
                    }
                },
                {
                    "step_name": "response_synthesis",
                    "timestamp": "2025-03-18T18:00:03.000Z",
                    "data": {
                        "response": "The capital of France is Paris.",
                        "sources": [
                            {
                                "document_id": "doc1",
                                "chunk_id": "chunk1",
                                "relevance_score": 0.95,
                                "excerpt": "Paris is the capital of France.",
                                "filename": "geography.txt",
                                "tags": ["geography", "europe"],
                                "folder": "/"
                            }
                        ]
                    }
                },
                {
                    "step_name": "response_evaluation",
                    "timestamp": "2025-03-18T18:00:04.000Z",
                    "data": {
                        "evaluation": {
                            "factual_accuracy": 8,
                            "completeness": 7,
                            "relevance": 9,
                            "hallucination_detected": False,
                            "hallucination_details": "No hallucinations detected.",
                            "overall_score": 8,
                            "strengths": ["Accurate information", "Clear explanation"],
                            "weaknesses": ["Could be more detailed"],
                            "improvement_suggestions": ["Add more context about France"]
                        }
                    }
                }
            ],
            "final_response": {
                "text": "The capital of France is Paris.",
                "timestamp": "2025-03-18T18:00:05.000Z"
            }
        })
        logger.log_step = MagicMock()
        return logger
    
    @pytest.fixture
    def llm_provider(self):
        """Create a mock LLM provider"""
        provider = AsyncMock()
        analysis_json = {
            "process_efficiency": {
                "assessment": "The process was efficient.",
                "issues_identified": [],
                "recommendations": []
            },
            "retrieval_quality": {
                "assessment": "Retrieval was effective.",
                "issues_identified": [],
                "recommendations": []
            },
            "response_quality": {
                "assessment": "Response was accurate but could be more detailed.",
                "issues_identified": ["Limited detail"],
                "recommendations": ["Add more context"]
            },
            "overall_assessment": {
                "strengths": ["Accurate information", "Efficient process"],
                "weaknesses": ["Limited detail"],
                "recommendations": ["Add more context"]
            }
        }
        provider.generate = AsyncMock(return_value={"response": json.dumps(analysis_json)})
        return provider
    
    @pytest.mark.asyncio
    async def test_generate_report(self, process_logger, llm_provider):
        """Test the generate_report method"""
        # Create an audit report generator
        generator = AuditReportGenerator(
            process_logger=process_logger,
            llm_provider=llm_provider
        )
        
        # Call the generate_report method
        result = await generator.generate_report(
            query_id="test-query-id",
            include_llm_analysis=True
        )
        
        # Check the result
        assert "query_id" in result
        assert "query" in result
        assert "timestamp" in result
        assert "process_summary" in result
        assert "information_sources" in result
        assert "reasoning_trace" in result
        assert "verification_status" in result
        assert "execution_timeline" in result
        assert "response_quality" in result
        
        # Check that the process logger was called
        process_logger.get_process_log.assert_called_once_with("test-query-id")
        process_logger.log_step.assert_called_once()
        
        # Check that the LLM provider was called
        llm_provider.generate.assert_called_once()

if __name__ == "__main__":
    pytest.main(["-xvs", "test_response_quality.py"])

================
File: tests/unit/test_retrieval_judge.py
================
"""
Unit tests for the Retrieval Judge
"""
import pytest
import json
from unittest.mock import AsyncMock, MagicMock, patch

from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.ollama_client import OllamaClient


@pytest.fixture
def mock_ollama_client():
    """Create a mock OllamaClient"""
    client = AsyncMock(spec=OllamaClient)
    return client


@pytest.fixture
def retrieval_judge(mock_ollama_client):
    """Create a RetrievalJudge with a mock OllamaClient"""
    return RetrievalJudge(ollama_client=mock_ollama_client, model="test-model")


@pytest.fixture
def sample_chunks():
    """Create sample chunks for testing"""
    return [
        {
            "chunk_id": "chunk1",
            "content": "This is a sample chunk about artificial intelligence and machine learning.",
            "metadata": {
                "document_id": "doc1",
                "filename": "ai_basics.md",
                "tags": "ai,machine learning",
                "folder": "/tech"
            },
            "distance": 0.2
        },
        {
            "chunk_id": "chunk2",
            "content": "Neural networks are a subset of machine learning and are at the core of deep learning algorithms.",
            "metadata": {
                "document_id": "doc1",
                "filename": "ai_basics.md",
                "tags": "ai,machine learning,neural networks",
                "folder": "/tech"
            },
            "distance": 0.3
        },
        {
            "chunk_id": "chunk3",
            "content": "Python is a popular programming language for data science and machine learning.",
            "metadata": {
                "document_id": "doc2",
                "filename": "programming.md",
                "tags": "python,programming",
                "folder": "/tech/programming"
            },
            "distance": 0.5
        }
    ]


class TestRetrievalJudge:
    """Tests for the RetrievalJudge class"""

    @pytest.mark.asyncio
    async def test_analyze_query(self, retrieval_judge, mock_ollama_client):
        """Test analyze_query method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": json.dumps({
                "complexity": "moderate",
                "parameters": {
                    "k": 8,
                    "threshold": 0.5,
                    "reranking": True
                },
                "justification": "This is a moderately complex query that requires specific information about neural networks."
            })
        }

        # Call the method
        result = await retrieval_judge.analyze_query("How do neural networks work?")

        # Verify the result
        assert result["complexity"] == "moderate"
        assert result["parameters"]["k"] == 8
        assert result["parameters"]["threshold"] == 0.5
        assert result["parameters"]["reranking"] is True
        assert "justification" in result

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "analyze the query complexity" in prompt.lower()

    @pytest.mark.asyncio
    async def test_evaluate_chunks(self, retrieval_judge, mock_ollama_client, sample_chunks):
        """Test evaluate_chunks method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": json.dumps({
                "relevance_scores": {
                    "1": 0.9,
                    "2": 0.8,
                    "3": 0.4
                },
                "needs_refinement": False,
                "justification": "The first two chunks are highly relevant to neural networks."
            })
        }

        # Call the method
        result = await retrieval_judge.evaluate_chunks("How do neural networks work?", sample_chunks)

        # Verify the result
        assert "relevance_scores" in result
        assert "chunk1" in result["relevance_scores"]
        assert result["relevance_scores"]["chunk1"] == 0.9
        assert result["relevance_scores"]["chunk2"] == 0.8
        assert result["relevance_scores"]["chunk3"] == 0.4
        assert result["needs_refinement"] is False
        assert "justification" in result

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "evaluate the relevance" in prompt.lower()
        assert "ai_basics.md" in prompt  # Sample chunk metadata

    @pytest.mark.asyncio
    async def test_refine_query(self, retrieval_judge, mock_ollama_client, sample_chunks):
        """Test refine_query method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": "How do neural networks function in deep learning algorithms?"
        }

        # Call the method
        result = await retrieval_judge.refine_query("How do neural networks work?", sample_chunks)

        # Verify the result
        assert result == "How do neural networks function in deep learning algorithms?"

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "refine the user's query" in prompt.lower()

    @pytest.mark.asyncio
    async def test_optimize_context(self, retrieval_judge, mock_ollama_client, sample_chunks):
        """Test optimize_context method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": json.dumps({
                "optimized_order": [2, 1],  # Chunk indices (1-based)
                "excluded_chunks": [3],  # Exclude chunk3
                "justification": "Ordered chunks for logical flow and excluded less relevant chunk."
            })
        }

        # Call the method
        result = await retrieval_judge.optimize_context("How do neural networks work?", sample_chunks)

        # Verify the result
        assert len(result) == 2
        assert result[0]["chunk_id"] == "chunk2"  # First in optimized order
        assert result[1]["chunk_id"] == "chunk1"  # Second in optimized order
        # chunk3 should be excluded

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "optimize the assembly" in prompt.lower()

    @pytest.mark.asyncio
    async def test_analyze_query_error_handling(self, retrieval_judge, mock_ollama_client):
        """Test error handling in analyze_query method"""
        # Mock the LLM response with invalid JSON
        mock_ollama_client.generate.return_value = {
            "response": "This is not valid JSON"
        }

        # Call the method
        result = await retrieval_judge.analyze_query("How do neural networks work?")

        # Verify default values are returned
        assert result["complexity"] == "moderate"
        assert result["parameters"]["k"] == 10
        assert result["parameters"]["threshold"] == 0.4
        assert result["parameters"]["reranking"] is True
        assert "Failed to parse" in result["justification"]

    @pytest.mark.asyncio
    async def test_extract_chunks_sample(self, retrieval_judge, sample_chunks):
        """Test _extract_chunks_sample method"""
        # Add more chunks to the sample
        extended_chunks = sample_chunks + [
            {
                "chunk_id": f"chunk{i}",
                "content": f"This is sample chunk {i} with some content that should be truncated if too long.",
                "metadata": {
                    "document_id": f"doc{i//2}",
                    "filename": f"file{i}.md",
                    "tags": "tag1,tag2",
                    "folder": "/folder"
                },
                "distance": 0.1 * i
            }
            for i in range(4, 10)
        ]

        # Call the method with max_chunks=3 and a small max_length
        result = retrieval_judge._extract_chunks_sample(extended_chunks, max_chunks=3, max_length=100)

        # Verify the result
        assert len(result) == 3  # Should only return 3 chunks
        assert all("chunk_id" in chunk for chunk in result)
        
        # Verify chunks are sorted by distance (ascending)
        distances = [chunk.get("distance", 1.0) for chunk in result]
        assert all(distances[i] <= distances[i+1] for i in range(len(distances)-1))
        
        # Verify content is truncated
        total_content_length = sum(len(chunk.get("content", "")) for chunk in result)
        assert total_content_length <= 100

================
File: tests/unit/test_schema_inspector.py
================
"""
Unit tests for the schema inspector
"""
import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch

from app.db.schema_inspector import SchemaInspector
from app.rag.tools.postgresql_tool import PostgreSQLTool

@pytest.fixture
def schema_inspector():
    """Create a schema inspector instance for testing"""
    return SchemaInspector()

@pytest.fixture
def postgresql_tool():
    """Create a PostgreSQL tool instance for testing"""
    return PostgreSQLTool()

@pytest.mark.asyncio
async def test_get_schemas(schema_inspector):
    """Test getting schemas"""
    # Mock the connection manager
    with patch('app.db.schema_inspector.connection_manager') as mock_conn_manager:
        # Setup mock connection
        mock_conn = AsyncMock()
        mock_conn.fetch.return_value = [
            {"schema_name": "public", "owner": "postgres", "description": "standard public schema"},
            {"schema_name": "app", "owner": "app_user", "description": "application schema"}
        ]
        
        # Setup connection manager mocks
        mock_conn_manager.get_connection_type.return_value = 'postgres'
        mock_conn_manager.get_postgres_connection.return_value = mock_conn
        mock_conn_manager.release_postgres_connection = AsyncMock()
        
        # Call the method
        result = await schema_inspector.get_schemas("test_conn_id")
        
        # Verify results
        assert len(result) == 2
        assert result[0]["schema_name"] == "public"
        assert result[1]["schema_name"] == "app"
        
        # Verify mocks were called correctly
        mock_conn_manager.get_connection_type.assert_called_once_with("test_conn_id")
        mock_conn_manager.get_postgres_connection.assert_called_once_with("test_conn_id")
        mock_conn.fetch.assert_called_once()
        mock_conn_manager.release_postgres_connection.assert_called_once_with("test_conn_id", mock_conn)

@pytest.mark.asyncio
async def test_get_tables(schema_inspector):
    """Test getting tables"""
    # Mock the connection manager
    with patch('app.db.schema_inspector.connection_manager') as mock_conn_manager:
        # Setup mock connection
        mock_conn = AsyncMock()
        mock_conn.fetch.return_value = [
            {
                "table_name": "users", 
                "owner": "postgres", 
                "description": "User accounts",
                "row_estimate": 1000,
                "total_size": "1024 kB",
                "type": "table"
            },
            {
                "table_name": "documents", 
                "owner": "postgres", 
                "description": "Document storage",
                "row_estimate": 5000,
                "total_size": "8192 kB",
                "type": "table"
            }
        ]
        mock_conn.fetchval.return_value = 1042  # Exact count for first table
        
        # Setup connection manager mocks
        mock_conn_manager.get_connection_type.return_value = 'postgres'
        mock_conn_manager.get_postgres_connection.return_value = mock_conn
        mock_conn_manager.release_postgres_connection = AsyncMock()
        
        # Call the method
        result = await schema_inspector.get_tables("test_conn_id", "public")
        
        # Verify results
        assert len(result) == 2
        assert result[0]["table_name"] == "users"
        assert result[1]["table_name"] == "documents"
        assert result[0]["exact_row_count"] == 1042
        
        # Verify mocks were called correctly
        mock_conn_manager.get_connection_type.assert_called_once_with("test_conn_id")
        mock_conn_manager.get_postgres_connection.assert_called_once_with("test_conn_id")
        assert mock_conn.fetch.call_count == 1
        assert mock_conn.fetchval.call_count == 1
        mock_conn_manager.release_postgres_connection.assert_called_once_with("test_conn_id", mock_conn)

@pytest.mark.asyncio
async def test_postgresql_tool_get_schemas(postgresql_tool):
    """Test PostgreSQL tool get_schemas operation"""
    # Mock the schema inspector
    with patch('app.rag.tools.postgresql_tool.schema_inspector') as mock_schema_inspector:
        # Setup mock schema inspector
        mock_schema_inspector.get_schemas = AsyncMock()
        mock_schema_inspector.get_schemas.return_value = [
            {"schema_name": "public", "owner": "postgres", "description": "standard public schema"},
            {"schema_name": "app", "owner": "app_user", "description": "application schema"}
        ]
        
        # Mock connection manager
        with patch('app.rag.tools.postgresql_tool.connection_manager') as mock_conn_manager:
            mock_conn_manager.get_connection_type.return_value = 'postgres'
            
            # Call the method
            result = await postgresql_tool.execute({
                "operation": "get_schemas",
                "connection_id": "test_conn_id"
            })
            
            # Verify results
            assert "schemas" in result
            assert len(result["schemas"]) == 2
            assert result["schemas"][0]["schema_name"] == "public"
            assert result["schemas"][1]["schema_name"] == "app"
            assert "execution_time" in result
            
            # Verify mocks were called correctly
            mock_conn_manager.get_connection_type.assert_called_once_with("test_conn_id")
            mock_schema_inspector.get_schemas.assert_called_once_with("test_conn_id")

@pytest.mark.asyncio
async def test_postgresql_tool_explain_query(postgresql_tool):
    """Test PostgreSQL tool explain_query operation"""
    # Mock the connection manager
    with patch('app.rag.tools.postgresql_tool.connection_manager') as mock_conn_manager:
        # Setup mock connection
        mock_conn = AsyncMock()
        mock_conn.fetch.return_value = [
            ("Seq Scan on users  (cost=0.00..25.88 rows=6 width=90) (actual time=0.019..0.021 rows=3 loops=1)",),
            ("  Filter: ((email)::text ~~ '%example.com'::text)",),
            ("  Rows Removed by Filter: 7",),
            ("Planning Time: 0.066 ms",),
            ("Execution Time: 0.048 ms",)
        ]
        
        # Setup connection manager mocks
        mock_conn_manager.get_connection_type.return_value = 'postgres'
        mock_conn_manager.get_postgres_connection.return_value = mock_conn
        mock_conn_manager.release_postgres_connection = AsyncMock()
        
        # Call the method
        result = await postgresql_tool.execute({
            "operation": "explain_query",
            "connection_id": "test_conn_id",
            "query": "SELECT * FROM users WHERE email LIKE '%example.com'",
            "explain_type": "analyze"
        })
        
        # Verify results
        assert "query" in result
        assert "plan_text" in result
        assert result["query"] == "SELECT * FROM users WHERE email LIKE '%example.com'"
        assert "Seq Scan on users" in result["plan_text"]
        assert "execution_time" in result
        
        # Verify mocks were called correctly
        mock_conn_manager.get_connection_type.assert_called_once_with("test_conn_id")
        mock_conn_manager.get_postgres_connection.assert_called_once_with("test_conn_id")
        mock_conn.fetch.assert_called_once()
        mock_conn_manager.release_postgres_connection.assert_called_once_with("test_conn_id", mock_conn)

@pytest.mark.asyncio
async def test_postgresql_tool_vector_search(postgresql_tool):
    """Test PostgreSQL tool vector_search operation"""
    # Mock the schema inspector and connection manager
    with patch('app.rag.tools.postgresql_tool.schema_inspector') as mock_schema_inspector, \
         patch('app.rag.tools.postgresql_tool.connection_manager') as mock_conn_manager:
        
        # Setup mock schema inspector
        mock_schema_inspector.get_pgvector_info = AsyncMock()
        mock_schema_inspector.get_pgvector_info.return_value = {
            "installed": True,
            "version": "0.5.0"
        }
        
        # Setup mock connection
        mock_conn = AsyncMock()
        mock_conn.fetch.return_value = [
            {"id": 1, "text": "Sample text 1", "distance": 0.15},
            {"id": 2, "text": "Sample text 2", "distance": 0.25},
            {"id": 3, "text": "Sample text 3", "distance": 0.35}
        ]
        
        # Setup connection manager mocks
        mock_conn_manager.get_connection_type.return_value = 'postgres'
        mock_conn_manager.get_postgres_connection.return_value = mock_conn
        mock_conn_manager.release_postgres_connection = AsyncMock()
        
        # Call the method
        result = await postgresql_tool.execute({
            "operation": "vector_search",
            "connection_id": "test_conn_id",
            "table_name": "embeddings",
            "column_name": "embedding",
            "vector": [0.1, 0.2, 0.3, 0.4, 0.5],
            "distance_type": "cosine",
            "limit": 5
        })
        
        # Verify results
        assert "results" in result
        assert len(result["results"]) == 3
        assert result["results"][0]["id"] == 1
        assert result["results"][0]["text"] == "Sample text 1"
        assert result["results"][0]["distance"] == 0.15
        assert result["count"] == 3
        assert result["distance_type"] == "cosine"
        assert "execution_time" in result
        
        # Verify mocks were called correctly
        mock_conn_manager.get_connection_type.assert_called_once_with("test_conn_id")
        mock_schema_inspector.get_pgvector_info.assert_called_once_with("test_conn_id")
        mock_conn_manager.get_postgres_connection.assert_called_once_with("test_conn_id")
        mock_conn.fetch.assert_called_once()
        mock_conn_manager.release_postgres_connection.assert_called_once_with("test_conn_id", mock_conn)

================
File: tests/unit/test_security_utils.py
================
#!/usr/bin/env python3
"""
Unit tests for security utilities in app/core/security.py
"""

import pytest
import time
from datetime import datetime, timedelta
from jose import jwt, JWTError
from uuid import uuid4

from app.core.security import (
    verify_password,
    get_password_hash,
    create_access_token,
    create_refresh_token,
    decode_token,
    verify_refresh_token
)
from app.core.config import SETTINGS


class TestPasswordUtils:
    """Tests for password hashing and verification functions"""
    
    def test_password_hash_and_verify(self):
        """Test that password hashing and verification work correctly"""
        # Test with a simple password
        password = "testpassword123"
        hashed = get_password_hash(password)
        
        # Verify the hash is not the plain password
        assert hashed != password
        
        # Verify the password against the hash
        assert verify_password(password, hashed) is True
        
        # Verify incorrect password fails
        assert verify_password("wrongpassword", hashed) is False
    
    def test_password_hash_different_each_time(self):
        """Test that password hashing produces different hashes each time"""
        password = "testpassword123"
        hash1 = get_password_hash(password)
        hash2 = get_password_hash(password)
        
        # Hashes should be different due to salt
        assert hash1 != hash2
        
        # But both should verify the password
        assert verify_password(password, hash1) is True
        assert verify_password(password, hash2) is True


class TestJWTFunctions:
    """Tests for JWT token creation and validation functions"""
    
    def test_create_access_token(self):
        """Test creating an access token"""
        # Create test data
        user_id = str(uuid4())
        username = "testuser"
        token_data = {
            "sub": username,
            "user_id": user_id,
            "aud": SETTINGS.jwt_audience,
            "iss": SETTINGS.jwt_issuer,
            "jti": str(uuid4())
        }
        
        # Create token with default expiry
        token = create_access_token(data=token_data)
        
        # Decode and verify token
        payload = decode_token(token)
        
        # Check claims
        assert payload["sub"] == username
        assert payload["user_id"] == user_id
        assert payload["token_type"] == "access"
        assert "exp" in payload
        assert "iat" in payload
        
        # Check expiry is in the future
        now = datetime.utcnow().timestamp()
        assert payload["exp"] > now
        
        # Check expiry is set correctly (within a small margin of error)
        expected_exp = now + SETTINGS.access_token_expire_minutes * 60
        assert abs(payload["exp"] - expected_exp) < 10  # Within 10 seconds
    
    def test_create_access_token_with_custom_expiry(self):
        """Test creating an access token with custom expiry"""
        # Create test data
        user_id = str(uuid4())
        username = "testuser"
        token_data = {
            "sub": username,
            "user_id": user_id
        }
        
        # Create token with custom expiry (30 minutes)
        custom_expiry = timedelta(minutes=30)
        token = create_access_token(data=token_data, expires_delta=custom_expiry)
        
        # Decode and verify token
        payload = decode_token(token)
        
        # Check expiry is set correctly (within a small margin of error)
        now = datetime.utcnow().timestamp()
        expected_exp = now + 30 * 60
        assert abs(payload["exp"] - expected_exp) < 10  # Within 10 seconds
    
    def test_create_refresh_token(self):
        """Test creating a refresh token"""
        # Create test data
        user_id = str(uuid4())
        username = "testuser"
        token_data = {
            "sub": username,
            "user_id": user_id,
            "aud": SETTINGS.jwt_audience,
            "iss": SETTINGS.jwt_issuer,
            "jti": str(uuid4())
        }
        
        # Create refresh token
        token = create_refresh_token(data=token_data)
        
        # Decode and verify token
        payload = decode_token(token)
        
        # Check claims
        assert payload["sub"] == username
        assert payload["user_id"] == user_id
        assert payload["token_type"] == "refresh"
        assert "exp" in payload
        assert "iat" in payload
        
        # Check expiry is in the future
        now = datetime.utcnow().timestamp()
        assert payload["exp"] > now
        
        # Check expiry is set correctly (within a small margin of error)
        expected_exp = now + 7 * 24 * 60 * 60  # 7 days in seconds
        assert abs(payload["exp"] - expected_exp) < 10  # Within 10 seconds
    
    def test_verify_refresh_token(self):
        """Test verifying a refresh token"""
        # Create test data
        user_id = str(uuid4())
        username = "testuser"
        token_data = {
            "sub": username,
            "user_id": user_id,
            "aud": SETTINGS.jwt_audience,
            "iss": SETTINGS.jwt_issuer,
            "jti": str(uuid4())
        }
        
        # Create refresh token
        token = create_refresh_token(data=token_data)
        
        # Verify refresh token
        payload = verify_refresh_token(token)
        
        # Check payload is returned
        assert payload is not None
        assert payload["sub"] == username
        assert payload["user_id"] == user_id
        assert payload["token_type"] == "refresh"
    
    def test_verify_refresh_token_with_access_token(self):
        """Test verifying an access token as a refresh token (should fail)"""
        # Create test data
        user_id = str(uuid4())
        username = "testuser"
        token_data = {
            "sub": username,
            "user_id": user_id
        }
        
        # Create access token
        token = create_access_token(data=token_data)
        
        # Try to verify as refresh token
        payload = verify_refresh_token(token)
        
        # Should fail (return None)
        assert payload is None
    
    def test_decode_token_with_invalid_signature(self):
        """Test decoding a token with invalid signature"""
        # Create test data
        user_id = str(uuid4())
        username = "testuser"
        token_data = {
            "sub": username,
            "user_id": user_id
        }
        
        # Create token
        token = create_access_token(data=token_data)
        
        # Tamper with the token (change the last character)
        tampered_token = token[:-1] + ('X' if token[-1] != 'X' else 'Y')
        
        # Try to decode tampered token
        with pytest.raises(JWTError):
            decode_token(tampered_token)
    
    def test_decode_token_with_expired_token(self):
        """Test decoding an expired token"""
        # Create test data
        user_id = str(uuid4())
        username = "testuser"
        token_data = {
            "sub": username,
            "user_id": user_id
        }
        
        # Create token that expires immediately
        token = create_access_token(data=token_data, expires_delta=timedelta(seconds=1))
        
        # Wait for token to expire
        time.sleep(2)
        
        # Try to decode expired token
        with pytest.raises(JWTError):
            decode_token(token)
    
    def test_decode_token_with_malformed_token(self):
        """Test decoding a malformed token"""
        # Try to decode a malformed token
        with pytest.raises(JWTError):
            decode_token("not.a.valid.token")
        
        # Try to decode an empty token
        with pytest.raises(JWTError):
            decode_token("")


if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/unit/test_semantic_chunker.py
================
"""
Unit tests for the Semantic Chunker
"""
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
import json

from app.rag.chunkers.semantic_chunker import SemanticChunker
from langchain.schema import Document as LangchainDocument

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        [500, 1000, 1500]
        """
    }
    return client

@pytest.fixture
def sample_text():
    """Create a sample text for testing"""
    return """
    This is the first section of the document. It contains information about the introduction.
    The introduction sets the stage for the rest of the document and provides context.
    
    This is the second section of the document. It contains information about the main topic.
    The main topic is discussed in detail with examples and explanations.
    
    This is the third section of the document. It contains information about the conclusion.
    The conclusion summarizes the main points and provides next steps.
    """

@pytest.mark.asyncio
async def test_semantic_chunker_split_text_async(mock_ollama_client, sample_text):
    """Test that the SemanticChunker correctly splits text asynchronously"""
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client)
    
    # Mock the _identify_semantic_boundaries method to return predictable results
    async def mock_identify_boundaries(text):
        # Return a simple chunk
        return [text]
    
    # Patch the _identify_semantic_boundaries method
    with patch.object(chunker, '_identify_semantic_boundaries', side_effect=mock_identify_boundaries):
        # Test async splitting
        chunks = await chunker.split_text_async(sample_text)
        
        # Verify result
        assert len(chunks) == 1

@pytest.mark.asyncio
async def test_semantic_chunker_caching(mock_ollama_client, sample_text):
    """Test that the SemanticChunker caches results"""
    # For this test, we need to use a sample text that's longer than chunk_size
    long_sample = sample_text * 10  # Make it long enough to trigger chunking
    
    # Create semantic chunker with mock client and caching enabled
    chunker = SemanticChunker(ollama_client=mock_ollama_client, cache_enabled=True, chunk_size=500, chunk_overlap=50)
    
    # Create a simple implementation for _semantic_chunking_async that we can track
    original_method = chunker._semantic_chunking_async
    call_count = 0
    
    async def tracked_method(text):
        nonlocal call_count
        call_count += 1
        # Just return a simple chunk for testing
        return ["Test chunk 1", "Test chunk 2"]
    
    # Replace the method
    chunker._semantic_chunking_async = tracked_method
    
    try:
        # First call should use our tracked method
        chunks1 = await chunker.split_text_async(long_sample)
        
        # Second call should use the cache
        chunks2 = await chunker.split_text_async(long_sample)
        
        # Verify results are the same
        assert chunks1 == chunks2
        
        # Verify the method was called only once
        assert call_count == 1
    finally:
        # Restore the original method
        chunker._semantic_chunking_async = original_method

@pytest.mark.asyncio
async def test_semantic_chunker_short_text(mock_ollama_client):
    """Test that the SemanticChunker handles short text correctly"""
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client, chunk_size=1000)
    
    # Short text that doesn't need chunking
    short_text = "This is a short text that doesn't need chunking."
    
    # Test async splitting
    chunks = await chunker.split_text_async(short_text)
    
    # Verify result
    assert len(chunks) == 1
    assert chunks[0] == short_text
    
    # Verify LLM was not called
    assert mock_ollama_client.generate.call_count == 0

@pytest.mark.asyncio
async def test_semantic_chunker_error_handling(mock_ollama_client, sample_text):
    """Test that the SemanticChunker handles errors gracefully"""
    # For this test, we need to use a sample text that's longer than chunk_size
    long_sample = sample_text * 10  # Make it long enough to trigger chunking
    
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client, chunk_size=500, chunk_overlap=50)
    
    # Save original methods
    original_semantic_chunking = chunker._semantic_chunking_async
    original_fallback = chunker._fallback_chunking
    
    # Create a method that directly calls _fallback_chunking to bypass the normal flow
    async def error_method(text):
        # This will force the use of fallback chunking
        return chunker._fallback_chunking(text)
    
    # Create a predictable fallback method
    def mock_fallback(text):
        return ["Fallback chunk 1", "Fallback chunk 2"]
    
    # Replace the methods
    chunker._semantic_chunking_async = error_method
    chunker._fallback_chunking = mock_fallback
    
    try:
        # Test async splitting with our modified methods
        chunks = await chunker.split_text_async(long_sample)
        
        # Verify fallback to simple chunking
        assert len(chunks) == 2
        assert chunks[0] == "Fallback chunk 1"
        assert chunks[1] == "Fallback chunk 2"
    finally:
        # Restore the original methods
        chunker._semantic_chunking_async = original_semantic_chunking
        chunker._fallback_chunking = original_fallback

@pytest.mark.asyncio
async def test_semantic_chunker_apply_overlap(mock_ollama_client):
    """Test that the SemanticChunker applies overlap correctly"""
    # Create semantic chunker with mock client and significant overlap
    chunker = SemanticChunker(ollama_client=mock_ollama_client, chunk_overlap=10)
    
    # Create test chunks with enough length for overlap
    original_chunks = [
        "This is the first chunk of text that is long enough for overlap.",
        "This is the second chunk of text that is also long enough.",
        "This is the third chunk of text with sufficient length."
    ]
    
    # Apply overlap
    overlapped_chunks = chunker._apply_overlap(original_chunks)
    
    # Verify result
    assert len(overlapped_chunks) == 3
    assert overlapped_chunks[0] == original_chunks[0]
    assert overlapped_chunks[1].startswith(original_chunks[0][-10:])
    assert overlapped_chunks[2].startswith(original_chunks[1][-10:])

@pytest.mark.asyncio
async def test_semantic_chunker_integration_with_langchain(mock_ollama_client, sample_text):
    """Test that the SemanticChunker works with Langchain documents"""
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client)
    
    # Create Langchain document
    doc = LangchainDocument(page_content=sample_text, metadata={"source": "test"})
    
    # Split document
    split_docs = chunker.split_documents([doc])
    
    # Verify result
    assert len(split_docs) > 0
    assert all(isinstance(d, LangchainDocument) for d in split_docs)
    assert all("source" in d.metadata for d in split_docs)

@pytest.mark.asyncio
async def test_semantic_chunker_long_text_processing(mock_ollama_client):
    """Test that the SemanticChunker handles long text correctly"""
    # Create a long text that exceeds the max_llm_context_length
    long_text = "This is section " + ". ".join([f"Part {i}" for i in range(1000)])
    
    # Create semantic chunker with mock client and small max context
    chunker = SemanticChunker(
        ollama_client=mock_ollama_client,
        max_llm_context_length=2000
    )
    
    # Mock the _process_long_text method to return predictable results
    async def mock_process_long_text(text):
        # Return a chunk for each 1000 characters
        chunks = []
        for i in range(0, len(text), 1000):
            end = min(i + 1000, len(text))
            chunks.append(text[i:end])
        return chunks
    
    # Patch the method
    with patch.object(chunker, '_process_long_text', side_effect=mock_process_long_text):
        # Test async splitting
        chunks = await chunker.split_text_async(long_text)
        
        # Verify result
        assert len(chunks) > 1
        assert len(chunks) == (len(long_text) + 999) // 1000  # Ceiling division

================
File: tests/unit/test_tools.py
================
"""
Unit tests for the Tool interface and implementations
"""
import pytest
import asyncio
import math
from unittest.mock import MagicMock, AsyncMock

from app.rag.tools import Tool, ToolRegistry, RAGTool, CalculatorTool, DatabaseTool


class TestTool(Tool):
    """Test implementation of the Tool abstract base class"""
    
    def __init__(self, name="test_tool", description="Test tool"):
        super().__init__(name=name, description=description)
    
    async def execute(self, input_data):
        return {"result": f"Executed {self.name} with input: {input_data}"}
    
    def get_input_schema(self):
        return {
            "type": "object",
            "properties": {
                "test_param": {
                    "type": "string",
                    "description": "Test parameter"
                }
            }
        }
    
    def get_output_schema(self):
        return {
            "type": "object",
            "properties": {
                "result": {
                    "type": "string",
                    "description": "Test result"
                }
            }
        }
    
    def get_examples(self):
        return [
            {
                "input": {"test_param": "test_value"},
                "output": {"result": "Executed test_tool with input: {'test_param': 'test_value'}"}
            }
        ]


class TestToolRegistry:
    """Tests for the ToolRegistry class"""
    
    def test_register_and_get_tool(self):
        """Test registering and retrieving a tool"""
        registry = ToolRegistry()
        tool = TestTool()
        
        # Register tool
        registry.register_tool(tool)
        
        # Get tool
        retrieved_tool = registry.get_tool("test_tool")
        
        assert retrieved_tool is tool
        assert retrieved_tool.name == "test_tool"
        assert retrieved_tool.get_description() == "Test tool"
    
    def test_list_tools(self):
        """Test listing registered tools"""
        registry = ToolRegistry()
        tool1 = TestTool(name="tool1", description="Tool 1")
        tool2 = TestTool(name="tool2", description="Tool 2")
        
        # Register tools
        registry.register_tool(tool1)
        registry.register_tool(tool2)
        
        # List tools
        tools = registry.list_tools()
        
        assert len(tools) == 2
        assert tools[0]["name"] == "tool1"
        assert tools[0]["description"] == "Tool 1"
        assert tools[1]["name"] == "tool2"
        assert tools[1]["description"] == "Tool 2"
    
    def test_get_tool_examples(self):
        """Test getting examples for a tool"""
        registry = ToolRegistry()
        tool = TestTool()
        
        # Register tool
        registry.register_tool(tool)
        
        # Get examples
        examples = registry.get_tool_examples("test_tool")
        
        assert len(examples) == 1
        assert examples[0]["input"] == {"test_param": "test_value"}
        assert examples[0]["output"] == {"result": "Executed test_tool with input: {'test_param': 'test_value'}"}
    
    def test_get_nonexistent_tool(self):
        """Test getting a tool that doesn't exist"""
        registry = ToolRegistry()
        
        # Get nonexistent tool
        tool = registry.get_tool("nonexistent")
        
        assert tool is None


class TestRAGTool:
    """Tests for the RAGTool implementation"""
    
    @pytest.mark.asyncio
    async def test_rag_tool_execute(self):
        """Test executing the RAG tool"""
        # Create mock RAG engine
        mock_rag_engine = AsyncMock()
        mock_rag_engine.retrieve.return_value = [
            {
                "content": "Test content",
                "metadata": {"document_id": "doc123"},
                "score": 0.95
            }
        ]
        
        # Create RAG tool
        rag_tool = RAGTool(rag_engine=mock_rag_engine)
        
        # Execute tool
        result = await rag_tool.execute({
            "query": "Test query",
            "top_k": 3
        })
        
        # Check result
        assert "chunks" in result
        assert len(result["chunks"]) == 1
        assert result["chunks"][0]["content"] == "Test content"
        assert result["chunks"][0]["metadata"] == {"document_id": "doc123"}
        assert result["chunks"][0]["score"] == 0.95
        assert "sources" in result
        assert "doc123" in result["sources"]
        assert "execution_time" in result
        
        # Check RAG engine was called correctly
        mock_rag_engine.retrieve.assert_called_once_with(
            query="Test query",
            top_k=3,
            filters={}
        )
    
    def test_rag_tool_schemas(self):
        """Test RAG tool schemas"""
        # Create mock RAG engine
        mock_rag_engine = MagicMock()
        
        # Create RAG tool
        rag_tool = RAGTool(rag_engine=mock_rag_engine)
        
        # Check input schema
        input_schema = rag_tool.get_input_schema()
        assert input_schema["type"] == "object"
        assert "query" in input_schema["properties"]
        assert "top_k" in input_schema["properties"]
        assert "filters" in input_schema["properties"]
        assert input_schema["required"] == ["query"]
        
        # Check output schema
        output_schema = rag_tool.get_output_schema()
        assert output_schema["type"] == "object"
        assert "chunks" in output_schema["properties"]
        assert "sources" in output_schema["properties"]
        assert "execution_time" in output_schema["properties"]
        
        # Check examples
        examples = rag_tool.get_examples()
        assert len(examples) > 0
        assert "input" in examples[0]
        assert "output" in examples[0]


class TestCalculatorTool:
    """Tests for the CalculatorTool implementation"""
    
    @pytest.mark.asyncio
    async def test_basic_arithmetic(self):
        """Test basic arithmetic operations"""
        calculator = CalculatorTool()
        
        # Test addition
        result = await calculator.execute({"expression": "2 + 3"})
        assert "result" in result
        assert result["result"] == 5
        
        # Test subtraction
        result = await calculator.execute({"expression": "10 - 4"})
        assert result["result"] == 6
        
        # Test multiplication
        result = await calculator.execute({"expression": "3 * 5"})
        assert result["result"] == 15
        
        # Test division
        result = await calculator.execute({"expression": "20 / 4"})
        assert result["result"] == 5
        
        # Test order of operations
        result = await calculator.execute({"expression": "2 + 3 * 4"})
        assert result["result"] == 14
        
        # Test parentheses
        result = await calculator.execute({"expression": "(2 + 3) * 4"})
        assert result["result"] == 20
    
    @pytest.mark.asyncio
    async def test_math_functions(self):
        """Test mathematical functions"""
        calculator = CalculatorTool()
        
        # Test square root
        result = await calculator.execute({"expression": "sqrt(16)"})
        assert result["result"] == 4
        
        # Test power
        result = await calculator.execute({"expression": "pow(2, 3)"})
        assert result["result"] == 8
        
        # Test trigonometric functions
        result = await calculator.execute({"expression": "sin(radians(30))"})
        assert math.isclose(result["result"], 0.5, abs_tol=1e-10)
        
        result = await calculator.execute({"expression": "cos(radians(60))"})
        assert math.isclose(result["result"], 0.5, abs_tol=1e-10)
        
        # Test logarithmic functions
        result = await calculator.execute({"expression": "log10(100)"})
        assert result["result"] == 2
    
    @pytest.mark.asyncio
    async def test_variables(self):
        """Test variable substitution"""
        calculator = CalculatorTool()
        
        # Test simple variable
        result = await calculator.execute({
            "expression": "x + 5",
            "variables": {"x": 10}
        })
        assert result["result"] == 15
        
        # Test multiple variables
        result = await calculator.execute({
            "expression": "x * y + z",
            "variables": {"x": 2, "y": 3, "z": 4}
        })
        assert result["result"] == 10
        
        # Test variables in functions
        result = await calculator.execute({
            "expression": "sqrt(x) + pow(y, 2)",
            "variables": {"x": 16, "y": 3}
        })
        assert result["result"] == 13
    
    @pytest.mark.asyncio
    async def test_unit_conversion(self):
        """Test unit conversions"""
        calculator = CalculatorTool()
        
        # Test length conversion
        result = await calculator.execute({
            "expression": "5",
            "unit_conversion": "km_to_miles"
        })
        assert math.isclose(result["result"], 3.10686, abs_tol=1e-5)
        assert "steps" in result
        assert len(result["steps"]) == 2
        
        # Test temperature conversion
        result = await calculator.execute({
            "expression": "100",
            "unit_conversion": "c_to_f"
        })
        assert result["result"] == 212.0
        assert "steps" in result
        assert len(result["steps"]) == 2
        
        # Test weight conversion
        result = await calculator.execute({
            "expression": "10",
            "unit_conversion": "kg_to_lb"
        })
        assert math.isclose(result["result"], 22.0462, abs_tol=1e-4)
    
    @pytest.mark.asyncio
    async def test_precision(self):
        """Test result precision"""
        calculator = CalculatorTool()
        
        # Test with precision
        result = await calculator.execute({
            "expression": "1 / 3",
            "precision": 2
        })
        assert result["result"] == 0.33
        assert "steps" in result
        assert len(result["steps"]) == 1
        
        # Test with precision and unit conversion
        result = await calculator.execute({
            "expression": "10",
            "unit_conversion": "km_to_miles",
            "precision": 3
        })
        assert result["result"] == 6.214
        assert "steps" in result
        assert len(result["steps"]) == 3  # Initial result, conversion, and rounding
    
    @pytest.mark.asyncio
    async def test_error_handling(self):
        """Test error handling"""
        calculator = CalculatorTool()
        
        # Test division by zero
        result = await calculator.execute({"expression": "1 / 0"})
        assert "error" in result
        
        # Test invalid expression
        result = await calculator.execute({"expression": "1 + * 2"})
        assert "error" in result
        
        # Test invalid function
        result = await calculator.execute({"expression": "invalid_func(10)"})
        assert "error" in result
        
        # Test missing required parameter
        result = await calculator.execute({})
        assert "error" in result
        
        # Test invalid unit conversion
        result = await calculator.execute({
            "expression": "10",
            "unit_conversion": "invalid_conversion"
        })
        assert "error" in result
    
    def test_calculator_schemas(self):
        """Test calculator tool schemas"""
        calculator = CalculatorTool()
        
        # Check input schema
        input_schema = calculator.get_input_schema()
        assert input_schema["type"] == "object"
        assert "expression" in input_schema["properties"]
        assert "variables" in input_schema["properties"]
        assert "precision" in input_schema["properties"]
        assert "unit_conversion" in input_schema["properties"]
        assert input_schema["required"] == ["expression"]
        
        # Check output schema
        output_schema = calculator.get_output_schema()
        assert output_schema["type"] == "object"
        assert "result" in output_schema["properties"]
        assert "steps" in output_schema["properties"]
        assert "execution_time" in output_schema["properties"]
        assert "error" in output_schema["properties"]
        
        # Check examples
        examples = calculator.get_examples()
        assert len(examples) > 0
        assert "input" in examples[0]
        assert "output" in examples[0]


class TestDatabaseTool:
    """Tests for the DatabaseTool implementation"""
    
    @pytest.mark.asyncio
    async def test_sqlite_query(self, tmp_path):
        """Test querying a SQLite database"""
        import sqlite3
        import os
        
        # Create a test database
        db_path = os.path.join(tmp_path, "test.db")
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Create a test table
        cursor.execute("""
            CREATE TABLE test_table (
                id INTEGER PRIMARY KEY,
                name TEXT,
                value REAL
            )
        """)
        
        # Insert test data
        test_data = [
            (1, "item1", 10.5),
            (2, "item2", 20.75),
            (3, "item3", 15.25)
        ]
        cursor.executemany("INSERT INTO test_table VALUES (?, ?, ?)", test_data)
        conn.commit()
        conn.close()
        
        # Create database tool
        db_tool = DatabaseTool(data_dir=str(tmp_path))
        
        # Test simple query
        result = await db_tool.execute({
            "query": "SELECT * FROM test_table",
            "source": "test.db"
        })
        
        # Check result
        assert "results" in result
        assert len(result["results"]) == 3
        assert result["results"][0]["id"] == 1
        assert result["results"][0]["name"] == "item1"
        assert result["results"][0]["value"] == 10.5
        assert "columns" in result
        assert result["columns"] == ["id", "name", "value"]
        assert "row_count" in result
        assert result["row_count"] == 3
        assert "execution_time" in result
        
        # Test query with filter
        result = await db_tool.execute({
            "query": "SELECT * FROM test_table WHERE value > ?",
            "source": "test.db",
            "params": [15.0]
        })
        
        # Check result
        assert len(result["results"]) == 2
        assert result["results"][0]["name"] == "item2"
        assert result["results"][1]["name"] == "item3"
        
        # Test query with limit
        result = await db_tool.execute({
            "query": "SELECT * FROM test_table ORDER BY value DESC",
            "source": "test.db",
            "limit": 1
        })
        
        # Check result
        assert len(result["results"]) == 1
        assert result["results"][0]["name"] == "item2"
    
    @pytest.mark.asyncio
    async def test_csv_query(self, tmp_path):
        """Test querying a CSV file"""
        import pandas as pd
        import os
        
        # Create a test CSV file
        csv_path = os.path.join(tmp_path, "test.csv")
        df = pd.DataFrame({
            "id": [1, 2, 3],
            "name": ["product1", "product2", "product3"],
            "price": [9.99, 19.99, 29.99],
            "category": ["A", "B", "A"]
        })
        df.to_csv(csv_path, index=False)
        
        # Create database tool
        db_tool = DatabaseTool(data_dir=str(tmp_path))
        
        # Test simple query
        result = await db_tool.execute({
            "query": "SELECT * FROM data",
            "source": "test.csv"
        })
        
        # Check result
        assert "results" in result
        assert len(result["results"]) == 3
        assert result["results"][0]["id"] == 1
        assert result["results"][0]["name"] == "product1"
        assert result["results"][0]["price"] == 9.99
        assert "columns" in result
        assert set(result["columns"]) == set(["id", "name", "price", "category"])
        
        # Test query with filter
        result = await db_tool.execute({
            "query": "SELECT * FROM data WHERE category = 'A'",
            "source": "test.csv"
        })
        
        # Check result
        assert len(result["results"]) == 2
        assert result["results"][0]["name"] == "product1"
        assert result["results"][1]["name"] == "product3"
        
        # Test aggregation query
        result = await db_tool.execute({
            "query": "SELECT category, AVG(price) as avg_price FROM data GROUP BY category",
            "source": "test.csv"
        })
        
        # Check result
        assert len(result["results"]) == 2
        assert result["results"][0]["category"] == "A"
        assert result["results"][1]["category"] == "B"
        assert abs(result["results"][0]["avg_price"] - 19.99) < 0.01
        assert abs(result["results"][1]["avg_price"] - 19.99) < 0.01
    
    @pytest.mark.asyncio
    async def test_json_query(self, tmp_path):
        """Test querying a JSON file"""
        import json
        import os
        
        # Create a test JSON file
        json_path = os.path.join(tmp_path, "test.json")
        data = [
            {"id": 1, "name": "user1", "email": "user1@example.com", "active": True},
            {"id": 2, "name": "user2", "email": "user2@example.com", "active": False},
            {"id": 3, "name": "user3", "email": "user3@example.com", "active": True}
        ]
        with open(json_path, 'w') as f:
            json.dump(data, f)
        
        # Create database tool
        db_tool = DatabaseTool(data_dir=str(tmp_path))
        
        # Test simple query
        result = await db_tool.execute({
            "query": "SELECT * FROM data",
            "source": "test.json"
        })
        
        # Check result
        assert "results" in result
        assert len(result["results"]) == 3
        assert result["results"][0]["id"] == 1
        assert result["results"][0]["name"] == "user1"
        assert result["results"][0]["email"] == "user1@example.com"
        assert result["results"][0]["active"] == 1  # SQLite converts boolean to integer
        
        # Test query with filter
        result = await db_tool.execute({
            "query": "SELECT * FROM data WHERE active = 1",
            "source": "test.json"
        })
        
        # Check result
        assert len(result["results"]) == 2
        assert result["results"][0]["name"] == "user1"
        assert result["results"][1]["name"] == "user3"
    
    @pytest.mark.asyncio
    async def test_error_handling(self):
        """Test error handling"""
        db_tool = DatabaseTool()
        
        # Test missing query
        result = await db_tool.execute({
            "source": "test.db"
        })
        assert "error" in result
        
        # Test missing source
        result = await db_tool.execute({
            "query": "SELECT * FROM test"
        })
        assert "error" in result
        
        # Test invalid source
        result = await db_tool.execute({
            "query": "SELECT * FROM test",
            "source": "nonexistent.db"
        })
        assert "error" in result
    
    def test_database_schemas(self):
        """Test database tool schemas"""
        db_tool = DatabaseTool()
        
        # Check input schema
        input_schema = db_tool.get_input_schema()
        assert input_schema["type"] == "object"
        assert "query" in input_schema["properties"]
        assert "source" in input_schema["properties"]
        assert "params" in input_schema["properties"]
        assert "limit" in input_schema["properties"]
        assert input_schema["required"] == ["query", "source"]
        
        # Check output schema
        output_schema = db_tool.get_output_schema()
        assert output_schema["type"] == "object"
        assert "results" in output_schema["properties"]
        assert "columns" in output_schema["properties"]
        assert "row_count" in output_schema["properties"]
        assert "execution_time" in output_schema["properties"]
        assert "error" in output_schema["properties"]
        
        # Check examples
        examples = db_tool.get_examples()
        assert len(examples) > 0
        assert "input" in examples[0]
        assert "output" in examples[0]

================
File: tests/utils/create_test_pdf.py
================
#!/usr/bin/env python3
"""
Utility script to create a PDF file from the technical specifications text file.
This is needed because the Metis RAG system supports PDF files, and we need to
test all supported file formats in our end-to-end test.

Requirements:
pip install reportlab
"""

import os
import sys
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.enums import TA_CENTER, TA_LEFT
from reportlab.lib import colors

def create_pdf_from_text(txt_file, pdf_file):
    """
    Create a PDF file from a text file, preserving formatting as much as possible.
    
    Args:
        txt_file: Path to the input text file
        pdf_file: Path to the output PDF file
    """
    print(f"Creating PDF from {txt_file}...")
    
    # Read the text file
    with open(txt_file, 'r') as f:
        content = f.read()
    
    # Split into lines and process
    lines = content.split('\n')
    
    # Create styles
    styles = getSampleStyleSheet()
    
    # Create a custom bullet point style
    bullet_style = styles['Normal'].clone('BulletPoint')
    bullet_style.leftIndent = 20
    bullet_style.spaceBefore = 2
    bullet_style.spaceAfter = 2
    
    # Create the PDF document
    doc = SimpleDocTemplate(
        pdf_file,
        pagesize=letter,
        rightMargin=72,
        leftMargin=72,
        topMargin=72,
        bottomMargin=72
    )
    
    # Container for document elements
    elements = []
    
    # Process line by line
    current_line_type = None
    for line in lines:
        # Skip empty lines
        if not line.strip():
            elements.append(Spacer(1, 6))
            continue
        
        # Process heading levels
        if line.startswith('# '):
            elements.append(Paragraph(line[2:], styles['Heading1']))
        elif line.startswith('## '):
            elements.append(Paragraph(line[3:], styles['Heading2']))
        elif line.startswith('### '):
            elements.append(Paragraph(line[4:], styles['Heading3']))
        # Process bullet points
        elif line.strip().startswith('- '):
            bullet_text = "• " + line.strip()[2:]
            elements.append(Paragraph(bullet_text, bullet_style))
        # Regular text
        else:
            elements.append(Paragraph(line, styles['Normal']))
    
    # Build the PDF
    doc.build(elements)
    
    print(f"PDF created successfully: {pdf_file}")

if __name__ == "__main__":
    # Ensure the utils directory exists
    os.makedirs("tests/utils", exist_ok=True)
    
    # Ensure the data/test_docs directory exists
    os.makedirs("data/test_docs", exist_ok=True)
    
    # Path to the input text file and output PDF file
    txt_file = "data/test_docs/smart_home_technical_specs.txt"
    pdf_file = "data/test_docs/smart_home_technical_specs.pdf"
    
    if not os.path.exists(txt_file):
        print(f"Error: Input file not found: {txt_file}")
        sys.exit(1)
    
    create_pdf_from_text(txt_file, pdf_file)

================
File: tests/utils/test_auth_helper.py
================
#!/usr/bin/env python3
"""
Authentication helper for Metis RAG end-to-end tests.
This module provides functions to authenticate with the Metis RAG API
and maintain session state between requests.

NOTE: This approach may encounter issues with event loops and async code.
For a more reliable approach, see scripts/test_api_directly.py and
tests/authentication_setup_guide.md.
"""

import os
import json
import logging
import requests
from typing import Dict, Optional, Tuple
from fastapi.testclient import TestClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_auth_helper")

def authenticate_with_session(base_url: str, username: str, password: str) -> Tuple[requests.Session, Optional[str]]:
    """
    Authenticate with the API using a requests.Session to maintain cookies and state.
    
    Args:
        base_url: Base URL of the API (e.g., http://localhost:8000)
        username: Username for authentication
        password: Password for authentication
        
    Returns:
        Tuple of (session, token) where session is a requests.Session object and token is the access token
    """
    session = requests.Session()
    
    # Try to authenticate
    try:
        logger.info(f"Authenticating with username: {username}")
        
        # Use form data for token endpoint
        login_response = session.post(
            f"{base_url}/api/auth/token",
            data={
                "username": username,
                "password": password,
                "grant_type": "password"
            }
        )
        
        if login_response.status_code == 200:
            token_data = login_response.json()
            access_token = token_data.get("access_token")
            
            if access_token:
                logger.info("Authentication successful")
                
                # Set the token in the session headers for subsequent requests
                session.headers.update({"Authorization": f"Bearer {access_token}"})
                
                # Also set the token in a cookie
                session.cookies.set("auth_token", access_token)
                
                return session, access_token
            else:
                logger.error("No access token in response")
        else:
            logger.error(f"Authentication failed: {login_response.status_code} - {login_response.text}")
            
            # Try to register a new test user if login fails
            register_response = session.post(
                f"{base_url}/api/auth/register",
                json={
                    "username": f"{username}_new",
                    "email": f"{username}_new@example.com",
                    "password": password,
                    "full_name": "Test User",
                    "is_active": True,
                    "is_admin": False
                }
            )
            
            if register_response.status_code == 200:
                logger.info(f"Registered new test user: {username}_new")
                
                # Try to login with the new user
                login_response = session.post(
                    f"{base_url}/api/auth/token",
                    data={
                        "username": f"{username}_new",
                        "password": password,
                        "grant_type": "password"
                    }
                )
                
                if login_response.status_code == 200:
                    token_data = login_response.json()
                    access_token = token_data.get("access_token")
                    
                    if access_token:
                        logger.info("Authentication successful with new user")
                        
                        # Set the token in the session headers for subsequent requests
                        session.headers.update({"Authorization": f"Bearer {access_token}"})
                        
                        # Also set the token in a cookie
                        session.cookies.set("auth_token", access_token)
                        
                        return session, access_token
                    else:
                        logger.error("No access token in response for new user")
                else:
                    logger.error(f"Login with new user failed: {login_response.status_code} - {login_response.text}")
            else:
                logger.error(f"Registration failed: {register_response.status_code} - {register_response.text}")
    
    except Exception as e:
        logger.error(f"Authentication error: {str(e)}")
    
    return session, None

def configure_test_client(app, username: str = "testuser", password: str = "testpassword") -> TestClient:
    """
    Configure a FastAPI TestClient with authentication.
    
    Args:
        app: FastAPI application
        username: Username for authentication
        password: Password for authentication
        
    Returns:
        Configured TestClient
    """
    # Create a TestClient that will maintain cookies between requests
    client = TestClient(app)
    
    # Try to authenticate
    try:
        logger.info(f"Authenticating TestClient with username: {username}")
        
        # Use form data for token endpoint
        login_response = client.post(
            "/api/auth/token",
            data={
                "username": username,
                "password": password,
                "grant_type": "password"
            }
        )
        
        if login_response.status_code == 200:
            token_data = login_response.json()
            access_token = token_data.get("access_token")
            
            if access_token:
                logger.info("TestClient authentication successful")
                
                # Set the token in the client's headers for subsequent requests
                client.headers["Authorization"] = f"Bearer {access_token}"
                
                # Save cookies to a file for debugging
                with open("cookies.txt", "w") as f:
                    f.write(str(client.cookies))
                
                return client
            else:
                logger.error("No access token in TestClient response")
        else:
            logger.error(f"TestClient authentication failed: {login_response.status_code} - {login_response.text}")
            
            # Try to register a new test user if login fails
            register_response = client.post(
                "/api/auth/register",
                json={
                    "username": f"{username}_new",
                    "email": f"{username}_new@example.com",
                    "password": password,
                    "full_name": "Test User",
                    "is_active": True,
                    "is_admin": False
                }
            )
            
            if register_response.status_code == 200:
                logger.info(f"Registered new test user: {username}_new")
                
                # Try to login with the new user
                login_response = client.post(
                    "/api/auth/token",
                    data={
                        "username": f"{username}_new",
                        "password": password,
                        "grant_type": "password"
                    }
                )
                
                if login_response.status_code == 200:
                    token_data = login_response.json()
                    access_token = token_data.get("access_token")
                    
                    if access_token:
                        logger.info("TestClient authentication successful with new user")
                        
                        # Set the token in the client's headers for subsequent requests
                        client.headers["Authorization"] = f"Bearer {access_token}"
                        
                        return client
                    else:
                        logger.error("No access token in TestClient response for new user")
                else:
                    logger.error(f"TestClient login with new user failed: {login_response.status_code} - {login_response.text}")
            else:
                logger.error(f"TestClient registration failed: {register_response.status_code} - {register_response.text}")
    
    except Exception as e:
        logger.error(f"TestClient authentication error: {str(e)}")
    
    return client

def verify_authentication(client: TestClient) -> bool:
    """
    Verify that the client is authenticated by making a request to a protected endpoint.
    
    Args:
        client: TestClient to verify
        
    Returns:
        True if authenticated, False otherwise
    """
    try:
        # Try to access a protected endpoint
        response = client.get("/api/auth/me")
        
        if response.status_code == 200:
            user_data = response.json()
            logger.info(f"Authenticated as user: {user_data.get('username')}")
            return True
        else:
            logger.error(f"Authentication verification failed: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        logger.error(f"Authentication verification error: {str(e)}")
        return False

if __name__ == "__main__":
    # Example usage
    from app.main import app
    
    # Configure the test client
    client = configure_test_client(app)
    
    # Verify authentication
    is_authenticated = verify_authentication(client)
    print(f"Is authenticated: {is_authenticated}")

================
File: tests/authentication_setup_guide.md
================
# Metis RAG Authentication Testing Guide

This guide provides solutions for authentication issues when testing the Metis RAG system.

## Authentication Issues Identified

1. **TestClient Authentication Issues**: The FastAPI TestClient has limitations when working with async code and authentication, particularly with event loops.

2. **Database Connection Issues**: When using TestClient with async database connections, event loop conflicts can occur.

3. **Token Handling**: The TestClient doesn't maintain cookies/session state properly between requests.

## Solutions

### Solution 1: Direct API Testing

The most reliable approach is to use direct API calls with the `requests` library instead of TestClient. This approach:

- Avoids event loop conflicts
- Properly maintains authentication state
- Works with the actual running API

We've implemented this solution in `scripts/test_api_directly.py`, which successfully:
- Authenticates with the API
- Uploads and processes documents
- Executes queries against the RAG system
- Retrieves meaningful responses with citations

### Solution 2: TestClient with Session Fixation

If you need to use TestClient for integration with pytest, you can:

1. Use direct requests to obtain an authentication token
2. Manually set the token in the TestClient headers
3. Verify authentication before proceeding with tests

```python
# Example of TestClient with manual token setting
import requests
from fastapi.testclient import TestClient
from app.main import app

# Get token using direct requests
def get_auth_token():
    response = requests.post(
        "http://localhost:8000/api/auth/token",
        data={
            "username": "testuser",
            "password": "testpassword",
            "grant_type": "password"
        }
    )
    return response.json().get("access_token")

# Configure TestClient with token
def get_authenticated_client():
    client = TestClient(app)
    token = get_auth_token()
    client.headers["Authorization"] = f"Bearer {token}"
    return client
```

### Solution 3: Separate Authentication Service

For more complex testing scenarios:

1. Create a separate authentication service that runs independently
2. Use this service to generate tokens for testing
3. Configure your tests to use these pre-generated tokens

## Test User Management

For reliable testing:

1. **Create dedicated test users** with known credentials
2. Use a **unique test user for each test run** to avoid conflicts
3. Clean up test users after testing if possible

## API Endpoint Verification

Our testing confirmed the following endpoints work correctly:

- `/api/auth/register` - User registration
- `/api/auth/token` - Token acquisition
- `/api/auth/me` - User verification
- `/api/documents/upload` - Document upload
- `/api/documents/process` - Document processing
- `/api/chat/query` - RAG queries

Some endpoints returned 500 errors:
- `/api/documents/{id}` - Document info retrieval
- `/api/documents/{id}` - Document deletion

These errors suggest potential bugs in the document management endpoints, but they don't affect the core RAG functionality.

## Query Results

Our testing confirmed that the RAG system correctly:
- Retrieves relevant information from uploaded documents
- Provides accurate answers to queries
- Includes citations to source documents

Example query: "What is the battery life of the motion sensor?"
Response: "Based on the provided documents, I found information about the motion sensor's battery life. According to [1], the Motion Sensor (Device ID: SH-MS100) has a Battery Life of 2 years."

## Conclusion

The Metis RAG system's core functionality (authentication, document processing, and querying) works correctly. The authentication issues in the test suite are related to the testing approach rather than the system itself.

By using direct API calls or properly configuring TestClient, you can successfully test the entire system end-to-end.

================
File: tests/authentication_testing_guide.md
================
# Metis RAG Authentication Testing Guide

This guide provides instructions for running the authentication and authorization tests for the Metis RAG application. These tests verify the security features implemented according to the [Authentication Implementation Plan](../docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md).

## Test Structure

The authentication tests are organized into three levels:

1. **Unit Tests**: Test individual functions in isolation
2. **Integration Tests**: Test interactions between components
3. **End-to-End Tests**: Test complete user flows and scenarios

### Unit Tests

Located in `tests/unit/`:

- `test_security_utils.py`: Tests for JWT token functions and password hashing utilities

### Integration Tests

Located in `tests/integration/`:

- `test_auth_endpoints.py`: Tests for authentication API endpoints (login, register, refresh, etc.)
- `test_permissions_db.py`: Tests for database-level permissions (Row Level Security)
- `test_permissions_vector.py`: Tests for vector database security (metadata filtering)

### End-to-End Tests

Located in `tests/e2e/`:

- `test_auth_flows.py`: Tests for complete authentication flows (register, login, access, refresh, logout)
- `test_permission_scenarios.py`: Tests for complex permission scenarios with multiple users, roles, and organizations

## Prerequisites

Before running the tests, ensure you have:

1. Set up the Metis RAG application according to the setup instructions
2. Created a test database (separate from your production database)
3. Installed the required testing dependencies:
   ```
   pip install pytest pytest-asyncio httpx
   ```

## Running the Tests

### Running All Tests

To run all authentication tests:

```bash
pytest tests/unit/test_security_utils.py tests/integration/test_auth_endpoints.py tests/integration/test_permissions_db.py tests/integration/test_permissions_vector.py tests/e2e/test_auth_flows.py tests/e2e/test_permission_scenarios.py -v
```

### Running Unit Tests Only

```bash
pytest tests/unit/test_security_utils.py -v
```

### Running Integration Tests Only

```bash
pytest tests/integration/test_auth_endpoints.py tests/integration/test_permissions_db.py tests/integration/test_permissions_vector.py -v
```

### Running End-to-End Tests Only

```bash
pytest tests/e2e/test_auth_flows.py tests/e2e/test_permission_scenarios.py -v
```

### Running Individual Test Files

```bash
# Example: Run only the security utils tests
pytest tests/unit/test_security_utils.py -v

# Example: Run only the auth endpoints tests
pytest tests/integration/test_auth_endpoints.py -v
```

### Running Specific Test Functions

```bash
# Example: Run only the password hash test
pytest tests/unit/test_security_utils.py::TestPasswordUtils::test_password_hash_and_verify -v

# Example: Run only the login test
pytest tests/integration/test_auth_endpoints.py::TestAuthEndpoints::test_login_success -v
```

## Test Environment Setup

The tests require a running instance of the Metis RAG application. You can set up a test environment using:

```bash
# Set environment variables for testing
export METIS_RAG_ENV=test
export METIS_RAG_DB_URL=sqlite:///test.db

# Run the application in test mode
python scripts/run_app.py
```

In a separate terminal, run the tests:

```bash
pytest tests/unit/test_security_utils.py -v
```

## Test Data

The tests create temporary test users, documents, and other data as needed. This data is cleaned up after each test to avoid polluting the test database.

For end-to-end tests, the application should be running with a clean database to ensure consistent results.

## Troubleshooting

### Common Issues

1. **Database connection errors**: Ensure the test database exists and is accessible
2. **Permission errors**: Ensure the application is running with the correct permissions
3. **Token validation errors**: Check that the JWT secret key is correctly set in the test environment

### Debugging Tests

To enable more verbose output:

```bash
pytest tests/unit/test_security_utils.py -vv
```

To enable debug logging:

```bash
pytest tests/unit/test_security_utils.py -v --log-cli-level=DEBUG
```

## Security Considerations

These tests verify the security features of the Metis RAG application, including:

1. **Authentication**: User identity verification with JWT tokens
2. **Authorization**: Access control for documents and conversations
3. **Row Level Security**: Database-level access control
4. **Vector Database Security**: Metadata filtering for vector searches
5. **Persistent User-Document Relationships**: Maintaining relationships across authentication events

The tests are designed to ensure that these security features work correctly and that unauthorized access is properly prevented.

## Additional Resources

- [Authentication Implementation Plan](../docs/Metis_RAG_Authentication_Implementation_Detailed_Plan.md)
- [Authentication Testing Plan](../docs/Metis_RAG_Authentication_Testing_Plan.md)

================
File: tests/conftest.py
================
import pytest
import sys
import os
from pathlib import Path

# Add the project root to sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

@pytest.fixture
def client():
    """
    Create a TestClient instance for each test.
    This ensures that dependency overrides are correctly applied.
    """
    from fastapi.testclient import TestClient  # Import INSIDE the fixture
    from app.main import app  # Import your FastAPI app

    client = TestClient(app)
    yield client

@pytest.fixture
def test_dir():
    """
    Return the directory containing test files
    """
    return Path(__file__).parent / "data"

@pytest.fixture
def temp_upload_dir(tmpdir):
    """
    Return a temporary directory for uploads
    """
    from app.core.config import UPLOAD_DIR
    original_upload_dir = UPLOAD_DIR
    
    # Temporarily override the upload directory
    import app.core.config
    app.core.config.UPLOAD_DIR = str(tmpdir)
    
    yield str(tmpdir)
    
    # Restore the original upload directory
    app.core.config.UPLOAD_DIR = original_upload_dir

================
File: tests/e2e_test_README.md
================
# Metis RAG End-to-End Test

This directory contains an end-to-end test for the Metis RAG system. The test evaluates the complete pipeline from document upload and processing to query response generation and evaluation.

## Test Components

The end-to-end test consists of the following components:

1. **Test Documents**: Sample documents in all supported formats (PDF, TXT, CSV, MD) containing complementary information about a fictional "SmartHome" system.
   - `smart_home_technical_specs.txt`/`.pdf` - Technical specifications (PDF generated from TXT)
   - `smart_home_user_guide.txt` - User guide and troubleshooting
   - `smart_home_device_comparison.csv` - Structured device data
   - `smart_home_developer_reference.md` - API documentation

2. **PDF Generation Utility**: A script to convert the technical specifications from text to PDF.
   - `tests/utils/create_test_pdf.py`

3. **End-to-End Test Script**: The main test script that runs all test cases.
   - `tests/test_metis_rag_e2e.py`

4. **Test Runner**: A runner script that sets up the environment and executes the test.
   - `run_metis_rag_e2e_test.py`

## Test Coverage

The end-to-end test evaluates:

- Document upload and processing for all supported file formats
- Chunking and embedding functionality
- Single-document query processing
- Multi-document (cross-document) retrieval
- Complex queries requiring synthesis
- Response quality and factual accuracy
- Citation quality
- System performance

## Prerequisites

- Python 3.8+
- ReportLab library (for PDF generation)
- Access to an instance of the Metis RAG system

## Running the Test

### Option 1: Using the Test Runner

The simplest way to run the test is to use the provided runner script:

```bash
python run_metis_rag_e2e_test.py
```

This script will:
1. Check and install required dependencies
2. Create necessary directories
3. Generate the PDF file from the text file
4. Run the end-to-end test
5. Collect and organize the test results

### Option 2: Running Individual Components

If you prefer to run the components individually:

1. Generate the PDF:
```bash
python tests/utils/create_test_pdf.py
```

2. Run the test:
```bash
pytest -xvs tests/test_metis_rag_e2e.py
```

## Test Results

The test generates several JSON result files with detailed information about each aspect of the testing:

- `test_e2e_upload_results.json` - Document upload and processing results
- `test_e2e_single_doc_results.json` - Single-document query results
- `test_e2e_multi_doc_results.json` - Multi-document query results
- `test_e2e_complex_results.json` - Complex query results
- `test_e2e_citation_results.json` - Citation quality results
- `test_e2e_performance_results.json` - Performance metrics
- `test_e2e_comprehensive_report.json` - Comprehensive report combining all results

All result files are moved to the `test_results` directory after test execution.

## Test Structure

The test follows this structure:

1. **Setup**: Create test directories and files
2. **Document Upload**: Upload all test documents to the system
3. **Document Processing**: Process the uploaded documents for RAG
4. **Query Testing**:
   - Test single-document queries
   - Test multi-document queries
   - Test complex queries
5. **Quality Assessment**:
   - Evaluate factual accuracy
   - Evaluate citation quality
6. **Performance Testing**: Measure system performance
7. **Cleanup**: Remove test documents and generate final report

## Modifying the Test

To modify the test for specific needs:

- Add or modify test documents in the `data/test_docs` directory
- Update the `TEST_DOCUMENTS` dictionary in `test_metis_rag_e2e.py`
- Add or modify test queries in the query test cases
- Adjust the expected facts for the queries

## Troubleshooting

### Common Issues

1. **PDF Generation Fails**:
   - Ensure ReportLab is installed: `pip install reportlab`
   - Check if the text file exists at the expected location

2. **Document Upload Fails**:
   - Verify the Metis RAG system is running
   - Check API endpoints are accessible
   - Ensure all test files exist in the correct locations

3. **Tests Fail with Low Factual Accuracy**:
   - Check that the documents contain the expected information
   - Verify the expected facts in the test cases match the document content
   - Check that the RAG system is properly configured for retrieval

4. **Authentication Issues**:
   - See the detailed troubleshooting guide in `authentication_setup_guide.md`
   - Try using the direct API testing approach described below

## Alternative Testing Approach: Direct API Testing

If you encounter authentication issues with the TestClient approach, we provide an alternative testing method that uses direct API calls:

### Option 3: Using the Direct API Test

```bash
python run_api_test.py
```

This approach:
- Requires the Metis RAG API to be running (`python -m uvicorn app.main:app --reload`)
- Uses the requests library instead of TestClient
- Properly handles authentication
- Avoids event loop conflicts

### Direct API Test Components

1. **API Test Script**: Tests the API directly using HTTP requests.
   - `scripts/test_api_directly.py`

2. **API Test Runner**: A runner script that checks if the API is running and executes the test.
   - `run_api_test.py`

### Direct API Test Results

The direct API test generates these result files:
- `api_test_upload_results.json` - Document upload and processing results
- `api_test_query_results.json` - Query results

All result files are saved to the `test_results` directory.

For detailed information about authentication issues and solutions, please refer to `tests/authentication_setup_guide.md`.

================
File: tests/metis_rag_e2e_test_plan.md
================
# Metis RAG End-to-End Test Plan

## Overview

This document outlines a comprehensive end-to-end test for the Metis RAG system. The test will evaluate the entire pipeline from document upload and processing to query response generation, using all supported file formats (PDF, TXT, CSV, MD).

## Test Objectives

1. Verify document upload and processing across all supported file formats
2. Test the chunking and embedding functionality
3. Validate cross-document retrieval capabilities
4. Assess query refinement and response quality
5. Evaluate factual accuracy and citation quality
6. Measure system performance metrics

## Test Documents Design

We will create a set of test documents with complementary and interlinking information across different file formats:

### 1. Technical Documentation (PDF)
- Filename: `smart_home_technical_specs.pdf`
- Content focus: Technical specifications, architecture, and implementation details
- Key information: Device specifications, system architecture, connectivity protocols

### 2. User Guide (TXT)
- Filename: `smart_home_user_guide.txt`
- Content focus: User instructions, setup procedures, troubleshooting
- Key information: Configuration steps, usage scenarios, maintenance procedures

### 3. Device Comparison (CSV)
- Filename: `smart_home_device_comparison.csv`
- Content focus: Structured data on various smart home devices
- Key information: Model numbers, prices, features, compatibility data

### 4. Developer Reference (Markdown)
- Filename: `smart_home_developer_reference.md`
- Content focus: API documentation, code examples, integration guidelines
- Key information: API endpoints, authentication methods, sample code, best practices

## Test Document Content

The test documents will contain information about a fictional "SmartHome" system, with each document containing complementary information that creates opportunities for testing cross-document retrieval and synthesis.

### Example Content for Technical Documentation (PDF)

```
# SmartHome Technical Specifications

## System Architecture

The SmartHome system follows a hub-and-spoke architecture with the following components:

### SmartHome Hub (Model SH-100)
- Central processing unit: ARM Cortex-A53 quad-core @ 1.4GHz
- Memory: 2GB RAM
- Storage: 16GB eMMC
- Connectivity: Wi-Fi (802.11ac), Bluetooth 5.0, Zigbee 3.0, Z-Wave
- Power: 5V DC, 2A

### Communication Protocols
The SmartHome system uses the following protocols for device communication:
- SmartHome Connect (SHC) - proprietary protocol for secure device communication
- MQTT for lightweight messaging
- CoAP for constrained devices
- HTTP/HTTPS for cloud connectivity

### Security Features
- End-to-end encryption (AES-256)
- Secure boot
- Automatic security updates
- Certificate-based device authentication
- OAuth 2.0 for API authentication
```

### Example Content for User Guide (TXT)

```
SmartHome User Guide
====================

Getting Started
--------------

1. Unbox your SmartHome Hub (Model SH-100) and connect it to power using the provided adapter.
2. Download the SmartHome mobile app from the App Store or Google Play.
3. Launch the app and follow the on-screen instructions to create an account.
4. Connect your hub to your home Wi-Fi network through the app.
5. Once connected, the hub will automatically check for and install the latest firmware.

Adding Devices
-------------

To add a new device to your SmartHome system:

1. Press the "Add Device" button in the mobile app.
2. Select the device type from the list or scan the QR code on the device.
3. Put the device in pairing mode according to its instructions (usually by holding a button for 5 seconds).
4. Follow the app instructions to complete the pairing process.

Troubleshooting
--------------

Hub LED Status Indicators:
- Solid Blue: Hub is powered and working normally
- Blinking Blue: Hub is in pairing mode
- Solid Red: Hub is starting up
- Blinking Red: Hub has no internet connection
- Alternating Red/Blue: Hub is updating firmware

Common Issues:

1. Devices won't connect:
   - Ensure the device is within range of the hub (30-50 feet for most devices)
   - Check that the device is in pairing mode
   - Verify the device is compatible with SmartHome (see compatible devices list)

2. Hub offline:
   - Check your internet connection
   - Restart your router
   - Power cycle the hub by unplugging it for 10 seconds, then plugging it back in
```

### Example Content for Device Comparison (CSV)

```
Device ID,Device Name,Category,Protocol,Battery Life,Price,Indoor Range,Outdoor Range,Water Resistant,Voice Control,Hub Required
SH-MS100,Motion Sensor,Sensor,Zigbee,2 years,$24.99,40 ft,25 ft,No,No,Yes
SH-DS100,Door Sensor,Sensor,Zigbee,2 years,$19.99,50 ft,30 ft,No,No,Yes
SH-LS100,Light Switch,Switch,Z-Wave,N/A,$34.99,100 ft,75 ft,No,Yes,Yes
SH-PL100,Smart Plug,Plug,Wi-Fi,N/A,$29.99,150 ft,100 ft,No,Yes,No
SH-BL100,Smart Bulb,Lighting,Bluetooth,N/A,$15.99,30 ft,N/A,No,Yes,No
SH-KB100,Smart Keypad,Security,SHC,1 year,$79.99,N/A,N/A,Yes,No,Yes
SH-CM100,Smart Camera,Security,Wi-Fi,8 hours,$129.99,N/A,100 ft,Yes,Yes,No
SH-TH100,Temperature/Humidity Sensor,Sensor,Zigbee,18 months,$39.99,60 ft,45 ft,Yes,No,Yes
SH-WV100,Water Valve Controller,Plumbing,Z-Wave,N/A,$89.99,50 ft,N/A,Yes,Yes,Yes
SH-RC100,Remote Control,Controller,RF,$79.99,1 year,75 ft,50 ft,No,Yes,Yes
```

### Example Content for Developer Reference (Markdown)

```markdown
# SmartHome Developer Reference

## API Overview

The SmartHome system provides a RESTful API that allows developers to integrate with and extend the functionality of their SmartHome installation.

### Base URL

All API requests should be made to:

```
https://api.smarthome.example.com/v1
```

### Authentication

The API uses OAuth 2.0 for authentication. To obtain an access token:

1. Register your application at the [SmartHome Developer Portal](https://developer.smarthome.example.com)
2. Implement the OAuth 2.0 authorization code flow
3. Request the appropriate scopes for your application

Example authorization request:

```http
GET https://auth.smarthome.example.com/authorize?
  response_type=code&
  client_id=YOUR_CLIENT_ID&
  redirect_uri=YOUR_REDIRECT_URI&
  scope=devices.read devices.write
```

## API Endpoints

### Devices

#### List all devices

```http
GET /devices
```

Response:

```json
{
  "devices": [
    {
      "id": "device_123456",
      "name": "Living Room Motion Sensor",
      "type": "motion_sensor",
      "model": "SH-MS100",
      "connected": true,
      "battery": 87,
      "last_event": "2025-03-20T15:30:45Z"
    },
    {
      "id": "device_789012",
      "name": "Front Door Sensor",
      "type": "door_sensor",
      "model": "SH-DS100",
      "connected": true,
      "battery": 92,
      "last_event": "2025-03-21T08:15:22Z"
    }
  ]
}
```

#### Get device details

```http
GET /devices/{device_id}
```

#### Update device

```http
PATCH /devices/{device_id}
```

Request body:

```json
{
  "name": "Updated Device Name",
  "room": "bedroom"
}
```

### Events

#### Get recent events

```http
GET /events
```

Parameters:
- `limit`: Maximum number of events to return (default: 50, max: 500)
- `device_id`: Filter events by device ID
- `event_type`: Filter events by type (motion, door, button, etc.)
- `start_time`: ISO 8601 formatted timestamp
- `end_time`: ISO 8601 formatted timestamp

## Webhook Integration

You can register webhook URLs to receive real-time notifications when events occur in the SmartHome system.

To register a webhook:

```http
POST /webhooks
```

Request body:

```json
{
  "url": "https://your-server.example.com/smarthome-webhook",
  "events": ["motion", "door", "button"],
  "secret": "your_webhook_secret"
}
```

The secret will be used to sign webhook requests with an HMAC, allowing you to verify that requests come from the SmartHome system.
```

## Test Queries

We will develop a series of test queries designed to evaluate different aspects of the RAG system:

### Single-Document Queries

1. "What are the specifications of the SmartHome Hub?"
   - Target: PDF document
   - Expected facts: ARM Cortex-A53, 2GB RAM, 16GB storage, connectivity options

2. "How do I troubleshoot when devices won't connect?"
   - Target: TXT document
   - Expected facts: Check device range, ensure pairing mode, verify compatibility

3. "What is the battery life of the motion sensor?"
   - Target: CSV document
   - Expected facts: 2 years, device ID SH-MS100

4. "How do I authenticate with the SmartHome API?"
   - Target: MD document
   - Expected facts: OAuth 2.0, authorization code flow, developer portal

### Multi-Document Queries

5. "Compare the Motion Sensor and Door Sensor specifications and setup process."
   - Target: CSV + TXT documents
   - Expected facts: Specifications from CSV, setup process from TXT

6. "Explain how to integrate a motion sensor with a third-party application."
   - Target: MD + PDF + TXT documents
   - Expected facts: API details from MD, technical specifications from PDF, setup from TXT

7. "What security features does the SmartHome system provide for both users and developers?"
   - Target: PDF + MD documents
   - Expected facts: End-to-end encryption from PDF, OAuth and HMAC from MD

### Complex Analysis Queries

8. "Which devices require the hub and what protocols do they use?"
   - Target: CSV + PDF documents
   - Expected facts: Hub-required devices from CSV, protocol details from PDF

9. "If I want to create a water leak detection system, which devices should I use and how would I set them up?"
   - Target: All documents
   - Expected synthesis across documents

10. "What's the difference between Zigbee and Z-Wave devices in the SmartHome ecosystem?"
    - Target: PDF + CSV documents
    - Expected synthesis of technical differences and practical implications

## Test Implementation Structure

The implementation will follow this structure:

1. **Setup**: Create test directories and files
2. **Document Creation**: Generate all test documents with complementary content
3. **Test Functions**:
   - Document upload and processing
   - Single-document query testing
   - Multi-document query testing
   - Complex query testing
   - Response quality assessment
4. **Metrics Calculation**:
   - Factual accuracy
   - Response completeness
   - Citation quality
   - Performance metrics
5. **Results Reporting**: Generate detailed test reports

## Code Implementation

The end-to-end test will be implemented as a single comprehensive script with clear sections. Below is a pseudocode outline of the implementation:

```python
#!/usr/bin/env python3
"""
End-to-End test for the Metis RAG system.
This test evaluates the complete pipeline from document upload to query response.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime

import pytest
from fastapi.testclient import TestClient
from io import BytesIO

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_metis_rag_e2e")

# Import necessary components
from app.main import app
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.models.document import Document, Chunk

# Test client
client = TestClient(app)

# Test document content (defined as variables or loaded from files)
PDF_CONTENT = """# SmartHome Technical Specifications..."""
TXT_CONTENT = """SmartHome User Guide..."""
CSV_CONTENT = """Device ID,Device Name,Category..."""
MD_CONTENT = """# SmartHome Developer Reference..."""

# Test queries with expected facts
TEST_QUERIES = [
    {
        "query": "What are the specifications of the SmartHome Hub?",
        "expected_facts": ["ARM Cortex-A53", "2GB RAM", "16GB storage", "Wi-Fi", "Bluetooth 5.0", "Zigbee 3.0", "Z-Wave"],
        "target_docs": ["smart_home_technical_specs.pdf"]
    },
    # Additional queries...
]

@pytest.fixture
def test_documents_dir():
    """Create a directory for test documents"""
    temp_dir = tempfile.mkdtemp(prefix="metis_rag_e2e_test_")
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.fixture
def create_test_documents(test_documents_dir):
    """Create test documents of different formats with complementary information"""
    # Implementation to create all document files
    # Return paths to created documents

@pytest.fixture
async def setup_vector_store():
    """Set up a separate vector store for testing"""
    # Implementation

@pytest.fixture
async def setup_rag_engine(setup_vector_store):
    """Set up RAG engine with test vector store"""
    # Implementation

@pytest.mark.asyncio
async def test_document_upload_and_processing():
    """Test uploading and processing of all document types"""
    # Implementation

@pytest.mark.asyncio
async def test_single_document_queries(setup_rag_engine):
    """Test queries that target a single document"""
    # Implementation

@pytest.mark.asyncio
async def test_multi_document_queries(setup_rag_engine):
    """Test queries that require information from multiple documents"""
    # Implementation

@pytest.mark.asyncio
async def test_complex_queries(setup_rag_engine):
    """Test complex queries requiring synthesis and analysis"""
    # Implementation

@pytest.mark.asyncio
async def test_response_quality(setup_rag_engine):
    """Test quality aspects like factual accuracy, completeness, and citations"""
    # Implementation

@pytest.mark.asyncio
async def test_system_performance():
    """Test performance metrics like processing time and response time"""
    # Implementation

@pytest.mark.asyncio
async def test_end_to_end_workflow():
    """Test the complete workflow from upload to query response"""
    # Implementation

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])
```

## Test Document Creation

For the actual test implementation, we'll need to create physical files for each format. The PDF file creation will require a PDF generation library like ReportLab, PyFPDF, or WeasyPrint. The other file formats (TXT, CSV, MD) can be created using standard file I/O operations.

## Test Execution Process

The test execution will follow these steps:

1. Create a clean test environment with separate directories for test documents and vector store
2. Generate all test documents with the designed content
3. Upload and process each document
4. Run all test queries and collect responses
5. Evaluate responses against expected facts
6. Calculate quality metrics
7. Generate a comprehensive test report

## Next Steps

To implement this test plan:

1. Switch to Code mode to implement the actual test script
2. Create helper functions for document generation, especially for PDF files
3. Implement the test queries and evaluation logic
4. Run the tests in a clean environment to ensure accurate results

The implementation should take into account potential environment-specific issues, such as dependencies for PDF handling and vector store persistence.

================
File: tests/README.md
================
# Metis RAG Testing Framework

This directory contains test scripts for verifying the functionality and quality of the Metis RAG (Retrieval Augmented Generation) system.

## Test Scripts

### Entity Preservation Test (`test_rag_entity_preservation.py`)

This script tests the Metis RAG system's ability to preserve named entities during query refinement, ensure minimum context requirements are met, and handle citations properly.

#### Features

- Creates a test environment with synthetic documents about fictional entities
- Executes a series of increasingly complex queries against the RAG system
- Analyzes the results to verify entity preservation, context selection, and citation handling
- Generates detailed reports for each query and a summary report

#### Usage

```bash
python tests/test_rag_entity_preservation.py
```

#### Test Queries

The test includes five queries of increasing complexity:

1. **Basic Entity Query**: "Tell me about Stabilium and its applications in quantum computing."
2. **Multi-Entity Query**: "Compare the properties of Stabilium and Quantum Resonance Modulation in cold fusion experiments."
3. **Query with Potential Ambiguity**: "What are the differences between Stabilium QRM-12X and earlier versions?"
4. **Query Requiring Context Synthesis**: "How does Stabilium interact with Heisenberg's Uncertainty Principle when used in quantum entanglement experiments?"
5. **Query with Specialized Terminology**: "Explain the role of Stabilium in facilitating quantum tunneling through non-Euclidean space-time manifolds."

#### Results

The test generates the following outputs:

- Individual query reports in `tests/results/query_<query_id>/report.md`
- A summary report in `tests/results/summary_report.md`
- Detailed logs in `tests/results/entity_preservation_test_results.log`

## Lessons Learned

During the development and testing of the Metis RAG system, we encountered several important insights:

1. **Log-Based Metrics Extraction**: The RAG engine doesn't directly expose metrics like chunks retrieved and used in its response. We had to implement a custom log handler to capture and extract this information from log messages. This approach is more robust than trying to modify the core RAG engine to expose these metrics directly.

2. **Entity Preservation Importance**: Our tests confirmed that preserving named entities during query refinement is critical for maintaining the user's intent. The system successfully identified and preserved entities like "Stabilium", "Heisenberg's Uncertainty Principle", and "QRM-12X" even when refining queries.

3. **Context Selection Trade-offs**: The tests revealed that the system retrieves many chunks (15) but uses only a subset (3-6) in the final context. This selective approach balances comprehensive information retrieval with focused, relevant responses.

4. **Retrieval Judge Effectiveness**: The Retrieval Judge component successfully refined queries and optimized context assembly, demonstrating its value in improving RAG quality. For example, it refined the basic query to focus on specific applications of Stabilium in quantum computing.

5. **Testing with Fictional Entities**: Using fictional entities (like Stabilium) for testing proved effective as it allowed us to create controlled test cases without relying on external knowledge. This approach ensures tests are reproducible and focused on system behavior rather than factual accuracy.

6. **Importance of Detailed Logging**: Comprehensive logging was essential for debugging and understanding the system's behavior. The logs provided insights into query refinement, chunk selection, and context optimization that weren't visible in the final response.

These lessons have informed our approach to RAG system development and testing, leading to more robust and effective implementations.

================
File: tests/run_api_test.py
================
#!/usr/bin/env python3
"""
Runner script for the Metis RAG API test.
This script:
1. Verifies the API is running
2. Runs the direct API test
"""

import os
import sys
import subprocess
import time
import requests

def check_api_running():
    """Check if the Metis RAG API is running"""
    print("Checking if Metis RAG API is running...")
    
    try:
        response = requests.get("http://localhost:8000/api/health")
        if response.status_code == 200:
            print("✓ Metis RAG API is running")
            return True
        else:
            print(f"✗ Metis RAG API returned unexpected status: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("✗ Metis RAG API is not running at http://localhost:8000")
        return False
    except Exception as e:
        print(f"✗ Error checking API: {str(e)}")
        return False

def run_api_test():
    """Run the direct API test"""
    try:
        print("\n" + "="*80)
        print("Running Metis RAG API Test")
        print("="*80 + "\n")
        
        # Check if the test script exists
        if not os.path.exists("scripts/test_api_directly.py"):
            print("✗ Error: test_api_directly.py not found!")
            return 1
        
        # Run the test
        start_time = time.time()
        result = subprocess.run(
            [sys.executable, "scripts/test_api_directly.py"], 
            check=False
        )
        end_time = time.time()
        
        # Print summary
        print("\n" + "="*80)
        print(f"Test execution completed in {end_time - start_time:.2f} seconds")
        if result.returncode == 0:
            print("✓ API test completed successfully!")
        else:
            print(f"✗ API test failed with exit code {result.returncode}")
        print("="*80 + "\n")
        
        return result.returncode
        
    except Exception as e:
        print(f"✗ Error running API test: {str(e)}")
        return 1

def main():
    """Main function"""
    print("\nMetis RAG API Test Runner\n")
    
    # Check if API is running
    if not check_api_running():
        print("\nPlease start the Metis RAG API before running the test.")
        print("You can start it with: python -m uvicorn app.main:app --reload")
        return 1
    
    # Run API test
    return run_api_test()

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/run_authentication_test.py
================
#!/usr/bin/env python3
"""
Test script for JWT authentication in Metis RAG

This script tests the JWT authentication system by:
1. Registering a new user (if needed)
2. Logging in to get access and refresh tokens
3. Using the access token to access a protected endpoint
4. Refreshing the access token using the refresh token
5. Using the new access token to access a protected endpoint

Usage:
    python run_authentication_test.py
"""

import requests
import json
import time
import sys
from urllib.parse import urljoin

# Configuration
BASE_URL = "http://localhost:8000"
API_PREFIX = "/api"
USERNAME = "testuser"
PASSWORD = "Test@password123"
EMAIL = "testuser@example.com"

# API endpoints
AUTH_REGISTER_URL = urljoin(BASE_URL, f"{API_PREFIX}/auth/register")
AUTH_TOKEN_URL = urljoin(BASE_URL, f"{API_PREFIX}/auth/token")
AUTH_REFRESH_URL = urljoin(BASE_URL, f"{API_PREFIX}/auth/refresh")
ME_URL = urljoin(BASE_URL, f"{API_PREFIX}/auth/me")

def print_header(title):
    """Print a formatted header"""
    print("\n" + "=" * 80)
    print(f" {title} ".center(80, "="))
    print("=" * 80)

def print_json(data):
    """Print JSON data in a formatted way"""
    print(json.dumps(data, indent=4))

def register_user():
    """Register a new user"""
    print_header("REGISTERING NEW USER")
    
    user_data = {
        "username": USERNAME,
        "email": EMAIL,
        "password": PASSWORD,
        "full_name": "Test User"
    }
    
    print(f"Registering user: {USERNAME}")
    try:
        response = requests.post(AUTH_REGISTER_URL, json=user_data)
        
        if response.status_code == 200:
            print("User registered successfully!")
            print_json(response.json())
            return True
        else:
            print(f"Registration failed with status code: {response.status_code}")
            print_json(response.json())
            
            # If user already exists, that's fine
            if response.status_code == 400 and "already exists" in response.text:
                print("User already exists, continuing with login test.")
                return True
            
            return False
    except Exception as e:
        print(f"Error during registration: {str(e)}")
        return False

def login_user():
    """Login and get access token"""
    print_header("LOGGING IN")
    
    login_data = {
        "username": USERNAME,
        "password": PASSWORD
    }
    
    print(f"Logging in as: {USERNAME}")
    try:
        response = requests.post(
            AUTH_TOKEN_URL, 
            data=login_data,  # Note: using form data, not JSON
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        if response.status_code == 200:
            token_data = response.json()
            print("Login successful!")
            print("Token data:")
            print_json(token_data)
            return token_data
        else:
            print(f"Login failed with status code: {response.status_code}")
            print_json(response.json())
            return None
    except Exception as e:
        print(f"Error during login: {str(e)}")
        return None

def get_user_profile(access_token):
    """Get user profile using access token"""
    print_header("ACCESSING PROTECTED ENDPOINT")
    
    print("Getting user profile with access token")
    try:
        response = requests.get(
            ME_URL,
            headers={"Authorization": f"Bearer {access_token}"}
        )
        
        if response.status_code == 200:
            user_data = response.json()
            print("User profile retrieved successfully!")
            print_json(user_data)
            return True
        else:
            print(f"Profile retrieval failed with status code: {response.status_code}")
            print_json(response.json())
            return False
    except Exception as e:
        print(f"Error getting user profile: {str(e)}")
        return False

def refresh_token(refresh_token):
    """Refresh access token using refresh token"""
    print_header("REFRESHING ACCESS TOKEN")
    
    refresh_data = {
        "refresh_token": refresh_token
    }
    
    print("Refreshing access token")
    try:
        response = requests.post(
            AUTH_REFRESH_URL,
            json=refresh_data
        )
        
        if response.status_code == 200:
            new_token_data = response.json()
            print("Token refreshed successfully!")
            print("New token data:")
            print_json(new_token_data)
            return new_token_data
        else:
            print(f"Token refresh failed with status code: {response.status_code}")
            print_json(response.json())
            return None
    except Exception as e:
        print(f"Error refreshing token: {str(e)}")
        return None

def main():
    """Main function to run the authentication test"""
    print_header("JWT AUTHENTICATION TEST")
    
    # Step 1: Register a new user (if needed)
    if not register_user():
        print("Registration failed, exiting.")
        sys.exit(1)
    
    # Step 2: Login to get tokens
    token_data = login_user()
    if not token_data:
        print("Login failed, exiting.")
        sys.exit(1)
    
    access_token = token_data["access_token"]
    refresh_token_str = token_data["refresh_token"]
    
    # Step 3: Access protected endpoint with access token
    if not get_user_profile(access_token):
        print("Failed to access protected endpoint, exiting.")
        sys.exit(1)
    
    # Optional: Wait a moment to simulate time passing
    print("\nWaiting for 2 seconds...\n")
    time.sleep(2)
    
    # Step 4: Refresh the token
    new_token_data = refresh_token(refresh_token_str)
    if not new_token_data:
        print("Token refresh failed, exiting.")
        sys.exit(1)
    
    new_access_token = new_token_data["access_token"]
    
    # Step 5: Access protected endpoint with new access token
    print_header("ACCESSING PROTECTED ENDPOINT WITH NEW TOKEN")
    if not get_user_profile(new_access_token):
        print("Failed to access protected endpoint with new token, exiting.")
        sys.exit(1)
    
    print_header("TEST COMPLETED SUCCESSFULLY")
    print("The JWT authentication system is working correctly!")

if __name__ == "__main__":
    main()

================
File: tests/run_metis_rag_e2e_demo.py
================
#!/usr/bin/env python3
"""
Runner script for the Metis RAG End-to-End Demo Test.
This script:
1. Creates necessary directories
2. Generates PDF file from text
3. Runs the end-to-end demo test
"""

import os
import sys
import subprocess
import time

def check_requirements():
    """Check if required packages are installed"""
    try:
        import reportlab
        print("✓ ReportLab is installed")
    except ImportError:
        print("✗ ReportLab is not installed. Installing...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "reportlab"])
        print("✓ ReportLab installed successfully")

def create_directories():
    """Create necessary directories"""
    directories = [
        "data/test_docs",
        "tests/utils",
        "test_results"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"✓ Directory created/verified: {directory}")

def generate_pdf():
    """Generate PDF from the text file"""
    if not os.path.exists("data/test_docs/smart_home_technical_specs.txt"):
        print("✗ Error: smart_home_technical_specs.txt not found!")
        print("  Please ensure you have created all test documents first.")
        sys.exit(1)
    
    try:
        print("Generating PDF from text file...")
        if os.path.exists("tests/utils/create_test_pdf.py"):
            # Run the script directly
            subprocess.run(
                [sys.executable, "tests/utils/create_test_pdf.py"],
                check=True
            )
            
            # Verify the PDF was created
            if os.path.exists("data/test_docs/smart_home_technical_specs.pdf"):
                print("✓ PDF file created successfully")
            else:
                print("✗ PDF file was not created")
                sys.exit(1)
        else:
            print("✗ Error: create_test_pdf.py not found!")
            sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"✗ Error generating PDF: {e}")
        sys.exit(1)

def run_demo_test():
    """Run the end-to-end demo test"""
    try:
        print("\n" + "="*80)
        print("Running Metis RAG End-to-End Demo Test")
        print("="*80 + "\n")
        
        # Check if the test script exists
        if not os.path.exists("tests/test_metis_rag_e2e_demo.py"):
            print("✗ Error: test_metis_rag_e2e_demo.py not found!")
            sys.exit(1)
        
        # Run the test
        start_time = time.time()
        result = subprocess.run(
            [sys.executable, "tests/test_metis_rag_e2e_demo.py"], 
            check=False
        )
        end_time = time.time()
        
        # Print summary
        print("\n" + "="*80)
        print(f"Test execution completed in {end_time - start_time:.2f} seconds")
        if result.returncode == 0:
            print("✓ Demo test completed successfully!")
        else:
            print(f"✗ Demo test failed with exit code {result.returncode}")
        print("="*80 + "\n")
        
        # Move result files to results directory
        result_files = [
            "test_e2e_demo_upload_results.json",
            "test_e2e_demo_query_results.json",
            "test_e2e_demo_comprehensive_report.json"
        ]
        
        for file in result_files:
            if os.path.exists(file):
                # Create destination path
                dest_path = os.path.join("test_results", file)
                
                # Copy file to destination
                with open(file, 'r') as src_file:
                    content = src_file.read()
                
                with open(dest_path, 'w') as dest_file:
                    dest_file.write(content)
                
                print(f"✓ Copied {file} to test_results directory")
                
                # Remove original file
                os.remove(file)
        
        return result.returncode
        
    except Exception as e:
        print(f"✗ Error running demo test: {e}")
        return 1

def main():
    """Main function to run the test"""
    print("\nMetis RAG End-to-End Demo Test Runner\n")
    
    # Check requirements
    check_requirements()
    
    # Create directories
    create_directories()
    
    # Generate PDF
    generate_pdf()
    
    # Run demo test
    return run_demo_test()

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/run_metis_rag_e2e_test.py
================
#!/usr/bin/env python3
"""
Runner script for the Metis RAG End-to-End test.
This script:
1. Creates necessary directories
2. Generates PDF file from text
3. Runs the end-to-end test suite
"""

import os
import sys
import subprocess
import importlib.util
import time
import shutil

def check_requirements():
    """Check if required packages are installed"""
    try:
        import reportlab
        print("✓ ReportLab is installed")
    except ImportError:
        print("✗ ReportLab is not installed. Installing...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "reportlab"])
        print("✓ ReportLab installed successfully")

def create_directories():
    """Create necessary directories"""
    directories = [
        "data/test_docs",
        "tests/utils",
        "test_e2e_chroma",
        "test_results"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"✓ Directory created/verified: {directory}")

def generate_pdf():
    """Generate PDF from the text file"""
    if not os.path.exists("data/test_docs/smart_home_technical_specs.txt"):
        print("✗ Error: smart_home_technical_specs.txt not found!")
        print("  Please ensure you have created all test documents first.")
        sys.exit(1)
    
    try:
        print("Generating PDF from text file...")
        if os.path.exists("tests/utils/create_test_pdf.py"):
            # Run the script directly
            subprocess.run(
                [sys.executable, "tests/utils/create_test_pdf.py"],
                check=True
            )
            
            # Verify the PDF was created
            if os.path.exists("data/test_docs/smart_home_technical_specs.pdf"):
                print("✓ PDF file created successfully")
            else:
                print("✗ PDF file was not created")
                sys.exit(1)
        else:
            print("✗ Error: create_test_pdf.py not found!")
            sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"✗ Error generating PDF: {e}")
        sys.exit(1)

def run_e2e_test():
    """Run the end-to-end test"""
    try:
        print("\n" + "="*80)
        print("Running Metis RAG End-to-End Test")
        print("="*80 + "\n")
        
        # Check if the test script exists
        if not os.path.exists("tests/test_metis_rag_e2e.py"):
            print("✗ Error: test_metis_rag_e2e.py not found!")
            sys.exit(1)
        
        # Run the test
        start_time = time.time()
        result = subprocess.run(
            ["pytest", "-xvs", "tests/test_metis_rag_e2e.py"], 
            check=False
        )
        end_time = time.time()
        
        # Print summary
        print("\n" + "="*80)
        print(f"Test execution completed in {end_time - start_time:.2f} seconds")
        if result.returncode == 0:
            print("✓ All tests passed successfully!")
        else:
            print(f"✗ Tests failed with exit code {result.returncode}")
        print("="*80 + "\n")
        
        # Move result files to results directory
        result_files = [
            "test_e2e_upload_results.json",
            "test_e2e_single_doc_results.json",
            "test_e2e_multi_doc_results.json",
            "test_e2e_complex_results.json",
            "test_e2e_citation_results.json",
            "test_e2e_performance_results.json",
            "test_e2e_comprehensive_report.json"
        ]
        
        for file in result_files:
            if os.path.exists(file):
                shutil.move(file, os.path.join("test_results", file))
                print(f"✓ Moved {file} to test_results directory")
        
        return result.returncode
        
    except Exception as e:
        print(f"✗ Error running tests: {e}")
        return 1
def create_test_user():
    """Create test user for authentication"""
    print("Creating test user for authentication...")
    try:
        # Run the script to create a test user
        result = subprocess.run(
            [sys.executable, "scripts/create_test_user.py"],
            check=False
        )
        
        if result.returncode == 0:
            print("✓ Test user created/verified successfully")
        else:
            print(f"✗ Error creating test user (exit code {result.returncode})")
            print("  Tests may fail if authentication is required.")
    except Exception as e:
        print(f"✗ Error running create_test_user.py: {e}")
        print("  Tests may fail if authentication is required.")

def initialize_chroma_db():
    """Initialize ChromaDB for testing"""
    print("Initializing ChromaDB for testing...")
    try:
        # Run the script to initialize ChromaDB
        result = subprocess.run(
            [sys.executable, "scripts/initialize_test_chroma.py"],
            check=False
        )
        
        if result.returncode == 0:
            print("✓ ChromaDB initialized successfully")
            return True
        else:
            print(f"✗ Error initializing ChromaDB (exit code {result.returncode})")
            print("  Tests may fail if ChromaDB is not properly initialized.")
            return False
    except Exception as e:
        print(f"✗ Error running initialize_test_chroma.py: {e}")
        print("  Tests may fail if ChromaDB is not properly initialized.")
        return False

def main():
    """Main function to run the test"""
    print("\nMetis RAG End-to-End Test Runner\n")
    
    # Check requirements
    check_requirements()
    
    # Create directories
    create_directories()
    
    # Generate PDF
    generate_pdf()
    
    # Create test user
    create_test_user()
    
    # Initialize ChromaDB
    initialize_chroma_db()
    
    # Run tests
    return run_e2e_test()
    return run_e2e_test()

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/simple_auth_test.py
================
#!/usr/bin/env python3
"""
Simple authentication test for Metis RAG.
This script tests authentication using direct API calls without TestClient.
"""

import os
import sys
import logging
import requests
import json
from typing import Dict, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("simple_auth_test")

def authenticate_with_api(base_url: str, username: str, password: str) -> Tuple[bool, Optional[str]]:
    """
    Authenticate with the API using direct requests.
    
    Args:
        base_url: Base URL of the API (e.g., http://localhost:8000)
        username: Username for authentication
        password: Password for authentication
        
    Returns:
        Tuple of (success, token) where success is a boolean and token is the access token if successful
    """
    try:
        logger.info(f"Authenticating with username: {username}")
        
        # Use form data for token endpoint
        login_response = requests.post(
            f"{base_url}/api/auth/token",
            data={
                "username": username,
                "password": password,
                "grant_type": "password"
            }
        )
        
        if login_response.status_code == 200:
            token_data = login_response.json()
            access_token = token_data.get("access_token")
            
            if access_token:
                logger.info("✓ Authentication successful")
                return True, access_token
            else:
                logger.error("✗ No access token in response")
                return False, None
        else:
            logger.error(f"✗ Authentication failed: {login_response.status_code} - {login_response.text}")
            return False, None
            
    except Exception as e:
        logger.error(f"✗ Authentication error: {str(e)}")
        return False, None

def register_test_user(base_url: str, username: str, password: str) -> bool:
    """
    Register a new test user.
    
    Args:
        base_url: Base URL of the API
        username: Username for the new user
        password: Password for the new user
        
    Returns:
        True if registration was successful, False otherwise
    """
    try:
        logger.info(f"Registering new test user: {username}")
        
        register_response = requests.post(
            f"{base_url}/api/auth/register",
            json={
                "username": username,
                "email": f"{username}@example.com",
                "password": password,
                "full_name": "Test User",
                "is_active": True,
                "is_admin": False
            }
        )
        
        if register_response.status_code == 200:
            logger.info(f"✓ User {username} registered successfully")
            return True
        else:
            logger.error(f"✗ Registration failed: {register_response.status_code} - {register_response.text}")
            return False
            
    except Exception as e:
        logger.error(f"✗ Registration error: {str(e)}")
        return False

def test_protected_endpoint(base_url: str, token: str) -> bool:
    """
    Test accessing a protected endpoint.
    
    Args:
        base_url: Base URL of the API
        token: Access token
        
    Returns:
        True if access was successful, False otherwise
    """
    try:
        logger.info("Testing access to protected endpoint")
        
        headers = {"Authorization": f"Bearer {token}"}
        
        # Try different protected endpoints
        endpoints = [
            "/api/documents",
            "/api/auth/me",
            "/api/system/status"
        ]
        
        for endpoint in endpoints:
            response = requests.get(f"{base_url}{endpoint}", headers=headers)
            
            if response.status_code == 200:
                logger.info(f"✓ Successfully accessed {endpoint}")
                return True
            else:
                logger.warning(f"✗ Failed to access {endpoint}: {response.status_code} - {response.text}")
        
        logger.error("✗ Could not access any protected endpoint")
        return False
            
    except Exception as e:
        logger.error(f"✗ Error accessing protected endpoint: {str(e)}")
        return False

def main():
    """Main function"""
    logger.info("Starting simple authentication test...")
    
    base_url = "http://localhost:8000"
    
    # Test with existing user
    success, token = authenticate_with_api(base_url, "testuser", "testpassword")
    
    if not success:
        logger.warning("Authentication with existing user failed, trying with a new user")
        
        # Try with a unique username
        import uuid
        test_username = f"testuser_{uuid.uuid4().hex[:8]}"
        test_password = "testpassword"
        
        # Register new user
        if register_test_user(base_url, test_username, test_password):
            # Try to authenticate with the new user
            success, token = authenticate_with_api(base_url, test_username, test_password)
    
    if success and token:
        # Test accessing a protected endpoint
        if test_protected_endpoint(base_url, token):
            logger.info("✓ Authentication test passed successfully!")
            return 0
        else:
            logger.error("✗ Could not access protected endpoints")
            return 1
    else:
        logger.error("✗ Authentication test failed")
        return 1

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/test_auth_simple.py
================
#!/usr/bin/env python3
"""
Simple script to test authentication with a known user.
"""

import asyncio
import logging
import sys
import requests
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_auth_simple")

# Base URL for the API
BASE_URL = "http://localhost:8000/api"

# Test user credentials
TEST_USER = {
    "username": "testuser123",
    "password": "testpassword123"
}

def test_authentication():
    """Test authentication using direct API calls"""
    logger.info(f"Testing authentication with username: {TEST_USER['username']}")
    
    # Try to get a token
    try:
        response = requests.post(
            f"{BASE_URL}/auth/token",
            data={
                "username": TEST_USER["username"],
                "password": TEST_USER["password"]
            }
        )
        
        if response.status_code == 200:
            token_data = response.json()
            logger.info("Authentication successful!")
            logger.info(f"Access token: {token_data['access_token'][:20]}...")
            
            # Test accessing a protected endpoint
            headers = {"Authorization": f"Bearer {token_data['access_token']}"}
            
            # Try to get user info
            me_response = requests.get(f"{BASE_URL}/auth/me", headers=headers)
            if me_response.status_code == 200:
                user_data = me_response.json()
                logger.info(f"Successfully accessed protected endpoint: /auth/me")
                logger.info(f"User data: {json.dumps(user_data, indent=2)}")
            else:
                logger.error(f"Failed to access protected endpoint: {me_response.status_code} - {me_response.text}")
            
            # Try to upload a document
            with open("test_document.txt", "w") as f:
                f.write("This is a test document for authentication testing.")
            
            with open("test_document.txt", "rb") as f:
                files = {"file": ("test_document.txt", f, "text/plain")}
                upload_response = requests.post(
                    f"{BASE_URL}/documents/upload",
                    headers=headers,
                    files=files,
                    data={"tags": "test,auth", "folder": "/test"}
                )
            
            if upload_response.status_code == 200:
                result = upload_response.json()
                logger.info(f"Document uploaded successfully: {result['document_id']}")
            else:
                logger.error(f"Failed to upload document: {upload_response.status_code} - {upload_response.text}")
            
            return True
        else:
            logger.error(f"Authentication failed: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        logger.error(f"Error during authentication test: {str(e)}")
        return False

def main():
    """Main function"""
    logger.info("Starting authentication test...")
    
    # Test authentication
    success = test_authentication()
    
    # Print summary
    logger.info("\n" + "="*80)
    logger.info("Authentication Test Summary")
    logger.info("="*80)
    logger.info(f"Authentication: {'✓ Success' if success else '✗ Failed'}")
    logger.info("="*80)
    
    # Return success code
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/test_authentication.py
================
#!/usr/bin/env python3
"""
Test script to verify authentication with the Metis RAG API.
This script tests both direct API calls and the TestClient approach.
"""

import os
import sys
import logging
import requests
from fastapi.testclient import TestClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_authentication")

# Import app and authentication helper
sys.path.append(".")  # Add current directory to path
from app.main import app
from tests.utils.test_auth_helper import (
    configure_test_client,
    verify_authentication,
    authenticate_with_session
)

def test_direct_api_authentication():
    """Test authentication using direct API calls with requests"""
    logger.info("Testing direct API authentication...")
    
    base_url = "http://localhost:8000"
    
    # Test with valid credentials
    session, token = authenticate_with_session(base_url, "testuser", "testpassword")
    
    if token:
        logger.info("✓ Authentication successful with valid credentials")
        
        # Test accessing a protected endpoint
        response = session.get(f"{base_url}/api/documents")
        
        if response.status_code == 200:
            logger.info("✓ Successfully accessed protected endpoint")
            return True
        else:
            logger.error(f"✗ Failed to access protected endpoint: {response.status_code} - {response.text}")
    else:
        logger.error("✗ Authentication failed with valid credentials")
    
    # Test with invalid credentials
    session, token = authenticate_with_session(base_url, "testuser", "wrongpassword")
    
    if token:
        logger.error("✗ Authentication succeeded with invalid credentials (unexpected)")
    else:
        logger.info("✓ Authentication correctly failed with invalid credentials")
    
    return False

def test_testclient_authentication():
    """Test authentication using TestClient"""
    logger.info("Testing TestClient authentication...")
    
    # Configure TestClient with valid credentials
    client = configure_test_client(app, "testuser", "testpassword")
    
    # Verify authentication
    if verify_authentication(client):
        logger.info("✓ TestClient authentication successful with valid credentials")
        
        # Test accessing a protected endpoint
        response = client.get("/api/documents")
        
        if response.status_code == 200:
            logger.info("✓ TestClient successfully accessed protected endpoint")
            return True
        else:
            logger.error(f"✗ TestClient failed to access protected endpoint: {response.status_code} - {response.text}")
    else:
        logger.error("✗ TestClient authentication failed with valid credentials")
    
    # Configure TestClient with invalid credentials
    client = configure_test_client(app, "testuser", "wrongpassword")
    
    # Verify authentication (should fail)
    if verify_authentication(client):
        logger.error("✗ TestClient authentication succeeded with invalid credentials (unexpected)")
    else:
        logger.info("✓ TestClient authentication correctly failed with invalid credentials")
    
    return False

def main():
    """Main function"""
    logger.info("Starting authentication tests...")
    
    # Test direct API authentication
    direct_api_success = test_direct_api_authentication()
    
    # Test TestClient authentication
    testclient_success = test_testclient_authentication()
    
    # Print summary
    logger.info("\n" + "="*80)
    logger.info("Authentication Test Summary")
    logger.info("="*80)
    logger.info(f"Direct API Authentication: {'✓ Success' if direct_api_success else '✗ Failed'}")
    logger.info(f"TestClient Authentication: {'✓ Success' if testclient_success else '✗ Failed'}")
    logger.info("="*80)
    
    # Return success if either test passed
    return 0 if (direct_api_success or testclient_success) else 1

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/test_background_tasks.py
================
"""
Performance tests for the Background Task System
"""
import asyncio
import time
import random
import pytest
from typing import List, Dict, Any

from app.tasks.task_manager import TaskManager
from app.tasks.task_models import Task, TaskStatus, TaskPriority
from app.tasks.resource_monitor import ResourceMonitor
from app.tasks.scheduler import Scheduler
from app.tasks.example_tasks import register_example_handlers

# Test task handler
async def test_task_handler(task: Task) -> Dict[str, Any]:
    """
    Test task handler that simulates work
    
    Args:
        task: Task to execute
        
    Returns:
        Task result
    """
    # Get parameters
    duration = task.params.get("duration", 1.0)
    should_fail = task.params.get("should_fail", False)
    
    # Update progress periodically
    total_steps = max(1, int(duration))
    for step in range(1, total_steps + 1):
        # Update progress
        progress = (step / total_steps) * 100
        task.update_progress(progress)
        
        # Simulate work
        await asyncio.sleep(1.0)
    
    # Simulate failure if requested
    if should_fail:
        raise ValueError("Task failed as requested")
    
    # Return result
    return {
        "duration": duration,
        "steps": total_steps
    }

@pytest.fixture
async def task_manager():
    """
    Fixture for task manager
    
    Returns:
        TaskManager instance
    """
    # Create task manager
    manager = TaskManager(max_concurrent_tasks=10)
    
    # Register test handler
    manager.register_task_handler("test", test_task_handler)
    
    # Register example handlers
    register_example_handlers(manager)
    
    # Start task manager
    await manager.start()
    
    yield manager
    
    # Stop task manager
    await manager.stop()

@pytest.mark.asyncio
async def test_task_execution(task_manager):
    """
    Test basic task execution
    
    Args:
        task_manager: TaskManager fixture
    """
    # Submit a task
    task_id = await task_manager.submit(
        name="Test Task",
        task_type="test",
        params={"duration": 2.0}
    )
    
    # Wait for task to complete
    for _ in range(10):
        task = task_manager.get_task(task_id)
        if task and task.status in (TaskStatus.COMPLETED, TaskStatus.FAILED):
            break
        await asyncio.sleep(0.5)
    
    # Check task status
    task = task_manager.get_task(task_id)
    assert task is not None
    assert task.status == TaskStatus.COMPLETED
    assert task.result is not None
    assert task.result["duration"] == 2.0
    assert task.result["steps"] == 2

@pytest.mark.asyncio
async def test_task_failure(task_manager):
    """
    Test task failure handling
    
    Args:
        task_manager: TaskManager fixture
    """
    # Submit a task that will fail
    task_id = await task_manager.submit(
        name="Failing Task",
        task_type="test",
        params={"duration": 1.0, "should_fail": True}
    )
    
    # Wait for task to complete
    for _ in range(10):
        task = task_manager.get_task(task_id)
        if task and task.status in (TaskStatus.COMPLETED, TaskStatus.FAILED):
            break
        await asyncio.sleep(0.5)
    
    # Check task status
    task = task_manager.get_task(task_id)
    assert task is not None
    assert task.status == TaskStatus.FAILED
    assert task.error is not None
    assert "Task failed as requested" in task.error

@pytest.mark.asyncio
async def test_task_cancellation(task_manager):
    """
    Test task cancellation
    
    Args:
        task_manager: TaskManager fixture
    """
    # Submit a long-running task
    task_id = await task_manager.submit(
        name="Long Task",
        task_type="test",
        params={"duration": 10.0}
    )
    
    # Wait for task to start
    await asyncio.sleep(1.0)
    
    # Cancel the task
    cancelled = await task_manager.cancel(task_id)
    
    # Check cancellation result
    assert cancelled
    
    # Check task status
    task = task_manager.get_task(task_id)
    assert task is not None
    assert task.status in (TaskStatus.CANCELLED, TaskStatus.FAILED)

@pytest.mark.asyncio
async def test_task_priorities(task_manager):
    """
    Test task priorities
    
    Args:
        task_manager: TaskManager fixture
    """
    # Submit tasks with different priorities
    low_task_id = await task_manager.submit(
        name="Low Priority Task",
        task_type="test",
        params={"duration": 1.0},
        priority=TaskPriority.LOW
    )
    
    normal_task_id = await task_manager.submit(
        name="Normal Priority Task",
        task_type="test",
        params={"duration": 1.0},
        priority=TaskPriority.NORMAL
    )
    
    high_task_id = await task_manager.submit(
        name="High Priority Task",
        task_type="test",
        params={"duration": 1.0},
        priority=TaskPriority.HIGH
    )
    
    critical_task_id = await task_manager.submit(
        name="Critical Priority Task",
        task_type="test",
        params={"duration": 1.0},
        priority=TaskPriority.CRITICAL
    )
    
    # Wait for tasks to complete
    for _ in range(20):
        tasks_completed = True
        for task_id in [low_task_id, normal_task_id, high_task_id, critical_task_id]:
            task = task_manager.get_task(task_id)
            if task and task.status not in (TaskStatus.COMPLETED, TaskStatus.FAILED):
                tasks_completed = False
                break
        if tasks_completed:
            break
        await asyncio.sleep(0.5)
    
    # Check that all tasks completed
    for task_id in [low_task_id, normal_task_id, high_task_id, critical_task_id]:
        task = task_manager.get_task(task_id)
        assert task is not None
        assert task.status == TaskStatus.COMPLETED

@pytest.mark.asyncio
async def test_concurrent_tasks(task_manager):
    """
    Test concurrent task execution
    
    Args:
        task_manager: TaskManager fixture
    """
    # Number of tasks to submit
    num_tasks = 20
    
    # Submit multiple tasks
    task_ids = []
    for i in range(num_tasks):
        task_id = await task_manager.submit(
            name=f"Concurrent Task {i}",
            task_type="test",
            params={"duration": random.uniform(0.5, 2.0)}
        )
        task_ids.append(task_id)
    
    # Wait for tasks to complete
    for _ in range(30):
        tasks_completed = True
        for task_id in task_ids:
            task = task_manager.get_task(task_id)
            if task and task.status not in (TaskStatus.COMPLETED, TaskStatus.FAILED):
                tasks_completed = False
                break
        if tasks_completed:
            break
        await asyncio.sleep(0.5)
    
    # Check that all tasks completed
    completed_count = 0
    for task_id in task_ids:
        task = task_manager.get_task(task_id)
        if task and task.status == TaskStatus.COMPLETED:
            completed_count += 1
    
    # At least 80% of tasks should complete successfully
    assert completed_count >= int(num_tasks * 0.8)

@pytest.mark.asyncio
async def test_resource_monitor():
    """
    Test resource monitor
    """
    # Create resource monitor
    monitor = ResourceMonitor()
    
    # Start monitoring
    await monitor.start()
    
    try:
        # Get resource usage
        usage = monitor.get_resource_usage()
        
        # Check that resource usage is reported
        assert "cpu_percent" in usage
        assert "memory_percent" in usage
        assert "disk_percent" in usage
        
        # Get system load
        load = monitor.get_system_load()
        
        # Check that system load is between 0 and 1
        assert 0.0 <= load <= 1.0
        
        # Get recommended concurrency
        concurrency = monitor.get_recommended_concurrency(max_concurrency=10)
        
        # Check that recommended concurrency is reasonable
        assert 1 <= concurrency <= 10
    finally:
        # Stop monitoring
        await monitor.stop()

@pytest.mark.asyncio
async def test_example_tasks(task_manager):
    """
    Test example task handlers
    
    Args:
        task_manager: TaskManager fixture
    """
    # Submit document processing task
    doc_task_id = await task_manager.submit(
        name="Process Document",
        task_type="document_processing",
        params={"document_id": "doc123"}
    )
    
    # Submit vector store update task
    vector_task_id = await task_manager.submit(
        name="Update Vector Store",
        task_type="vector_store_update",
        params={"document_ids": ["doc123", "doc456", "doc789"]}
    )
    
    # Submit report generation task
    report_task_id = await task_manager.submit(
        name="Generate Report",
        task_type="report_generation",
        params={"report_type": "summary", "document_ids": ["doc123", "doc456"]}
    )
    
    # Submit system maintenance task
    maintenance_task_id = await task_manager.submit(
        name="System Maintenance",
        task_type="system_maintenance",
        params={"maintenance_type": "cleanup"}
    )
    
    # Wait for tasks to complete
    task_ids = [doc_task_id, vector_task_id, report_task_id, maintenance_task_id]
    for _ in range(30):
        tasks_completed = True
        for task_id in task_ids:
            task = task_manager.get_task(task_id)
            if task and task.status not in (TaskStatus.COMPLETED, TaskStatus.FAILED):
                tasks_completed = False
                break
        if tasks_completed:
            break
        await asyncio.sleep(1.0)
    
    # Check that all tasks completed
    for task_id in task_ids:
        task = task_manager.get_task(task_id)
        assert task is not None
        assert task.status == TaskStatus.COMPLETED
        assert task.result is not None

@pytest.mark.asyncio
async def test_task_dependencies(task_manager):
    """
    Test task dependencies
    
    Args:
        task_manager: TaskManager fixture
    """
    # Submit first task
    first_task_id = await task_manager.submit(
        name="First Task",
        task_type="test",
        params={"duration": 1.0}
    )
    
    # Submit second task that depends on first task
    second_task_id = await task_manager.submit(
        name="Second Task",
        task_type="test",
        params={"duration": 1.0},
        dependencies=[first_task_id]
    )
    
    # Submit third task that depends on second task
    third_task_id = await task_manager.submit(
        name="Third Task",
        task_type="test",
        params={"duration": 1.0},
        dependencies=[second_task_id]
    )
    
    # Wait for tasks to complete
    for _ in range(30):
        tasks_completed = True
        for task_id in [first_task_id, second_task_id, third_task_id]:
            task = task_manager.get_task(task_id)
            if task and task.status not in (TaskStatus.COMPLETED, TaskStatus.FAILED):
                tasks_completed = False
                break
        if tasks_completed:
            break
        await asyncio.sleep(0.5)
    
    # Check that all tasks completed
    for task_id in [first_task_id, second_task_id, third_task_id]:
        task = task_manager.get_task(task_id)
        assert task is not None
        assert task.status == TaskStatus.COMPLETED

@pytest.mark.asyncio
async def test_performance():
    """
    Test system performance under load
    """
    # Create task manager with high concurrency
    manager = TaskManager(max_concurrent_tasks=20)
    
    # Register test handler
    manager.register_task_handler("test", test_task_handler)
    
    # Start task manager
    await manager.start()
    
    try:
        # Number of tasks to submit
        num_tasks = 100
        
        # Submit multiple tasks
        start_time = time.time()
        task_ids = []
        for i in range(num_tasks):
            task_id = await manager.submit(
                name=f"Performance Task {i}",
                task_type="test",
                params={"duration": random.uniform(0.1, 0.5)}
            )
            task_ids.append(task_id)
        
        # Wait for tasks to complete
        for _ in range(60):
            tasks_completed = True
            for task_id in task_ids:
                task = manager.get_task(task_id)
                if task and task.status not in (TaskStatus.COMPLETED, TaskStatus.FAILED):
                    tasks_completed = False
                    break
            if tasks_completed:
                break
            await asyncio.sleep(0.5)
        
        # Calculate elapsed time
        elapsed_time = time.time() - start_time
        
        # Check that all tasks completed
        completed_count = 0
        for task_id in task_ids:
            task = manager.get_task(task_id)
            if task and task.status == TaskStatus.COMPLETED:
                completed_count += 1
        
        # At least 90% of tasks should complete successfully
        assert completed_count >= int(num_tasks * 0.9)
        
        # Calculate throughput
        throughput = completed_count / elapsed_time
        print(f"Throughput: {throughput:.2f} tasks/second")
        
        # Get task manager stats
        stats = manager.get_stats()
        print(f"Task manager stats: {stats}")
    finally:
        # Stop task manager
        await manager.stop()

================
File: tests/test_chat_api.py
================
import requests
import json

# Test the chat API endpoint
url = "http://localhost:8000/api/chat/query"
payload = {
    "message": "Hello, how are you?",
    "use_rag": True,
    "stream": False
}

headers = {
    "Content-Type": "application/json"
}

try:
    response = requests.post(url, json=payload, headers=headers)
    print(f"Status code: {response.status_code}")
    print(f"Response: {response.text}")
except Exception as e:
    print(f"Error: {str(e)}")

================
File: tests/test_chunking_judge_phase1.py
================
"""
Test script for Phase 1 of the LLM-enhanced RAG system: Chunking Judge component.
This script tests the Chunking Judge's ability to analyze different document types
and recommend appropriate chunking strategies.
"""
import asyncio
import os
import sys
import json
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("chunking_judge_test")

# Add the app directory to the Python path if needed
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.document import Document
from app.rag.agents.chunking_judge import ChunkingJudge
from app.core.config import USE_CHUNKING_JUDGE

class MockOllamaClient:
    """Mock Ollama client for testing when the real server is not available"""
    async def generate(self, prompt, model, stream=False):
        """Mock generate method"""
        # Extract the filename from the prompt
        filename = None
        if "Document Filename:" in prompt:
            lines = prompt.split("\n")
            for line in lines:
                if line.startswith("Document Filename:"):
                    filename = line.split("Document Filename:")[1].strip()
                    break
        
        # Return a mock response based on the document type
        if filename and filename.lower().endswith(".md"):
            return {
                "response": """
                {
                    "strategy": "markdown",
                    "parameters": {
                        "chunk_size": 800,
                        "chunk_overlap": 100
                    },
                    "justification": "This is a structured markdown document with clear headers."
                }
                """
            }
        elif filename and filename.lower().endswith(".csv"):
            return {
                "response": """
                {
                    "strategy": "token",
                    "parameters": {
                        "chunk_size": 600,
                        "chunk_overlap": 75
                    },
                    "justification": "This is a CSV file with tabular data that benefits from token-based chunking."
                }
                """
            }
        else:
            return {
                "response": """
                {
                    "strategy": "recursive",
                    "parameters": {
                        "chunk_size": 1000,
                        "chunk_overlap": 150
                    },
                    "justification": "This is a plain text document with paragraphs that benefits from recursive chunking."
                }
                """
            }

async def test_chunking_judge():
    """Test the Chunking Judge with different document types"""
    logger.info("\n===== Testing Chunking Judge Component (Phase 1) =====\n")
    
    # Use mock client for testing
    logger.info("Using mock Ollama client for testing.")
    ollama_client = MockOllamaClient()
    
    # Create Chunking Judge
    chunking_judge = ChunkingJudge(ollama_client=ollama_client)
    
    # Test documents - using files from the project root
    test_files = [
        "test_document.txt",
        "technical_documentation.md",
        "test_data.csv"
    ]
    
    results = []
    
    for filename in test_files:
        if not os.path.exists(filename):
            logger.warning(f"Warning: {filename} not found, skipping...")
            continue
            
        logger.info(f"\n----- Testing with {filename} -----\n")
        
        # Read document content
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Create document object
        document = Document(
            id=f"test-{filename}",
            filename=filename,
            content=content
        )
        
        # Analyze document with Chunking Judge
        logger.info(f"Analyzing document: {filename}")
        analysis_result = await chunking_judge.analyze_document(document)
        
        # Print analysis result
        logger.info("\nChunking Judge Analysis Result:")
        logger.info(f"Strategy: {analysis_result['strategy']}")
        logger.info(f"Parameters: chunk_size={analysis_result['parameters']['chunk_size']}, " +
              f"chunk_overlap={analysis_result['parameters']['chunk_overlap']}")
        logger.info(f"Justification: {analysis_result['justification']}")
        
        # Store results
        result = {
            "filename": filename,
            "strategy": analysis_result["strategy"],
            "parameters": analysis_result["parameters"],
            "justification": analysis_result["justification"]
        }
        results.append(result)
        
        logger.info("\n" + "="*50)
    
    # Save results to a JSON file
    results_file = "chunking_judge_test_results.json"
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"\nTest results saved to {results_file}")
    
    return results

async def main():
    """Main function"""
    logger.info("Starting Chunking Judge Phase 1 test...")
    
    # Check if Chunking Judge is enabled
    if not USE_CHUNKING_JUDGE:
        logger.warning("Warning: Chunking Judge is disabled in configuration.")
        logger.warning("To enable it, set USE_CHUNKING_JUDGE=True in .env or app/core/config.py")
    
    results = await test_chunking_judge()
    
    # Print summary
    logger.info("\n===== Chunking Judge Test Summary =====")
    for result in results:
        logger.info(f"File: {result['filename']}")
        logger.info(f"Strategy: {result['strategy']}")
        logger.info(f"Parameters: chunk_size={result['parameters']['chunk_size']}, " +
              f"chunk_overlap={result['parameters']['chunk_overlap']}")
        logger.info("---")
    
    logger.info("\nChunking Judge Phase 1 test completed.")

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_chunking_judge_real.py
================
"""
Test script for Phase 1 of the LLM-enhanced RAG system: Chunking Judge component.
This script tests the Chunking Judge's ability to analyze different document types
and recommend appropriate chunking strategies using the real Ollama client.
"""
import asyncio
import os
import sys
import json
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("chunking_judge_test")

# Add the app directory to the Python path if needed
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.document import Document
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.ollama_client import OllamaClient
from app.core.config import USE_CHUNKING_JUDGE, CHUNKING_JUDGE_MODEL

async def test_chunking_judge_with_real_ollama():
    """Test the Chunking Judge with different document types using the real Ollama client"""
    logger.info("\n===== Testing Chunking Judge Component with Real Ollama (Phase 1) =====\n")
    
    # Use real Ollama client
    logger.info(f"Using real Ollama client with model: {CHUNKING_JUDGE_MODEL}")
    ollama_client = OllamaClient()
    
    # Create Chunking Judge
    chunking_judge = ChunkingJudge(ollama_client=ollama_client)
    
    # Test documents - using files from the project root
    test_files = [
        "test_document.txt",
        "technical_documentation.md",
        "test_data.csv"
    ]
    
    results = []
    
    for filename in test_files:
        if not os.path.exists(filename):
            logger.warning(f"Warning: {filename} not found, skipping...")
            continue
            
        logger.info(f"\n----- Testing with {filename} -----\n")
        
        # Read document content
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Create document object
        document = Document(
            id=f"test-{filename}",
            filename=filename,
            content=content
        )
        
        # Analyze document with Chunking Judge
        logger.info(f"Analyzing document: {filename}")
        analysis_result = await chunking_judge.analyze_document(document)
        
        # Print analysis result
        logger.info("\nChunking Judge Analysis Result:")
        logger.info(f"Strategy: {analysis_result['strategy']}")
        logger.info(f"Parameters: chunk_size={analysis_result['parameters']['chunk_size']}, " +
              f"chunk_overlap={analysis_result['parameters']['chunk_overlap']}")
        logger.info(f"Justification: {analysis_result['justification']}")
        
        # Store results
        result = {
            "filename": filename,
            "strategy": analysis_result["strategy"],
            "parameters": analysis_result["parameters"],
            "justification": analysis_result["justification"]
        }
        results.append(result)
        
        logger.info("\n" + "="*50)
    
    # Save results to a JSON file
    results_file = "chunking_judge_real_results.json"
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"\nTest results saved to {results_file}")
    
    return results

async def main():
    """Main function"""
    logger.info("Starting Chunking Judge Phase 1 test with real Ollama...")
    
    # Check if Chunking Judge is enabled
    if not USE_CHUNKING_JUDGE:
        logger.warning("Warning: Chunking Judge is disabled in configuration.")
        logger.warning("To enable it, set USE_CHUNKING_JUDGE=True in .env or app/core/config.py")
    
    results = await test_chunking_judge_with_real_ollama()
    
    # Print summary
    logger.info("\n===== Chunking Judge Test Summary =====")
    for result in results:
        logger.info(f"File: {result['filename']}")
        logger.info(f"Strategy: {result['strategy']}")
        logger.info(f"Parameters: chunk_size={result['parameters']['chunk_size']}, " +
              f"chunk_overlap={result['parameters']['chunk_overlap']}")
        logger.info("---")
    
    logger.info("\nChunking Judge Phase 1 test with real Ollama completed.")

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_db_connection_simple.py
================
import asyncio
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text
from dotenv import load_dotenv

async def test_connection():
    # Load environment variables from .env file
    load_dotenv()
    
    # Get database URL from environment
    DATABASE_URL = os.getenv("DATABASE_URL")
    print(f"DATABASE_URL from environment: {DATABASE_URL}")
    
    # Ensure we're using the correct URL
    if not DATABASE_URL or "asyncpg" not in DATABASE_URL:
        DATABASE_URL = "postgresql+asyncpg://postgres:postgres@localhost:5432/metis_rag"
        print(f"Using hardcoded DATABASE_URL: {DATABASE_URL}")
    
    print(f"Testing connection to {DATABASE_URL}")
    engine = create_async_engine(DATABASE_URL)
    async_session = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)
    
    try:
        async with async_session() as session:
            result = await session.execute(text("SELECT 1"))
            print(f"Connection successful: {result.scalar()}")
    except Exception as e:
        print(f"Connection failed: {str(e)}")
    finally:
        await engine.dispose()

if __name__ == "__main__":
    asyncio.run(test_connection())

================
File: tests/test_db_connection.py
================
import asyncio
import os
from sqlalchemy.ext.asyncio import create_async_engine
from dotenv import load_dotenv

async def test_connection():
    # Load environment variables from .env file
    load_dotenv()
    
    # Get database URL from environment
    database_url = os.getenv("DATABASE_URL")
    print(f"DATABASE_URL from environment: {database_url}")
    
    # Ensure we're using the correct URL
    if not database_url or "asyncpg" not in database_url:
        database_url = "postgresql+asyncpg://postgres:postgres@localhost:5432/metis_rag"
        print(f"Using hardcoded DATABASE_URL: {database_url}")
    
    print(f"Testing connection to {database_url}")
    
    # Create engine
    engine = create_async_engine(
        database_url,
        echo=True,  # Enable SQL logging
    )
    
    # Test connection
    try:
        async with engine.begin() as conn:
            # Execute a simple query
            from sqlalchemy import text
            result = await conn.execute(text("SELECT 1"))
            print(f"Connection successful: {result.scalar()}")
    except Exception as e:
        print(f"Connection failed: {str(e)}")
    finally:
        await engine.dispose()

if __name__ == "__main__":
    asyncio.run(test_connection())

================
File: tests/test_db_simple.py
================
import asyncio
from app.db.session import AsyncSessionLocal
from sqlalchemy import text

async def test_db_connection():
    print("Testing database connection...")
    try:
        async with AsyncSessionLocal() as session:
            result = await session.execute(text("SELECT 1"))
            print(f"Connection successful: {result.scalar()}")
    except Exception as e:
        print(f"Connection failed: {str(e)}")

if __name__ == "__main__":
    asyncio.run(test_db_connection())

================
File: tests/test_document_processing_performance.py
================
import os
import time
import asyncio
import pytest
import statistics
import tempfile
import uuid
from typing import List, Dict, Any
from pathlib import Path

from app.rag.document_processor import DocumentProcessor
from app.rag.document_analysis_service import DocumentAnalysisService
from app.rag.vector_store import VectorStore
from app.db.repositories.document_repository import DocumentRepository
from app.db.session import SessionLocal
from app.models.document import Document
from app.core.config import SETTINGS, UPLOAD_DIR, DATABASE_TYPE
from app.db.models import Folder

# Test configuration
TEST_FILE_SIZES = [
    ("small", 5),      # 5 KB
    ("medium", 50),    # 50 KB
    ("large", 500),    # 500 KB
    ("xlarge", 2000)   # 2 MB
]

TEST_CHUNKING_STRATEGIES = [
    "recursive",
    "token",
    "markdown",
    "semantic"
]

# Number of times to run each test for averaging
NUM_RUNS = 3

@pytest.fixture
def db_session():
    """Create a database session for testing"""
    session = SessionLocal()
    try:
        yield session
    finally:
        session.close()

@pytest.fixture
def document_repository(db_session):
    """Create a document repository for testing"""
    return DocumentRepository(db_session)

@pytest.fixture
def document_processor():
    """Create a document processor for testing"""
    return DocumentProcessor()

@pytest.fixture
def document_analysis_service():
    """Create a document analysis service for testing"""
    return DocumentAnalysisService()

@pytest.fixture
def vector_store():
    """Create a vector store for testing"""
    return VectorStore()

def generate_test_file(size_kb: int, file_type: str = "txt"):
    """Generate a test file of the specified size"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix=f".{file_type}")
    
    try:
        # Generate content
        content = ""
        if file_type == "txt":
            # Generate paragraphs of text
            paragraph = "This is a test paragraph for document processing performance testing. " * 10
            paragraphs_needed = (size_kb * 1024) // len(paragraph)
            content = "\n\n".join([paragraph for _ in range(paragraphs_needed)])
        elif file_type == "md":
            # Generate markdown content
            paragraph = "This is a test paragraph for document processing performance testing. " * 10
            paragraphs_needed = (size_kb * 1024) // (len(paragraph) + 20)  # Account for markdown syntax
            
            content = "# Test Document\n\n"
            for i in range(paragraphs_needed):
                content += f"## Section {i+1}\n\n{paragraph}\n\n"
        
        # Write content to file
        with os.fdopen(fd, 'w') as f:
            f.write(content)
        
        return path
    except Exception as e:
        os.close(fd)
        os.unlink(path)
        raise e

async def process_document(document_processor, document: Document, chunking_strategy: str):
    """Process a document and measure performance"""
    start_time = time.time()
    
    # Configure processor with the specified chunking strategy
    processor = DocumentProcessor(
        chunking_strategy=chunking_strategy,
        chunk_size=SETTINGS.chunk_size,
        chunk_overlap=SETTINGS.chunk_overlap
    )
    
    # Process document
    processed_document = await processor.process_document(document)
    
    elapsed_time = time.time() - start_time
    
    # Convert UUID to string if needed
    document_id_str = str(document.id) if document.id else None
    
    # Create document directory path
    document_dir = os.path.join(UPLOAD_DIR, document_id_str)
    document_file_path = os.path.join(document_dir, document.filename)
    
    return {
        "document_id": document_id_str,
        "filename": document.filename,
        "chunking_strategy": chunking_strategy,
        "elapsed_time": elapsed_time,
        "chunk_count": len(processed_document.chunks),
        "file_size": os.path.getsize(document_file_path)
    }

async def analyze_document(document_analysis_service, document: Document):
    """Analyze a document and measure performance"""
    start_time = time.time()
    
    # Analyze document
    analysis_result = await document_analysis_service.analyze_document(document)
    
    elapsed_time = time.time() - start_time
    
    # Convert UUID to string if needed
    document_id_str = str(document.id) if document.id else None
    
    # Create document directory path
    document_dir = os.path.join(UPLOAD_DIR, document_id_str)
    document_file_path = os.path.join(document_dir, document.filename)
    
    return {
        "document_id": document_id_str,
        "filename": document.filename,
        "elapsed_time": elapsed_time,
        "recommended_strategy": analysis_result.get("strategy"),
        "file_size": os.path.getsize(document_file_path)
    }

def ensure_test_folder_exists(document_repository):
    """Ensure the test folder exists"""
    # Create the test folder using the document repository's helper method
    document_repository._ensure_folder_exists("/test")

async def run_processing_tests(document_repository, document_processor, vector_store):
    """Run document processing performance tests"""
    results = []
    
    # Create test directory if it doesn't exist
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    
    # Ensure the test folder exists in the database
    ensure_test_folder_exists(document_repository)
    
    for size_name, size_kb in TEST_FILE_SIZES:
        for file_type in ["txt", "md"]:
            # Generate test file
            file_path = generate_test_file(size_kb, file_type)
            file_name = f"test_{size_name}.{file_type}"
            
            try:
                # Create document in the database
                document = document_repository.create_document(
                    filename=file_name,
                    content="",
                    metadata={"file_type": file_type, "test_size": size_name},
                    folder="/test"
                )
                
                # Get document ID as string
                document_id_str = str(document.id)
                
                # Create document directory
                document_dir = os.path.join(UPLOAD_DIR, document_id_str)
                os.makedirs(document_dir, exist_ok=True)
                
                # Copy test file to document directory
                document_file_path = os.path.join(document_dir, file_name)
                with open(file_path, 'r') as src, open(document_file_path, 'w') as dst:
                    dst.write(src.read())
                
                # Test each chunking strategy
                for strategy in TEST_CHUNKING_STRATEGIES:
                    strategy_results = []
                    
                    for _ in range(NUM_RUNS):
                        try:
                            result = await process_document(document_processor, document, strategy)
                            strategy_results.append(result)
                        except Exception as e:
                            print(f"Error processing document {file_name}: {e}")
                            raise
                    
                    # Calculate average metrics
                    avg_time = statistics.mean([r["elapsed_time"] for r in strategy_results])
                    avg_chunk_count = statistics.mean([r["chunk_count"] for r in strategy_results])
                    
                    results.append({
                        "document_id": document_id_str,
                        "filename": file_name,
                        "file_type": file_type,
                        "size_name": size_name,
                        "size_kb": size_kb,
                        "chunking_strategy": strategy,
                        "avg_elapsed_time": avg_time,
                        "avg_chunk_count": avg_chunk_count,
                        "num_runs": NUM_RUNS
                    })
            finally:
                # Clean up test file
                try:
                    os.unlink(file_path)
                except:
                    pass
    
    return results

async def run_analysis_tests(document_repository, document_analysis_service):
    """Run document analysis performance tests"""
    results = []
    
    # Create test directory if it doesn't exist
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    
    # Ensure the test folder exists in the database
    ensure_test_folder_exists(document_repository)
    
    for size_name, size_kb in TEST_FILE_SIZES:
        for file_type in ["txt", "md"]:
            # Generate test file
            file_path = generate_test_file(size_kb, file_type)
            file_name = f"test_analysis_{size_name}.{file_type}"
            
            try:
                # Create document in the database
                document = document_repository.create_document(
                    filename=file_name,
                    content="",
                    metadata={"file_type": file_type, "test_size": size_name},
                    folder="/test"
                )
                
                # Get document ID as string
                document_id_str = str(document.id)
                
                # Create document directory
                document_dir = os.path.join(UPLOAD_DIR, document_id_str)
                os.makedirs(document_dir, exist_ok=True)
                
                # Copy test file to document directory
                document_file_path = os.path.join(document_dir, file_name)
                with open(file_path, 'r') as src, open(document_file_path, 'w') as dst:
                    dst.write(src.read())
                
                # Run analysis multiple times
                analysis_results = []
                for _ in range(NUM_RUNS):
                    try:
                        result = await analyze_document(document_analysis_service, document)
                        analysis_results.append(result)
                    except Exception as e:
                        print(f"Error analyzing document {file_name}: {e}")
                        raise
                
                # Calculate average metrics
                avg_time = statistics.mean([r["elapsed_time"] for r in analysis_results])
                
                results.append({
                    "document_id": document_id_str,
                    "filename": file_name,
                    "file_type": file_type,
                    "size_name": size_name,
                    "size_kb": size_kb,
                    "avg_elapsed_time": avg_time,
                    "recommended_strategy": analysis_results[0]["recommended_strategy"],
                    "num_runs": NUM_RUNS
                })
            finally:
                # Clean up test file
                try:
                    os.unlink(file_path)
                except:
                    pass
    
    return results

@pytest.mark.asyncio
async def test_document_processing_performance(document_repository, document_processor, vector_store):
    """Test document processing performance"""
    print(f"\nRunning document processing performance test with {len(TEST_FILE_SIZES)} file sizes, {len(TEST_CHUNKING_STRATEGIES)} chunking strategies")
    
    # Run processing tests
    results = await run_processing_tests(document_repository, document_processor, vector_store)
    
    # Print results
    print("\nDocument Processing Performance Results:")
    print(f"{'File':<20} | {'Size':<10} | {'Strategy':<15} | {'Avg Time (s)':<12} | {'Avg Chunks':<10}")
    print("-" * 80)
    
    for result in results:
        print(f"{result['filename']:<20} | {result['size_kb']} KB{' ':<5} | {result['chunking_strategy']:<15} | {result['avg_elapsed_time']:<12.2f} | {result['avg_chunk_count']:<10.0f}")
    
    # Calculate overall metrics by strategy
    strategies = {}
    for strategy in TEST_CHUNKING_STRATEGIES:
        strategy_results = [r for r in results if r["chunking_strategy"] == strategy]
        avg_time = statistics.mean([r["avg_elapsed_time"] for r in strategy_results])
        strategies[strategy] = avg_time
    
    print("\nAverage Processing Time by Strategy:")
    for strategy, avg_time in strategies.items():
        print(f"{strategy:<15}: {avg_time:.2f}s")
    
    # Calculate overall metrics by file size
    sizes = {}
    for size_name, size_kb in TEST_FILE_SIZES:
        size_results = [r for r in results if r["size_name"] == size_name]
        avg_time = statistics.mean([r["avg_elapsed_time"] for r in size_results])
        sizes[size_name] = avg_time
    
    print("\nAverage Processing Time by File Size:")
    for size_name, avg_time in sizes.items():
        print(f"{size_name:<10}: {avg_time:.2f}s")

@pytest.mark.asyncio
async def test_document_analysis_performance(document_repository, document_analysis_service):
    """Test document analysis performance"""
    print(f"\nRunning document analysis performance test with {len(TEST_FILE_SIZES)} file sizes")
    
    # Run analysis tests
    results = await run_analysis_tests(document_repository, document_analysis_service)
    
    # Print results
    print("\nDocument Analysis Performance Results:")
    print(f"{'File':<20} | {'Size':<10} | {'Avg Time (s)':<12} | {'Recommended Strategy':<20}")
    print("-" * 80)
    
    for result in results:
        print(f"{result['filename']:<20} | {result['size_kb']} KB{' ':<5} | {result['avg_elapsed_time']:<12.2f} | {result['recommended_strategy']:<20}")
    
    # Calculate overall metrics by file size
    sizes = {}
    for size_name, size_kb in TEST_FILE_SIZES:
        size_results = [r for r in results if r["size_name"] == size_name]
        avg_time = statistics.mean([r["avg_elapsed_time"] for r in size_results])
        sizes[size_name] = avg_time
    
    print("\nAverage Analysis Time by File Size:")
    for size_name, avg_time in sizes.items():
        print(f"{size_name:<10}: {avg_time:.2f}s")

if __name__ == "__main__":
    # Run the tests directly if this file is executed
    asyncio.run(test_document_processing_performance(
        DocumentRepository(SessionLocal()),
        DocumentProcessor(),
        VectorStore()
    ))
    asyncio.run(test_document_analysis_performance(
        DocumentRepository(SessionLocal()),
        DocumentAnalysisService()
    ))

================
File: tests/test_document_repository.py
================
import asyncio
import logging
import sys
import os
import nest_asyncio

# Add the parent directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from sqlalchemy.ext.asyncio import AsyncSession
from app.db.session import AsyncSessionLocal, engine
from app.db.repositories.document_repository import DocumentRepository

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Apply nest_asyncio to allow nested event loops
nest_asyncio.apply()

async def test_create_document():
    """Test creating a document using the DocumentRepository"""
    # Create a new session
    session = AsyncSessionLocal()
    try:
        # Create a document repository
        document_repository = DocumentRepository(session)
        
        # Create a document
        document = await document_repository.create_document(
            filename="test_document.txt",
            content="This is a test document.",
            metadata={"source": "test"},
            tags=["test", "sample"],
            folder="/"
        )
        
        logger.info(f"Document created: {document.id}")
        return document
    except Exception as e:
        logger.error(f"Error creating document: {str(e)}")
        raise
    finally:
        await session.close()

async def main():
    """Main function"""
    try:
        document = await test_create_document()
        logger.info(f"Test completed successfully. Document ID: {document.id}")
    except Exception as e:
        logger.error(f"Test failed: {str(e)}")

if __name__ == "__main__":
    # Install the uvloop event loop policy if available
    try:
        import uvloop
        uvloop.install()
    except ImportError:
        pass
    
    # Run the main function
    asyncio.run(main())

================
File: tests/test_document_upload_enhanced.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Enhanced Document Upload</title>
    <link rel="stylesheet" href="app/static/css/styles.css">
    <link rel="stylesheet" href="app/static/css/document-manager.css">
    <link rel="stylesheet" href="app/static/css/document-upload-enhanced.css">
    <style>
        body {
            background-color: #121212;
            color: #f0f0f0;
            font-family: Arial, sans-serif;
        }
        .documents-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        .section-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="documents-container">
        <div class="document-section">
            <div class="section-header">
                <h2>Document Management (Test)</h2>
            </div>
            
            <div class="upload-form">
                <h3>Upload Documents</h3>
                <div id="drop-zone" class="drop-zone">
                    <p>Drag and drop files here or click to select</p>
                    <p class="supported-formats">Supported formats: PDF, Word, Text, CSV, Markdown, HTML, JSON, XML</p>
                    <form id="upload-form">
                        <div class="upload-input">
                            <input type="file" id="document-file" accept=".pdf,.txt,.csv,.md,.docx,.doc,.rtf,.html,.json,.xml" multiple required>
                        </div>
                        
                        <div class="form-row">
                            <div class="form-group">
                                <label for="doc-tags">Tags (comma separated)</label>
                                <input type="text" id="doc-tags" placeholder="e.g. important, reference, work">
                            </div>
                            
                            <div class="form-group">
                                <label for="doc-folder">Folder</label>
                                <select id="doc-folder">
                                    <option value="/">Root</option>
                                    <!-- Folders will be loaded dynamically -->
                                </select>
                            </div>
                        </div>
                        
                        <button type="submit">Upload Documents</button>
                    </form>
                </div>
                
                <div id="file-list" class="file-list">
                    <!-- Selected files will be displayed here -->
                </div>
                
                <div class="progress-container">
                    <div class="overall-progress">
                        <label>Overall Progress:</label>
                        <div class="progress-bar" id="overall-progress">
                            <div class="progress-bar-fill" id="overall-progress-fill"></div>
                        </div>
                    </div>
                    <div id="file-progress-list">
                        <!-- Individual file progress bars will be added here -->
                    </div>
                </div>
            </div>
            
            <div class="filter-section">
                <div class="filter-title">
                    <h3>Filter Documents</h3>
                    <button id="filter-toggle" class="filter-toggle">
                        <i class="fas fa-chevron-down"></i>
                    </button>
                </div>
                
                <div id="filter-content" class="filter-content">
                    <div class="filter-section">
                        <div class="filter-section-title">Tags</div>
                        <div id="filter-tags" class="filter-tags">
                            <!-- Tags will be loaded dynamically -->
                        </div>
                    </div>
                    
                    <div class="filter-section">
                        <div class="filter-section-title">Folders</div>
                        <div id="filter-folders" class="filter-folders">
                            <!-- Folders will be loaded dynamically -->
                        </div>
                    </div>
                    
                    <div class="filter-actions">
                        <button id="apply-filters">Apply Filters</button>
                        <button id="clear-filters">Clear Filters</button>
                    </div>
                </div>
            </div>
            
            <div class="document-section">
                <h3>Document Processing Options</h3>
                <div class="processing-options">
                    <div class="form-row">
                        <div class="form-group">
                            <label for="chunking-strategy">Chunking Strategy</label>
                            <select id="chunking-strategy">
                                <option value="recursive">Recursive (Default)</option>
                                <option value="token">Token-based</option>
                                <option value="markdown">Markdown Headers</option>
                            </select>
                            <div class="param-description">Choose how documents are split into chunks</div>
                        </div>
                        
                        <div class="form-group">
                            <label for="chunk-size">Chunk Size</label>
                            <input type="number" id="chunk-size" placeholder="Default: 1000" min="100" max="4000">
                            <div class="param-description">Size of each chunk in characters</div>
                        </div>
                        
                        <div class="form-group">
                            <label for="chunk-overlap">Chunk Overlap</label>
                            <input type="number" id="chunk-overlap" placeholder="Default: 200" min="0" max="1000">
                            <div class="param-description">Overlap between chunks in characters</div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div id="document-list" class="document-grid">
                <!-- Documents will be loaded dynamically -->
                <div class="document-loading">No documents loaded in test mode</div>
            </div>
        </div>
    </div>

    <!-- Load Font Awesome for icons -->
    <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>
    
    <!-- Load the enhanced document upload script -->
    <script src="app/static/js/document-upload-enhanced.js"></script>
    
    <!-- Test script to show notifications -->
    <script>
        // Define showNotification function for testing
        window.showNotification = function(message, type = 'info') {
            const notification = document.createElement('div');
            notification.className = `notification ${type}`;
            notification.style.position = 'fixed';
            notification.style.top = '20px';
            notification.style.right = '20px';
            notification.style.backgroundColor = type === 'warning' ? '#ff9800' : '#2e8b57';
            notification.style.color = 'white';
            notification.style.padding = '10px 15px';
            notification.style.borderRadius = '4px';
            notification.style.boxShadow = '0 2px 10px rgba(0, 0, 0, 0.2)';
            notification.style.zIndex = '1000';
            notification.style.maxWidth = '300px';
            notification.textContent = message;
            
            // Add close button
            const closeBtn = document.createElement('span');
            closeBtn.innerHTML = '&times;';
            closeBtn.style.marginLeft = '10px';
            closeBtn.style.cursor = 'pointer';
            closeBtn.style.fontWeight = 'bold';
            closeBtn.onclick = function() {
                document.body.removeChild(notification);
            };
            notification.appendChild(closeBtn);
            
            // Add to body
            document.body.appendChild(notification);
            
            // Auto remove after 5 seconds
            setTimeout(() => {
                if (document.body.contains(notification)) {
                    document.body.removeChild(notification);
                }
            }, 5000);
        };
    </script>
</body>
</html>

================
File: tests/test_document_upload.py
================
#!/usr/bin/env python3
"""
Script to test document upload with authentication.
"""

import requests
import json
import os

# Base URL for the API
BASE_URL = "http://localhost:8000/api"

# Test user credentials
TEST_USER = {
    "username": "testuser123",
    "password": "testpassword123"
}

def main():
    print("Testing document upload with authentication...")
    
    # Get authentication token
    print(f"Authenticating as {TEST_USER['username']}...")
    response = requests.post(
        f"{BASE_URL}/auth/token",
        data={
            "username": TEST_USER["username"],
            "password": TEST_USER["password"]
        }
    )
    
    if response.status_code != 200:
        print(f"Authentication failed: {response.status_code} - {response.text}")
        return False
    
    token_data = response.json()
    access_token = token_data["access_token"]
    print(f"Authentication successful, received token: {access_token[:20]}...")
    
    # Create a test document
    with open("test_upload_document.txt", "w") as f:
        f.write("This is a test document for upload testing.")
    
    # Upload the document
    print("Uploading document...")
    headers = {"Authorization": f"Bearer {access_token}"}
    
    with open("test_upload_document.txt", "rb") as f:
        files = {"file": ("test_upload_document.txt", f, "text/plain")}
        response = requests.post(
            f"{BASE_URL}/documents/upload",
            headers=headers,
            files=files,
            data={"tags": "test,upload", "folder": "/test"}
        )
    
    if response.status_code == 200:
        result = response.json()
        print(f"Document uploaded successfully: {result['document_id']}")
        return True
    else:
        print(f"Document upload failed: {response.status_code} - {response.text}")
        return False

if __name__ == "__main__":
    success = main()
    print("\n" + "="*80)
    print(f"Document Upload Test: {'✓ Success' if success else '✗ Failed'}")
    print("="*80)

================
File: tests/test_edge_cases.py
================
#!/usr/bin/env python3
"""
Edge case test suite for the Metis RAG system.
This test suite focuses on unusual inputs, error handling, and system resilience.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
import random
import string
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, MagicMock, AsyncMock
from io import BytesIO

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_edge_cases")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document, Chunk
from app.models.chat import ChatQuery
from app.main import app

# Test client
client = TestClient(app)

# Helper functions
def generate_random_string(length):
    """Generate a random string of specified length"""
    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))

def generate_binary_file(size_kb):
    """Generate a binary file of specified size in KB"""
    return os.urandom(size_kb * 1024)

@pytest.fixture
def test_dir():
    """Create a temporary directory for test files"""
    temp_dir = tempfile.mkdtemp(prefix="metis_rag_edge_")
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.mark.asyncio
async def test_empty_document():
    """Test handling of empty documents"""
    # Create an empty document
    empty_doc = Document(
        id=str(uuid.uuid4()),
        filename="empty.txt",
        content=""
    )
    
    # Process the document
    processor = DocumentProcessor()
    
    try:
        processed_doc = await processor.process_document(empty_doc)
        
        # Check if processing succeeded
        assert len(processed_doc.chunks) == 0, "Empty document should result in no chunks"
        logger.info("Empty document processed successfully with 0 chunks")
        
    except Exception as e:
        # Check if the error is handled gracefully
        logger.info(f"Empty document processing resulted in error: {str(e)}")
        assert False, f"Empty document should be handled gracefully, but got error: {str(e)}"

@pytest.mark.asyncio
async def test_very_long_document(test_dir):
    """Test handling of very long documents"""
    # Create a very long document (10 MB)
    file_path = os.path.join(test_dir, "very_long.txt")
    content = generate_random_string(10 * 1024 * 1024 // 10)  # 10 MB of text (approximated)
    
    with open(file_path, "w") as f:
        f.write(content)
    
    # Create document object
    long_doc = Document(
        id=str(uuid.uuid4()),
        filename="very_long.txt",
        content=content
    )
    
    # Process the document
    processor = DocumentProcessor()
    
    try:
        processed_doc = await processor.process_document(long_doc)
        
        # Check if processing succeeded
        assert len(processed_doc.chunks) > 0, "Long document should be chunked"
        logger.info(f"Very long document processed successfully with {len(processed_doc.chunks)} chunks")
        
    except Exception as e:
        # Check if the error is handled gracefully
        logger.info(f"Very long document processing resulted in error: {str(e)}")
        assert False, f"Very long document should be handled gracefully, but got error: {str(e)}"

@pytest.mark.asyncio
async def test_malformed_document(test_dir):
    """Test handling of malformed documents"""
    # Create a malformed document (binary data with text extension)
    file_path = os.path.join(test_dir, "malformed.txt")
    content = generate_binary_file(10)  # 10 KB of binary data
    
    with open(file_path, "wb") as f:
        f.write(content)
    
    # Try to upload the malformed document via API
    with open(file_path, "rb") as f:
        response = client.post(
            "/api/documents/upload",
            files={"file": ("malformed.txt", f, "text/plain")}
        )
    
    # Check if the API handles the malformed document gracefully
    logger.info(f"Malformed document upload response: {response.status_code}")
    logger.info(f"Response content: {response.content}")
    
    # The API should either accept the file (status 200) or reject it with a clear error
    if response.status_code == 200:
        # If accepted, try to process it
        upload_data = response.json()
        document_id = upload_data["document_id"]
        
        process_response = client.post(
            "/api/documents/process",
            json={"document_ids": [document_id]}
        )
        
        logger.info(f"Malformed document processing response: {process_response.status_code}")
        logger.info(f"Process response content: {process_response.content}")
        
        # Clean up
        client.delete(f"/api/documents/{document_id}")
    
    # We don't assert specific behavior here, just log what happens
    # The important thing is that the system doesn't crash

@pytest.mark.asyncio
async def test_very_long_query():
    """Test handling of very long queries"""
    # Create a very long query (10 KB)
    long_query = generate_random_string(10 * 1024)
    
    # Try to query the system
    response = client.post(
        "/api/chat/query",
        json={
            "message": long_query,
            "use_rag": True,
            "stream": False
        }
    )
    
    # Check if the API handles the long query gracefully
    logger.info(f"Very long query response: {response.status_code}")
    
    # The API should either process the query or reject it with a clear error
    assert response.status_code in [200, 400, 422], f"Unexpected status code: {response.status_code}"
    
    if response.status_code == 200:
        logger.info("Very long query was processed successfully")
    else:
        logger.info(f"Very long query was rejected with status {response.status_code}")
        logger.info(f"Response content: {response.content}")

@pytest.mark.asyncio
async def test_special_characters_query():
    """Test handling of queries with special characters"""
    # Create queries with special characters
    special_queries = [
        "What is RAG? <script>alert('XSS')</script>",
        "SELECT * FROM documents WHERE content LIKE '%secret%'",
        "curl -X POST http://example.com --data 'payload'",
        "Document\0with\0null\0bytes",
        "Multi\nline\nquery\nwith\nnewlines",
        "Query with emoji 🔍 and Unicode characters 你好",
        "Query with control characters \x01\x02\x03\x04\x05"
    ]
    
    results = []
    
    for query in special_queries:
        # Try to query the system
        response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        # Check if the API handles the special query gracefully
        logger.info(f"Special query '{query[:20]}...' response: {response.status_code}")
        
        # Store results
        results.append({
            "query": query,
            "status_code": response.status_code,
            "success": response.status_code == 200
        })
        
        # The API should either process the query or reject it with a clear error
        assert response.status_code in [200, 400, 422], f"Unexpected status code: {response.status_code}"
    
    # Save results to file
    results_path = "test_special_queries_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Special queries test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_network_failure():
    """Test handling of network failures when communicating with Ollama"""
    # Create a mock RAG engine with a failing Ollama client
    ollama_client = AsyncMock()
    ollama_client.generate.side_effect = Exception("Simulated network failure")
    
    rag_engine = RAGEngine(
        ollama_client=ollama_client,
        vector_store=AsyncMock()
    )
    
    # Try to query the RAG engine
    try:
        response = await rag_engine.query(
            query="Test query",
            use_rag=False,  # Disable RAG to focus on Ollama failure
            stream=False
        )
        
        # Check if the error is handled gracefully
        logger.info(f"Network failure response: {response}")
        assert "error" in response, "Network failure should result in an error response"
        
    except Exception as e:
        # If an exception is raised, the error wasn't handled gracefully
        logger.error(f"Network failure resulted in unhandled exception: {str(e)}")
        assert False, f"Network failure should be handled gracefully, but got exception: {str(e)}"

@pytest.mark.asyncio
async def test_invalid_model():
    """Test handling of invalid model name"""
    # Try to query with an invalid model name
    response = client.post(
        "/api/chat/query",
        json={
            "message": "Test query",
            "model": "non_existent_model_12345",
            "use_rag": False,
            "stream": False
        }
    )
    
    # Check if the API handles the invalid model gracefully
    logger.info(f"Invalid model response: {response.status_code}")
    logger.info(f"Response content: {response.content}")
    
    # The API should either fall back to a default model or return a clear error
    assert response.status_code in [200, 400, 422], f"Unexpected status code: {response.status_code}"

@pytest.mark.asyncio
async def test_concurrent_document_processing():
    """Test handling of concurrent document processing requests"""
    # Create multiple small documents
    document_count = 5
    document_ids = []
    
    for i in range(document_count):
        # Create a small document
        content = f"Test document {i}\n" * 100
        
        # Upload the document
        file_obj = BytesIO(content.encode())
        file_obj.name = f"concurrent_test_{i}.txt"
        
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (file_obj.name, file_obj, "text/plain")}
        )
        
        assert upload_response.status_code == 200
        upload_data = upload_response.json()
        document_ids.append(upload_data["document_id"])
    
    # Process documents concurrently
    async def process_document(doc_id):
        process_response = client.post(
            "/api/documents/process",
            json={"document_ids": [doc_id]}
        )
        return {
            "document_id": doc_id,
            "status_code": process_response.status_code,
            "success": process_response.status_code == 200
        }
    
    # Create tasks for concurrent processing
    tasks = [process_document(doc_id) for doc_id in document_ids]
    
    # Execute tasks concurrently
    results = await asyncio.gather(*tasks)
    
    # Check results
    success_count = sum(1 for r in results if r["success"])
    logger.info(f"Concurrent processing results: {success_count}/{len(results)} successful")
    
    # Clean up
    for doc_id in document_ids:
        client.delete(f"/api/documents/{doc_id}")
    
    # Save results to file
    results_path = "test_concurrent_processing_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Concurrent processing test results saved to {os.path.abspath(results_path)}")
    
    # Assert that at least some documents were processed successfully
    assert success_count > 0, "No documents were processed successfully"

@pytest.mark.asyncio
async def test_invalid_file_types():
    """Test handling of invalid file types"""
    invalid_files = [
        ("executable.exe", generate_binary_file(10), "application/octet-stream"),
        ("image.jpg", generate_binary_file(10), "image/jpeg"),
        ("archive.zip", generate_binary_file(10), "application/zip"),
        ("empty.txt", b"", "text/plain")
    ]
    
    results = []
    
    for filename, content, content_type in invalid_files:
        # Try to upload the invalid file
        file_obj = BytesIO(content)
        file_obj.name = filename
        
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (filename, file_obj, content_type)}
        )
        
        # Check if the API handles the invalid file gracefully
        logger.info(f"Invalid file '{filename}' upload response: {upload_response.status_code}")
        
        # Store results
        result = {
            "filename": filename,
            "content_type": content_type,
            "status_code": upload_response.status_code,
            "accepted": upload_response.status_code == 200
        }
        
        # If the file was accepted, try to process it
        if upload_response.status_code == 200:
            upload_data = upload_response.json()
            document_id = upload_data["document_id"]
            
            process_response = client.post(
                "/api/documents/process",
                json={"document_ids": [document_id]}
            )
            
            result["process_status_code"] = process_response.status_code
            result["process_success"] = process_response.status_code == 200
            
            # Clean up
            client.delete(f"/api/documents/{document_id}")
        
        results.append(result)
    
    # Save results to file
    results_path = "test_invalid_files_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Invalid file types test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_malformed_requests():
    """Test handling of malformed API requests"""
    malformed_requests = [
        # Missing required fields
        {"endpoint": "/api/chat/query", "data": {}, "method": "POST"},
        # Invalid data types
        {"endpoint": "/api/chat/query", "data": {"message": 123, "use_rag": "yes"}, "method": "POST"},
        # Extra fields
        {"endpoint": "/api/chat/query", "data": {"message": "Test", "invalid_field": "value"}, "method": "POST"},
        # Invalid JSON
        {"endpoint": "/api/documents/process", "data": "not_json", "method": "POST"},
        # Invalid HTTP method
        {"endpoint": "/api/documents/upload", "data": {}, "method": "PUT"}
    ]
    
    results = []
    
    for req in malformed_requests:
        # Try to make the malformed request
        try:
            if req["method"] == "POST":
                if req["data"] == "not_json":
                    # Send invalid JSON
                    response = client.post(
                        req["endpoint"],
                        data="not_json",
                        headers={"Content-Type": "application/json"}
                    )
                else:
                    # Send normal JSON
                    response = client.post(req["endpoint"], json=req["data"])
            elif req["method"] == "PUT":
                response = client.put(req["endpoint"], json=req["data"])
            else:
                response = client.get(req["endpoint"])
            
            # Check if the API handles the malformed request gracefully
            logger.info(f"Malformed request to {req['endpoint']} response: {response.status_code}")
            
            # Store results
            results.append({
                "endpoint": req["endpoint"],
                "method": req["method"],
                "data": str(req["data"]),
                "status_code": response.status_code,
                "is_error": response.status_code >= 400
            })
            
            # The API should return an error for malformed requests
            assert response.status_code >= 400, f"Malformed request should return error, got {response.status_code}"
            
        except Exception as e:
            # If an exception is raised, the error wasn't handled gracefully
            logger.error(f"Malformed request resulted in unhandled exception: {str(e)}")
            results.append({
                "endpoint": req["endpoint"],
                "method": req["method"],
                "data": str(req["data"]),
                "exception": str(e),
                "is_error": True
            })
    
    # Save results to file
    results_path = "test_malformed_requests_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Malformed requests test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_vector_store_resilience():
    """Test vector store resilience to invalid operations"""
    # Create a vector store
    vector_store = VectorStore()
    
    # Test operations that might cause issues
    edge_cases = [
        # Search with empty query
        {"operation": "search", "args": {"query": ""}},
        # Search with very long query
        {"operation": "search", "args": {"query": generate_random_string(10000)}},
        # Search with invalid filter
        {"operation": "search", "args": {"query": "test", "filter_criteria": {"invalid": "filter"}}},
        # Delete non-existent document
        {"operation": "delete", "args": {"document_id": "non_existent_id"}},
        # Update metadata for non-existent document
        {"operation": "update_metadata", "args": {"document_id": "non_existent_id", "metadata_update": {"tag": "value"}}}
    ]
    
    results = []
    
    for case in edge_cases:
        try:
            if case["operation"] == "search":
                await vector_store.search(**case["args"])
            elif case["operation"] == "delete":
                vector_store.delete_document(**case["args"])
            elif case["operation"] == "update_metadata":
                await vector_store.update_document_metadata(**case["args"])
            
            # If we get here, the operation didn't raise an exception
            logger.info(f"Vector store {case['operation']} succeeded with args: {case['args']}")
            results.append({
                "operation": case["operation"],
                "args": str(case["args"]),
                "success": True
            })
            
        except Exception as e:
            # The operation raised an exception
            logger.info(f"Vector store {case['operation']} failed with args: {case['args']}, error: {str(e)}")
            results.append({
                "operation": case["operation"],
                "args": str(case["args"]),
                "success": False,
                "error": str(e)
            })
    
    # Save results to file
    results_path = "test_vector_store_resilience_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Vector store resilience test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_generate_edge_case_report():
    """Generate a comprehensive edge case test report"""
    # Check if all result files exist
    result_files = [
        "test_special_queries_results.json",
        "test_concurrent_processing_results.json",
        "test_invalid_files_results.json",
        "test_malformed_requests_results.json",
        "test_vector_store_resilience_results.json"
    ]
    
    missing_files = [f for f in result_files if not os.path.exists(f)]
    
    if missing_files:
        logger.warning(f"Missing result files: {missing_files}")
        logger.warning("Run the individual edge case tests first")
        return
    
    # Load all results
    results = {}
    
    for file_path in result_files:
        with open(file_path, "r") as f:
            results[file_path] = json.load(f)
    
    # Generate report
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "special_queries": {
                "total": len(results["test_special_queries_results.json"]),
                "success": sum(1 for r in results["test_special_queries_results.json"] if r["success"])
            },
            "concurrent_processing": {
                "total": len(results["test_concurrent_processing_results.json"]),
                "success": sum(1 for r in results["test_concurrent_processing_results.json"] if r["success"])
            },
            "invalid_files": {
                "total": len(results["test_invalid_files_results.json"]),
                "accepted": sum(1 for r in results["test_invalid_files_results.json"] if r["accepted"]),
                "processed": sum(1 for r in results["test_invalid_files_results.json"] if r.get("process_success", False))
            },
            "malformed_requests": {
                "total": len(results["test_malformed_requests_results.json"]),
                "error_responses": sum(1 for r in results["test_malformed_requests_results.json"] if r["is_error"])
            },
            "vector_store_resilience": {
                "total": len(results["test_vector_store_resilience_results.json"]),
                "success": sum(1 for r in results["test_vector_store_resilience_results.json"] if r["success"])
            }
        },
        "detailed_results": results
    }
    
    # Save report
    report_path = "edge_case_test_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Edge case test report saved to {os.path.abspath(report_path)}")
    
    # Generate HTML report
    html_report = f"""<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Edge Case Test Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        .section {{ margin-bottom: 30px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .success {{ color: green; }}
        .failure {{ color: red; }}
    </style>
</head>
<body>
    <h1>Metis RAG Edge Case Test Report</h1>
    <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="section">
        <h2>Summary</h2>
        <table>
            <tr>
                <th>Test Category</th>
                <th>Total Tests</th>
                <th>Success Rate</th>
            </tr>
            <tr>
                <td>Special Character Queries</td>
                <td>{report["summary"]["special_queries"]["total"]}</td>
                <td>{report["summary"]["special_queries"]["success"]}/{report["summary"]["special_queries"]["total"]} ({report["summary"]["special_queries"]["success"]/report["summary"]["special_queries"]["total"]*100:.1f}%)</td>
            </tr>
            <tr>
                <td>Concurrent Document Processing</td>
                <td>{report["summary"]["concurrent_processing"]["total"]}</td>
                <td>{report["summary"]["concurrent_processing"]["success"]}/{report["summary"]["concurrent_processing"]["total"]} ({report["summary"]["concurrent_processing"]["success"]/report["summary"]["concurrent_processing"]["total"]*100:.1f}%)</td>
            </tr>
            <tr>
                <td>Invalid File Types</td>
                <td>{report["summary"]["invalid_files"]["total"]}</td>
                <td>Accepted: {report["summary"]["invalid_files"]["accepted"]}/{report["summary"]["invalid_files"]["total"]} ({report["summary"]["invalid_files"]["accepted"]/report["summary"]["invalid_files"]["total"]*100:.1f}%)<br>
                Processed: {report["summary"]["invalid_files"]["processed"]}/{report["summary"]["invalid_files"]["accepted"] if report["summary"]["invalid_files"]["accepted"] > 0 else 1} ({(report["summary"]["invalid_files"]["processed"]/report["summary"]["invalid_files"]["accepted"]*100 if report["summary"]["invalid_files"]["accepted"] > 0 else 0):.1f}%)</td>
            </tr>
            <tr>
                <td>Malformed API Requests</td>
                <td>{report["summary"]["malformed_requests"]["total"]}</td>
                <td>Error Responses: {report["summary"]["malformed_requests"]["error_responses"]}/{report["summary"]["malformed_requests"]["total"]} ({report["summary"]["malformed_requests"]["error_responses"]/report["summary"]["malformed_requests"]["total"]*100:.1f}%)</td>
            </tr>
            <tr>
                <td>Vector Store Resilience</td>
                <td>{report["summary"]["vector_store_resilience"]["total"]}</td>
                <td>{report["summary"]["vector_store_resilience"]["success"]}/{report["summary"]["vector_store_resilience"]["total"]} ({report["summary"]["vector_store_resilience"]["success"]/report["summary"]["vector_store_resilience"]["total"]*100:.1f}%)</td>
            </tr>
        </table>
    </div>
    
    <div class="section">
        <h2>Special Character Queries</h2>
        <table>
            <tr>
                <th>Query</th>
                <th>Status Code</th>
                <th>Result</th>
            </tr>
            {("".join([f"<tr><td>{r['query'][:50]}...</td><td>{r['status_code']}</td><td class=\"{'success' if r['success'] else 'failure'}\">{('Success' if r['success'] else 'Failure')}</td></tr>" for r in results["test_special_queries_results.json"]]))}
        </table>
    </div>
    
    <div class="section">
        <h2>Invalid File Types</h2>
        <table>
            <tr>
                <th>Filename</th>
                <th>Content Type</th>
                <th>Upload Status</th>
                <th>Processing Status</th>
            </tr>
            {("".join([f"<tr><td>{r['filename']}</td><td>{r['content_type']}</td><td class=\"{'success' if r['accepted'] else 'failure'}\">{r['status_code']} ({('Accepted' if r['accepted'] else 'Rejected')})</td><td class=\"{'success' if r.get('process_success', False) else 'failure'}\">{r.get('process_status_code', 'N/A')} ({('Success' if r.get('process_success', False) else 'N/A' if not r['accepted'] else 'Failure')})</td></tr>" for r in results["test_invalid_files_results.json"]]))}
        </table>
    </div>
    
    <div class="section">
        <h2>Malformed API Requests</h2>
        <table>
            <tr>
                <th>Endpoint</th>
                <th>Method</th>
                <th>Data</th>
                <th>Status Code</th>
                <th>Result</th>
            </tr>
            {("".join([f"<tr><td>{r['endpoint']}</td><td>{r['method']}</td><td>{r['data'][:30]}...</td><td>{r.get('status_code', 'Exception')}</td><td class=\"{'success' if r['is_error'] else 'failure'}\">{('Error Response' if r['is_error'] else 'Unexpected Success')}</td></tr>" for r in results["test_malformed_requests_results.json"]]))}
        </table>
    </div>
    
    <div class="section">
        <h2>Vector Store Resilience</h2>
        <table>
            <tr>
                <th>Operation</th>
                <th>Arguments</th>
                <th>Result</th>
                <th>Error</th>
            </tr>
            {("".join([f"<tr><td>{r['operation']}</td><td>{r['args'][:30]}...</td><td class=\"{'success' if r['success'] else 'failure'}\">{('Success' if r['success'] else 'Failure')}</td><td>{r.get('error', 'N/A')}</td></tr>" for r in results["test_vector_store_resilience_results.json"]]))}
        </table>
    </div>
</body>
</html>
"""
    
    html_report_path = "edge_case_test_report.html"
    with open(html_report_path, "w") as f:
        f.write(html_report)
    
    logger.info(f"HTML edge case report saved to {os.path.abspath(html_report_path)}")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_entity_preservation.py
================
#!/usr/bin/env python3
"""
Simplified test script to verify the entity preservation fix in the retrieval judge.
"""

import asyncio
import logging
import sys
import os
import uuid
from typing import Dict, Any, List

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger("test_entity_preservation")

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment variables from .env.test
from dotenv import load_dotenv
load_dotenv(".env.test")

async def test_entity_preservation():
    """Test entity preservation in query refinement"""
    logger.info("Testing entity preservation in query refinement...")
    
    # Import the necessary modules
    from app.rag.agents.retrieval_judge import RetrievalJudge
    
    # Create a mock RetrievalJudge class that doesn't require external dependencies
    class MockRetrievalJudge(RetrievalJudge):
        def __init__(self):
            # Skip initialization that requires external dependencies
            pass
        
        async def refine_query(self, query: str, chunks: List[Dict[str, Any]]) -> str:
            # Simulate query refinement with a typo in entity names
            if "Stabilium" in query:
                refined = query.replace("Stabilium", "Stabilim")
                logger.info(f"Simulated refinement (with typo): '{query}' -> '{refined}'")
                # Now call our fixed _parse_refined_query method to fix the typo
                return self._parse_refined_query(refined, query)
            elif "Microsoft" in query:
                refined = query.replace("Microsoft", "Microsft")
                logger.info(f"Simulated refinement (with typo): '{query}' -> '{refined}'")
                return self._parse_refined_query(refined, query)
            elif "Quantum" in query:
                refined = query.replace("Quantum", "Quantom")
                logger.info(f"Simulated refinement (with typo): '{query}' -> '{refined}'")
                return self._parse_refined_query(refined, query)
            else:
                # No typo simulation for other queries
                logger.info(f"No typo simulation for: '{query}'")
                return self._parse_refined_query(query, query)
    
    # Create a mock retrieval judge
    retrieval_judge = MockRetrievalJudge()
    
    # Test queries with entity names
    test_queries = [
        "tell me about Stabilium",
        "what is Quantum Resonance Modulation",
        "explain the Heisenberg Uncertainty Principle",
        "information about Microsoft Azure",
        "details on SpaceX Starship"
    ]
    
    for query in test_queries:
        logger.info(f"Testing query: {query}")
        
        # Create a mock chunks list for testing
        mock_chunks = [
            {
                "chunk_id": str(uuid.uuid4()),
                "content": f"This is a document about {query.split()[-1]}.",
                "metadata": {"filename": "test.txt"},
                "distance": 0.2
            }
        ]
        
        # Test query refinement
        refined_query = await retrieval_judge.refine_query(query, mock_chunks)
        
        # Check if the entity name is preserved
        entity_name = query.split()[-1]
        if entity_name in refined_query:
            logger.info(f"✅ Entity '{entity_name}' preserved in refined query: '{refined_query}'")
        else:
            logger.error(f"❌ Entity '{entity_name}' NOT preserved in refined query: '{refined_query}'")
            
        # Add a separator for readability
        logger.info("-" * 50)

async def main():
    """Main function to run all tests"""
    logger.info("Starting entity preservation test...")
    
    # Test entity preservation
    await test_entity_preservation()
    
    logger.info("All tests completed!")

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_file_handling.py
================
#!/usr/bin/env python3
"""
Test suite for evaluating file handling capabilities in the Metis RAG system.
This test suite focuses on different file types, multiple file uploads, and large files.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
import random
import string

import pytest
from fastapi.testclient import TestClient
from io import BytesIO

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_file_handling")

# Import RAG components
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document
from app.main import app

# Test client
client = TestClient(app)

# Test file templates
TEST_FILE_TEMPLATES = {
    "txt": {
        "content": """This is a test text file for the Metis RAG system.
It contains multiple lines of text that will be processed and indexed.
The document processor should handle plain text files efficiently.
This file will be used to test the text file processing capabilities."""
    },
    "md": {
        "content": """# Test Markdown File

## Introduction
This is a test markdown file for the Metis RAG system.

## Features
- Supports headers
- Supports lists
- Supports **bold** and *italic* text

## Code Examples
```python
def test_function():
    return "This is a test"
```

## Conclusion
The document processor should handle markdown files correctly."""
    },
    "csv": {
        "content": """id,name,description,value
1,Item 1,This is the first item,100
2,Item 2,This is the second item,200
3,Item 3,This is the third item,300
4,Item 4,This is the fourth item,400
5,Item 5,This is the fifth item,500"""
    }
}

def generate_random_text(size_kb):
    """Generate random text of specified size in KB"""
    chars = string.ascii_letters + string.digits + string.punctuation + ' \n\t'
    # 1 KB is approximately 1000 characters
    return ''.join(random.choice(chars) for _ in range(size_kb * 1000))

@pytest.fixture
def test_files_dir():
    """Create a temporary directory for test files"""
    temp_dir = tempfile.mkdtemp(prefix="metis_rag_test_")
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.fixture
def create_test_files(test_files_dir):
    """Create test files of different types"""
    file_paths = {}
    
    # Create basic test files
    for ext, template in TEST_FILE_TEMPLATES.items():
        file_path = os.path.join(test_files_dir, f"test.{ext}")
        with open(file_path, "w") as f:
            f.write(template["content"])
        file_paths[ext] = file_path
    
    # Create a large text file (1 MB)
    large_file_path = os.path.join(test_files_dir, "large_file.txt")
    with open(large_file_path, "w") as f:
        f.write(generate_random_text(1024))  # 1024 KB = 1 MB
    file_paths["large_txt"] = large_file_path
    
    return file_paths

@pytest.fixture
def document_processor():
    """Create a document processor instance"""
    return DocumentProcessor()

@pytest.mark.asyncio
async def test_file_type_support(document_processor, create_test_files):
    """Test processing of different file types"""
    results = []
    
    for file_type, file_path in create_test_files.items():
        if file_type == "large_txt":
            continue  # Skip large file for this test
            
        logger.info(f"Testing file type: {file_type}")
        
        # Create a document object
        with open(file_path, "r") as f:
            content = f.read()
            
        document = Document(
            id=str(uuid.uuid4()),
            filename=os.path.basename(file_path),
            content=content
        )
        
        # Process the document
        try:
            processed_doc = await document_processor.process_document(document)
            
            # Check if chunks were created
            chunk_count = len(processed_doc.chunks)
            success = chunk_count > 0
            
            logger.info(f"Processed {file_type} file: {success}, {chunk_count} chunks created")
            
            # Store results
            results.append({
                "file_type": file_type,
                "success": success,
                "chunk_count": chunk_count,
                "avg_chunk_size": sum(len(chunk.content) for chunk in processed_doc.chunks) / chunk_count if chunk_count > 0 else 0
            })
            
            # Assertions
            assert success, f"Failed to process {file_type} file"
            assert chunk_count > 0, f"No chunks created for {file_type} file"
            
        except Exception as e:
            logger.error(f"Error processing {file_type} file: {str(e)}")
            results.append({
                "file_type": file_type,
                "success": False,
                "error": str(e)
            })
            assert False, f"Error processing {file_type} file: {str(e)}"
    
    # Save results to file
    results_path = "test_file_type_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"File type test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_chunking_strategies(document_processor, create_test_files):
    """Test different chunking strategies"""
    results = []
    
    chunking_strategies = ["recursive", "token", "markdown"]
    file_path = create_test_files["md"]  # Use markdown file for testing chunking strategies
    
    with open(file_path, "r") as f:
        content = f.read()
    
    for strategy in chunking_strategies:
        logger.info(f"Testing chunking strategy: {strategy}")
        
        # Create a custom document processor with the specified chunking strategy
        custom_processor = DocumentProcessor(chunking_strategy=strategy)
        
        # Create a document object
        document = Document(
            id=str(uuid.uuid4()),
            filename=os.path.basename(file_path),
            content=content
        )
        
        # Process the document
        try:
            processed_doc = await custom_processor.process_document(document)
            
            # Check if chunks were created
            chunk_count = len(processed_doc.chunks)
            success = chunk_count > 0
            
            logger.info(f"Processed with {strategy} strategy: {success}, {chunk_count} chunks created")
            
            # Store results
            results.append({
                "chunking_strategy": strategy,
                "success": success,
                "chunk_count": chunk_count,
                "avg_chunk_size": sum(len(chunk.content) for chunk in processed_doc.chunks) / chunk_count if chunk_count > 0 else 0
            })
            
            # Assertions
            assert success, f"Failed to process with {strategy} strategy"
            assert chunk_count > 0, f"No chunks created with {strategy} strategy"
            
        except Exception as e:
            logger.error(f"Error processing with {strategy} strategy: {str(e)}")
            results.append({
                "chunking_strategy": strategy,
                "success": False,
                "error": str(e)
            })
            assert False, f"Error processing with {strategy} strategy: {str(e)}"
    
    # Save results to file
    results_path = "test_chunking_strategy_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"Chunking strategy test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_large_file_handling(document_processor, create_test_files):
    """Test handling of large files"""
    results = []
    
    file_path = create_test_files["large_txt"]
    
    logger.info(f"Testing large file handling")
    
    # Create a document object
    with open(file_path, "r") as f:
        content = f.read()
        
    document = Document(
        id=str(uuid.uuid4()),
        filename=os.path.basename(file_path),
        content=content
    )
    
    # Process the document
    try:
        start_time = datetime.now()
        processed_doc = await document_processor.process_document(document)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Check if chunks were created
        chunk_count = len(processed_doc.chunks)
        success = chunk_count > 0
        
        file_size_kb = len(content) / 1000
        processing_speed = file_size_kb / processing_time if processing_time > 0 else 0
        
        logger.info(f"Processed large file: {success}, {chunk_count} chunks created")
        logger.info(f"Processing time: {processing_time:.2f} seconds")
        logger.info(f"Processing speed: {processing_speed:.2f} KB/s")
        
        # Store results
        results.append({
            "file_size_kb": file_size_kb,
            "success": success,
            "chunk_count": chunk_count,
            "processing_time_seconds": processing_time,
            "processing_speed_kb_per_second": processing_speed,
            "avg_chunk_size": sum(len(chunk.content) for chunk in processed_doc.chunks) / chunk_count if chunk_count > 0 else 0
        })
        
        # Assertions
        assert success, "Failed to process large file"
        assert chunk_count > 0, "No chunks created for large file"
        
    except Exception as e:
        logger.error(f"Error processing large file: {str(e)}")
        results.append({
            "file_size_kb": len(content) / 1000,
            "success": False,
            "error": str(e)
        })
        assert False, f"Error processing large file: {str(e)}"
    
    # Save results to file
    results_path = "test_large_file_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"Large file test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_api_file_upload(create_test_files):
    """Test file upload through API"""
    results = []
    
    for file_type, file_path in create_test_files.items():
        if file_type == "large_txt":
            continue  # Skip large file for API test to avoid timeouts
            
        logger.info(f"Testing API upload for file type: {file_type}")
        
        # Read file content
        with open(file_path, "rb") as f:
            file_content = f.read()
            
        # Create file-like object for upload
        file_obj = BytesIO(file_content)
        file_obj.name = os.path.basename(file_path)
        
        # Upload the file
        try:
            upload_response = client.post(
                "/api/documents/upload",
                files={"file": (file_obj.name, file_obj, f"text/{file_type}")}
            )
            
            # Check upload response
            assert upload_response.status_code == 200, f"Upload failed with status {upload_response.status_code}"
            upload_data = upload_response.json()
            assert upload_data["success"] is True, "Upload response indicates failure"
            assert "document_id" in upload_data, "No document_id in upload response"
            
            document_id = upload_data["document_id"]
            
            # Process the document
            process_response = client.post(
                "/api/documents/process",
                json={"document_ids": [document_id]}
            )
            
            # Check process response
            assert process_response.status_code == 200, f"Processing failed with status {process_response.status_code}"
            process_data = process_response.json()
            assert process_data["success"] is True, "Process response indicates failure"
            
            # Get document info
            info_response = client.get(f"/api/documents/{document_id}")
            
            # Check info response
            assert info_response.status_code == 200, f"Info request failed with status {info_response.status_code}"
            doc_info = info_response.json()
            
            # Store results
            results.append({
                "file_type": file_type,
                "document_id": document_id,
                "filename": file_obj.name,
                "success": True,
                "chunk_count": doc_info.get("chunk_count", 0) if isinstance(doc_info, dict) else 0
            })
            
            # Clean up - delete the document
            delete_response = client.delete(f"/api/documents/{document_id}")
            assert delete_response.status_code == 200, f"Delete failed with status {delete_response.status_code}"
            
        except Exception as e:
            logger.error(f"Error in API test for {file_type} file: {str(e)}")
            results.append({
                "file_type": file_type,
                "filename": os.path.basename(file_path),
                "success": False,
                "error": str(e)
            })
            assert False, f"Error in API test for {file_type} file: {str(e)}"
    
    # Save results to file
    results_path = "test_api_upload_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"API upload test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_multiple_file_upload(create_test_files):
    """Test uploading and processing multiple files simultaneously"""
    results = []
    
    # Skip large file for this test
    file_paths = {k: v for k, v in create_test_files.items() if k != "large_txt"}
    
    logger.info(f"Testing multiple file upload with {len(file_paths)} files")
    
    # Upload all files
    document_ids = []
    
    for file_type, file_path in file_paths.items():
        # Read file content
        with open(file_path, "rb") as f:
            file_content = f.read()
            
        # Create file-like object for upload
        file_obj = BytesIO(file_content)
        file_obj.name = os.path.basename(file_path)
        
        # Upload the file
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (file_obj.name, file_obj, f"text/{file_type}")}
        )
        
        # Check upload response
        assert upload_response.status_code == 200, f"Upload failed with status {upload_response.status_code}"
        upload_data = upload_response.json()
        assert upload_data["success"] is True, "Upload response indicates failure"
        
        document_ids.append(upload_data["document_id"])
    
    # Process all documents at once
    try:
        start_time = datetime.now()
        
        process_response = client.post(
            "/api/documents/process",
            json={"document_ids": document_ids}
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Check process response
        assert process_response.status_code == 200, f"Processing failed with status {process_response.status_code}"
        process_data = process_response.json()
        assert process_data["success"] is True, "Process response indicates failure"
        
        # Get info for all documents
        for doc_id in document_ids:
            info_response = client.get(f"/api/documents/{doc_id}")
            assert info_response.status_code == 200, f"Info request failed for document {doc_id}"
        
        # Store results
        results.append({
            "file_count": len(document_ids),
            "document_ids": document_ids,
            "success": True,
            "processing_time_seconds": processing_time,
            "processing_speed_files_per_second": len(document_ids) / processing_time if processing_time > 0 else 0
        })
        
        # Clean up - delete all documents
        for doc_id in document_ids:
            delete_response = client.delete(f"/api/documents/{doc_id}")
            assert delete_response.status_code == 200, f"Delete failed for document {doc_id}"
            
    except Exception as e:
        logger.error(f"Error in multiple file upload test: {str(e)}")
        results.append({
            "file_count": len(document_ids),
            "success": False,
            "error": str(e)
        })
        assert False, f"Error in multiple file upload test: {str(e)}"
    
    # Save results to file
    results_path = "test_multiple_upload_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"Multiple file upload test results saved to {os.path.abspath(results_path)}")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_fixes.py
================
import asyncio
import logging
import sys
import os
import uuid
import nest_asyncio
from datetime import datetime

# Add the parent directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("test_fixes")

# Apply nest_asyncio to allow nested event loops
nest_asyncio.apply()

from sqlalchemy.ext.asyncio import AsyncSession
from app.db.session import AsyncSessionLocal
from app.db.repositories.analytics_repository import AnalyticsRepository
from app.db.repositories.conversation_repository import ConversationRepository

async def test_analytics_repository_fix():
    """Test the fix for the analytics repository"""
    logger.info("Testing analytics repository fix...")
    
    # Create a new session
    session = AsyncSessionLocal()
    try:
        # Create repository
        analytics_repo = AnalyticsRepository(session)
        
        # Test create_analytics_query method
        try:
            query = await analytics_repo.create_analytics_query(
                query="Test query",
                model="gemma3:4b",
                use_rag=True,
                response_time_ms=100.0,
                token_count=50,
                document_ids=["test-doc-1", "test-doc-2"],
                query_type="simple",
                successful=True
            )
            logger.info(f"Successfully created analytics query with ID: {query.id}")
            return True
        except Exception as e:
            logger.error(f"Error testing analytics repository fix: {str(e)}")
            return False
    finally:
        await session.close()

async def test_citation_fix():
    """Test the fix for the citation foreign key validation"""
    logger.info("Testing citation fix...")
    
    # Create a new session
    session = AsyncSessionLocal()
    try:
        # Create conversation repository
        conv_repo = ConversationRepository(session)
        
        # Create a test conversation
        conversation = await conv_repo.create_conversation(user_id="test_user")
        
        # Add a message to the conversation
        message = await conv_repo.add_message(
            conversation_id=conversation.id,
            content="Test message",
            role="assistant"
        )
        
        # Test the validation logic directly
        from app.db.models import Chunk, Document
        from sqlalchemy import select
        
        # Generate non-existent IDs
        non_existent_chunk_id = uuid.uuid4()
        non_existent_doc_id = uuid.uuid4()
        
        # Check if the validation code correctly identifies non-existent IDs
        stmt = select(Chunk).filter(Chunk.id == non_existent_chunk_id)
        result = await session.execute(stmt)
        chunk = result.scalars().first()
        
        if chunk is None:
            logger.info("Validation correctly identified non-existent chunk_id")
        else:
            logger.error("Validation failed to identify non-existent chunk_id")
            return False
            
        stmt = select(Document).filter(Document.id == non_existent_doc_id)
        result = await session.execute(stmt)
        document = result.scalars().first()
        
        if document is None:
            logger.info("Validation correctly identified non-existent document_id")
        else:
            logger.error("Validation failed to identify non-existent document_id")
            return False
        
        logger.info("Citation validation logic test passed")
        return True
    except Exception as e:
        logger.error(f"Error testing citation fix: {str(e)}")
        return False
    finally:
        await session.close()

async def main():
    """Run all tests"""
    try:
        analytics_result = await test_analytics_repository_fix()
        citation_result = await test_citation_fix()
        
        logger.info(f"Analytics repository fix test: {'PASSED' if analytics_result else 'FAILED'}")
        logger.info(f"Citation fix test: {'PASSED' if citation_result else 'FAILED'}")
    except Exception as e:
        logger.error(f"Test failed: {str(e)}")

if __name__ == "__main__":
    # Install the uvloop event loop policy if available
    try:
        import uvloop
        uvloop.install()
    except ImportError:
        pass
    
    # Run the main function
    asyncio.run(main())

================
File: tests/test_metis_rag_e2e_demo.py
================
#!/usr/bin/env python3
"""
End-to-End test demonstration for the Metis RAG system.
This is a simplified version that doesn't require authentication.
"""

import os
import json
import logging
from datetime import datetime
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_metis_rag_e2e_demo")

# Test documents - paths relative to project root
TEST_DOCUMENTS = {
    "technical_specs": {
        "path": "data/test_docs/smart_home_technical_specs.pdf",
        "type": "pdf",
        "content_type": "application/pdf"
    },
    "user_guide": {
        "path": "data/test_docs/smart_home_user_guide.txt",
        "type": "txt",
        "content_type": "text/plain"
    },
    "device_comparison": {
        "path": "data/test_docs/smart_home_device_comparison.csv",
        "type": "csv",
        "content_type": "text/csv"
    },
    "developer_reference": {
        "path": "data/test_docs/smart_home_developer_reference.md",
        "type": "md",
        "content_type": "text/markdown"
    }
}

# Test queries with expected facts to be present in responses
SINGLE_DOC_QUERIES = [
    {
        "query": "What are the specifications of the SmartHome Hub?",
        "expected_facts": [
            "ARM Cortex-A53",
            "quad-core",
            "1.4GHz",
            "2GB RAM",
            "16GB eMMC",
            "Wi-Fi",
            "Bluetooth 5.0",
            "Zigbee 3.0",
            "Z-Wave",
            "5V DC"
        ],
        "target_docs": ["technical_specs"]
    },
    {
        "query": "How do I troubleshoot when devices won't connect?",
        "expected_facts": [
            "within range",
            "30-50 feet",
            "pairing mode",
            "compatible with SmartHome"
        ],
        "target_docs": ["user_guide"]
    }
]

MULTI_DOC_QUERIES = [
    {
        "query": "Compare the Motion Sensor and Door Sensor specifications and setup process.",
        "expected_facts": [
            "SH-MS100",
            "SH-DS100",
            "Zigbee",
            "2 years",
            "pairing mode",
            "Add Device"
        ],
        "target_docs": ["device_comparison", "user_guide"]
    }
]

def verify_test_documents():
    """Verify that all test documents exist"""
    missing_docs = []
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        if not os.path.exists(doc_info["path"]):
            missing_docs.append(doc_info["path"])
    
    if missing_docs:
        logger.error(f"Missing test documents: {', '.join(missing_docs)}")
        logger.error("Please ensure all test documents are created before running the test.")
        return False
    else:
        logger.info("All test documents verified.")
        return True

def simulate_document_processing():
    """Simulate document processing for demonstration purposes"""
    logger.info("Simulating document processing...")
    
    results = []
    
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        logger.info(f"Processing document: {doc_id} ({doc_info['path']})")
        
        # Check if file exists
        if not os.path.exists(doc_info['path']):
            logger.error(f"File not found: {doc_info['path']}")
            results.append({
                "document_id": f"simulated_{doc_id}",
                "document_type": doc_info['type'],
                "filename": os.path.basename(doc_info['path']),
                "success": False,
                "error": "File not found"
            })
            continue
        
        # Get file size
        file_size = os.path.getsize(doc_info['path'])
        
        # Simulate processing time based on file size
        processing_time = file_size / 100000  # Simulate 1 second per 100KB
        time.sleep(min(processing_time, 0.5))  # Cap at 0.5 seconds for demo
        
        # Simulate chunk count based on file size and type
        chunk_count = max(1, int(file_size / 2000))  # Roughly 1 chunk per 2KB
        
        # Store results
        results.append({
            "document_id": f"simulated_{doc_id}",
            "document_type": doc_info['type'],
            "filename": os.path.basename(doc_info['path']),
            "success": True,
            "file_size_bytes": file_size,
            "processing_time_seconds": processing_time,
            "chunk_count": chunk_count
        })
        
        logger.info(f"Successfully processed {doc_id} into {chunk_count} chunks")
    
    # Save results to file
    results_path = "test_e2e_demo_upload_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Document processing results saved to {os.path.abspath(results_path)}")
    
    return results

def simulate_query_responses():
    """Simulate query responses for demonstration purposes"""
    logger.info("Simulating query responses...")
    
    results = []
    
    # Combine all query types
    all_queries = SINGLE_DOC_QUERIES + MULTI_DOC_QUERIES
    
    for test_case in all_queries:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        target_docs = test_case["target_docs"]
        
        logger.info(f"Processing query: '{query}'")
        
        # Simulate query processing time
        processing_time = 1.0 + (0.5 * len(target_docs))  # Base time + extra for multi-doc
        time.sleep(0.2)  # Just a short delay for demo
        
        # Simulate response with expected facts
        # In a real test, this would be the actual response from the RAG system
        simulated_response = f"Here is information about your query: '{query}'\n\n"
        
        # Include some of the expected facts in the simulated response
        fact_count = max(1, int(len(expected_facts) * 0.8))  # Include 80% of facts
        included_facts = expected_facts[:fact_count]
        
        for fact in included_facts:
            simulated_response += f"- {fact}\n"
        
        # Add some citations
        simulated_response += "\nThis information comes from "
        for i, doc in enumerate(target_docs):
            simulated_response += f"[{i+1}] {doc}"
            if i < len(target_docs) - 1:
                simulated_response += " and "
        
        # Calculate fact percentage
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        # Store results
        results.append({
            "query": query,
            "answer": simulated_response,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "processing_time_seconds": processing_time,
            "success": fact_percentage >= 70
        })
        
        logger.info(f"Query processed. Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
    
    # Save results to file
    results_path = "test_e2e_demo_query_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Query results saved to {os.path.abspath(results_path)}")
    
    return results

def generate_comprehensive_report(doc_results, query_results):
    """Generate a comprehensive report of all test results"""
    logger.info("Generating comprehensive report...")
    
    # Calculate overall statistics
    doc_success_rate = sum(1 for r in doc_results if r["success"]) / len(doc_results) * 100
    query_success_rate = sum(1 for r in query_results if r["success"]) / len(query_results) * 100
    avg_fact_percentage = sum(r["fact_percentage"] for r in query_results) / len(query_results)
    
    # Create report
    report = {
        "test_name": "Metis RAG End-to-End Demo Test",
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "document_count": len(doc_results),
            "document_success_rate": doc_success_rate,
            "query_count": len(query_results),
            "query_success_rate": query_success_rate,
            "average_fact_percentage": avg_fact_percentage
        },
        "results": {
            "document_processing": doc_results,
            "query_responses": query_results
        }
    }
    
    # Save report to file
    report_path = "test_e2e_demo_comprehensive_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Comprehensive report saved to {os.path.abspath(report_path)}")
    
    return report

def run_demo_test():
    """Run the complete demo test"""
    logger.info("Starting Metis RAG End-to-End Demo Test")
    
    # Verify test documents
    if not verify_test_documents():
        logger.error("Test documents verification failed. Aborting test.")
        return False
    
    # Simulate document processing
    doc_results = simulate_document_processing()
    
    # Simulate query responses
    query_results = simulate_query_responses()
    
    # Generate comprehensive report
    report = generate_comprehensive_report(doc_results, query_results)
    
    # Print summary
    logger.info("\n" + "="*80)
    logger.info("Metis RAG End-to-End Demo Test Summary")
    logger.info("="*80)
    logger.info(f"Documents processed: {len(doc_results)}")
    logger.info(f"Document success rate: {report['summary']['document_success_rate']:.1f}%")
    logger.info(f"Queries processed: {len(query_results)}")
    logger.info(f"Query success rate: {report['summary']['query_success_rate']:.1f}%")
    logger.info(f"Average fact percentage: {report['summary']['average_fact_percentage']:.1f}%")
    logger.info("="*80)
    
    logger.info("End-to-End Demo Test completed successfully")
    return True

if __name__ == "__main__":
    run_demo_test()

================
File: tests/test_metis_rag_e2e.py
================
#!/usr/bin/env python3
"""
End-to-End test for the Metis RAG system.
This test evaluates the complete pipeline from document upload to query response.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
import time

import pytest
from fastapi.testclient import TestClient
from io import BytesIO
import requests

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_metis_rag_e2e")

import os
from unittest import mock

# Set environment variables for testing
os.environ["CHROMA_DB_DIR"] = "test_e2e_chroma"
os.environ["CACHE_DIR"] = "data/test_cache"

# Import RAG components
from app.main import app
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document, Chunk

# Test client
client = TestClient(app)

# Test documents - paths relative to project root
TEST_DOCUMENTS = {
    "technical_specs": {
        "path": "data/test_docs/smart_home_technical_specs.pdf",
        "type": "pdf",
        "content_type": "application/pdf"
    },
    "user_guide": {
        "path": "data/test_docs/smart_home_user_guide.txt",
        "type": "txt",
        "content_type": "text/plain"
    },
    "device_comparison": {
        "path": "data/test_docs/smart_home_device_comparison.csv",
        "type": "csv",
        "content_type": "text/csv"
    },
    "developer_reference": {
        "path": "data/test_docs/smart_home_developer_reference.md",
        "type": "md",
        "content_type": "text/markdown"
    }
}

# Test queries with expected facts to be present in responses
SINGLE_DOC_QUERIES = [
    {
        "query": "What are the specifications of the SmartHome Hub?",
        "expected_facts": [
            "ARM Cortex-A53",
            "quad-core",
            "1.4GHz",
            "2GB RAM",
            "16GB eMMC",
            "Wi-Fi",
            "Bluetooth 5.0",
            "Zigbee 3.0",
            "Z-Wave",
            "5V DC"
        ],
        "target_docs": ["technical_specs"]
    },
    {
        "query": "How do I troubleshoot when devices won't connect?",
        "expected_facts": [
            "within range",
            "30-50 feet",
            "pairing mode",
            "compatible with SmartHome"
        ],
        "target_docs": ["user_guide"]
    },
    {
        "query": "What is the battery life of the motion sensor?",
        "expected_facts": [
            "Motion Sensor",
            "SH-MS100",
            "2 years"
        ],
        "target_docs": ["device_comparison"]
    },
    {
        "query": "How do I authenticate with the SmartHome API?",
        "expected_facts": [
            "OAuth 2.0",
            "access token",
            "Developer Portal",
            "authorization code"
        ],
        "target_docs": ["developer_reference"]
    }
]

MULTI_DOC_QUERIES = [
    {
        "query": "Compare the Motion Sensor and Door Sensor specifications and setup process.",
        "expected_facts": [
            "SH-MS100",
            "SH-DS100",
            "Zigbee",
            "2 years",
            "pairing mode",
            "Add Device"
        ],
        "target_docs": ["device_comparison", "user_guide"]
    },
    {
        "query": "Explain how to integrate a motion sensor with a third-party application.",
        "expected_facts": [
            "API",
            "webhook",
            "OAuth",
            "motion.active",
            "real-time updates",
            "WebSocket"
        ],
        "target_docs": ["developer_reference", "device_comparison"]
    },
    {
        "query": "What security features does the SmartHome system provide for both users and developers?",
        "expected_facts": [
            "End-to-end encryption",
            "AES-256",
            "Certificate-based",
            "OAuth 2.0",
            "webhook signature",
            "HMAC"
        ],
        "target_docs": ["technical_specs", "developer_reference"]
    }
]

COMPLEX_QUERIES = [
    {
        "query": "Which devices require the hub and what protocols do they use?",
        "expected_facts": [
            "Hub Required",
            "Yes",
            "Zigbee",
            "Z-Wave",
            "SHC",
            "RF"
        ],
        "target_docs": ["device_comparison", "technical_specs"]
    },
    {
        "query": "If I want to create a water leak detection system, which devices should I use and how would I set them up?",
        "expected_facts": [
            "Leak Detector",
            "SH-LD100",
            "Water Valve Controller",
            "SH-WV100",
            "Add Device",
            "pairing mode",
            "automation"
        ],
        "target_docs": ["device_comparison", "user_guide", "developer_reference"]
    },
    {
        "query": "What's the difference between Zigbee and Z-Wave devices in the SmartHome ecosystem?",
        "expected_facts": [
            "Zigbee",
            "Z-Wave",
            "range",
            "protocol",
            "devices"
        ],
        "target_docs": ["technical_specs", "device_comparison"]
    }
]

def authenticate_client():
    """Authenticate the test client"""
    logger.info("Authenticating test client...")
    
    # Create a unique test user for this test run
    import uuid
    import requests
    
    test_username = f"testuser_{uuid.uuid4().hex[:8]}"
    test_password = "testpassword"
    base_url = "http://localhost:8000"
    
    # Use direct requests for authentication
    try:
        # Register the new test user
        logger.info(f"Registering new test user: {test_username}")
        register_response = requests.post(
            f"{base_url}/api/auth/register",
            json={
                "username": test_username,
                "email": f"{test_username}@example.com",
                "password": test_password,
                "full_name": "Test User",
                "is_active": True,
                "is_admin": False
            }
        )
        
        if register_response.status_code != 200:
            logger.error(f"Registration failed: {register_response.status_code} - {register_response.text}")
            return False
            
        logger.info(f"User {test_username} registered successfully")
        
        # Authenticate with the new user
        login_response = requests.post(
            f"{base_url}/api/auth/token",
            data={
                "username": test_username,
                "password": test_password,
                "grant_type": "password"
            }
        )
        
        if login_response.status_code != 200:
            logger.error(f"Login failed: {login_response.status_code} - {login_response.text}")
            return False
            
        token_data = login_response.json()
        access_token = token_data.get("access_token")
        
        if not access_token:
            logger.error("No access token in response")
            return False
        
        logger.info(f"Successfully obtained token: {access_token[:10]}...")
            
        # Set the token in the client's headers for subsequent requests
        client.headers.update({"Authorization": f"Bearer {access_token}"})
        
        # Verify authentication
        verify_response = client.get("/api/auth/me")
        if verify_response.status_code == 200:
            logger.info(f"Successfully authenticated as {test_username}")
            return True
        else:
            logger.error(f"Authentication verification failed: {verify_response.status_code} - {verify_response.text}")
            return False
            
    except Exception as e:
        logger.error(f"Authentication error: {str(e)}")
    
    # If we get here, authentication failed
    logger.warning("Authentication failed. Tests may not work correctly.")
    return False

def verify_test_documents():
    """Verify that all test documents exist"""
    missing_docs = []
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        if not os.path.exists(doc_info["path"]):
            missing_docs.append(doc_info["path"])
    
    if missing_docs:
        logger.error(f"Missing test documents: {', '.join(missing_docs)}")
        logger.error("Please ensure all test documents are created before running the test.")
        pytest.fail(f"Missing test documents: {', '.join(missing_docs)}")
    else:
        logger.info("All test documents verified.")

def test_document_upload_and_processing():
    """Test uploading and processing of all document types"""
    # First verify all test documents exist
    verify_test_documents()
    
    # Authenticate the client
    authenticate_client()
    
    uploaded_docs = {}
    results = []
    
    # Upload and process each document
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        logger.info(f"Testing upload and processing for {doc_id} ({doc_info['path']})")
        
        # Read file content
        try:
            with open(doc_info['path'], 'rb') as f:
                file_content = f.read()
        except Exception as e:
            pytest.fail(f"Failed to read file {doc_info['path']}: {str(e)}")
        
        # Create file-like object for upload
        file_obj = BytesIO(file_content)
        file_obj.name = os.path.basename(doc_info['path'])
        
        # Upload the file
        try:
            upload_response = client.post(
                "/api/documents/upload",
                files={"file": (file_obj.name, file_obj, doc_info['content_type'])}
            )
            
            # Check upload response
            assert upload_response.status_code == 200, f"Upload failed with status {upload_response.status_code}: {upload_response.text}"
            upload_data = upload_response.json()
            assert upload_data["success"] is True, f"Upload response indicates failure: {upload_data}"
            assert "document_id" in upload_data, "No document_id in upload response"
            
            document_id = upload_data["document_id"]
            uploaded_docs[doc_id] = document_id
            
            logger.info(f"Successfully uploaded {doc_id}, document_id: {document_id}")
            
            # Process the document
            process_response = client.post(
                "/api/documents/process",
                json={"document_ids": [document_id]}
            )
            
            # Check process response
            assert process_response.status_code == 200, f"Processing failed with status {process_response.status_code}: {process_response.text}"
            process_data = process_response.json()
            assert process_data["success"] is True, f"Process response indicates failure: {process_data}"
            
            logger.info(f"Successfully processed {doc_id}")
            
            # Get document info
            info_response = client.get(f"/api/documents/{document_id}")
            
            # Check info response
            assert info_response.status_code == 200, f"Info request failed with status {info_response.status_code}: {info_response.text}"
            doc_info_data = info_response.json()
            
            # Store results
            results.append({
                "document_id": document_id,
                "document_type": doc_info['type'],
                "filename": file_obj.name,
                "success": True,
                "chunk_count": doc_info_data.get("chunk_count", 0) if isinstance(doc_info_data, dict) else 0
            })
            
        except Exception as e:
            logger.error(f"Error in upload/processing test for {doc_id}: {str(e)}")
            results.append({
                "document_id": doc_id,
                "document_type": doc_info['type'],
                "filename": os.path.basename(doc_info['path']),
                "success": False,
                "error": str(e)
            })
            pytest.fail(f"Error in upload/processing test for {doc_id}: {str(e)}")
    
    # Save results to file
    results_path = "test_e2e_upload_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Upload and processing results saved to {os.path.abspath(results_path)}")
    
    # Return the uploaded document IDs for further tests
    return uploaded_docs

def test_single_document_queries():
    """Test queries that target a single document"""
    # Get document IDs from the upload test
    uploaded_docs = test_document_upload_and_processing()
    
    results = []
    
    for test_case in SINGLE_DOC_QUERIES:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        
        logger.info(f"Testing single-document query: '{query}'")
        
        # Execute query with RAG
        response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        # Check query response
        assert response.status_code == 200, f"Query failed with status {response.status_code}: {response.text}"
        response_data = response.json()
        assert "message" in response_data, "No message in query response"
        
        answer = response_data["message"]
        logger.info(f"Answer: {answer[:100]}...")
        
        # Check if response contains expected facts
        fact_count = sum(1 for fact in expected_facts if fact.lower() in answer.lower())
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        logger.info(f"Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "success": fact_percentage >= 70  # Consider successful if at least 70% of facts are found
        })
        
        # Assert minimum factual accuracy
        assert fact_percentage >= 70, f"Query '{query}' has low factual accuracy: {fact_percentage:.1f}%"
    
    # Save results to file
    results_path = "test_e2e_single_doc_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Single document query results saved to {os.path.abspath(results_path)}")

def test_multi_document_queries():
    """Test queries that require information from multiple documents"""
    # Get document IDs from the upload test
    uploaded_docs = test_document_upload_and_processing()
    
    results = []
    
    for test_case in MULTI_DOC_QUERIES:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        
        logger.info(f"Testing multi-document query: '{query}'")
        
        # Execute query with RAG
        response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        # Check query response
        assert response.status_code == 200, f"Query failed with status {response.status_code}: {response.text}"
        response_data = response.json()
        assert "message" in response_data, "No message in query response"
        
        answer = response_data["message"]
        logger.info(f"Answer: {answer[:100]}...")
        
        # Check if response contains expected facts
        fact_count = sum(1 for fact in expected_facts if fact.lower() in answer.lower())
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        logger.info(f"Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "success": fact_percentage >= 60  # Consider successful if at least 60% of facts are found (multi-doc is harder)
        })
        
        # Assert minimum factual accuracy
        assert fact_percentage >= 60, f"Query '{query}' has low factual accuracy: {fact_percentage:.1f}%"
    
    # Save results to file
    results_path = "test_e2e_multi_doc_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Multi-document query results saved to {os.path.abspath(results_path)}")

def test_complex_queries():
    """Test complex queries requiring synthesis and analysis"""
    # Get document IDs from the upload test
    uploaded_docs = test_document_upload_and_processing()
    
    results = []
    
    for test_case in COMPLEX_QUERIES:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        
        logger.info(f"Testing complex query: '{query}'")
        
        # Execute query with RAG
        response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False,
                "enable_query_refinement": True  # Enable query refinement for complex queries
            }
        )
        
        # Check query response
        assert response.status_code == 200, f"Query failed with status {response.status_code}: {response.text}"
        response_data = response.json()
        assert "message" in response_data, "No message in query response"
        
        answer = response_data["message"]
        logger.info(f"Answer: {answer[:100]}...")
        
        # Check if response contains expected facts
        fact_count = sum(1 for fact in expected_facts if fact.lower() in answer.lower())
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        logger.info(f"Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "success": fact_percentage >= 60  # Consider successful if at least 60% of facts are found
        })
        
        # Assert minimum factual accuracy
        assert fact_percentage >= 60, f"Query '{query}' has low factual accuracy: {fact_percentage:.1f}%"
    
    # Save results to file
    results_path = "test_e2e_complex_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Complex query results saved to {os.path.abspath(results_path)}")

def test_citation_quality():
    """Test quality of citations in RAG responses"""
    # Get document IDs from the upload test
    uploaded_docs = test_document_upload_and_processing()
    
    # Combine all query types for citation testing
    all_queries = SINGLE_DOC_QUERIES + MULTI_DOC_QUERIES + COMPLEX_QUERIES
    
    results = []
    
    for test_case in all_queries[:3]:  # Test a subset for efficiency
        query = test_case["query"]
        
        logger.info(f"Testing citation quality for query: '{query}'")
        
        # Execute query with RAG
        response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        # Check query response
        assert response.status_code == 200, f"Query failed with status {response.status_code}: {response.text}"
        response_data = response.json()
        assert "message" in response_data, "No message in query response"
        
        answer = response_data["message"]
        
        # Check for citation markers
        has_citation_markers = "[" in answer and "]" in answer
        
        # Count citations
        citation_count = 0
        for i in range(1, 10):  # Check for citation markers [1] through [9]
            if f"[{i}]" in answer:
                citation_count += 1
        
        logger.info(f"Has citation markers: {has_citation_markers}, Citation count: {citation_count}")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "has_citation_markers": has_citation_markers,
            "citation_count": citation_count,
            "success": has_citation_markers and citation_count > 0
        })
        
        # Assert citation quality
        assert has_citation_markers, f"Query '{query}' response lacks citation markers"
        assert citation_count > 0, f"Query '{query}' response has no citations"
    
    # Save results to file
    results_path = "test_e2e_citation_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Citation quality results saved to {os.path.abspath(results_path)}")

def test_system_performance():
    """Test performance metrics of the RAG system"""
    # Get document IDs from the upload test
    uploaded_docs = test_document_upload_and_processing()
    
    results = []
    
    # Test query performance
    sample_queries = [
        SINGLE_DOC_QUERIES[0]["query"],
        MULTI_DOC_QUERIES[0]["query"],
        COMPLEX_QUERIES[0]["query"]
    ]
    
    for query in sample_queries:
        logger.info(f"Testing performance for query: '{query}'")
        
        # Measure response time
        start_time = time.time()
        
        response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        response_time = time.time() - start_time
        
        # Check query response
        assert response.status_code == 200, f"Query failed with status {response.status_code}: {response.text}"
        response_data = response.json()
        
        logger.info(f"Response time: {response_time:.2f} seconds")
        
        # Store results
        results.append({
            "query": query,
            "response_time_seconds": response_time,
            "success": response_time < 10  # Consider successful if response time is under 10 seconds
        })
        
        # Assert reasonable performance
        assert response_time < 10, f"Query '{query}' has slow response time: {response_time:.2f} seconds"
    
    # Save results to file
    results_path = "test_e2e_performance_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Performance test results saved to {os.path.abspath(results_path)}")

def test_end_to_end_cleanup():
    """Clean up test documents and generate final report"""
    # Get document IDs from the upload test
    uploaded_docs = test_document_upload_and_processing()
    
    # Clean up all test documents
    for doc_type, doc_id in uploaded_docs.items():
        logger.info(f"Cleaning up document: {doc_type} (ID: {doc_id})")
        
        try:
            delete_response = client.delete(f"/api/documents/{doc_id}")
            assert delete_response.status_code == 200, f"Delete failed with status {delete_response.status_code}: {delete_response.text}"
            
            delete_data = delete_response.json()
            assert delete_data["success"] is True, f"Delete response indicates failure: {delete_data}"
            
            logger.info(f"Successfully deleted document: {doc_type} (ID: {doc_id})")
            
        except Exception as e:
            logger.error(f"Error deleting document {doc_type} (ID: {doc_id}): {str(e)}")
            pytest.fail(f"Error deleting document {doc_type} (ID: {doc_id}): {str(e)}")
    
    # Collect all results files
    result_files = [
        "test_e2e_upload_results.json",
        "test_e2e_single_doc_results.json",
        "test_e2e_multi_doc_results.json",
        "test_e2e_complex_results.json",
        "test_e2e_citation_results.json",
        "test_e2e_performance_results.json"
    ]
    
    # Combine results into a single report
    all_results = {}
    
    for file_path in result_files:
        if os.path.exists(file_path):
            with open(file_path, "r") as f:
                results = json.load(f)
            
            test_name = file_path.replace("test_e2e_", "").replace("_results.json", "")
            all_results[test_name] = results
    
    # Save comprehensive report
    report_path = "test_e2e_comprehensive_report.json"
    with open(report_path, "w") as f:
        json.dump({
            "test_name": "Metis RAG End-to-End Test",
            "timestamp": datetime.now().isoformat(),
            "results": all_results
        }, f, indent=2)
    
    logger.info(f"End-to-end test complete. Comprehensive report saved to {os.path.abspath(report_path)}")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_performance.py
================
#!/usr/bin/env python3
"""
Performance benchmarking test suite for the Metis RAG system.
This test suite measures response time, throughput, and resource utilization.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
import psutil
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
import statistics
import concurrent.futures
from io import BytesIO

import pytest
from fastapi.testclient import TestClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_performance")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document, Chunk
from app.models.chat import ChatQuery
from app.main import app

# Test client
client = TestClient(app)

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test queries for benchmarking
BENCHMARK_QUERIES = [
    "What is the architecture of Metis RAG?",
    "How does the document processing pipeline work?",
    "What is the role of the vector store?",
    "How does the frontend interface with the API layer?",
    "What is the purpose of the LLM integration component?",
    "Explain the chunking strategies used in document processing.",
    "How does the system handle metadata filtering?",
    "What technologies are used in the frontend layer?",
    "How does the RAG engine retrieve relevant information?",
    "What is the overall purpose of the Metis RAG system?"
]

@pytest.fixture
def test_document_dir():
    """Create a temporary directory for test documents"""
    temp_dir = tempfile.mkdtemp(prefix="metis_rag_perf_")
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.fixture
def create_test_document(test_document_dir):
    """Create a test document for performance testing"""
    file_path = os.path.join(test_document_dir, "technical_doc.md")
    with open(file_path, "w") as f:
        f.write(TEST_DOCUMENT)
    return file_path

@pytest.fixture
async def setup_vector_store(create_test_document):
    """Set up vector store with test document"""
    # Use a separate directory for test ChromaDB
    test_chroma_dir = "test_perf_chroma"
    os.makedirs(test_chroma_dir, exist_ok=True)
    
    # Initialize vector store
    vector_store = VectorStore(persist_directory=test_chroma_dir)
    
    # Create Document object
    with open(create_test_document, "r") as f:
        content = f.read()
        
    doc = Document(
        id="test_doc",
        filename=os.path.basename(create_test_document),
        content=content,
        tags=["technical", "documentation"],
        folder="/test"
    )
    
    # Create a single chunk for simplicity
    doc.chunks = [
        Chunk(
            id="test_chunk_0",
            content=content,
            metadata={
                "index": 0,
                "source": os.path.basename(create_test_document)
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(doc)
    
    return vector_store, doc

@pytest.fixture
async def setup_rag_engine(setup_vector_store):
    """Set up RAG engine with test vector store"""
    vector_store, document = await setup_vector_store
    rag_engine = RAGEngine(vector_store=vector_store)
    return rag_engine, document

def get_system_metrics():
    """Get current system resource utilization"""
    process = psutil.Process(os.getpid())
    return {
        "cpu_percent": process.cpu_percent(),
        "memory_percent": process.memory_percent(),
        "memory_mb": process.memory_info().rss / (1024 * 1024),  # Convert to MB
        "threads": process.num_threads(),
        "system_cpu_percent": psutil.cpu_percent(),
        "system_memory_percent": psutil.virtual_memory().percent
    }

@pytest.mark.asyncio
async def test_query_response_time(setup_rag_engine):
    """Benchmark query response time"""
    rag_engine, document = await setup_rag_engine
    
    results = []
    
    for query in BENCHMARK_QUERIES:
        # Warm-up query to initialize any lazy-loaded components
        await rag_engine.query(
            query="Warm-up query",
            use_rag=True,
            stream=False
        )
        
        # Measure response time
        start_time = time.time()
        
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            stream=False
        )
        
        end_time = time.time()
        response_time_ms = (end_time - start_time) * 1000
        
        # Get system metrics
        metrics = get_system_metrics()
        
        # Log results
        logger.info(f"Query: {query}")
        logger.info(f"Response time: {response_time_ms:.2f} ms")
        logger.info(f"CPU: {metrics['cpu_percent']:.1f}%, Memory: {metrics['memory_mb']:.1f} MB")
        
        # Store results
        results.append({
            "query": query,
            "response_time_ms": response_time_ms,
            "answer_length": len(response.get("answer", "")),
            "sources_count": len(response.get("sources", [])),
            **metrics
        })
    
    # Calculate statistics
    response_times = [r["response_time_ms"] for r in results]
    stats = {
        "min_response_time_ms": min(response_times),
        "max_response_time_ms": max(response_times),
        "avg_response_time_ms": statistics.mean(response_times),
        "median_response_time_ms": statistics.median(response_times),
        "stddev_response_time_ms": statistics.stdev(response_times) if len(response_times) > 1 else 0,
        "total_queries": len(response_times)
    }
    
    # Save results to file
    results_path = "test_response_time_results.json"
    with open(results_path, "w") as f:
        json.dump({
            "results": results,
            "statistics": stats
        }, f, indent=2)
    
    logger.info(f"Response time test results saved to {os.path.abspath(results_path)}")
    logger.info(f"Average response time: {stats['avg_response_time_ms']:.2f} ms")
    
    # Assert reasonable response time (adjust threshold as needed)
    assert stats["avg_response_time_ms"] < 5000, f"Average response time too high: {stats['avg_response_time_ms']:.2f} ms"

@pytest.mark.asyncio
async def test_throughput(setup_rag_engine):
    """Benchmark system throughput with concurrent queries"""
    rag_engine, document = await setup_rag_engine
    
    results = []
    concurrency_levels = [1, 2, 5, 10]  # Number of concurrent queries
    
    for concurrency in concurrency_levels:
        logger.info(f"Testing throughput with concurrency level: {concurrency}")
        
        # Create a list of queries (repeat the benchmark queries if needed)
        queries = BENCHMARK_QUERIES * (concurrency // len(BENCHMARK_QUERIES) + 1)
        queries = queries[:concurrency]  # Limit to the desired concurrency level
        
        # Function to execute a single query
        async def execute_query(query):
            start_time = time.time()
            
            response = await rag_engine.query(
                query=query,
                use_rag=True,
                stream=False
            )
            
            end_time = time.time()
            return {
                "query": query,
                "response_time_ms": (end_time - start_time) * 1000,
                "answer_length": len(response.get("answer", "")),
                "sources_count": len(response.get("sources", []))
            }
        
        # Execute queries concurrently
        start_time = time.time()
        
        # Create tasks for all queries
        tasks = [execute_query(query) for query in queries]
        
        # Wait for all tasks to complete
        query_results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time_seconds = end_time - start_time
        
        # Calculate throughput
        throughput_qps = concurrency / total_time_seconds if total_time_seconds > 0 else 0
        
        # Get system metrics
        metrics = get_system_metrics()
        
        # Calculate statistics
        response_times = [r["response_time_ms"] for r in query_results]
        stats = {
            "min_response_time_ms": min(response_times),
            "max_response_time_ms": max(response_times),
            "avg_response_time_ms": statistics.mean(response_times),
            "median_response_time_ms": statistics.median(response_times),
            "stddev_response_time_ms": statistics.stdev(response_times) if len(response_times) > 1 else 0
        }
        
        # Log results
        logger.info(f"Concurrency: {concurrency}, Throughput: {throughput_qps:.2f} queries/second")
        logger.info(f"Avg response time: {stats['avg_response_time_ms']:.2f} ms")
        logger.info(f"CPU: {metrics['cpu_percent']:.1f}%, Memory: {metrics['memory_mb']:.1f} MB")
        
        # Store results
        results.append({
            "concurrency": concurrency,
            "total_time_seconds": total_time_seconds,
            "throughput_queries_per_second": throughput_qps,
            "query_count": len(query_results),
            "response_time_stats": stats,
            "system_metrics": metrics,
            "query_results": query_results
        })
    
    # Save results to file
    results_path = "test_throughput_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Throughput test results saved to {os.path.abspath(results_path)}")
    
    # Assert reasonable throughput scaling
    throughputs = [r["throughput_queries_per_second"] for r in results]
    assert throughputs[-1] > 0, "Throughput should be greater than zero"
    
    # Ideally, throughput should scale with concurrency, but this depends on system resources
    # This is a simple check that throughput doesn't collapse under load
    assert throughputs[-1] >= throughputs[0] * 0.5, "Throughput collapsed under load"

@pytest.mark.asyncio
async def test_resource_utilization(setup_rag_engine):
    """Benchmark resource utilization during sustained load"""
    rag_engine, document = await setup_rag_engine
    
    results = []
    duration_seconds = 30  # Duration of the test
    query_interval_seconds = 1  # Time between queries
    
    logger.info(f"Testing resource utilization over {duration_seconds} seconds")
    
    # Function to execute queries at regular intervals
    async def query_task():
        start_time = time.time()
        query_count = 0
        
        while time.time() - start_time < duration_seconds:
            # Select a query
            query = BENCHMARK_QUERIES[query_count % len(BENCHMARK_QUERIES)]
            
            # Execute query
            await rag_engine.query(
                query=query,
                use_rag=True,
                stream=False
            )
            
            query_count += 1
            
            # Wait for the next interval
            await asyncio.sleep(query_interval_seconds)
            
        return query_count
    
    # Function to monitor system resources
    async def monitor_task():
        start_time = time.time()
        metrics_list = []
        
        while time.time() - start_time < duration_seconds:
            # Get system metrics
            metrics = get_system_metrics()
            metrics["timestamp"] = time.time() - start_time
            metrics_list.append(metrics)
            
            # Wait before next measurement
            await asyncio.sleep(1)
            
        return metrics_list
    
    # Run both tasks concurrently
    query_task_obj = asyncio.create_task(query_task())
    monitor_task_obj = asyncio.create_task(monitor_task())
    
    # Wait for both tasks to complete
    query_count = await query_task_obj
    metrics_list = await monitor_task_obj
    
    # Calculate statistics
    cpu_percentages = [m["cpu_percent"] for m in metrics_list]
    memory_mbs = [m["memory_mb"] for m in metrics_list]
    
    stats = {
        "avg_cpu_percent": statistics.mean(cpu_percentages),
        "max_cpu_percent": max(cpu_percentages),
        "avg_memory_mb": statistics.mean(memory_mbs),
        "max_memory_mb": max(memory_mbs),
        "query_count": query_count,
        "queries_per_second": query_count / duration_seconds
    }
    
    # Log results
    logger.info(f"Executed {query_count} queries over {duration_seconds} seconds")
    logger.info(f"Average CPU: {stats['avg_cpu_percent']:.1f}%, Max CPU: {stats['max_cpu_percent']:.1f}%")
    logger.info(f"Average Memory: {stats['avg_memory_mb']:.1f} MB, Max Memory: {stats['max_memory_mb']:.1f} MB")
    
    # Store results
    results = {
        "duration_seconds": duration_seconds,
        "query_interval_seconds": query_interval_seconds,
        "query_count": query_count,
        "queries_per_second": query_count / duration_seconds,
        "statistics": stats,
        "metrics_timeline": metrics_list
    }
    
    # Save results to file
    results_path = "test_resource_utilization_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Resource utilization test results saved to {os.path.abspath(results_path)}")
    
    # No specific assertions for resource utilization, as acceptable values depend on the system

@pytest.mark.asyncio
async def test_api_performance(create_test_document):
    """Benchmark API endpoint performance"""
    results = []
    
    # Test document upload and processing performance
    with open(create_test_document, "rb") as f:
        file_content = f.read()
    
    # Measure upload performance
    upload_times = []
    document_ids = []
    
    for i in range(5):  # Upload the same document 5 times
        file_obj = BytesIO(file_content)
        file_obj.name = f"perf_test_{i}.md"
        
        start_time = time.time()
        
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (file_obj.name, file_obj, "text/markdown")}
        )
        
        end_time = time.time()
        upload_time_ms = (end_time - start_time) * 1000
        
        assert upload_response.status_code == 200
        upload_data = upload_response.json()
        document_ids.append(upload_data["document_id"])
        
        upload_times.append(upload_time_ms)
        
        logger.info(f"Upload {i+1}: {upload_time_ms:.2f} ms")
    
    # Measure processing performance
    start_time = time.time()
    
    process_response = client.post(
        "/api/documents/process",
        json={"document_ids": document_ids}
    )
    
    end_time = time.time()
    process_time_ms = (end_time - start_time) * 1000
    
    assert process_response.status_code == 200
    
    logger.info(f"Processing {len(document_ids)} documents: {process_time_ms:.2f} ms")
    
    # Measure query performance
    query_times = []
    
    for query in BENCHMARK_QUERIES[:5]:  # Use first 5 queries
        start_time = time.time()
        
        query_response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        end_time = time.time()
        query_time_ms = (end_time - start_time) * 1000
        
        assert query_response.status_code == 200
        
        query_times.append(query_time_ms)
        
        logger.info(f"Query: {query}")
        logger.info(f"Query time: {query_time_ms:.2f} ms")
    
    # Calculate statistics
    upload_stats = {
        "min_upload_time_ms": min(upload_times),
        "max_upload_time_ms": max(upload_times),
        "avg_upload_time_ms": statistics.mean(upload_times)
    }
    
    query_stats = {
        "min_query_time_ms": min(query_times),
        "max_query_time_ms": max(query_times),
        "avg_query_time_ms": statistics.mean(query_times)
    }
    
    # Store results
    results = {
        "upload_performance": {
            "document_count": len(document_ids),
            "upload_times_ms": upload_times,
            "statistics": upload_stats
        },
        "processing_performance": {
            "document_count": len(document_ids),
            "process_time_ms": process_time_ms,
            "process_time_per_document_ms": process_time_ms / len(document_ids)
        },
        "query_performance": {
            "query_count": len(query_times),
            "query_times_ms": query_times,
            "statistics": query_stats
        }
    }
    
    # Save results to file
    results_path = "test_api_performance_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"API performance test results saved to {os.path.abspath(results_path)}")
    
    # Clean up - delete all documents
    for doc_id in document_ids:
        client.delete(f"/api/documents/{doc_id}")

@pytest.mark.asyncio
async def test_generate_performance_report():
    """Generate a comprehensive performance report"""
    # Check if all result files exist
    result_files = [
        "test_response_time_results.json",
        "test_throughput_results.json",
        "test_resource_utilization_results.json",
        "test_api_performance_results.json"
    ]
    
    missing_files = [f for f in result_files if not os.path.exists(f)]
    
    if missing_files:
        logger.warning(f"Missing result files: {missing_files}")
        logger.warning("Run the individual performance tests first")
        return
    
    # Load all results
    results = {}
    
    for file_path in result_files:
        with open(file_path, "r") as f:
            results[file_path] = json.load(f)
    
    # Generate report
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "response_time": {
                "avg_ms": results["test_response_time_results.json"]["statistics"]["avg_response_time_ms"],
                "min_ms": results["test_response_time_results.json"]["statistics"]["min_response_time_ms"],
                "max_ms": results["test_response_time_results.json"]["statistics"]["max_response_time_ms"]
            },
            "throughput": {
                "max_qps": max(r["throughput_queries_per_second"] for r in results["test_throughput_results.json"]),
                "concurrency_levels": [r["concurrency"] for r in results["test_throughput_results.json"]]
            },
            "resource_utilization": {
                "avg_cpu_percent": results["test_resource_utilization_results.json"]["statistics"]["avg_cpu_percent"],
                "max_cpu_percent": results["test_resource_utilization_results.json"]["statistics"]["max_cpu_percent"],
                "avg_memory_mb": results["test_resource_utilization_results.json"]["statistics"]["avg_memory_mb"],
                "max_memory_mb": results["test_resource_utilization_results.json"]["statistics"]["max_memory_mb"]
            },
            "api_performance": {
                "avg_upload_time_ms": results["test_api_performance_results.json"]["upload_performance"]["statistics"]["avg_upload_time_ms"],
                "avg_query_time_ms": results["test_api_performance_results.json"]["query_performance"]["statistics"]["avg_query_time_ms"],
                "process_time_per_document_ms": results["test_api_performance_results.json"]["processing_performance"]["process_time_per_document_ms"]
            }
        },
        "detailed_results": results
    }
    
    # Save report
    report_path = "performance_benchmark_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Performance benchmark report saved to {os.path.abspath(report_path)}")
    
    # Generate HTML report
    html_report = f"""<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Performance Benchmark Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        .section {{ margin-bottom: 30px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .chart {{ width: 100%; height: 300px; margin-top: 20px; }}
    </style>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <h1>Metis RAG Performance Benchmark Report</h1>
    <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="section">
        <h2>Summary</h2>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Average Response Time</td>
                <td>{report["summary"]["response_time"]["avg_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Maximum Throughput</td>
                <td>{report["summary"]["throughput"]["max_qps"]:.2f} queries/second</td>
            </tr>
            <tr>
                <td>Average CPU Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_cpu_percent"]:.1f}%</td>
            </tr>
            <tr>
                <td>Average Memory Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_memory_mb"]:.1f} MB</td>
            </tr>
            <tr>
                <td>Average Upload Time</td>
                <td>{report["summary"]["api_performance"]["avg_upload_time_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Average Query Time (API)</td>
                <td>{report["summary"]["api_performance"]["avg_query_time_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Document Processing Time</td>
                <td>{report["summary"]["api_performance"]["process_time_per_document_ms"]:.2f} ms per document</td>
            </tr>
        </table>
    </div>
    
    <div class="section">
        <h2>Response Time</h2>
        <p>Statistics for query response time across {len(results["test_response_time_results.json"]["results"])} test queries.</p>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Minimum</td>
                <td>{report["summary"]["response_time"]["min_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Maximum</td>
                <td>{report["summary"]["response_time"]["max_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Average</td>
                <td>{report["summary"]["response_time"]["avg_ms"]:.2f} ms</td>
            </tr>
        </table>
        
        <div class="chart">
            <canvas id="responseTimeChart"></canvas>
        </div>
    </div>
    
    <div class="section">
        <h2>Throughput</h2>
        <p>Throughput measurements at different concurrency levels.</p>
        <table>
            <tr>
                <th>Concurrency</th>
                <th>Throughput (queries/second)</th>
                <th>Average Response Time (ms)</th>
            </tr>
            {"".join(f"<tr><td>{r['concurrency']}</td><td>{r['throughput_queries_per_second']:.2f}</td><td>{r['response_time_stats']['avg_response_time_ms']:.2f}</td></tr>" for r in results["test_throughput_results.json"])}
        </table>
        
        <div class="chart">
            <canvas id="throughputChart"></canvas>
        </div>
    </div>
    
    <div class="section">
        <h2>Resource Utilization</h2>
        <p>Resource utilization during sustained load over {results["test_resource_utilization_results.json"]["duration_seconds"]} seconds.</p>
        <table>
            <tr>
                <th>Metric</th>
                <th>Average</th>
                <th>Maximum</th>
            </tr>
            <tr>
                <td>CPU Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_cpu_percent"]:.1f}%</td>
                <td>{report["summary"]["resource_utilization"]["max_cpu_percent"]:.1f}%</td>
            </tr>
            <tr>
                <td>Memory Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_memory_mb"]:.1f} MB</td>
                <td>{report["summary"]["resource_utilization"]["max_memory_mb"]:.1f} MB</td>
            </tr>
        </table>
        
        <div class="chart">
            <canvas id="resourceChart"></canvas>
        </div>
    </div>
    
    <div class="section">
        <h2>API Performance</h2>
        <p>Performance measurements for API endpoints.</p>
        <h3>Upload Performance</h3>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Average Upload Time</td>
                <td>{report["summary"]["api_performance"]["avg_upload_time_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Document Processing Time</td>
                <td>{report["summary"]["api_performance"]["process_time_per_document_ms"]:.2f} ms per document</td>
            </tr>
        </table>
        
        <h3>Query Performance</h3>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Average Query Time</td>
                <td>{report["summary"]["api_performance"]["avg_query_time_ms"]:.2f} ms</td>
            </tr>
        </table>
    </div>
    
    <script>
        // Response Time Chart
        const responseTimeCtx = document.getElementById('responseTimeChart').getContext('2d');
        const responseTimeChart = new Chart(responseTimeCtx, {{
            type: 'bar',
            data: {{
                labels: {json.dumps([r["query"][:20] + "..." for r in results["test_response_time_results.json"]["results"]])},
                datasets: [{{
                    label: 'Response Time (ms)',
                    data: {json.dumps([r["response_time_ms"] for r in results["test_response_time_results.json"]["results"]])},
                    backgroundColor: 'rgba(54, 162, 235, 0.5)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1
                }}]
            }},
            options: {{
                scales: {{
                    y: {{
                        beginAtZero: true,
                        title: {{
                            display: true,
                            text: 'Response Time (ms)'
                        }}
                    }}
                }}
            }}
        }});
        
        // Throughput Chart
        const throughputCtx = document.getElementById('throughputChart').getContext('2d');
        const throughputChart = new Chart(throughputCtx, {{
            type: 'line',
            data: {{
                labels: {json.dumps([r["concurrency"] for r in results["test_throughput_results.json"]])},
                datasets: [
                    {{
                        label: 'Throughput (queries/second)',
                        data: {json.dumps([r["throughput_queries_per_second"] for r in results["test_throughput_results.json"]])},
                        backgroundColor: 'rgba(75, 192, 192, 0.5)',
                        borderColor: 'rgba(75, 192, 192, 1)',
                        borderWidth: 2,
                        yAxisID: 'y'
                    }},
                    {{
                        label: 'Avg Response Time (ms)',
                        data: {json.dumps([r["response_time_stats"]["avg_response_time_ms"] for r in results["test_throughput_results.json"]])},
                        backgroundColor: 'rgba(255, 99, 132, 0.5)',
                        borderColor: 'rgba(255, 99, 132, 1)',
                        borderWidth: 2,
                        yAxisID: 'y1'
                    }}
                ]
            }},
            options: {{
                scales: {{
                    y: {{
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {{
                            display: true,
                            text: 'Throughput (queries/second)'
                        }}
                    }},
                    y1: {{
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {{
                            display: true,
                            text: 'Response Time (ms)'
                        }},
                        grid: {{
                            drawOnChartArea: false
                        }}
                    }}
                }}
            }}
        }});
        
        // Resource Utilization Chart
        const resourceCtx = document.getElementById('resourceChart').getContext('2d');
        const resourceData = {json.dumps(results["test_resource_utilization_results.json"]["metrics_timeline"])};
        const resourceChart = new Chart(resourceCtx, {{
            type: 'line',
            data: {{
                labels: resourceData.map(m => m.timestamp.toFixed(1) + 's'),
                datasets: [
                    {{
                        label: 'CPU Usage (%)',
                        data: resourceData.map(m => m.cpu_percent),
                        backgroundColor: 'rgba(255, 99, 132, 0.5)',
                        borderColor: 'rgba(255, 99, 132, 1)',
                        borderWidth: 2,
                        yAxisID: 'y'
                    }},
                    {{
                        label: 'Memory Usage (MB)',
                        data: resourceData.map(m => m.memory_mb),
                        backgroundColor: 'rgba(54, 162, 235, 0.5)',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 2,
                        yAxisID: 'y1'
                    }}
                ]
            }},
            options: {{
                scales: {{
                    y: {{
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {{
                            display: true,
                            text: 'CPU Usage (%)'
                        }}
                    }},
                    y1: {{
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {{
                            display: true,
                            text: 'Memory Usage (MB)'
                        }},
                        grid: {{
                            drawOnChartArea: false
                        }}
                    }}
                }}
            }}
        }});
    </script>
</body>
</html>
"""
    
    html_report_path = "performance_benchmark_report.html"
    with open(html_report_path, "w") as f:
        f.write(html_report)
    
    logger.info(f"HTML performance report saved to {os.path.abspath(html_report_path)}")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_query_performance.py
================
import os
import time
import asyncio
import pytest
import statistics
from typing import List, Dict, Any
from uuid import UUID

from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.db.repositories.document_repository import DocumentRepository
from app.db.repositories.analytics_repository import AnalyticsRepository
from app.db.session import SessionLocal
from app.core.config import SETTINGS

# Test configuration
TEST_QUERIES = [
    "What is the main purpose of the RAG system?",
    "How does the document analysis service work?",
    "What are the key components of the LangGraph integration?",
    "Explain the database schema for documents and chunks",
    "How does the memory integration enhance the system?",
    "What performance optimizations are implemented in the system?",
    "How does the response quality pipeline work?",
    "What is the purpose of the background task system?",
    "How are documents processed and analyzed?",
    "What are the main features of the Metis RAG system?"
]

# Number of times to run each query for averaging
NUM_RUNS = 3

# Maximum acceptable response time for simple queries (in seconds)
MAX_RESPONSE_TIME = 6.0

@pytest.fixture
def db_session():
    """Create a database session for testing"""
    session = SessionLocal()
    try:
        yield session
    finally:
        session.close()

@pytest.fixture
def document_repository(db_session):
    """Create a document repository for testing"""
    return DocumentRepository(db_session)

@pytest.fixture
def analytics_repository(db_session):
    """Create an analytics repository for testing"""
    return AnalyticsRepository(db_session)

@pytest.fixture
def vector_store():
    """Create a vector store for testing"""
    return VectorStore()

@pytest.fixture
def rag_engine():
    """Create a RAG engine for testing"""
    return RAGEngine()

async def run_query(rag_engine, query: str, use_rag: bool = True):
    """Run a query and measure performance"""
    start_time = time.time()
    
    response = await rag_engine.query(
        query=query,
        model=SETTINGS.default_model,
        use_rag=use_rag,
        stream=False
    )
    
    elapsed_time = time.time() - start_time
    
    return {
        "query": query,
        "response": response.get("answer", ""),
        "elapsed_time": elapsed_time,
        "sources": response.get("sources", []),
        "token_count": response.get("token_count", 0)
    }

async def run_query_batch(rag_engine, queries: List[str], use_rag: bool = True, runs: int = 1):
    """Run a batch of queries multiple times and collect performance metrics"""
    results = []
    
    for query in queries:
        query_results = []
        for _ in range(runs):
            result = await run_query(rag_engine, query, use_rag)
            query_results.append(result)
        
        # Calculate average metrics
        avg_time = statistics.mean([r["elapsed_time"] for r in query_results])
        avg_token_count = statistics.mean([r["token_count"] for r in query_results])
        
        results.append({
            "query": query,
            "avg_elapsed_time": avg_time,
            "avg_token_count": avg_token_count,
            "num_runs": runs,
            "use_rag": use_rag,
            "response": query_results[0]["response"],  # Just use the first response
            "sources": query_results[0]["sources"]  # Just use the first sources
        })
    
    return results

def record_performance_results(analytics_repository, results: List[Dict[str, Any]]):
    """Record performance results in the analytics repository"""
    for result in results:
        analytics_repository.create_analytics_query(
            query=result["query"],
            model=SETTINGS.default_model,
            use_rag=result["use_rag"],
            response_time_ms=result["avg_elapsed_time"] * 1000,
            token_count=result["avg_token_count"],
            document_ids=[s.get("document_id") for s in result.get("sources", []) if s.get("document_id")],
            query_type="performance_test",
            successful=True
        )

@pytest.mark.asyncio
async def test_rag_query_performance(rag_engine, analytics_repository):
    """Test RAG query performance"""
    print(f"\nRunning RAG query performance test with {len(TEST_QUERIES)} queries, {NUM_RUNS} runs each")
    
    # Run queries with RAG
    results = await run_query_batch(rag_engine, TEST_QUERIES, use_rag=True, runs=NUM_RUNS)
    
    # Record results
    record_performance_results(analytics_repository, results)
    
    # Print results
    print("\nRAG Query Performance Results:")
    print(f"{'Query':<50} | {'Avg Time (s)':<12} | {'Avg Tokens':<10} | {'Sources':<10}")
    print("-" * 90)
    
    for result in results:
        query_display = result["query"][:47] + "..." if len(result["query"]) > 50 else result["query"].ljust(50)
        print(f"{query_display} | {result['avg_elapsed_time']:<12.2f} | {result['avg_token_count']:<10.0f} | {len(result['sources']):<10}")
    
    # Calculate overall metrics
    avg_time = statistics.mean([r["avg_elapsed_time"] for r in results])
    max_time = max([r["avg_elapsed_time"] for r in results])
    min_time = min([r["avg_elapsed_time"] for r in results])
    avg_tokens = statistics.mean([r["avg_token_count"] for r in results])
    
    print("-" * 90)
    print(f"Overall Average: {avg_time:.2f}s, Min: {min_time:.2f}s, Max: {max_time:.2f}s, Avg Tokens: {avg_tokens:.0f}")
    
    # Assert performance requirements
    assert max_time <= MAX_RESPONSE_TIME, f"Maximum response time ({max_time:.2f}s) exceeds limit ({MAX_RESPONSE_TIME}s)"

@pytest.mark.asyncio
async def test_non_rag_query_performance(rag_engine, analytics_repository):
    """Test non-RAG query performance (LLM only)"""
    print(f"\nRunning non-RAG query performance test with {len(TEST_QUERIES)} queries, {NUM_RUNS} runs each")
    
    # Run queries without RAG
    results = await run_query_batch(rag_engine, TEST_QUERIES, use_rag=False, runs=NUM_RUNS)
    
    # Record results
    record_performance_results(analytics_repository, results)
    
    # Print results
    print("\nNon-RAG Query Performance Results:")
    print(f"{'Query':<50} | {'Avg Time (s)':<12} | {'Avg Tokens':<10}")
    print("-" * 80)
    
    for result in results:
        query_display = result["query"][:47] + "..." if len(result["query"]) > 50 else result["query"].ljust(50)
        print(f"{query_display} | {result['avg_elapsed_time']:<12.2f} | {result['avg_token_count']:<10.0f}")
    
    # Calculate overall metrics
    avg_time = statistics.mean([r["avg_elapsed_time"] for r in results])
    max_time = max([r["avg_elapsed_time"] for r in results])
    min_time = min([r["avg_elapsed_time"] for r in results])
    avg_tokens = statistics.mean([r["avg_token_count"] for r in results])
    
    print("-" * 80)
    print(f"Overall Average: {avg_time:.2f}s, Min: {min_time:.2f}s, Max: {max_time:.2f}s, Avg Tokens: {avg_tokens:.0f}")

@pytest.mark.asyncio
async def test_performance_comparison():
    """Compare RAG vs non-RAG performance"""
    print("\nComparing RAG vs non-RAG performance")
    
    # This test will be implemented to analyze the results from the analytics repository
    # and compare the performance of RAG vs non-RAG queries
    
    # For now, we'll just print a placeholder message
    print("Performance comparison test will be implemented in a future update")

if __name__ == "__main__":
    # Run the tests directly if this file is executed
    asyncio.run(test_rag_query_performance(RAGEngine(), AnalyticsRepository(SessionLocal())))
    asyncio.run(test_non_rag_query_performance(RAGEngine(), AnalyticsRepository(SessionLocal())))
    asyncio.run(test_performance_comparison())

================
File: tests/test_query_refinement_fix.py
================
#!/usr/bin/env python3
"""
Test script to verify the query refinement fixes.

This script tests the following fixes:
1. Entity preservation in query refinement
2. Minimum context requirements
3. Chunk ID validation in citations
"""

import asyncio
import logging
import sys
import os
import uuid
from typing import Dict, Any, List

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger("test_query_refinement_fix")

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the necessary modules
from app.rag.rag_engine import RAGEngine
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.ollama_client import OllamaClient
from app.models.chat import Message

async def test_entity_preservation():
    """Test entity preservation in query refinement"""
    logger.info("Testing entity preservation in query refinement...")
    
    # Create a retrieval judge
    ollama_client = OllamaClient()
    retrieval_judge = RetrievalJudge(ollama_client=ollama_client)
    
    # Test queries with entity names
    test_queries = [
        "tell me about Stabilium",
        "what is Quantum Resonance Modulation",
        "explain the Heisenberg Uncertainty Principle",
        "information about Microsoft Azure",
        "details on SpaceX Starship"
    ]
    
    for query in test_queries:
        logger.info(f"Testing query: {query}")
        
        # Create a mock chunks list for testing
        mock_chunks = [
            {
                "chunk_id": str(uuid.uuid4()),
                "content": f"This is a document about {query.split()[-1]}.",
                "metadata": {"filename": "test.txt"},
                "distance": 0.2
            }
        ]
        
        # Test query refinement
        refined_query = await retrieval_judge.refine_query(query, mock_chunks)
        
        # Check if the entity name is preserved
        entity_name = query.split()[-1]
        if entity_name in refined_query:
            logger.info(f"✅ Entity '{entity_name}' preserved in refined query: '{refined_query}'")
        else:
            logger.error(f"❌ Entity '{entity_name}' NOT preserved in refined query: '{refined_query}'")
            
        # Add a separator for readability
        logger.info("-" * 50)

async def test_minimum_context():
    """Test minimum context requirements"""
    logger.info("Testing minimum context requirements...")
    
    # Create a RAG engine
    rag_engine = RAGEngine()
    
    # Test query
    query = "tell me about Stabilium"
    
    # Execute the query
    response = await rag_engine.query(
        query=query,
        use_rag=True,
        stream=False
    )
    
    # Check the response
    if response and "answer" in response:
        answer = response["answer"]
        sources = response.get("sources", [])
        
        logger.info(f"Query: {query}")
        logger.info(f"Answer length: {len(answer)} characters")
        logger.info(f"Number of sources: {len(sources)}")
        
        # Check if we have a minimum number of sources
        if len(sources) >= 3:
            logger.info("✅ Minimum context requirement met")
        else:
            logger.warning(f"⚠️ Minimum context requirement not met: only {len(sources)} sources")
            
        # Log the first 100 characters of the answer
        logger.info(f"Answer preview: {answer[:100]}...")
    else:
        logger.error("❌ Failed to get a response from the RAG engine")
    
    # Add a separator for readability
    logger.info("-" * 50)

async def test_citation_handling():
    """Test citation handling with non-existent chunk IDs"""
    logger.info("Testing citation handling...")
    
    # Create a RAG engine
    rag_engine = RAGEngine()
    
    # Create a conversation with history
    conversation_id = str(uuid.uuid4())
    conversation_history = [
        Message(id=1, conversation_id=conversation_id, role="user", content="Hello"),
        Message(id=2, conversation_id=conversation_id, role="assistant", content="Hi there! How can I help you?")
    ]
    
    # Test query
    query = "tell me about Stabilium and Quantum Resonance"
    
    # Execute the query
    response = await rag_engine.query(
        query=query,
        use_rag=True,
        stream=False,
        conversation_history=conversation_history
    )
    
    # Check the response
    if response and "answer" in response:
        answer = response["answer"]
        sources = response.get("sources", [])
        
        logger.info(f"Query: {query}")
        logger.info(f"Answer length: {len(answer)} characters")
        logger.info(f"Number of sources: {len(sources)}")
        
        # Check if we have valid sources
        valid_sources = [s for s in sources if s.document_id]
        if len(valid_sources) > 0:
            logger.info(f"✅ Valid sources found: {len(valid_sources)}")
        else:
            logger.warning("⚠️ No valid sources found")
            
        # Log the first 100 characters of the answer
        logger.info(f"Answer preview: {answer[:100]}...")
    else:
        logger.error("❌ Failed to get a response from the RAG engine")
    
    # Add a separator for readability
    logger.info("-" * 50)

async def main():
    """Main function to run all tests"""
    logger.info("Starting query refinement fix tests...")
    
    # Test entity preservation
    await test_entity_preservation()
    
    # Test minimum context requirements
    await test_minimum_context()
    
    # Test citation handling
    await test_citation_handling()
    
    logger.info("All tests completed!")

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_rag_entity_preservation.py
================
#!/usr/bin/env python3
"""
Comprehensive test script to verify the Metis RAG system fixes:
- Entity preservation in query refinement
- Minimum context requirements
- Citation handling

This script tests the system with increasingly complex queries to verify
that entity names are preserved, sufficient context is retrieved, and
citations are properly handled.
"""

import asyncio
import logging
import sys
import os
import json
import uuid
import time
import re
from typing import Dict, Any, List, Optional
from datetime import datetime

# Custom log handler to capture log messages
class LogCaptureHandler(logging.Handler):
    def __init__(self):
        super().__init__()
        self.logs = []
        
    def emit(self, record):
        self.logs.append(record)
        
    def get_logs(self):
        return self.logs
        
    def clear(self):
        self.logs = []

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("tests/results/entity_preservation_test_results.log")
    ]
)

logger = logging.getLogger("test_rag_entity_preservation")

# Create a log capture handler for RAG engine logs
log_capture_handler = LogCaptureHandler()
rag_logger = logging.getLogger("app.rag.rag_engine")
rag_logger.addHandler(log_capture_handler)

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment variables from .env.test
from dotenv import load_dotenv
load_dotenv(".env.test")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.models.document import Document, Chunk
from app.rag.agents.retrieval_judge import RetrievalJudge

# Test document content
TEST_DOCUMENTS = {
    "stabilium_overview": {
        "filename": "stabilium_overview.md",
        "content": """# Stabilium: A Revolutionary Material for Quantum Computing

## Overview

Stabilium is a synthetic metamaterial developed in 2023 that exhibits remarkable quantum stability properties. 
It is primarily composed of a lattice structure of modified graphene sheets interspersed with rare earth elements, 
creating a material that can maintain quantum coherence at higher temperatures than previously thought possible.

## Key Properties

- **Quantum Coherence**: Stabilium can maintain quantum states for up to 300 microseconds at temperatures approaching 20K, 
  a significant improvement over previous materials that required temperatures below 1K.
- **Superconductivity**: Exhibits zero electrical resistance at temperatures up to 15K.
- **Magnetic Field Resistance**: Maintains its properties in magnetic fields up to 2 Tesla.
- **Scalability**: Can be manufactured in sheets up to 10cm² while maintaining uniform properties.

## Applications in Quantum Computing

Stabilium has revolutionized quantum computing by enabling the development of more practical quantum processors:

1. **Qubit Stability**: When used as a substrate for superconducting qubits, Stabilium increases coherence times by a factor of 5-10.
2. **Reduced Cooling Requirements**: Quantum computers using Stabilium components can operate with less extensive cooling systems.
3. **Error Correction**: The increased stability reduces the overhead required for quantum error correction.
4. **Quantum Memory**: Stabilium-based quantum memory cells can store quantum states reliably for extended periods.

## Integration with Quantum Resonance Modulation

When combined with Quantum Resonance Modulation (QRM) techniques, Stabilium enables:
- Dynamic qubit coupling with reduced crosstalk
- Faster gate operations with higher fidelity
- Improved readout accuracy for quantum measurements

## Current Research

Current research focuses on:
- Developing Stabilium QRM-12X, an enhanced version with even longer coherence times
- Exploring applications in quantum networking and quantum cryptography
- Investigating integration with photonic quantum computing architectures
""",
        "tags": ["quantum computing", "materials science", "stabilium"],
        "folder": "/quantum_materials"
    },
    "qrm_overview": {
        "filename": "quantum_resonance_modulation.md",
        "content": """# Quantum Resonance Modulation: Principles and Applications

## Fundamental Principles

Quantum Resonance Modulation (QRM) is a technique developed to manipulate quantum states through precisely controlled 
electromagnetic field modulations. It works by creating resonant conditions that allow for selective excitation and 
manipulation of quantum systems while minimizing decoherence.

## Key Components

1. **Resonance Generators**: Devices that produce precisely tuned electromagnetic fields
2. **Quantum Field Modulators**: Systems that shape and direct the resonant fields
3. **Feedback Control Systems**: Real-time monitoring and adjustment mechanisms

## Properties in Cold Fusion Experiments

QRM has shown promising results in cold fusion research:

- **Energy Catalysis**: QRM can lower the energy barrier for nuclear fusion by creating quantum tunneling conditions
- **Reaction Stability**: Provides controlled modulation of nuclear forces
- **Heat Distribution**: Enables uniform energy distribution, preventing hotspots
- **Reproducibility**: Significantly improves the reproducibility of cold fusion experiments

## Comparison with Stabilium in Cold Fusion

| Property | Quantum Resonance Modulation | Stabilium |
|----------|------------------------------|-----------|
| Function | Active process that modulates quantum fields | Passive material that maintains quantum coherence |
| Energy Requirements | Requires continuous energy input | Minimal energy requirements once cooled |
| Scalability | Limited by equipment size | Highly scalable with material production |
| Temperature Range | Effective from 0.1K to 50K | Optimal performance at 5K-20K |
| Cost | High operational costs | High initial material cost, low operational cost |
| Maintenance | Regular calibration required | Minimal maintenance needed |

## Integration Approaches

The most successful cold fusion experiments combine QRM techniques with Stabilium components:

1. Stabilium lattices serve as the fusion substrate
2. QRM systems create the conditions for quantum tunneling
3. The combined approach increases fusion efficiency by approximately 300% compared to traditional methods

## Current Limitations

Despite its promise, QRM in cold fusion faces several challenges:

- High energy requirements for sustained operation
- Complex equipment that requires specialized expertise
- Difficulty in scaling beyond laboratory demonstrations
- Theoretical models that are still being refined

## Future Directions

Researchers are currently exploring:

- Miniaturized QRM systems for portable applications
- Integration with other quantum materials beyond Stabilium
- Application to other fields including quantum computing and medical imaging
""",
        "tags": ["quantum physics", "cold fusion", "QRM"],
        "folder": "/quantum_techniques"
    },
    "stabilium_versions": {
        "filename": "stabilium_versions_comparison.md",
        "content": """# Stabilium Versions: Comparative Analysis

## Evolution of Stabilium Technology

Since its initial development, Stabilium has undergone several iterations, each improving upon the quantum coherence properties and practical applications of this revolutionary material.

## Stabilium QRM-12X

The latest and most advanced version of Stabilium, designated QRM-12X, represents a significant leap forward in quantum material science.

### Key Improvements Over Earlier Versions

1. **Enhanced Coherence Time**: QRM-12X maintains quantum coherence for up to 500 microseconds, compared to 300 microseconds in the standard version.

2. **Temperature Tolerance**: Operates effectively at temperatures up to 28K, an improvement of 8K over the standard version.

3. **Integrated QRM Capabilities**: Unlike earlier versions that required external QRM equipment, QRM-12X has resonance modulation properties built into its molecular structure.

4. **Reduced Material Complexity**: Requires 40% fewer rare earth elements in its composition, making it more cost-effective and environmentally sustainable.

5. **Improved Scalability**: Can be manufactured in sheets up to 25cm², more than doubling the size capability of the original version.

## Stabilium Standard (First Generation)

The original Stabilium formulation established the baseline capabilities:

- Coherence time: 300 microseconds at 20K
- Superconductivity up to 15K
- Magnetic field resistance up to 2 Tesla
- Manufacturing scale limited to 10cm²

## Stabilium QRM-8 (Second Generation)

An intermediate version that introduced the first integration with QRM principles:

- Coherence time: 350 microseconds at 22K
- Superconductivity up to 18K
- Magnetic field resistance up to 2.5 Tesla
- Manufacturing scale up to 15cm²
- Required external QRM equipment for optimal performance

## Comparative Performance in Quantum Computing Applications

| Performance Metric | Standard Stabilium | QRM-8 | QRM-12X |
|--------------------|-------------------|-------|---------|
| Qubit Error Rate | 0.5% | 0.3% | 0.1% |
| Gate Operation Speed | 50ns | 35ns | 20ns |
| Power Consumption | 100% (baseline) | 75% | 45% |
| Manufacturing Cost | 100% (baseline) | 120% | 90% |
| Cooling Requirements | 100% (baseline) | 80% | 60% |

## Transition Recommendations

For organizations currently using earlier Stabilium versions:

1. **Research Applications**: Immediate upgrade to QRM-12X is recommended for cutting-edge research.
2. **Commercial Quantum Computing**: Phased transition from QRM-8 to QRM-12X over 6-12 months.
3. **Legacy Systems**: Standard Stabilium remains adequate for educational and non-critical applications.

## Future Development Roadmap

The Stabilium research consortium has announced plans for:

- **QRM-15X**: Currently in early development, expected to achieve coherence times exceeding 1 millisecond
- **Stabilium Flex**: A malleable variant that can be shaped into complex 3D structures
- **Room Temperature Stabilium**: Long-term research goal to create variants that maintain quantum properties at 273K
""",
        "tags": ["stabilium", "QRM-12X", "quantum materials", "version comparison"],
        "folder": "/quantum_materials/versions"
    },
    "quantum_physics": {
        "filename": "heisenberg_uncertainty_principle.md",
        "content": """# Heisenberg's Uncertainty Principle and Quantum Entanglement

## Fundamental Concepts

Heisenberg's Uncertainty Principle, formulated by Werner Heisenberg in 1927, is one of the cornerstones of quantum mechanics. It states that there is a fundamental limit to the precision with which complementary variables, such as position and momentum, can be known simultaneously.

Mathematically expressed as:
Δx · Δp ≥ ħ/2

Where:
- Δx is the uncertainty in position
- Δp is the uncertainty in momentum
- ħ is the reduced Planck constant

## Quantum Entanglement

Quantum entanglement is a phenomenon where two or more particles become correlated in such a way that the quantum state of each particle cannot be described independently of the others, regardless of the distance separating them.

## Interaction Between Uncertainty and Entanglement

The relationship between Heisenberg's Uncertainty Principle and quantum entanglement creates fascinating paradoxes and applications:

1. **Measurement Effects**: When entangled particles are measured, the uncertainty principle applies to both particles simultaneously.

2. **EPR Paradox**: Einstein, Podolsky, and Rosen proposed that entanglement could violate the uncertainty principle, leading to decades of theoretical and experimental work.

3. **Bell's Inequality**: John Bell showed that quantum mechanics predicts stronger correlations between entangled particles than would be possible in classical physics.

## Stabilium's Interaction with the Uncertainty Principle

Stabilium has unique properties that affect how the uncertainty principle manifests in quantum systems:

1. **Extended Coherence**: Stabilium's ability to maintain quantum coherence for extended periods allows for more precise measurements within the bounds of the uncertainty principle.

2. **Entanglement Preservation**: When quantum entanglement is established between particles in a Stabilium substrate, the entangled state persists longer than in conventional materials.

3. **Measurement Precision**: Stabilium-based quantum sensors can approach the theoretical limits imposed by the uncertainty principle more closely than previous technologies.

## Quantum Entanglement Experiments with Stabilium

Recent experiments have demonstrated several key interactions:

1. **Coherent Entanglement**: Stabilium can maintain entangled qubit pairs with fidelity above 99% for up to 200 microseconds.

2. **Uncertainty Boundary Testing**: Experiments using Stabilium have probed the exact boundaries of the uncertainty principle with unprecedented precision.

3. **Quantum Teleportation**: Stabilium-based quantum teleportation experiments have achieved record distances and fidelities.

4. **Quantum Memory**: Stabilium can store entangled states as a quantum memory, preserving the entanglement properties while subject to the constraints of the uncertainty principle.

## Theoretical Implications

The interaction between Stabilium, the uncertainty principle, and quantum entanglement has led to several theoretical advances:

1. **Refined Measurement Theory**: More precise models of quantum measurement that account for material properties.

2. **Extended Quantum Information Theory**: New frameworks for understanding how quantum information persists in coherent materials.

3. **Quantum Gravity Probes**: Proposals to use Stabilium-enhanced precision to test theories that unite quantum mechanics with general relativity.

## Practical Applications

These fundamental interactions enable several practical applications:

1. **Quantum Cryptography**: Enhanced security through more stable entangled key distribution.

2. **Quantum Sensing**: Approaching the fundamental limits of measurement precision.

3. **Quantum Computing**: More reliable quantum algorithms that depend on entanglement.

4. **Fundamental Physics Research**: New experimental approaches to test the foundations of quantum mechanics.
""",
        "tags": ["quantum physics", "uncertainty principle", "entanglement"],
        "folder": "/quantum_physics/fundamentals"
    },
    "advanced_quantum": {
        "filename": "quantum_tunneling_non_euclidean.md",
        "content": """# Quantum Tunneling Through Non-Euclidean Space-Time Manifolds

## Theoretical Framework

Quantum tunneling is a quantum mechanical phenomenon where particles penetrate energy barriers that would be insurmountable according to classical physics. When this phenomenon is extended to non-Euclidean space-time manifolds, it opens up revolutionary possibilities for quantum information transfer and energy transmission.

## Non-Euclidean Space-Time Manifolds

Non-Euclidean space-time manifolds represent geometries that deviate from flat Euclidean space. These can include:

1. **Hyperbolic Manifolds**: Negatively curved space-time regions
2. **Spherical Manifolds**: Positively curved space-time regions
3. **Topological Singularities**: Points where standard geometric properties break down
4. **Wormhole-Like Structures**: Theoretical shortcuts through space-time

## The Role of Stabilium in Quantum Tunneling

Stabilium has emerged as a critical enabling technology for controlled quantum tunneling through non-Euclidean space-time manifolds due to several unique properties:

1. **Quantum Coherence Preservation**: Stabilium maintains quantum states even when subjected to the extreme conditions near space-time curvature anomalies.

2. **Topological Protection**: The molecular structure of Stabilium provides topological protection to quantum information, preventing decoherence during tunneling events.

3. **Curvature Response**: Stabilium exhibits a unique response to space-time curvature, creating "tunneling-friendly" conditions at its boundaries.

4. **Quantum Field Stabilization**: When properly configured, Stabilium can stabilize quantum fields in regions of high space-time curvature.

## Experimental Demonstrations

Recent laboratory experiments have demonstrated several key capabilities:

1. **Micro-Scale Manifold Tunneling**: Quantum particles in Stabilium substrates have been observed tunneling through artificially created micro-scale non-Euclidean geometries with efficiency 300% higher than theoretical predictions for conventional materials.

2. **Information Preservation**: Quantum information encoded in particles tunneling through Stabilium-mediated pathways maintains fidelity above 92%, compared to near-total information loss in conventional tunneling.

3. **Multi-Particle Entangled Tunneling**: Entangled particle pairs can tunnel simultaneously while maintaining their entanglement, a phenomenon previously thought impossible.

## Theoretical Models

Several theoretical frameworks have been developed to explain Stabilium's exceptional tunneling properties:

1. **Modified Path Integral Formulation**: Extends Feynman's path integral to account for Stabilium's unique quantum properties in curved space-time.

2. **Topological Quantum Field Theory (TQFT)**: Describes how Stabilium's topological properties protect quantum information during tunneling events.

3. **Non-Commutative Geometry Models**: Provides mathematical tools to analyze tunneling in highly curved space-time regions where standard quantum mechanics breaks down.

## Applications in Advanced Quantum Technologies

The ability of Stabilium to facilitate quantum tunneling through non-Euclidean space-time manifolds enables several revolutionary applications:

1. **Quantum Teleportation Enhancement**: Improving quantum teleportation protocols by utilizing controlled tunneling pathways.

2. **Quantum Computing Acceleration**: Creating shortcuts between physically separated qubits, potentially overcoming connectivity limitations in quantum processors.

3. **Quantum Sensing in Curved Space-Time**: Developing sensors that can detect minute variations in space-time curvature with unprecedented sensitivity.

4. **Quantum Energy Transfer**: Enabling efficient energy transfer across normally prohibitive barriers, with potential applications in energy technology.

## Challenges and Limitations

Despite its promise, several challenges remain:

1. **Scalability**: Current experiments are limited to microscopic scales.

2. **Energy Requirements**: Establishing and maintaining the necessary conditions requires significant energy input.

3. **Theoretical Gaps**: Complete mathematical models unifying quantum mechanics and general relativity remain elusive.

4. **Measurement Difficulties**: Observing and measuring tunneling events without disrupting them presents significant technical challenges.

## Future Research Directions

Ongoing research focuses on:

1. **Macroscopic Tunneling**: Scaling up tunneling phenomena to larger systems.

2. **Reduced Energy Requirements**: Developing more efficient methods to create tunneling-favorable conditions.

3. **Integrated Systems**: Combining Stabilium with other quantum technologies for practical applications.

4. **Fundamental Physics**: Using Stabilium-mediated tunneling to test theories of quantum gravity and unified physics.
""",
        "tags": ["quantum tunneling", "non-Euclidean geometry", "advanced quantum physics", "stabilium applications"],
        "folder": "/quantum_physics/advanced"
    }
}

async def setup_test_environment():
    """
    Set up the test environment by creating a fresh vector store and adding test documents
    """
    logger.info("Setting up test environment...")
    
    # Create test results directory if it doesn't exist
    os.makedirs("tests/results", exist_ok=True)
    
    # Initialize vector store with a test directory
    test_dir = "tests/chroma_db/entity_preservation_test"
    
    # Remove existing test directory if it exists
    import shutil
    if os.path.exists(test_dir):
        shutil.rmtree(test_dir)
    
    # Create vector store
    vector_store = VectorStore(persist_directory=test_dir)
    
    # Add test documents
    document_ids = []
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        logger.info(f"Adding test document: {doc_info['filename']}")
        
        doc = Document(
            id=doc_id,
            filename=doc_info["filename"],
            content=doc_info["content"],
            tags=doc_info["tags"],
            folder=doc_info["folder"]
        )
        
        # Create chunks for the document
        # For simplicity, we'll create chunks of approximately 1000 characters
        content = doc_info["content"]
        chunk_size = 1000
        chunks = []
        
        for i in range(0, len(content), chunk_size):
            chunk_content = content[i:i+chunk_size]
            if len(chunk_content.strip()) > 0:
                chunk_id = f"{doc_id}_chunk_{i//chunk_size}"
                chunks.append(
                    Chunk(
                        id=chunk_id,
                        content=chunk_content,
                        metadata={
                            "index": i//chunk_size,
                            "source": doc_info["filename"],
                            "document_id": doc_id,
                            "filename": doc_info["filename"],
                            "tags": doc_info["tags"],
                            "folder": doc_info["folder"]
                        }
                    )
                )
        
        # Add chunks to document
        doc.chunks = chunks
        
        # Add document to vector store
        await vector_store.add_document(doc)
        document_ids.append(doc_id)
        
        logger.info(f"Added document {doc_id} with {len(chunks)} chunks")
    
    logger.info(f"Test environment setup complete with {len(document_ids)} documents")
    return vector_store

def extract_chunks_info_from_logs(logs):
    """
    Extract information about chunks retrieved and used from log messages
    """
    chunks_retrieved = 0
    chunks_used = 0
    refined_query = ""
    
    # Regular expressions to match log messages
    retrieved_pattern = r"Retrieved (\d+) chunks from vector store"
    used_pattern = r"Using (\d+) chunks after (optimization|Retrieval Judge optimization)"
    refined_pattern = r"Refined query: (.+)"
    
    # Extract information from logs
    for log in logs:
        message = log.getMessage()
        
        # Check for chunks retrieved
        retrieved_match = re.search(retrieved_pattern, message)
        if retrieved_match:
            chunks_retrieved = int(retrieved_match.group(1))
            
        # Check for chunks used
        used_match = re.search(used_pattern, message)
        if used_match:
            chunks_used = int(used_match.group(1))
            
        # Check for refined query
        refined_match = re.search(refined_pattern, message)
        if refined_match:
            refined_query = refined_match.group(1)
    
    return {
        "chunks_retrieved": chunks_retrieved,
        "chunks_used": chunks_used,
        "refined_query": refined_query
    }

async def test_query(rag_engine, query, query_id):
    """
    Test a query and analyze the results
    """
    logger.info(f"Testing query {query_id}: {query}")
    
    # Create a results directory for this query
    query_results_dir = f"tests/results/query_{query_id}"
    os.makedirs(query_results_dir, exist_ok=True)
    
    # Clear previous logs
    log_capture_handler.clear()
    
    start_time = time.time()
    
    # Execute query with RAG
    response = await rag_engine.query(
        query=query,
        use_rag=True,
        top_k=10,
        stream=False
    )
    
    elapsed_time = time.time() - start_time
    
    # Log the response
    logger.info(f"Query {query_id} completed in {elapsed_time:.2f}s")
    
    # Extract chunks information from logs
    logs = log_capture_handler.get_logs()
    chunks_info = extract_chunks_info_from_logs(logs)
    
    # Extract and analyze results
    results = {
        "query_id": query_id,
        "query": query,
        "execution_time": elapsed_time,
        "answer": response.get("answer", ""),
        "sources": [],
        "chunks_retrieved": chunks_info["chunks_retrieved"],
        "chunks_used": chunks_info["chunks_used"],
        "refined_query": chunks_info["refined_query"] or response.get("metadata", {}).get("refined_query", "")
    }
    
    # Extract sources
    if "sources" in response and response["sources"]:
        for source in response["sources"]:
            source_info = {
                "document_id": source.document_id if hasattr(source, "document_id") else "",
                "chunk_id": source.chunk_id if hasattr(source, "chunk_id") else "",
                "relevance_score": source.relevance_score if hasattr(source, "relevance_score") else 0.0,
                "filename": source.filename if hasattr(source, "filename") else "",
                "excerpt": source.excerpt[:100] + "..." if hasattr(source, "excerpt") and len(source.excerpt) > 100 else source.excerpt if hasattr(source, "excerpt") else ""
            }
            results["sources"].append(source_info)
    
    # Analyze entity preservation
    entity_analysis = analyze_entity_preservation(query, results.get("refined_query", ""))
    results["entity_analysis"] = entity_analysis
    
    # Save results to file
    with open(f"{query_results_dir}/results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    # Generate a human-readable report
    generate_query_report(results, query_results_dir)
    
    return results

def analyze_entity_preservation(original_query, refined_query):
    """
    Analyze if entities in the original query are preserved in the refined query
    """
    if not refined_query:
        return {
            "entities_found": [],
            "entities_preserved": True,
            "analysis": "No query refinement was performed"
        }
    
    # Extract potential entities (capitalized words)
    import re
    entity_pattern = r'\b[A-Z][a-zA-Z0-9_-]*(?:\s+[A-Z][a-zA-Z0-9_-]*)*\b'
    entities = re.findall(entity_pattern, original_query)
    
    # Filter out common words that might be capitalized
    common_words = ["I", "A", "The", "What", "How", "When", "Where", "Why", "Is", "Are", "Tell", "Explain"]
    entities = [e for e in entities if e not in common_words]
    
    # Check if entities are preserved
    preserved_entities = []
    missing_entities = []
    
    for entity in entities:
        if entity in refined_query:
            preserved_entities.append(entity)
        else:
            # Check for partial matches (e.g., "Stabilium QRM-12X" might be split)
            entity_parts = entity.split()
            all_parts_found = all(part in refined_query for part in entity_parts)
            if all_parts_found:
                preserved_entities.append(entity)
            else:
                missing_entities.append(entity)
    
    return {
        "entities_found": entities,
        "entities_preserved": len(missing_entities) == 0,
        "preserved_entities": preserved_entities,
        "missing_entities": missing_entities,
        "analysis": f"Found {len(entities)} entities, {len(preserved_entities)} preserved, {len(missing_entities)} missing"
    }

def generate_query_report(results, output_dir):
    """
    Generate a human-readable report for a query test
    """
    report = f"""# Query Test Report: {results['query_id']}

## Query Information
- **Original Query**: {results['query']}
- **Refined Query**: {results.get('refined_query', 'No refinement performed')}
- **Execution Time**: {results['execution_time']:.2f} seconds

## Entity Preservation Analysis
- **Entities Found**: {', '.join(results['entity_analysis']['entities_found']) if results['entity_analysis']['entities_found'] else 'None detected'}
- **Entities Preserved**: {'Yes' if results['entity_analysis']['entities_preserved'] else 'No'}
"""

    if not results['entity_analysis']['entities_preserved']:
        report += f"- **Missing Entities**: {', '.join(results['entity_analysis']['missing_entities'])}\n"

    report += f"""
## Retrieval Statistics
- **Chunks Retrieved**: {results['chunks_retrieved']}
- **Chunks Used in Context**: {results['chunks_used']}

## Sources Used
"""

    for i, source in enumerate(results['sources']):
        report += f"""
### Source {i+1}
- **Document**: {source['filename']}
- **Relevance Score**: {source['relevance_score']:.2f}
- **Excerpt**: {source['excerpt']}
"""

    report += f"""
## Generated Answer
```
{results['answer']}
```
"""

    # Write report to file
    with open(f"{output_dir}/report.md", "w") as f:
        f.write(report)

async def run_test_suite():
    """
    Run the complete test suite
    """
    logger.info("Starting Metis RAG test suite...")
    
    # Set up test environment
    vector_store = await setup_test_environment()
    
    # Initialize RAG engine
    ollama_client = OllamaClient()
    retrieval_judge = RetrievalJudge(ollama_client=ollama_client)
    rag_engine = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    # Define test queries
    test_queries = [
        {
            "id": "basic_entity",
            "query": "Tell me about Stabilium and its applications in quantum computing."
        },
        {
            "id": "multi_entity",
            "query": "Compare the properties of Stabilium and Quantum Resonance Modulation in cold fusion experiments."
        },
        {
            "id": "potential_ambiguity",
            "query": "What are the differences between Stabilium QRM-12X and earlier versions?"
        },
        {
            "id": "context_synthesis",
            "query": "How does Stabilium interact with Heisenberg's Uncertainty Principle when used in quantum entanglement experiments?"
        },
        {
            "id": "specialized_terminology",
            "query": "Explain the role of Stabilium in facilitating quantum tunneling through non-Euclidean space-time manifolds."
        }
    ]
    
    # Run tests
    all_results = []
    for test in test_queries:
        result = await test_query(rag_engine, test["query"], test["id"])
        all_results.append(result)
        # Add a small delay between queries
        await asyncio.sleep(1)
    
    # Generate summary report
    generate_summary_report(all_results)
    
    logger.info("Test suite completed!")

def generate_summary_report(all_results):
    """
    Generate a summary report for all tests
    """
    summary = """# Metis RAG Test Suite Summary Report

## Overview
This report summarizes the results of testing the Metis RAG system with a series of increasingly complex queries
to verify the fixes implemented for entity preservation, minimum context requirements, and citation handling.

## Test Results Summary

| Query ID | Entity Preservation | Chunks Retrieved | Chunks Used | Execution Time (s) |
|----------|---------------------|------------------|-------------|-------------------|
"""

    for result in all_results:
        entity_status = "✅" if result["entity_analysis"]["entities_preserved"] else "❌"
        summary += f"| {result['query_id']} | {entity_status} | {result['chunks_retrieved']} | {result['chunks_used']} | {result['execution_time']:.2f} |\n"

    summary += """
## Detailed Analysis

### Entity Preservation

"""

    for result in all_results:
        summary += f"**Query {result['query_id']}**: "
        if result["entity_analysis"]["entities_preserved"]:
            summary += f"All entities preserved ({', '.join(result['entity_analysis']['entities_found'])})\n\n"
        else:
            summary += f"Some entities not preserved. Missing: {', '.join(result['entity_analysis']['missing_entities'])}\n\n"

    summary += """
### Context Selection

"""

    for result in all_results:
        summary += f"**Query {result['query_id']}**: Retrieved {result['chunks_retrieved']} chunks, used {result['chunks_used']} in final context\n\n"

    summary += """
### Source Relevance

"""

    for result in all_results:
        summary += f"**Query {result['query_id']}**: Used {len(result['sources'])} sources with relevance scores: "
        if result['sources']:
            scores = [f"{source['relevance_score']:.2f}" for source in result['sources']]
            summary += f"{', '.join(scores)}\n\n"
        else:
            summary += "No sources used\n\n"

    summary += """
## Conclusion

This test suite has verified the following aspects of the Metis RAG system:

1. **Entity Preservation**: The system's ability to preserve named entities during query refinement
2. **Minimum Context Requirements**: The system's ability to retrieve and use sufficient context
3. **Citation Handling**: The system's ability to track and cite sources properly

See individual test reports for detailed analysis of each query.
"""

    # Write summary to file
    with open("tests/results/summary_report.md", "w") as f:
        f.write(summary)

async def main():
    """Main function to run all tests"""
    logger.info("Starting entity preservation and RAG quality tests...")
    
    try:
        await run_test_suite()
    except Exception as e:
        logger.error(f"Error running test suite: {str(e)}", exc_info=True)
    
    logger.info("All tests completed!")

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_rag_quality.py
================
#!/usr/bin/env python3
"""
Test suite for evaluating the quality of RAG responses in the Metis RAG system.
This test suite focuses on factual accuracy, relevance, and citation quality.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

import pytest
from fastapi.testclient import TestClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_rag_quality")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document, Chunk
from app.models.chat import ChatQuery
from app.main import app

# Test client
client = TestClient(app)

# Test document content with known facts
TEST_DOCUMENTS = {
    "technical_doc": {
        "filename": "technical_documentation.md",
        "content": """# Metis RAG Technical Documentation

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
""",
        "tags": ["technical", "documentation", "architecture"],
        "folder": "/test"
    },
    "business_report": {
        "filename": "quarterly_report.txt",
        "content": """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
""",
        "tags": ["business", "report", "quarterly"],
        "folder": "/test"
    },
    "product_specs": {
        "filename": "product_specifications.csv",
        "content": """Product ID,Name,Category,Price,Features,Release Date
P001,MetisRAG Enterprise,Software,$4999,Advanced RAG capabilities|Multi-document retrieval|Enterprise security,2025-01-15
P002,MetisRAG Professional,Software,$1999,Standard RAG capabilities|Document management|API access,2025-01-15
P003,MetisRAG Basic,Software,$499,Basic RAG capabilities|Limited document storage|Web interface,2025-01-15
P004,MetisRAG API,Service,$0.10 per query,Pay-per-use|REST API|Developer documentation,2025-02-01
P005,MetisRAG Mobile,Mobile App,$9.99 per month,iOS and Android|Offline capabilities|Voice interface,2025-03-15
""",
        "tags": ["product", "specifications", "pricing"],
        "folder": "/test"
    }
}

# Test queries with expected facts to be present in responses
TEST_QUERIES = [
    {
        "query": "What is the architecture of Metis RAG?",
        "expected_facts": [
            "modular architecture",
            "Frontend Layer",
            "API Layer",
            "RAG Engine",
            "HTML, CSS, and JavaScript",
            "FastAPI"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What was the revenue reported in Q1 2025?",
        "expected_facts": [
            "$4.2M",
            "15% increase",
            "year-over-year",
            "net profit of $1.4M"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What are the components of the RAG engine?",
        "expected_facts": [
            "Document Processing",
            "Vector Store",
            "LLM Integration",
            "chunking",
            "embeddings",
            "Ollama"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What are the strategic initiatives for Q2?",
        "expected_facts": [
            "International Expansion",
            "European markets",
            "Enterprise Solution",
            "Strategic Partnerships",
            "Operational Efficiency"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What products are available and at what price points?",
        "expected_facts": [
            "MetisRAG Enterprise",
            "$4999",
            "MetisRAG Professional",
            "$1999",
            "MetisRAG Basic",
            "$499"
        ],
        "document_ids": ["product_specs"]
    },
    {
        "query": "What was the customer retention rate and NPS score?",
        "expected_facts": [
            "94%",
            "Net Promoter Score",
            "improved from 42 to 48"
        ],
        "document_ids": ["business_report"]
    }
]

# Test for multi-document queries
MULTI_DOC_QUERIES = [
    {
        "query": "Compare the MetisRAG Enterprise product with the RAG Engine architecture",
        "expected_facts": [
            "MetisRAG Enterprise",
            "$4999",
            "Advanced RAG capabilities",
            "RAG Engine",
            "Document Processing",
            "Vector Store"
        ],
        "document_ids": ["technical_doc", "product_specs"]
    },
    {
        "query": "What is the relationship between the Q1 financial performance and the product offerings?",
        "expected_facts": [
            "$4.2M in revenue",
            "MetisRAG Enterprise",
            "MetisRAG Professional",
            "MetisRAG Basic"
        ],
        "document_ids": ["business_report", "product_specs"]
    }
]

@pytest.fixture
def test_documents_dir():
    """Create a directory for test documents"""
    test_dir = "test_quality_docs"
    os.makedirs(test_dir, exist_ok=True)
    return test_dir

@pytest.fixture
def create_test_documents(test_documents_dir):
    """Create test documents with known facts"""
    document_paths = {}
    
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        file_path = os.path.join(test_documents_dir, doc_info["filename"])
        with open(file_path, "w") as f:
            f.write(doc_info["content"])
        document_paths[doc_id] = file_path
    
    return document_paths

@pytest.fixture
async def setup_vector_store(create_test_documents):
    """Set up vector store with test documents"""
    # Use a separate directory for test ChromaDB
    test_chroma_dir = "test_quality_chroma"
    os.makedirs(test_chroma_dir, exist_ok=True)
    
    # Initialize vector store
    vector_store = VectorStore(persist_directory=test_chroma_dir)
    
    # Create Document objects
    documents = {}
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        doc = Document(
            id=doc_id,
            filename=doc_info["filename"],
            content=doc_info["content"],
            tags=doc_info["tags"],
            folder=doc_info["folder"]
        )
        
        # Create a single chunk for simplicity in testing
        doc.chunks = [
            Chunk(
                id=f"{doc_id}_chunk_0",
                content=doc_info["content"],
                metadata={
                    "index": 0,
                    "source": doc_info["filename"]
                }
            )
        ]
        
        # Add document to vector store
        await vector_store.add_document(doc)
        documents[doc_id] = doc
    
    return vector_store, documents

@pytest.fixture
async def setup_rag_engine(setup_vector_store):
    """Set up RAG engine with test vector store"""
    vector_store, documents = await setup_vector_store
    rag_engine = RAGEngine(vector_store=vector_store)
    return rag_engine, documents

@pytest.mark.asyncio
async def test_factual_accuracy(setup_rag_engine):
    """Test factual accuracy of RAG responses"""
    rag_engine, documents = await setup_rag_engine
    
    results = []
    for test_case in TEST_QUERIES:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Check if response contains expected facts
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        # Count how many expected facts are present in the answer
        fact_count = sum(1 for fact in expected_facts if fact.lower() in answer.lower())
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        # Check if sources are from the expected documents
        expected_doc_ids = test_case["document_ids"]
        source_doc_ids = [source.document_id for source in sources]
        correct_sources = all(doc_id in source_doc_ids for doc_id in expected_doc_ids)
        
        # Log results
        logger.info(f"Query: {query}")
        logger.info(f"Answer: {answer[:100]}...")
        logger.info(f"Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
        logger.info(f"Correct sources: {correct_sources}")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "correct_sources": correct_sources,
            "sources": [
                {
                    "document_id": s.document_id,
                    "relevance_score": s.relevance_score
                }
                for s in sources
            ] if sources else []
        })
    
    # Save results to file
    results_path = "test_quality_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    # Assert minimum factual accuracy
    for i, result in enumerate(results):
        assert result["fact_percentage"] >= 70, f"Query {i+1} has low factual accuracy: {result['fact_percentage']:.1f}%"
        assert result["correct_sources"], f"Query {i+1} has incorrect sources"

@pytest.mark.asyncio
async def test_multi_document_retrieval(setup_rag_engine):
    """Test retrieval across multiple documents"""
    rag_engine, documents = await setup_rag_engine
    
    results = []
    for test_case in MULTI_DOC_QUERIES:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=5,  # Increase top_k for multi-document queries
            stream=False
        )
        
        # Check if response contains expected facts
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        # Count how many expected facts are present in the answer
        fact_count = sum(1 for fact in expected_facts if fact.lower() in answer.lower())
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        # Check if sources are from multiple documents
        source_doc_ids = set(source.document_id for source in sources)
        multi_doc_retrieval = len(source_doc_ids) > 1
        
        # Check if sources are from the expected documents
        expected_doc_ids = test_case["document_ids"]
        correct_sources = all(doc_id in source_doc_ids for doc_id in expected_doc_ids)
        
        # Log results
        logger.info(f"Multi-doc Query: {query}")
        logger.info(f"Answer: {answer[:100]}...")
        logger.info(f"Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
        logger.info(f"Multi-document retrieval: {multi_doc_retrieval}")
        logger.info(f"Correct sources: {correct_sources}")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "multi_doc_retrieval": multi_doc_retrieval,
            "correct_sources": correct_sources,
            "source_doc_ids": list(source_doc_ids)
        })
    
    # Save results to file
    results_path = "test_multi_doc_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    # Assert multi-document retrieval
    for i, result in enumerate(results):
        assert result["multi_doc_retrieval"], f"Query {i+1} failed to retrieve from multiple documents"
        assert result["fact_percentage"] >= 60, f"Query {i+1} has low factual accuracy: {result['fact_percentage']:.1f}%"

@pytest.mark.asyncio
async def test_citation_quality(setup_rag_engine):
    """Test quality of citations in RAG responses"""
    rag_engine, documents = await setup_rag_engine
    
    results = []
    for test_case in TEST_QUERIES:
        query = test_case["query"]
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Check citations in the answer
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        # Check if answer contains citation markers
        has_citation_markers = "[" in answer and "]" in answer
        
        # Check if citations in the answer correspond to sources
        citation_count = 0
        for i in range(1, 10):  # Check for citation markers [1] through [9]
            if f"[{i}]" in answer:
                citation_count += 1
        
        # Check if number of citations is reasonable
        reasonable_citation_count = 0 < citation_count <= len(sources) + 1  # Allow for one extra citation
        
        # Log results
        logger.info(f"Citation Query: {query}")
        logger.info(f"Has citation markers: {has_citation_markers}")
        logger.info(f"Citation count: {citation_count}")
        logger.info(f"Source count: {len(sources)}")
        
        # Store results
        results.append({
            "query": query,
            "has_citation_markers": has_citation_markers,
            "citation_count": citation_count,
            "source_count": len(sources),
            "reasonable_citation_count": reasonable_citation_count
        })
    
    # Save results to file
    results_path = "test_citation_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    # Assert citation quality
    for i, result in enumerate(results):
        assert result["has_citation_markers"], f"Query {i+1} response lacks citation markers"
        assert result["reasonable_citation_count"], f"Query {i+1} has unreasonable citation count"

@pytest.mark.asyncio
async def test_api_integration(create_test_documents):
    """Test integration with API endpoints"""
    # Test document upload and processing through API
    results = []
    
    for doc_id, doc_path in create_test_documents.items():
        doc_info = TEST_DOCUMENTS[doc_id]
        
        # Open the file for upload
        with open(doc_path, "rb") as f:
            # Upload the document
            upload_response = client.post(
                "/api/documents/upload",
                files={"file": (doc_info["filename"], f, "text/plain")}
            )
            
            # Check upload response
            assert upload_response.status_code == 200
            upload_data = upload_response.json()
            assert upload_data["success"] is True
            assert "document_id" in upload_data
            
            document_id = upload_data["document_id"]
            
            # Process the document
            process_response = client.post(
                "/api/documents/process",
                json={"document_ids": [document_id]}
            )
            
            # Check process response
            assert process_response.status_code == 200
            process_data = process_response.json()
            assert process_data["success"] is True
            
            # Test query with the uploaded document
            query = next((q["query"] for q in TEST_QUERIES if doc_id in q["document_ids"]), "What is this document about?")
            
            query_response = client.post(
                "/api/chat/query",
                json={
                    "message": query,
                    "use_rag": True,
                    "stream": False
                }
            )
            
            # Check query response
            assert query_response.status_code == 200
            query_data = query_response.json()
            assert "message" in query_data
            assert "conversation_id" in query_data
            
            # Store results
            results.append({
                "document_id": document_id,
                "filename": doc_info["filename"],
                "query": query,
                "response": query_data["message"]
            })
            
            # Clean up - delete the document
            delete_response = client.delete(f"/api/documents/{document_id}")
            assert delete_response.status_code == 200
            delete_data = delete_response.json()
            assert delete_data["success"] is True
    
    # Save results to file
    results_path = "test_api_integration_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_rag_retrieval.py
================
#!/usr/bin/env python3
"""
Test script for Metis RAG retrieval functionality.
This script creates test documents, uploads and processes them,
and then tests the RAG retrieval with specific queries.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_rag_retrieval")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.models.document import Document, Chunk

# Test document content
MARKDOWN_CONTENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

PDF_CONTENT = """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
"""

async def create_test_documents():
    """Create test documents for RAG testing"""
    logger.info("Creating test documents...")
    
    # Create directories for test documents
    os.makedirs("test_docs", exist_ok=True)
    
    # Create Markdown document
    markdown_path = os.path.join("test_docs", "technical_documentation.md")
    with open(markdown_path, "w") as f:
        f.write(MARKDOWN_CONTENT)
    
    # Create PDF-like text document (since we can't easily create a real PDF programmatically)
    pdf_path = os.path.join("test_docs", "quarterly_report.txt")
    with open(pdf_path, "w") as f:
        f.write(PDF_CONTENT)
    
    logger.info(f"Created test documents in {os.path.abspath('test_docs')}")
    return markdown_path, pdf_path

async def process_documents(vector_store, markdown_path, pdf_path):
    """Process the test documents and add them to the vector store"""
    logger.info("Processing test documents...")
    
    # Read file contents first
    with open(markdown_path, "r") as f:
        markdown_content = f.read()
    
    with open(pdf_path, "r") as f:
        pdf_content = f.read()
    
    # Create Document objects
    markdown_doc = Document(
        id=str(uuid.uuid4()),
        filename="technical_documentation.md",
        content=markdown_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    pdf_doc = Document(
        id=str(uuid.uuid4()),
        filename="quarterly_report.txt",
        content=pdf_content,
        tags=["business", "report", "quarterly"],
        folder="/test"
    )
    
    # Fix for ChromaDB metadata issue - convert tags to string
    # We'll modify the add_document method in vector_store.py to handle this properly
    class CustomVectorStore(VectorStore):
        async def add_document(self, document: Document) -> None:
            """Override to fix tags handling"""
            try:
                logger.info(f"Adding document {document.id} to vector store")
                
                # Make sure we have an Ollama client
                if self.ollama_client is None:
                    self.ollama_client = OllamaClient()
                
                # Prepare chunks for batch processing
                chunks_to_embed = [chunk for chunk in document.chunks if not chunk.embedding]
                chunk_contents = [chunk.content for chunk in chunks_to_embed]
                
                # Create embeddings in batch if possible
                if chunk_contents:
                    try:
                        # Batch embedding
                        embeddings = await self._batch_create_embeddings(chunk_contents)
                        
                        # Assign embeddings to chunks
                        for i, chunk in enumerate(chunks_to_embed):
                            chunk.embedding = embeddings[i]
                    except Exception as batch_error:
                        logger.warning(f"Batch embedding failed: {str(batch_error)}. Falling back to sequential embedding.")
                        # Fall back to sequential embedding
                        for chunk in chunks_to_embed:
                            chunk.embedding = await self.ollama_client.create_embedding(
                                text=chunk.content,
                                model=self.embedding_model
                            )
                
                # Add chunks to the collection
                for chunk in document.chunks:
                    if not chunk.embedding:
                        logger.warning(f"Chunk {chunk.id} has no embedding, skipping")
                        continue
                        
                    # Convert tags to string to avoid ChromaDB error
                    tags_str = ",".join(document.tags) if document.tags else ""
                    
                    self.collection.add(
                        ids=[chunk.id],
                        embeddings=[chunk.embedding],
                        documents=[chunk.content],
                        metadatas=[{
                            "document_id": document.id,
                            "chunk_index": chunk.metadata.get("index", 0),
                            "filename": document.filename,
                            "tags_str": tags_str,  # Use string instead of list
                            "folder": document.folder,
                            **chunk.metadata
                        }]
                    )
                
                logger.info(f"Added {len(document.chunks)} chunks to vector store for document {document.id}")
            except Exception as e:
                logger.error(f"Error adding document {document.id} to vector store: {str(e)}")
                raise
    
    # Use our custom vector store
    vector_store = CustomVectorStore()
    
    # Create chunks for Markdown document
    markdown_chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=markdown_content,
            metadata={
                "index": 0,
                "source": markdown_path
            }
        )
    ]
    
    # Create chunks for PDF document
    pdf_chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=pdf_content,
            metadata={
                "index": 0,
                "source": pdf_path
            }
        )
    ]
    
    # Assign chunks to documents
    markdown_doc.chunks = markdown_chunks
    pdf_doc.chunks = pdf_chunks
    
    # Add documents to vector store
    await vector_store.add_document(markdown_doc)
    await vector_store.add_document(pdf_doc)
    
    logger.info(f"Added documents to vector store: {markdown_doc.id}, {pdf_doc.id}")
    return markdown_doc, pdf_doc, vector_store

async def test_queries(rag_engine):
    """Test RAG retrieval with specific queries"""
    logger.info("Testing RAG retrieval with specific queries...")
    
    test_queries = [
        "What is the architecture of Metis RAG?",
        "What was the revenue reported in Q1 2025?",
        "What are the components of the RAG engine?",
        "What are the strategic initiatives for Q2?",
        "How does the vector store work?",
        "What was the customer retention rate?",
    ]
    
    results = []
    for query in test_queries:
        logger.info(f"Testing query: {query}")
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Log results
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        logger.info(f"Query: {query}")
        logger.info(f"Answer: {answer[:100]}...")
        logger.info(f"Sources: {len(sources)} chunks retrieved")
        
        # Store results for analysis
        results.append({
            "query": query,
            "answer": answer,
            "sources": [
                {
                    "document_id": s.document_id,
                    "chunk_id": s.chunk_id,
                    "relevance_score": s.relevance_score,
                    "excerpt": s.excerpt[:100] + "..." if len(s.excerpt) > 100 else s.excerpt
                }
                for s in sources
            ] if sources else []
        })
    
    # Save results to file
    results_path = os.path.join("test_docs", "rag_test_results.json")
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Test results saved to {os.path.abspath(results_path)}")
    return results

async def analyze_results(results):
    """Analyze the test results"""
    logger.info("Analyzing test results...")
    
    # Check if each query has sources
    queries_with_sources = sum(1 for r in results if r["sources"])
    logger.info(f"{queries_with_sources}/{len(results)} queries returned sources")
    
    # Check if answers contain source references
    answers_with_references = sum(1 for r in results if "[" in r["answer"] and "]" in r["answer"])
    logger.info(f"{answers_with_references}/{len(results)} answers contain source references")
    
    # Check for specific content in answers
    architecture_query = next((r for r in results if "architecture" in r["query"].lower()), None)
    revenue_query = next((r for r in results if "revenue" in r["query"].lower()), None)
    
    if architecture_query:
        has_architecture_info = any(keyword in architecture_query["answer"].lower() 
                                   for keyword in ["frontend", "api", "rag engine", "modular"])
        logger.info(f"Architecture query contains relevant information: {has_architecture_info}")
    
    if revenue_query:
        has_revenue_info = any(keyword in revenue_query["answer"].lower() 
                              for keyword in ["4.2", "million", "15%", "increase"])
        logger.info(f"Revenue query contains relevant information: {has_revenue_info}")
    
    # Overall assessment
    success_rate = (queries_with_sources / len(results)) * 100
    logger.info(f"Overall RAG retrieval success rate: {success_rate:.1f}%")
    
    return {
        "queries_with_sources": queries_with_sources,
        "total_queries": len(results),
        "answers_with_references": answers_with_references,
        "success_rate": success_rate
    }

async def main():
    """Main test function"""
    logger.info("Starting RAG retrieval test...")
    
    try:
        # Create test documents
        markdown_path, pdf_path = await create_test_documents()
        
        # Process documents - this now creates and returns our custom vector store
        markdown_doc, pdf_doc, vector_store = await process_documents(None, markdown_path, pdf_path)
        
        # Initialize RAG engine with our custom vector store
        rag_engine = RAGEngine(vector_store=vector_store)
        
        # Test queries
        results = await test_queries(rag_engine)
        
        # Analyze results
        analysis = await analyze_results(results)
        
        logger.info("RAG retrieval test completed successfully")
        logger.info(f"Success rate: {analysis['success_rate']:.1f}%")
        
    except Exception as e:
        logger.error(f"Error during RAG retrieval test: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_simplified_document_processing_with_db.py
================
"""
Simplified document processing test with database integration.

This test verifies that the document processor works correctly with the database,
using the adapter functions to convert between Pydantic and SQLAlchemy models.
"""
import os
import time
import asyncio
import pytest
import tempfile
import shutil
import uuid
from typing import List, Dict, Any
from pathlib import Path
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.rag.document_processor import DocumentProcessor
from app.models.document import Document
from app.db.repositories.document_repository import DocumentRepository
from app.db.models import Base, Folder
from app.db.session import SessionLocal, engine  # Updated import
from app.core.config import UPLOAD_DIR

# Test configuration
TEST_FILE_SIZES = [
    ("small", 5),      # 5 KB
]

TEST_CHUNKING_STRATEGIES = [
    "recursive",
]

# Number of times to run each test for averaging
NUM_RUNS = 1

@pytest.fixture
def temp_upload_dir():
    """Create a temporary directory for uploads"""
    temp_dir = tempfile.mkdtemp()
    original_upload_dir = UPLOAD_DIR
    
    # Temporarily override the upload directory
    import app.core.config
    app.core.config.UPLOAD_DIR = temp_dir
    
    yield temp_dir
    
    # Restore the original upload directory and clean up
    app.core.config.UPLOAD_DIR = original_upload_dir
    shutil.rmtree(temp_dir)

@pytest.fixture
def in_memory_db():
    """Create an in-memory SQLite database for testing"""
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    
    # Create root folder
    root_folder = Folder(path="/", name="Root", parent_path=None)
    session.add(root_folder)
    
    # Create test folder
    test_folder = Folder(path="/test/", name="Test", parent_path="/")
    session.add(test_folder)
    
    session.commit()
    
    yield session
    
    session.close()

@pytest.fixture
def document_repository(in_memory_db):
    """Create a document repository for testing"""
    return DocumentRepository(in_memory_db)

@pytest.fixture
def document_processor():
    """Create a document processor for testing"""
    return DocumentProcessor()

def generate_test_file(size_kb: int, file_type: str = "txt"):
    """Generate a test file of the specified size"""
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix=f".{file_type}")
    
    try:
        # Generate content
        content = ""
        if file_type == "txt":
            # Generate paragraphs of text
            paragraph = "This is a test paragraph for document processing performance testing. " * 10
            paragraphs_needed = (size_kb * 1024) // len(paragraph)
            content = "\n\n".join([paragraph for _ in range(paragraphs_needed)])
        elif file_type == "md":
            # Generate markdown content
            paragraph = "This is a test paragraph for document processing performance testing. " * 10
            paragraphs_needed = (size_kb * 1024) // (len(paragraph) + 20)  # Account for markdown syntax
            
            content = "# Test Document\n\n"
            for i in range(paragraphs_needed):
                content += f"## Section {i+1}\n\n{paragraph}\n\n"
        
        # Write content to file
        with os.fdopen(fd, 'w') as f:
            f.write(content)
        
        return path
    except Exception as e:
        os.close(fd)
        os.unlink(path)
        raise e

async def process_document(document_processor, document, chunking_strategy):
    """Process a document and measure performance"""
    start_time = time.time()
    
    # Configure processor with the specified chunking strategy
    processor = DocumentProcessor(
        chunking_strategy=chunking_strategy,
        chunk_size=1500,
        chunk_overlap=150
    )
    
    # Process document
    processed_document = await processor.process_document(document)
    
    elapsed_time = time.time() - start_time
    
    # Convert UUID to string if needed
    document_id_str = str(document.id) if document.id else None
    
    # Create document directory path
    document_dir = os.path.join(UPLOAD_DIR, document_id_str)
    document_file_path = os.path.join(document_dir, document.filename)
    
    return {
        "document_id": document_id_str,
        "filename": document.filename,
        "chunking_strategy": chunking_strategy,
        "elapsed_time": elapsed_time,
        "chunk_count": len(processed_document.chunks),
        "file_size": os.path.getsize(document_file_path)
    }

@pytest.mark.asyncio
async def test_document_processing_with_db(temp_upload_dir, document_repository, document_processor):
    """Test document processing with database integration"""
    print(f"\nRunning document processing test with {len(TEST_FILE_SIZES)} file sizes, {len(TEST_CHUNKING_STRATEGIES)} chunking strategies")
    
    # Create test directory if it doesn't exist
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    
    results = []
    
    for size_name, size_kb in TEST_FILE_SIZES:
        for file_type in ["txt"]:
            # Generate test file
            file_path = generate_test_file(size_kb, file_type)
            file_name = f"test_{size_name}.{file_type}"
            
            try:
                # Create document in the database
                document = document_repository.create_document(
                    filename=file_name,
                    content="",
                    metadata={"file_type": file_type, "test_size": size_name},
                    folder="/test"
                )
                
                # Get document ID as string
                document_id_str = str(document.id)
                
                # Create document directory
                document_dir = os.path.join(UPLOAD_DIR, document_id_str)
                os.makedirs(document_dir, exist_ok=True)
                
                # Copy test file to document directory
                document_file_path = os.path.join(document_dir, file_name)
                with open(file_path, 'r') as src, open(document_file_path, 'w') as dst:
                    dst.write(src.read())
                
                # Test each chunking strategy
                for strategy in TEST_CHUNKING_STRATEGIES:
                    strategy_results = []
                    
                    for _ in range(NUM_RUNS):
                        try:
                            # Get fresh document for each run
                            fresh_document = document_repository.get_document_with_chunks(document.id)
                            
                            # Process document
                            result = await process_document(document_processor, fresh_document, strategy)
                            strategy_results.append(result)
                            
                            # Save document with chunks
                            document_repository.save_document_with_chunks(fresh_document)
                            
                            # Verify that the document was saved correctly
                            saved_document = document_repository.get_document_with_chunks(document.id)
                            assert saved_document is not None
                            assert len(saved_document.chunks) > 0
                            
                        except Exception as e:
                            print(f"Error processing document {file_name}: {e}")
                            raise
                    
                    # Calculate average metrics
                    avg_time = sum([r["elapsed_time"] for r in strategy_results]) / len(strategy_results)
                    avg_chunk_count = sum([r["chunk_count"] for r in strategy_results]) / len(strategy_results)
                    
                    results.append({
                        "document_id": document_id_str,
                        "filename": file_name,
                        "file_type": file_type,
                        "size_name": size_name,
                        "size_kb": size_kb,
                        "chunking_strategy": strategy,
                        "avg_elapsed_time": avg_time,
                        "avg_chunk_count": avg_chunk_count,
                        "num_runs": NUM_RUNS
                    })
            finally:
                # Clean up test file
                try:
                    os.unlink(file_path)
                except:
                    pass
    
    # Print results
    print("\nDocument Processing Performance Results:")
    print(f"{'File':<20} | {'Size':<10} | {'Strategy':<15} | {'Avg Time (s)':<12} | {'Avg Chunks':<10}")
    print("-" * 80)
    
    for result in results:
        print(f"{result['filename']:<20} | {result['size_kb']} KB{' ':<5} | {result['chunking_strategy']:<15} | {result['avg_elapsed_time']:<12.2f} | {result['avg_chunk_count']:<10.0f}")
    
    # Calculate overall metrics by strategy
    strategies = {}
    for strategy in TEST_CHUNKING_STRATEGIES:
        strategy_results = [r for r in results if r["chunking_strategy"] == strategy]
        if strategy_results:
            avg_time = sum([r["avg_elapsed_time"] for r in strategy_results]) / len(strategy_results)
            strategies[strategy] = avg_time
    
    print("\nAverage Processing Time by Strategy:")
    for strategy, avg_time in strategies.items():
        print(f"{strategy:<15}: {avg_time:.2f}s")
    
    # Calculate overall metrics by file size
    sizes = {}
    for size_name, size_kb in TEST_FILE_SIZES:
        size_results = [r for r in results if r["size_name"] == size_name]
        if size_results:
            avg_time = sum([r["avg_elapsed_time"] for r in size_results]) / len(size_results)
            sizes[size_name] = avg_time
    
    print("\nAverage Processing Time by File Size:")
    for size_name, avg_time in sizes.items():
        print(f"{size_name:<10}: {avg_time:.2f}s")

if __name__ == "__main__":
    # Run the test directly if this file is executed
    import asyncio
    session = SessionLocal()
    try:
        asyncio.run(test_document_processing_with_db(
            tempfile.mkdtemp(),
            DocumentRepository(session),
            DocumentProcessor()
        ))
    finally:
        session.close()

================
File: tests/test_simplified_document_processing.py
================
import os
import time
import asyncio
import pytest
import statistics
import tempfile
import uuid
from typing import List, Dict, Any
from pathlib import Path

# Test configuration
TEST_FILE_SIZES = [
    ("small", 5),      # 5 KB
    ("medium", 50),    # 50 KB
    ("large", 500)     # 500 KB
]

TEST_CHUNKING_STRATEGIES = [
    "recursive",
    "token",
    "markdown"
]

# Number of times to run each test for averaging
NUM_RUNS = 2

class MockDocument:
    """Mock Document class for testing"""
    
    def __init__(self, id, filename, content="", metadata=None, folder="/"):
        self.id = id
        self.filename = filename
        self.content = content
        self.metadata = metadata or {}
        self.folder = folder
        self.chunks = []

class MockDocumentProcessor:
    """Mock DocumentProcessor for testing"""
    
    def __init__(self, chunking_strategy="recursive", chunk_size=1500, chunk_overlap=150):
        self.chunking_strategy = chunking_strategy
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    async def process_document(self, document):
        """
        Mock document processing
        """
        # Simulate processing time based on document size and chunking strategy
        file_size = len(document.content)
        
        # Different strategies have different processing times
        if self.chunking_strategy == "recursive":
            processing_time = 0.5 + (file_size * 0.0001)
        elif self.chunking_strategy == "token":
            processing_time = 0.7 + (file_size * 0.00015)
        elif self.chunking_strategy == "markdown":
            processing_time = 0.6 + (file_size * 0.00012)
        else:
            processing_time = 0.5 + (file_size * 0.0001)
        
        # Simulate processing
        await asyncio.sleep(processing_time)
        
        # Create chunks
        chunk_count = max(1, file_size // self.chunk_size)
        document.chunks = [{"id": f"chunk_{i}", "content": f"Chunk {i} content"} for i in range(chunk_count)]
        
        return document

class MockDocumentAnalysisService:
    """Mock DocumentAnalysisService for testing"""
    
    async def analyze_document(self, document):
        """
        Mock document analysis
        """
        # Simulate analysis time based on document size
        file_size = len(document.content)
        analysis_time = 0.3 + (file_size * 0.00005)
        
        # Simulate analysis
        await asyncio.sleep(analysis_time)
        
        # Determine recommended strategy based on file type
        file_type = document.metadata.get("file_type", "")
        if file_type == "md":
            strategy = "markdown"
        elif file_type == "code":
            strategy = "token"
        else:
            strategy = "recursive"
        
        return {
            "strategy": strategy,
            "parameters": {
                "chunk_size": 1500,
                "chunk_overlap": 150
            },
            "justification": f"Recommended strategy for {file_type} files"
        }

def generate_test_file(size_kb: int, file_type: str = "txt"):
    """Generate test content of the specified size"""
    # Generate content
    content = ""
    if file_type == "txt":
        # Generate paragraphs of text
        paragraph = "This is a test paragraph for document processing performance testing. " * 10
        paragraphs_needed = (size_kb * 1024) // len(paragraph)
        content = "\n\n".join([paragraph for _ in range(paragraphs_needed)])
    elif file_type == "md":
        # Generate markdown content
        paragraph = "This is a test paragraph for document processing performance testing. " * 10
        paragraphs_needed = (size_kb * 1024) // (len(paragraph) + 20)  # Account for markdown syntax
        
        content = "# Test Document\n\n"
        for i in range(paragraphs_needed):
            content += f"## Section {i+1}\n\n{paragraph}\n\n"
    
    return content

async def process_document(document_processor, document, chunking_strategy: str):
    """Process a document and measure performance"""
    start_time = time.time()
    
    # Configure processor with the specified chunking strategy
    processor = MockDocumentProcessor(
        chunking_strategy=chunking_strategy,
        chunk_size=1500,
        chunk_overlap=150
    )
    
    # Process document
    processed_document = await processor.process_document(document)
    
    elapsed_time = time.time() - start_time
    
    return {
        "document_id": str(document.id),
        "filename": document.filename,
        "chunking_strategy": chunking_strategy,
        "elapsed_time": elapsed_time,
        "chunk_count": len(processed_document.chunks),
        "file_size": len(document.content)
    }

async def analyze_document(document_analysis_service, document):
    """Analyze a document and measure performance"""
    start_time = time.time()
    
    # Analyze document
    analysis_result = await document_analysis_service.analyze_document(document)
    
    elapsed_time = time.time() - start_time
    
    return {
        "document_id": str(document.id),
        "filename": document.filename,
        "elapsed_time": elapsed_time,
        "recommended_strategy": analysis_result.get("strategy"),
        "file_size": len(document.content)
    }

async def run_processing_tests():
    """Run document processing performance tests"""
    results = []
    
    for size_name, size_kb in TEST_FILE_SIZES:
        for file_type in ["txt", "md"]:
            # Generate test content
            content = generate_test_file(size_kb, file_type)
            file_name = f"test_{size_name}.{file_type}"
            
            # Create document
            document_id = uuid.uuid4()
            document = MockDocument(
                id=document_id,
                filename=file_name,
                content=content,
                metadata={"file_type": file_type, "test_size": size_name},
                folder="/test"
            )
            
            # Test each chunking strategy
            for strategy in TEST_CHUNKING_STRATEGIES:
                strategy_results = []
                
                for _ in range(NUM_RUNS):
                    result = await process_document(None, document, strategy)
                    strategy_results.append(result)
                
                # Calculate average metrics
                avg_time = statistics.mean([r["elapsed_time"] for r in strategy_results])
                avg_chunk_count = statistics.mean([r["chunk_count"] for r in strategy_results])
                
                results.append({
                    "document_id": str(document.id),
                    "filename": file_name,
                    "file_type": file_type,
                    "size_name": size_name,
                    "size_kb": size_kb,
                    "chunking_strategy": strategy,
                    "avg_elapsed_time": avg_time,
                    "avg_chunk_count": avg_chunk_count,
                    "num_runs": NUM_RUNS
                })
    
    return results

async def run_analysis_tests():
    """Run document analysis performance tests"""
    results = []
    
    for size_name, size_kb in TEST_FILE_SIZES:
        for file_type in ["txt", "md"]:
            # Generate test content
            content = generate_test_file(size_kb, file_type)
            file_name = f"test_analysis_{size_name}.{file_type}"
            
            # Create document
            document_id = uuid.uuid4()
            document = MockDocument(
                id=document_id,
                filename=file_name,
                content=content,
                metadata={"file_type": file_type, "test_size": size_name},
                folder="/test"
            )
            
            # Create document analysis service
            document_analysis_service = MockDocumentAnalysisService()
            
            # Run analysis multiple times
            analysis_results = []
            for _ in range(NUM_RUNS):
                result = await analyze_document(document_analysis_service, document)
                analysis_results.append(result)
            
            # Calculate average metrics
            avg_time = statistics.mean([r["elapsed_time"] for r in analysis_results])
            
            results.append({
                "document_id": str(document.id),
                "filename": file_name,
                "file_type": file_type,
                "size_name": size_name,
                "size_kb": size_kb,
                "avg_elapsed_time": avg_time,
                "recommended_strategy": analysis_results[0]["recommended_strategy"],
                "num_runs": NUM_RUNS
            })
    
    return results

@pytest.mark.asyncio
async def test_document_processing_performance():
    """Test document processing performance"""
    print(f"\nRunning document processing performance test with {len(TEST_FILE_SIZES)} file sizes, {len(TEST_CHUNKING_STRATEGIES)} chunking strategies")
    
    # Run processing tests
    results = await run_processing_tests()
    
    # Print results
    print("\nDocument Processing Performance Results:")
    print(f"{'File':<20} | {'Size':<10} | {'Strategy':<15} | {'Avg Time (s)':<12} | {'Avg Chunks':<10}")
    print("-" * 80)
    
    for result in results:
        print(f"{result['filename']:<20} | {result['size_kb']} KB{' ':<5} | {result['chunking_strategy']:<15} | {result['avg_elapsed_time']:<12.2f} | {result['avg_chunk_count']:<10.0f}")
    
    # Calculate overall metrics by strategy
    strategies = {}
    for strategy in TEST_CHUNKING_STRATEGIES:
        strategy_results = [r for r in results if r["chunking_strategy"] == strategy]
        avg_time = statistics.mean([r["avg_elapsed_time"] for r in strategy_results])
        strategies[strategy] = avg_time
    
    print("\nAverage Processing Time by Strategy:")
    for strategy, avg_time in strategies.items():
        print(f"{strategy:<15}: {avg_time:.2f}s")
    
    # Calculate overall metrics by file size
    sizes = {}
    for size_name, size_kb in TEST_FILE_SIZES:
        size_results = [r for r in results if r["size_name"] == size_name]
        avg_time = statistics.mean([r["avg_elapsed_time"] for r in size_results])
        sizes[size_name] = avg_time
    
    print("\nAverage Processing Time by File Size:")
    for size_name, avg_time in sizes.items():
        print(f"{size_name:<10}: {avg_time:.2f}s")

@pytest.mark.asyncio
async def test_document_analysis_performance():
    """Test document analysis performance"""
    print(f"\nRunning document analysis performance test with {len(TEST_FILE_SIZES)} file sizes")
    
    # Run analysis tests
    results = await run_analysis_tests()
    
    # Print results
    print("\nDocument Analysis Performance Results:")
    print(f"{'File':<20} | {'Size':<10} | {'Avg Time (s)':<12} | {'Recommended Strategy':<20}")
    print("-" * 80)
    
    for result in results:
        print(f"{result['filename']:<20} | {result['size_kb']} KB{' ':<5} | {result['avg_elapsed_time']:<12.2f} | {result['recommended_strategy']:<20}")
    
    # Calculate overall metrics by file size
    sizes = {}
    for size_name, size_kb in TEST_FILE_SIZES:
        size_results = [r for r in results if r["size_name"] == size_name]
        avg_time = statistics.mean([r["avg_elapsed_time"] for r in size_results])
        sizes[size_name] = avg_time
    
    print("\nAverage Analysis Time by File Size:")
    for size_name, avg_time in sizes.items():
        print(f"{size_name:<10}: {avg_time:.2f}s")

if __name__ == "__main__":
    # Run the tests directly if this file is executed
    asyncio.run(test_document_processing_performance())
    asyncio.run(test_document_analysis_performance())

================
File: tests/test_simplified_performance.py
================
import time
import asyncio
import pytest
import statistics
from typing import List, Dict, Any

# Test configuration
TEST_QUERIES = [
    "What is the main purpose of the RAG system?",
    "How does the document analysis service work?",
    "What are the key components of the LangGraph integration?",
    "Explain the database schema for documents and chunks",
    "How does the memory integration enhance the system?"
]

# Number of times to run each query for averaging
NUM_RUNS = 3

# Maximum acceptable response time for simple queries (in seconds)
MAX_RESPONSE_TIME = 6.0

class MockRAGEngine:
    """Mock RAG Engine for testing"""
    
    async def query(self, query: str, model: str = "gemma3:12b", use_rag: bool = True, stream: bool = False, **kwargs):
        """
        Mock query method that simulates response generation
        """
        # Simulate processing time based on query length and whether RAG is used
        base_time = 0.5 + (len(query) * 0.01)
        if use_rag:
            # RAG queries take longer
            processing_time = base_time * 2
            sources = [
                {"document_id": "doc1", "chunk_id": "chunk1", "relevance_score": 0.95, "excerpt": "Sample excerpt 1"},
                {"document_id": "doc2", "chunk_id": "chunk2", "relevance_score": 0.85, "excerpt": "Sample excerpt 2"}
            ]
        else:
            processing_time = base_time
            sources = []
        
        # Simulate processing
        await asyncio.sleep(processing_time)
        
        # Generate mock response
        response = {
            "answer": f"This is a mock response to: {query}",
            "sources": sources,
            "token_count": len(query) * 5
        }
        
        # Add stream if requested
        if stream:
            async def token_generator():
                tokens = response["answer"].split()
                for token in tokens:
                    await asyncio.sleep(0.1)
                    yield token + " "
            
            response["stream"] = token_generator()
        
        return response

async def run_query(rag_engine, query: str, use_rag: bool = True):
    """Run a query and measure performance"""
    start_time = time.time()
    
    response = await rag_engine.query(
        query=query,
        model="gemma3:12b",
        use_rag=use_rag,
        stream=False
    )
    
    elapsed_time = time.time() - start_time
    
    return {
        "query": query,
        "response": response.get("answer", ""),
        "elapsed_time": elapsed_time,
        "sources": response.get("sources", []),
        "token_count": response.get("token_count", 0)
    }

async def run_query_batch(rag_engine, queries: List[str], use_rag: bool = True, runs: int = 1):
    """Run a batch of queries multiple times and collect performance metrics"""
    results = []
    
    for query in queries:
        query_results = []
        for _ in range(runs):
            result = await run_query(rag_engine, query, use_rag)
            query_results.append(result)
        
        # Calculate average metrics
        avg_time = statistics.mean([r["elapsed_time"] for r in query_results])
        avg_token_count = statistics.mean([r["token_count"] for r in query_results])
        
        results.append({
            "query": query,
            "avg_elapsed_time": avg_time,
            "avg_token_count": avg_token_count,
            "num_runs": runs,
            "use_rag": use_rag,
            "response": query_results[0]["response"],  # Just use the first response
            "sources": query_results[0]["sources"]  # Just use the first sources
        })
    
    return results

@pytest.mark.asyncio
async def test_rag_query_performance():
    """Test RAG query performance"""
    print(f"\nRunning RAG query performance test with {len(TEST_QUERIES)} queries, {NUM_RUNS} runs each")
    
    # Create mock RAG engine
    rag_engine = MockRAGEngine()
    
    # Run queries with RAG
    results = await run_query_batch(rag_engine, TEST_QUERIES, use_rag=True, runs=NUM_RUNS)
    
    # Print results
    print("\nRAG Query Performance Results:")
    print(f"{'Query':<50} | {'Avg Time (s)':<12} | {'Avg Tokens':<10} | {'Sources':<10}")
    print("-" * 90)
    
    for result in results:
        query_display = result["query"][:47] + "..." if len(result["query"]) > 50 else result["query"].ljust(50)
        print(f"{query_display} | {result['avg_elapsed_time']:<12.2f} | {result['avg_token_count']:<10.0f} | {len(result['sources']):<10}")
    
    # Calculate overall metrics
    avg_time = statistics.mean([r["avg_elapsed_time"] for r in results])
    max_time = max([r["avg_elapsed_time"] for r in results])
    min_time = min([r["avg_elapsed_time"] for r in results])
    avg_tokens = statistics.mean([r["avg_token_count"] for r in results])
    
    print("-" * 90)
    print(f"Overall Average: {avg_time:.2f}s, Min: {min_time:.2f}s, Max: {max_time:.2f}s, Avg Tokens: {avg_tokens:.0f}")
    
    # Assert performance requirements
    assert max_time <= MAX_RESPONSE_TIME, f"Maximum response time ({max_time:.2f}s) exceeds limit ({MAX_RESPONSE_TIME}s)"

@pytest.mark.asyncio
async def test_non_rag_query_performance():
    """Test non-RAG query performance (LLM only)"""
    print(f"\nRunning non-RAG query performance test with {len(TEST_QUERIES)} queries, {NUM_RUNS} runs each")
    
    # Create mock RAG engine
    rag_engine = MockRAGEngine()
    
    # Run queries without RAG
    results = await run_query_batch(rag_engine, TEST_QUERIES, use_rag=False, runs=NUM_RUNS)
    
    # Print results
    print("\nNon-RAG Query Performance Results:")
    print(f"{'Query':<50} | {'Avg Time (s)':<12} | {'Avg Tokens':<10}")
    print("-" * 80)
    
    for result in results:
        query_display = result["query"][:47] + "..." if len(result["query"]) > 50 else result["query"].ljust(50)
        print(f"{query_display} | {result['avg_elapsed_time']:<12.2f} | {result['avg_token_count']:<10.0f}")
    
    # Calculate overall metrics
    avg_time = statistics.mean([r["avg_elapsed_time"] for r in results])
    max_time = max([r["avg_elapsed_time"] for r in results])
    min_time = min([r["avg_elapsed_time"] for r in results])
    avg_tokens = statistics.mean([r["avg_token_count"] for r in results])
    
    print("-" * 80)
    print(f"Overall Average: {avg_time:.2f}s, Min: {min_time:.2f}s, Max: {max_time:.2f}s, Avg Tokens: {avg_tokens:.0f}")

@pytest.mark.asyncio
async def test_performance_comparison():
    """Compare RAG vs non-RAG performance"""
    print("\nComparing RAG vs non-RAG performance")
    
    # Create mock RAG engine
    rag_engine = MockRAGEngine()
    
    # Run queries with and without RAG
    rag_results = await run_query_batch(rag_engine, TEST_QUERIES, use_rag=True, runs=NUM_RUNS)
    non_rag_results = await run_query_batch(rag_engine, TEST_QUERIES, use_rag=False, runs=NUM_RUNS)
    
    # Calculate overall metrics
    rag_avg_time = statistics.mean([r["avg_elapsed_time"] for r in rag_results])
    non_rag_avg_time = statistics.mean([r["avg_elapsed_time"] for r in non_rag_results])
    
    # Print comparison
    print("\nPerformance Comparison:")
    print(f"RAG Average Response Time: {rag_avg_time:.2f}s")
    print(f"Non-RAG Average Response Time: {non_rag_avg_time:.2f}s")
    print(f"RAG Overhead: {rag_avg_time - non_rag_avg_time:.2f}s ({(rag_avg_time / non_rag_avg_time - 1) * 100:.1f}%)")

if __name__ == "__main__":
    # Run the tests directly if this file is executed
    asyncio.run(test_rag_query_performance())
    asyncio.run(test_non_rag_query_performance())
    asyncio.run(test_performance_comparison())

================
File: tests/try_rag_query.py
================
#!/usr/bin/env python3
"""
Simple script to try a query with the Metis RAG system.
"""
import asyncio
import logging
import sys
from typing import List, Dict, Any

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("try_rag_query")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.models.document import Document, Chunk

async def process_test_documents(vector_store: VectorStore) -> List[str]:
    """
    Process test documents and add them to the vector store.
    Returns a list of document IDs.
    """
    # Test documents with known content
    test_documents = {
        "technical_doc": {
            "filename": "technical_documentation.md",
            "content": """# Metis RAG Technical Documentation

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
""",
            "tags": ["technical", "documentation", "architecture"],
            "folder": "/test"
        },
        "business_report": {
            "filename": "quarterly_report.txt",
            "content": """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
""",
            "tags": ["business", "report", "quarterly"],
            "folder": "/test"
        }
    }
    
    document_ids = []
    
    # Create Document objects and add them to the vector store
    for doc_id, doc_info in test_documents.items():
        logger.info(f"Processing document: {doc_info['filename']}")
        
        doc = Document(
            id=doc_id,
            filename=doc_info["filename"],
            content=doc_info["content"],
            tags=doc_info["tags"],
            folder=doc_info["folder"]
        )
        
        # Create a single chunk for simplicity
        doc.chunks = [
            Chunk(
                id=f"{doc_id}_chunk_0",
                content=doc_info["content"],
                metadata={
                    "index": 0,
                    "source": doc_info["filename"],
                    "document_id": doc_id,
                    "filename": doc_info["filename"],
                    "tags": doc_info["tags"],
                    "folder": doc_info["folder"]
                }
            )
        ]
        
        # Add document to vector store
        await vector_store.add_document(doc)
        document_ids.append(doc_id)
        
        logger.info(f"Added document {doc_id} to vector store")
    
    return document_ids

async def try_query(query: str):
    """
    Try a query with the RAG system.
    """
    try:
        # Initialize vector store with a test directory
        vector_store = VectorStore(persist_directory="test_query_chroma")
        
        # Check if there are documents in the vector store
        stats = vector_store.get_stats()
        if stats["count"] == 0:
            logger.info("No documents in vector store, adding test documents")
            await process_test_documents(vector_store)
        else:
            logger.info(f"Vector store already contains {stats['count']} chunks")
        
        # Initialize RAG engine
        rag_engine = RAGEngine(vector_store=vector_store)
        
        # Execute query with RAG
        logger.info(f"Executing query: {query}")
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Print the response
        print("\n" + "="*80)
        print(f"QUERY: {query}")
        print("="*80)
        
        if "answer" in response:
            print("\nANSWER:")
            print(response["answer"])
        
        if "sources" in response and response["sources"]:
            print("\nSOURCES:")
            for i, source in enumerate(response["sources"]):
                print(f"[{i+1}] {source.filename} (Score: {source.relevance_score:.2f})")
                print(f"    Excerpt: {source.excerpt[:100]}...")
        
        print("="*80 + "\n")
        
        return response
    except Exception as e:
        logger.error(f"Error trying query: {str(e)}")
        raise

async def main():
    """
    Main function to try a query with the RAG system.
    """
    if len(sys.argv) > 1:
        # Use query from command line arguments
        query = " ".join(sys.argv[1:])
    else:
        # Use default queries
        queries = [
            "What is the architecture of Metis RAG?",
            "What was the revenue reported in Q1 2025?",
            "What are the components of the RAG engine?",
            "What are the strategic initiatives for Q2?",
        ]
        
        for query in queries:
            await try_query(query)
            # Add a small delay between queries
            await asyncio.sleep(1)
        
        return
    
    # Try the user-provided query
    await try_query(query)

if __name__ == "__main__":
    asyncio.run(main())

================
File: unused_code/app/rag/system_prompts/__init__.py
================
"""
System prompts package for Metis RAG.

This package contains various system prompts used by the RAG engine
to guide the behavior of the language model for different types of queries.

This is the original version of the file before the PromptManager refactoring.
"""

from app.rag.system_prompts.code_generation import (
    CODE_GENERATION_SYSTEM_PROMPT,
    PYTHON_CODE_GENERATION_PROMPT,
    JAVASCRIPT_CODE_GENERATION_PROMPT
)
from app.rag.system_prompts.rag import (
    RAG_SYSTEM_PROMPT
)
from app.rag.system_prompts.conversation import (
    CONVERSATION_WITH_CONTEXT_PROMPT,
    NEW_QUERY_WITH_CONTEXT_PROMPT,
    CONVERSATION_WITHOUT_CONTEXT_PROMPT
)

__all__ = [
    'CODE_GENERATION_SYSTEM_PROMPT',
    'PYTHON_CODE_GENERATION_PROMPT',
    'JAVASCRIPT_CODE_GENERATION_PROMPT',
    'RAG_SYSTEM_PROMPT',
    'CONVERSATION_WITH_CONTEXT_PROMPT',
    'NEW_QUERY_WITH_CONTEXT_PROMPT',
    'CONVERSATION_WITHOUT_CONTEXT_PROMPT'
]

================
File: unused_code/app/rag/system_prompts/conversation.py
================
"""
System prompts for conversation handling in Metis RAG.
"""

CONVERSATION_WITH_CONTEXT_PROMPT = """Context:
{context}

Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, acknowledge this using varied phrasing such as:
   * "I've searched the available documents but couldn't find information about [topic]."
   * "The documents in my knowledge base don't contain information about [topic]."
   * "I don't have document-based information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. If the context is insufficient, you may offer general knowledge with a clear disclaimer like:
   * "However, I can provide some general information about this topic if you'd like."
   * "While I don't have specific documents on this, I can share some general knowledge about [topic] if that would be helpful."
9. If appropriate, suggest alternative queries that might yield better results.
"""

NEW_QUERY_WITH_CONTEXT_PROMPT = """Context:
{context}

User Question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, acknowledge this using varied phrasing such as:
   * "I've searched the available documents but couldn't find information about [topic]."
   * "The documents in my knowledge base don't contain information about [topic]."
   * "I don't have document-based information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. This is a new conversation with no previous history - treat it as such.
9. If the context is insufficient, you may offer general knowledge with a clear disclaimer like:
   * "However, I can provide some general information about this topic if you'd like."
   * "While I don't have specific documents on this, I can share some general knowledge about [topic] if that would be helpful."
10. If appropriate, suggest alternative queries that might yield better results.
"""

CONVERSATION_WITHOUT_CONTEXT_PROMPT = """Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. Use the previous conversation to understand the context of the user's new question.
2. Pay attention to references like "it", "that", "there", etc., and resolve them based on the conversation history.
3. Provide a clear, direct answer to the user's question.
4. If the question refers to something not mentioned in the conversation history, ask for clarification.
5. Be helpful and informative in your response.
"""

================
File: unused_code/app/rag/system_prompts/rag.py
================
"""
System prompts for RAG responses in Metis RAG.
"""

# Original complex system prompt (commented out for reference)
"""
Original RAG_SYSTEM_PROMPT contained detailed instructions for:
- Role and capabilities
- Guidelines for using context
- Handling limited information
- Handling no information found
- Citation formatting
- Conversation handling
- Response style
"""

# Improved system prompt based on test results
RAG_SYSTEM_PROMPT = """You are a helpful assistant. Your primary role is to provide accurate information based on the documents available to you.

CORE GUIDELINES:
1. ALWAYS prioritize information from the provided documents in your responses.
2. NEVER fabricate document content or citations - only cite documents that actually exist in the context.
3. Use citations [1] ONLY when referring to specific documents that are present in the context.
4. Maintain a helpful, conversational tone while being honest about limitations.

WHEN DOCUMENTS CONTAIN INFORMATION:
- Provide clear, accurate information based on the documents.
- Use citations appropriately to reference specific documents.
- Synthesize information from multiple documents when relevant.

WHEN DOCUMENTS DON'T CONTAIN INFORMATION:
- Acknowledge the limitation using varied phrasing such as:
  * "I've searched the available documents but couldn't find information about [topic]."
  * "The documents in my knowledge base don't contain information about [topic]."
  * "I don't have document-based information about [topic]."
- THEN, you may offer general knowledge with a clear disclaimer like:
  * "However, I can provide some general information about this topic if you'd like."
  * "While I don't have specific documents on this, I can share some general knowledge about [topic] if that would be helpful."
- If appropriate, suggest alternative queries that might yield better results.

CONVERSATION HANDLING:
- Remember context from previous messages in the conversation.
- Respond directly to the user's query without unnecessary preambles.
- Be concise but thorough in your responses.
"""

================
File: venv_py310/bin/activate
================
# This file must be used with "source bin/activate" *from bash*
# you cannot run it directly

deactivate () {
    # reset old environment variables
    if [ -n "${_OLD_VIRTUAL_PATH:-}" ] ; then
        PATH="${_OLD_VIRTUAL_PATH:-}"
        export PATH
        unset _OLD_VIRTUAL_PATH
    fi
    if [ -n "${_OLD_VIRTUAL_PYTHONHOME:-}" ] ; then
        PYTHONHOME="${_OLD_VIRTUAL_PYTHONHOME:-}"
        export PYTHONHOME
        unset _OLD_VIRTUAL_PYTHONHOME
    fi

    # This should detect bash and zsh, which have a hash command that must
    # be called to get it to forget past commands.  Without forgetting
    # past commands the $PATH changes we made may not be respected
    if [ -n "${BASH:-}" -o -n "${ZSH_VERSION:-}" ] ; then
        hash -r 2> /dev/null
    fi

    if [ -n "${_OLD_VIRTUAL_PS1:-}" ] ; then
        PS1="${_OLD_VIRTUAL_PS1:-}"
        export PS1
        unset _OLD_VIRTUAL_PS1
    fi

    unset VIRTUAL_ENV
    unset VIRTUAL_ENV_PROMPT
    if [ ! "${1:-}" = "nondestructive" ] ; then
    # Self destruct!
        unset -f deactivate
    fi
}

# unset irrelevant variables
deactivate nondestructive

VIRTUAL_ENV=/Users/charleshoward/Metis_RAG/venv_py310
export VIRTUAL_ENV

_OLD_VIRTUAL_PATH="$PATH"
PATH="$VIRTUAL_ENV/"bin":$PATH"
export PATH

# unset PYTHONHOME if set
# this will fail if PYTHONHOME is set to the empty string (which is bad anyway)
# could use `if (set -u; : $PYTHONHOME) ;` in bash
if [ -n "${PYTHONHOME:-}" ] ; then
    _OLD_VIRTUAL_PYTHONHOME="${PYTHONHOME:-}"
    unset PYTHONHOME
fi

if [ -z "${VIRTUAL_ENV_DISABLE_PROMPT:-}" ] ; then
    _OLD_VIRTUAL_PS1="${PS1:-}"
    PS1='(venv_py310) '"${PS1:-}"
    export PS1
    VIRTUAL_ENV_PROMPT='(venv_py310) '
    export VIRTUAL_ENV_PROMPT
fi

# This should detect bash and zsh, which have a hash command that must
# be called to get it to forget past commands.  Without forgetting
# past commands the $PATH changes we made may not be respected
if [ -n "${BASH:-}" -o -n "${ZSH_VERSION:-}" ] ; then
    hash -r 2> /dev/null
fi

================
File: venv_py310/bin/activate.csh
================
# This file must be used with "source bin/activate.csh" *from csh*.
# You cannot run it directly.
# Created by Davide Di Blasi <davidedb@gmail.com>.
# Ported to Python 3.3 venv by Andrew Svetlov <andrew.svetlov@gmail.com>

alias deactivate 'test $?_OLD_VIRTUAL_PATH != 0 && setenv PATH "$_OLD_VIRTUAL_PATH" && unset _OLD_VIRTUAL_PATH; rehash; test $?_OLD_VIRTUAL_PROMPT != 0 && set prompt="$_OLD_VIRTUAL_PROMPT" && unset _OLD_VIRTUAL_PROMPT; unsetenv VIRTUAL_ENV; unsetenv VIRTUAL_ENV_PROMPT; test "\!:*" != "nondestructive" && unalias deactivate'

# Unset irrelevant variables.
deactivate nondestructive

setenv VIRTUAL_ENV /Users/charleshoward/Metis_RAG/venv_py310

set _OLD_VIRTUAL_PATH="$PATH"
setenv PATH "$VIRTUAL_ENV/"bin":$PATH"


set _OLD_VIRTUAL_PROMPT="$prompt"

if (! "$?VIRTUAL_ENV_DISABLE_PROMPT") then
    set prompt = '(venv_py310) '"$prompt"
    setenv VIRTUAL_ENV_PROMPT '(venv_py310) '
endif

alias pydoc python -m pydoc

rehash

================
File: venv_py310/bin/activate.fish
================
# This file must be used with "source <venv>/bin/activate.fish" *from fish*
# (https://fishshell.com/); you cannot run it directly.

function deactivate  -d "Exit virtual environment and return to normal shell environment"
    # reset old environment variables
    if test -n "$_OLD_VIRTUAL_PATH"
        set -gx PATH $_OLD_VIRTUAL_PATH
        set -e _OLD_VIRTUAL_PATH
    end
    if test -n "$_OLD_VIRTUAL_PYTHONHOME"
        set -gx PYTHONHOME $_OLD_VIRTUAL_PYTHONHOME
        set -e _OLD_VIRTUAL_PYTHONHOME
    end

    if test -n "$_OLD_FISH_PROMPT_OVERRIDE"
        set -e _OLD_FISH_PROMPT_OVERRIDE
        # prevents error when using nested fish instances (Issue #93858)
        if functions -q _old_fish_prompt
            functions -e fish_prompt
            functions -c _old_fish_prompt fish_prompt
            functions -e _old_fish_prompt
        end
    end

    set -e VIRTUAL_ENV
    set -e VIRTUAL_ENV_PROMPT
    if test "$argv[1]" != "nondestructive"
        # Self-destruct!
        functions -e deactivate
    end
end

# Unset irrelevant variables.
deactivate nondestructive

set -gx VIRTUAL_ENV /Users/charleshoward/Metis_RAG/venv_py310

set -gx _OLD_VIRTUAL_PATH $PATH
set -gx PATH "$VIRTUAL_ENV/"bin $PATH

# Unset PYTHONHOME if set.
if set -q PYTHONHOME
    set -gx _OLD_VIRTUAL_PYTHONHOME $PYTHONHOME
    set -e PYTHONHOME
end

if test -z "$VIRTUAL_ENV_DISABLE_PROMPT"
    # fish uses a function instead of an env var to generate the prompt.

    # Save the current fish_prompt function as the function _old_fish_prompt.
    functions -c fish_prompt _old_fish_prompt

    # With the original prompt function renamed, we can override with our own.
    function fish_prompt
        # Save the return status of the last command.
        set -l old_status $status

        # Output the venv prompt; color taken from the blue of the Python logo.
        printf "%s%s%s" (set_color 4B8BBE) '(venv_py310) ' (set_color normal)

        # Restore the return status of the previous command.
        echo "exit $old_status" | .
        # Output the original/"old" prompt.
        _old_fish_prompt
    end

    set -gx _OLD_FISH_PROMPT_OVERRIDE "$VIRTUAL_ENV"
    set -gx VIRTUAL_ENV_PROMPT '(venv_py310) '
end

================
File: venv_py310/bin/Activate.ps1
================
<#
.Synopsis
Activate a Python virtual environment for the current PowerShell session.

.Description
Pushes the python executable for a virtual environment to the front of the
$Env:PATH environment variable and sets the prompt to signify that you are
in a Python virtual environment. Makes use of the command line switches as
well as the `pyvenv.cfg` file values present in the virtual environment.

.Parameter VenvDir
Path to the directory that contains the virtual environment to activate. The
default value for this is the parent of the directory that the Activate.ps1
script is located within.

.Parameter Prompt
The prompt prefix to display when this virtual environment is activated. By
default, this prompt is the name of the virtual environment folder (VenvDir)
surrounded by parentheses and followed by a single space (ie. '(.venv) ').

.Example
Activate.ps1
Activates the Python virtual environment that contains the Activate.ps1 script.

.Example
Activate.ps1 -Verbose
Activates the Python virtual environment that contains the Activate.ps1 script,
and shows extra information about the activation as it executes.

.Example
Activate.ps1 -VenvDir C:\Users\MyUser\Common\.venv
Activates the Python virtual environment located in the specified location.

.Example
Activate.ps1 -Prompt "MyPython"
Activates the Python virtual environment that contains the Activate.ps1 script,
and prefixes the current prompt with the specified string (surrounded in
parentheses) while the virtual environment is active.

.Notes
On Windows, it may be required to enable this Activate.ps1 script by setting the
execution policy for the user. You can do this by issuing the following PowerShell
command:

PS C:\> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

For more information on Execution Policies: 
https://go.microsoft.com/fwlink/?LinkID=135170

#>
Param(
    [Parameter(Mandatory = $false)]
    [String]
    $VenvDir,
    [Parameter(Mandatory = $false)]
    [String]
    $Prompt
)

<# Function declarations --------------------------------------------------- #>

<#
.Synopsis
Remove all shell session elements added by the Activate script, including the
addition of the virtual environment's Python executable from the beginning of
the PATH variable.

.Parameter NonDestructive
If present, do not remove this function from the global namespace for the
session.

#>
function global:deactivate ([switch]$NonDestructive) {
    # Revert to original values

    # The prior prompt:
    if (Test-Path -Path Function:_OLD_VIRTUAL_PROMPT) {
        Copy-Item -Path Function:_OLD_VIRTUAL_PROMPT -Destination Function:prompt
        Remove-Item -Path Function:_OLD_VIRTUAL_PROMPT
    }

    # The prior PYTHONHOME:
    if (Test-Path -Path Env:_OLD_VIRTUAL_PYTHONHOME) {
        Copy-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME -Destination Env:PYTHONHOME
        Remove-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME
    }

    # The prior PATH:
    if (Test-Path -Path Env:_OLD_VIRTUAL_PATH) {
        Copy-Item -Path Env:_OLD_VIRTUAL_PATH -Destination Env:PATH
        Remove-Item -Path Env:_OLD_VIRTUAL_PATH
    }

    # Just remove the VIRTUAL_ENV altogether:
    if (Test-Path -Path Env:VIRTUAL_ENV) {
        Remove-Item -Path env:VIRTUAL_ENV
    }

    # Just remove VIRTUAL_ENV_PROMPT altogether.
    if (Test-Path -Path Env:VIRTUAL_ENV_PROMPT) {
        Remove-Item -Path env:VIRTUAL_ENV_PROMPT
    }

    # Just remove the _PYTHON_VENV_PROMPT_PREFIX altogether:
    if (Get-Variable -Name "_PYTHON_VENV_PROMPT_PREFIX" -ErrorAction SilentlyContinue) {
        Remove-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Scope Global -Force
    }

    # Leave deactivate function in the global namespace if requested:
    if (-not $NonDestructive) {
        Remove-Item -Path function:deactivate
    }
}

<#
.Description
Get-PyVenvConfig parses the values from the pyvenv.cfg file located in the
given folder, and returns them in a map.

For each line in the pyvenv.cfg file, if that line can be parsed into exactly
two strings separated by `=` (with any amount of whitespace surrounding the =)
then it is considered a `key = value` line. The left hand string is the key,
the right hand is the value.

If the value starts with a `'` or a `"` then the first and last character is
stripped from the value before being captured.

.Parameter ConfigDir
Path to the directory that contains the `pyvenv.cfg` file.
#>
function Get-PyVenvConfig(
    [String]
    $ConfigDir
) {
    Write-Verbose "Given ConfigDir=$ConfigDir, obtain values in pyvenv.cfg"

    # Ensure the file exists, and issue a warning if it doesn't (but still allow the function to continue).
    $pyvenvConfigPath = Join-Path -Resolve -Path $ConfigDir -ChildPath 'pyvenv.cfg' -ErrorAction Continue

    # An empty map will be returned if no config file is found.
    $pyvenvConfig = @{ }

    if ($pyvenvConfigPath) {

        Write-Verbose "File exists, parse `key = value` lines"
        $pyvenvConfigContent = Get-Content -Path $pyvenvConfigPath

        $pyvenvConfigContent | ForEach-Object {
            $keyval = $PSItem -split "\s*=\s*", 2
            if ($keyval[0] -and $keyval[1]) {
                $val = $keyval[1]

                # Remove extraneous quotations around a string value.
                if ("'""".Contains($val.Substring(0, 1))) {
                    $val = $val.Substring(1, $val.Length - 2)
                }

                $pyvenvConfig[$keyval[0]] = $val
                Write-Verbose "Adding Key: '$($keyval[0])'='$val'"
            }
        }
    }
    return $pyvenvConfig
}


<# Begin Activate script --------------------------------------------------- #>

# Determine the containing directory of this script
$VenvExecPath = Split-Path -Parent $MyInvocation.MyCommand.Definition
$VenvExecDir = Get-Item -Path $VenvExecPath

Write-Verbose "Activation script is located in path: '$VenvExecPath'"
Write-Verbose "VenvExecDir Fullname: '$($VenvExecDir.FullName)"
Write-Verbose "VenvExecDir Name: '$($VenvExecDir.Name)"

# Set values required in priority: CmdLine, ConfigFile, Default
# First, get the location of the virtual environment, it might not be
# VenvExecDir if specified on the command line.
if ($VenvDir) {
    Write-Verbose "VenvDir given as parameter, using '$VenvDir' to determine values"
}
else {
    Write-Verbose "VenvDir not given as a parameter, using parent directory name as VenvDir."
    $VenvDir = $VenvExecDir.Parent.FullName.TrimEnd("\\/")
    Write-Verbose "VenvDir=$VenvDir"
}

# Next, read the `pyvenv.cfg` file to determine any required value such
# as `prompt`.
$pyvenvCfg = Get-PyVenvConfig -ConfigDir $VenvDir

# Next, set the prompt from the command line, or the config file, or
# just use the name of the virtual environment folder.
if ($Prompt) {
    Write-Verbose "Prompt specified as argument, using '$Prompt'"
}
else {
    Write-Verbose "Prompt not specified as argument to script, checking pyvenv.cfg value"
    if ($pyvenvCfg -and $pyvenvCfg['prompt']) {
        Write-Verbose "  Setting based on value in pyvenv.cfg='$($pyvenvCfg['prompt'])'"
        $Prompt = $pyvenvCfg['prompt'];
    }
    else {
        Write-Verbose "  Setting prompt based on parent's directory's name. (Is the directory name passed to venv module when creating the virtual environment)"
        Write-Verbose "  Got leaf-name of $VenvDir='$(Split-Path -Path $venvDir -Leaf)'"
        $Prompt = Split-Path -Path $venvDir -Leaf
    }
}

Write-Verbose "Prompt = '$Prompt'"
Write-Verbose "VenvDir='$VenvDir'"

# Deactivate any currently active virtual environment, but leave the
# deactivate function in place.
deactivate -nondestructive

# Now set the environment variable VIRTUAL_ENV, used by many tools to determine
# that there is an activated venv.
$env:VIRTUAL_ENV = $VenvDir

if (-not $Env:VIRTUAL_ENV_DISABLE_PROMPT) {

    Write-Verbose "Setting prompt to '$Prompt'"

    # Set the prompt to include the env name
    # Make sure _OLD_VIRTUAL_PROMPT is global
    function global:_OLD_VIRTUAL_PROMPT { "" }
    Copy-Item -Path function:prompt -Destination function:_OLD_VIRTUAL_PROMPT
    New-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Description "Python virtual environment prompt prefix" -Scope Global -Option ReadOnly -Visibility Public -Value $Prompt

    function global:prompt {
        Write-Host -NoNewline -ForegroundColor Green "($_PYTHON_VENV_PROMPT_PREFIX) "
        _OLD_VIRTUAL_PROMPT
    }
    $env:VIRTUAL_ENV_PROMPT = $Prompt
}

# Clear PYTHONHOME
if (Test-Path -Path Env:PYTHONHOME) {
    Copy-Item -Path Env:PYTHONHOME -Destination Env:_OLD_VIRTUAL_PYTHONHOME
    Remove-Item -Path Env:PYTHONHOME
}

# Add the venv to the PATH
Copy-Item -Path Env:PATH -Destination Env:_OLD_VIRTUAL_PATH
$Env:PATH = "$VenvExecDir$([System.IO.Path]::PathSeparator)$Env:PATH"

================
File: venv_py310/bin/chroma
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from chromadb.cli.cli import app
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(app())

================
File: venv_py310/bin/coloredlogs
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from coloredlogs.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/distro
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from distro.distro import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/dotenv
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from dotenv.__main__ import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli())

================
File: venv_py310/bin/email_validator
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from email_validator.__main__ import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/f2py
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from numpy.f2py.f2py2e import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/fastapi
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from fastapi.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/httpx
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from httpx import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/huggingface-cli
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from huggingface_hub.commands.huggingface_cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/humanfriendly
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from humanfriendly.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/isympy
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from isympy import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/jsondiff
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-

from __future__ import print_function

import sys
import json
import jsonpatch
import argparse


parser = argparse.ArgumentParser(description='Diff two JSON files')
parser.add_argument('FILE1', type=argparse.FileType('r'))
parser.add_argument('FILE2', type=argparse.FileType('r'))
parser.add_argument('--indent', type=int, default=None,
                    help='Indent output by n spaces')
parser.add_argument('-u', '--preserve-unicode', action='store_true',
                    help='Output Unicode character as-is without using Code Point')
parser.add_argument('-v', '--version', action='version',
                    version='%(prog)s ' + jsonpatch.__version__)


def main():
    try:
        diff_files()
    except KeyboardInterrupt:
        sys.exit(1)


def diff_files():
    """ Diffs two JSON files and prints a patch """
    args = parser.parse_args()
    doc1 = json.load(args.FILE1)
    doc2 = json.load(args.FILE2)
    patch = jsonpatch.make_patch(doc1, doc2)
    if patch.patch:
        print(json.dumps(patch.patch, indent=args.indent, ensure_ascii=not(args.preserve_unicode)))
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: venv_py310/bin/jsonpatch
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-

import sys
import os.path
import json
import jsonpatch
import tempfile
import argparse


parser = argparse.ArgumentParser(
    description='Apply a JSON patch on a JSON file')
parser.add_argument('ORIGINAL', type=argparse.FileType('r'),
                    help='Original file')
parser.add_argument('PATCH', type=argparse.FileType('r'),
                    nargs='?', default=sys.stdin,
                    help='Patch file (read from stdin if omitted)')
parser.add_argument('--indent', type=int, default=None,
                    help='Indent output by n spaces')
parser.add_argument('-b', '--backup', action='store_true',
                    help='Back up ORIGINAL if modifying in-place')
parser.add_argument('-i', '--in-place', action='store_true',
                    help='Modify ORIGINAL in-place instead of to stdout')
parser.add_argument('-v', '--version', action='version',
                    version='%(prog)s ' + jsonpatch.__version__)
parser.add_argument('-u', '--preserve-unicode', action='store_true',
                    help='Output Unicode character as-is without using Code Point')

def main():
    try:
        patch_files()
    except KeyboardInterrupt:
        sys.exit(1)


def patch_files():
    """ Diffs two JSON files and prints a patch """
    args = parser.parse_args()
    doc = json.load(args.ORIGINAL)
    patch = json.load(args.PATCH)
    result = jsonpatch.apply_patch(doc, patch)

    if args.in_place:
        dirname = os.path.abspath(os.path.dirname(args.ORIGINAL.name))

        try:
            # Attempt to replace the file atomically.  We do this by
            # creating a temporary file in the same directory as the
            # original file so we can atomically move the new file over
            # the original later.  (This is done in the same directory
	    # because atomic renames do not work across mount points.)

            fd, pathname = tempfile.mkstemp(dir=dirname)
            fp = os.fdopen(fd, 'w')
            atomic = True

        except OSError:
            # We failed to create the temporary file for an atomic
            # replace, so fall back to non-atomic mode by backing up
            # the original (if desired) and writing a new file.

            if args.backup:
                os.rename(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')
            fp = open(args.ORIGINAL.name, 'w')
            atomic = False

    else:
        # Since we're not replacing the original file in-place, write
        # the modified JSON to stdout instead.

        fp = sys.stdout

    # By this point we have some sort of file object we can write the 
    # modified JSON to.
    
    json.dump(result, fp, indent=args.indent, ensure_ascii=not(args.preserve_unicode))
    fp.write('\n')

    if args.in_place:
        # Close the new file.  If we aren't replacing atomically, this
        # is our last step, since everything else is already in place.

        fp.close()

        if atomic:
            try:
                # Complete the atomic replace by linking the original
                # to a backup (if desired), fixing up the permissions
                # on the temporary file, and moving it into place.

                if args.backup:
                    os.link(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')
                os.chmod(pathname, os.stat(args.ORIGINAL.name).st_mode)
                os.rename(pathname, args.ORIGINAL.name)

            except OSError:
                # In the event we could not actually do the atomic
                # replace, unlink the original to move it out of the
                # way and finally move the temporary file into place.
                
                os.unlink(args.ORIGINAL.name)
                os.rename(pathname, args.ORIGINAL.name)


if __name__ == "__main__":
    main()

================
File: venv_py310/bin/jsonpointer
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-


import argparse
import json
import sys

import jsonpointer

parser = argparse.ArgumentParser(
    description='Resolve a JSON pointer on JSON files')

# Accept pointer as argument or as file
ptr_group = parser.add_mutually_exclusive_group(required=True)

ptr_group.add_argument('-f', '--pointer-file', type=argparse.FileType('r'),
                       nargs='?',
                       help='File containing a JSON pointer expression')

ptr_group.add_argument('POINTER', type=str, nargs='?',
                       help='A JSON pointer expression')

parser.add_argument('FILE', type=argparse.FileType('r'), nargs='+',
                    help='Files for which the pointer should be resolved')
parser.add_argument('--indent', type=int, default=None,
                    help='Indent output by n spaces')
parser.add_argument('-v', '--version', action='version',
                    version='%(prog)s ' + jsonpointer.__version__)


def main():
    try:
        resolve_files()
    except KeyboardInterrupt:
        sys.exit(1)


def parse_pointer(args):
    if args.POINTER:
        ptr = args.POINTER
    elif args.pointer_file:
        ptr = args.pointer_file.read().strip()
    else:
        parser.print_usage()
        sys.exit(1)

    return ptr


def resolve_files():
    """ Resolve a JSON pointer on JSON files """
    args = parser.parse_args()

    ptr = parse_pointer(args)

    for f in args.FILE:
        doc = json.load(f)
        try:
            result = jsonpointer.resolve_pointer(doc, ptr)
            print(json.dumps(result, indent=args.indent))
        except jsonpointer.JsonPointerException as e:
            print('Could not resolve pointer: %s' % str(e), file=sys.stderr)


if __name__ == "__main__":
    main()

================
File: venv_py310/bin/langchain-server
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from langchain.server import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/langsmith
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from langsmith.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/markdown-it
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from markdown_it.cli.parse import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/normalizer
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from charset_normalizer import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli.cli_detect())

================
File: venv_py310/bin/onnxruntime_test
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from onnxruntime.tools.onnxruntime_test import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/pip
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/pip3
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/pip3.10
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/py.test
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pytest import console_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(console_main())

================
File: venv_py310/bin/pygmentize
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pygments.cmdline import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/pyrsa-decrypt
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from rsa.cli import decrypt
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(decrypt())

================
File: venv_py310/bin/pyrsa-encrypt
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from rsa.cli import encrypt
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(encrypt())

================
File: venv_py310/bin/pyrsa-keygen
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from rsa.cli import keygen
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(keygen())

================
File: venv_py310/bin/pyrsa-priv2pub
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from rsa.util import private_to_public
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(private_to_public())

================
File: venv_py310/bin/pyrsa-sign
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from rsa.cli import sign
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(sign())

================
File: venv_py310/bin/pyrsa-verify
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from rsa.cli import verify
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(verify())

================
File: venv_py310/bin/pytest
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pytest import console_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(console_main())

================
File: venv_py310/bin/tqdm
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from tqdm.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/typer
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from typer.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/uvicorn
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from uvicorn.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/watchfiles
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from watchfiles.cli import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli())

================
File: venv_py310/bin/websockets
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from websockets.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/include/site/python3.10/greenlet/greenlet.h
================
/* -*- indent-tabs-mode: nil; tab-width: 4; -*- */

/* Greenlet object interface */

#ifndef Py_GREENLETOBJECT_H
#define Py_GREENLETOBJECT_H


#include <Python.h>

#ifdef __cplusplus
extern "C" {
#endif

/* This is deprecated and undocumented. It does not change. */
#define GREENLET_VERSION "1.0.0"

#ifndef GREENLET_MODULE
#define implementation_ptr_t void*
#endif

typedef struct _greenlet {
    PyObject_HEAD
    PyObject* weakreflist;
    PyObject* dict;
    implementation_ptr_t pimpl;
} PyGreenlet;

#define PyGreenlet_Check(op) (op && PyObject_TypeCheck(op, &PyGreenlet_Type))


/* C API functions */

/* Total number of symbols that are exported */
#define PyGreenlet_API_pointers 12

#define PyGreenlet_Type_NUM 0
#define PyExc_GreenletError_NUM 1
#define PyExc_GreenletExit_NUM 2

#define PyGreenlet_New_NUM 3
#define PyGreenlet_GetCurrent_NUM 4
#define PyGreenlet_Throw_NUM 5
#define PyGreenlet_Switch_NUM 6
#define PyGreenlet_SetParent_NUM 7

#define PyGreenlet_MAIN_NUM 8
#define PyGreenlet_STARTED_NUM 9
#define PyGreenlet_ACTIVE_NUM 10
#define PyGreenlet_GET_PARENT_NUM 11

#ifndef GREENLET_MODULE
/* This section is used by modules that uses the greenlet C API */
static void** _PyGreenlet_API = NULL;

#    define PyGreenlet_Type \
        (*(PyTypeObject*)_PyGreenlet_API[PyGreenlet_Type_NUM])

#    define PyExc_GreenletError \
        ((PyObject*)_PyGreenlet_API[PyExc_GreenletError_NUM])

#    define PyExc_GreenletExit \
        ((PyObject*)_PyGreenlet_API[PyExc_GreenletExit_NUM])

/*
 * PyGreenlet_New(PyObject *args)
 *
 * greenlet.greenlet(run, parent=None)
 */
#    define PyGreenlet_New                                        \
        (*(PyGreenlet * (*)(PyObject * run, PyGreenlet * parent)) \
             _PyGreenlet_API[PyGreenlet_New_NUM])

/*
 * PyGreenlet_GetCurrent(void)
 *
 * greenlet.getcurrent()
 */
#    define PyGreenlet_GetCurrent \
        (*(PyGreenlet * (*)(void)) _PyGreenlet_API[PyGreenlet_GetCurrent_NUM])

/*
 * PyGreenlet_Throw(
 *         PyGreenlet *greenlet,
 *         PyObject *typ,
 *         PyObject *val,
 *         PyObject *tb)
 *
 * g.throw(...)
 */
#    define PyGreenlet_Throw                 \
        (*(PyObject * (*)(PyGreenlet * self, \
                          PyObject * typ,    \
                          PyObject * val,    \
                          PyObject * tb))    \
             _PyGreenlet_API[PyGreenlet_Throw_NUM])

/*
 * PyGreenlet_Switch(PyGreenlet *greenlet, PyObject *args)
 *
 * g.switch(*args, **kwargs)
 */
#    define PyGreenlet_Switch                                              \
        (*(PyObject *                                                      \
           (*)(PyGreenlet * greenlet, PyObject * args, PyObject * kwargs)) \
             _PyGreenlet_API[PyGreenlet_Switch_NUM])

/*
 * PyGreenlet_SetParent(PyObject *greenlet, PyObject *new_parent)
 *
 * g.parent = new_parent
 */
#    define PyGreenlet_SetParent                                 \
        (*(int (*)(PyGreenlet * greenlet, PyGreenlet * nparent)) \
             _PyGreenlet_API[PyGreenlet_SetParent_NUM])

/*
 * PyGreenlet_GetParent(PyObject* greenlet)
 *
 * return greenlet.parent;
 *
 * This could return NULL even if there is no exception active.
 * If it does not return NULL, you are responsible for decrementing the
 * reference count.
 */
#     define PyGreenlet_GetParent                                    \
    (*(PyGreenlet* (*)(PyGreenlet*))                                 \
     _PyGreenlet_API[PyGreenlet_GET_PARENT_NUM])

/*
 * deprecated, undocumented alias.
 */
#     define PyGreenlet_GET_PARENT PyGreenlet_GetParent

#     define PyGreenlet_MAIN                                         \
    (*(int (*)(PyGreenlet*))                                         \
     _PyGreenlet_API[PyGreenlet_MAIN_NUM])

#     define PyGreenlet_STARTED                                      \
    (*(int (*)(PyGreenlet*))                                         \
     _PyGreenlet_API[PyGreenlet_STARTED_NUM])

#     define PyGreenlet_ACTIVE                                       \
    (*(int (*)(PyGreenlet*))                                         \
     _PyGreenlet_API[PyGreenlet_ACTIVE_NUM])




/* Macro that imports greenlet and initializes C API */
/* NOTE: This has actually moved to ``greenlet._greenlet._C_API``, but we
   keep the older definition to be sure older code that might have a copy of
   the header still works. */
#    define PyGreenlet_Import()                                               \
        {                                                                     \
            _PyGreenlet_API = (void**)PyCapsule_Import("greenlet._C_API", 0); \
        }

#endif /* GREENLET_MODULE */

#ifdef __cplusplus
}
#endif
#endif /* !Py_GREENLETOBJECT_H */

================
File: venv_py310/share/man/man1/isympy.1
================
'\" -*- coding: us-ascii -*-
.if \n(.g .ds T< \\FC
.if \n(.g .ds T> \\F[\n[.fam]]
.de URL
\\$2 \(la\\$1\(ra\\$3
..
.if \n(.g .mso www.tmac
.TH isympy 1 2007-10-8 "" ""
.SH NAME
isympy \- interactive shell for SymPy
.SH SYNOPSIS
'nh
.fi
.ad l
\fBisympy\fR \kx
.if (\nx>(\n(.l/2)) .nr x (\n(.l/5)
'in \n(.iu+\nxu
[\fB-c\fR | \fB--console\fR] [\fB-p\fR ENCODING | \fB--pretty\fR ENCODING] [\fB-t\fR TYPE | \fB--types\fR TYPE] [\fB-o\fR ORDER | \fB--order\fR ORDER] [\fB-q\fR | \fB--quiet\fR] [\fB-d\fR | \fB--doctest\fR] [\fB-C\fR | \fB--no-cache\fR] [\fB-a\fR | \fB--auto\fR] [\fB-D\fR | \fB--debug\fR] [
-- | PYTHONOPTIONS]
'in \n(.iu-\nxu
.ad b
'hy
'nh
.fi
.ad l
\fBisympy\fR \kx
.if (\nx>(\n(.l/2)) .nr x (\n(.l/5)
'in \n(.iu+\nxu
[
{\fB-h\fR | \fB--help\fR}
|
{\fB-v\fR | \fB--version\fR}
]
'in \n(.iu-\nxu
.ad b
'hy
.SH DESCRIPTION
isympy is a Python shell for SymPy. It is just a normal python shell
(ipython shell if you have the ipython package installed) that executes
the following commands so that you don't have to:
.PP
.nf
\*(T<
>>> from __future__ import division
>>> from sympy import *
>>> x, y, z = symbols("x,y,z")
>>> k, m, n = symbols("k,m,n", integer=True)
    \*(T>
.fi
.PP
So starting isympy is equivalent to starting python (or ipython) and
executing the above commands by hand. It is intended for easy and quick
experimentation with SymPy. For more complicated programs, it is recommended
to write a script and import things explicitly (using the "from sympy
import sin, log, Symbol, ..." idiom).
.SH OPTIONS
.TP
\*(T<\fB\-c \fR\*(T>\fISHELL\fR, \*(T<\fB\-\-console=\fR\*(T>\fISHELL\fR
Use the specified shell (python or ipython) as
console backend instead of the default one (ipython
if present or python otherwise).

Example: isympy -c python

\fISHELL\fR could be either
\&'ipython' or 'python'
.TP
\*(T<\fB\-p \fR\*(T>\fIENCODING\fR, \*(T<\fB\-\-pretty=\fR\*(T>\fIENCODING\fR
Setup pretty printing in SymPy. By default, the most pretty, unicode
printing is enabled (if the terminal supports it). You can use less
pretty ASCII printing instead or no pretty printing at all.

Example: isympy -p no

\fIENCODING\fR must be one of 'unicode',
\&'ascii' or 'no'.
.TP
\*(T<\fB\-t \fR\*(T>\fITYPE\fR, \*(T<\fB\-\-types=\fR\*(T>\fITYPE\fR
Setup the ground types for the polys. By default, gmpy ground types
are used if gmpy2 or gmpy is installed, otherwise it falls back to python
ground types, which are a little bit slower. You can manually
choose python ground types even if gmpy is installed (e.g., for testing purposes).

Note that sympy ground types are not supported, and should be used
only for experimental purposes.

Note that the gmpy1 ground type is primarily intended for testing; it the
use of gmpy even if gmpy2 is available.

This is the same as setting the environment variable
SYMPY_GROUND_TYPES to the given ground type (e.g.,
SYMPY_GROUND_TYPES='gmpy')

The ground types can be determined interactively from the variable
sympy.polys.domains.GROUND_TYPES inside the isympy shell itself.

Example: isympy -t python

\fITYPE\fR must be one of 'gmpy',
\&'gmpy1' or 'python'.
.TP
\*(T<\fB\-o \fR\*(T>\fIORDER\fR, \*(T<\fB\-\-order=\fR\*(T>\fIORDER\fR
Setup the ordering of terms for printing. The default is lex, which
orders terms lexicographically (e.g., x**2 + x + 1). You can choose
other orderings, such as rev-lex, which will use reverse
lexicographic ordering (e.g., 1 + x + x**2).

Note that for very large expressions, ORDER='none' may speed up
printing considerably, with the tradeoff that the order of the terms
in the printed expression will have no canonical order

Example: isympy -o rev-lax

\fIORDER\fR must be one of 'lex', 'rev-lex', 'grlex',
\&'rev-grlex', 'grevlex', 'rev-grevlex', 'old', or 'none'.
.TP
\*(T<\fB\-q\fR\*(T>, \*(T<\fB\-\-quiet\fR\*(T>
Print only Python's and SymPy's versions to stdout at startup, and nothing else.
.TP
\*(T<\fB\-d\fR\*(T>, \*(T<\fB\-\-doctest\fR\*(T>
Use the same format that should be used for doctests. This is
equivalent to '\fIisympy -c python -p no\fR'.
.TP
\*(T<\fB\-C\fR\*(T>, \*(T<\fB\-\-no\-cache\fR\*(T>
Disable the caching mechanism. Disabling the cache may slow certain
operations down considerably. This is useful for testing the cache,
or for benchmarking, as the cache can result in deceptive benchmark timings.

This is the same as setting the environment variable SYMPY_USE_CACHE
to 'no'.
.TP
\*(T<\fB\-a\fR\*(T>, \*(T<\fB\-\-auto\fR\*(T>
Automatically create missing symbols. Normally, typing a name of a
Symbol that has not been instantiated first would raise NameError,
but with this option enabled, any undefined name will be
automatically created as a Symbol. This only works in IPython 0.11.

Note that this is intended only for interactive, calculator style
usage. In a script that uses SymPy, Symbols should be instantiated
at the top, so that it's clear what they are.

This will not override any names that are already defined, which
includes the single character letters represented by the mnemonic
QCOSINE (see the "Gotchas and Pitfalls" document in the
documentation). You can delete existing names by executing "del
name" in the shell itself. You can see if a name is defined by typing
"'name' in globals()".

The Symbols that are created using this have default assumptions.
If you want to place assumptions on symbols, you should create them
using symbols() or var().

Finally, this only works in the top level namespace. So, for
example, if you define a function in isympy with an undefined
Symbol, it will not work.
.TP
\*(T<\fB\-D\fR\*(T>, \*(T<\fB\-\-debug\fR\*(T>
Enable debugging output. This is the same as setting the
environment variable SYMPY_DEBUG to 'True'. The debug status is set
in the variable SYMPY_DEBUG within isympy.
.TP
-- \fIPYTHONOPTIONS\fR
These options will be passed on to \fIipython (1)\fR shell.
Only supported when ipython is being used (standard python shell not supported).

Two dashes (--) are required to separate \fIPYTHONOPTIONS\fR
from the other isympy options.

For example, to run iSymPy without startup banner and colors:

isympy -q -c ipython -- --colors=NoColor
.TP
\*(T<\fB\-h\fR\*(T>, \*(T<\fB\-\-help\fR\*(T>
Print help output and exit.
.TP
\*(T<\fB\-v\fR\*(T>, \*(T<\fB\-\-version\fR\*(T>
Print isympy version information and exit.
.SH FILES
.TP
\*(T<\fI${HOME}/.sympy\-history\fR\*(T>
Saves the history of commands when using the python
shell as backend.
.SH BUGS
The upstreams BTS can be found at \(lahttps://github.com/sympy/sympy/issues\(ra
Please report all bugs that you find in there, this will help improve
the overall quality of SymPy.
.SH "SEE ALSO"
\fBipython\fR(1), \fBpython\fR(1)

================
File: venv_py310/pyvenv.cfg
================
home = /Users/charleshoward/.pyenv/versions/3.10.16/bin
include-system-site-packages = false
version = 3.10.16

================
File: .dockerignore
================
# Version control
.git
.gitignore

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg
venv/
venv_py310/

# Data directories
data/
uploads/
chroma_db/

# IDE files
.idea/
.vscode/
*.swp
*.swo

# Logs
logs/
*.log

# Test files
.coverage
htmlcov/
.pytest_cache/

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
.env

# IDE
.idea/
.vscode/
*.swp
*.swo

# Application specific
uploads/
chroma_db/
.env
mem0_data/

# OS specific
.DS_Store
Thumbs.db

================
File: alembic.ini
================
# A generic, single database configuration.

[alembic]
# path to migration scripts
script_location = alembic

# template used to generate migration files
# file_template = %%(rev)s_%%(slug)s

# timezone to use when rendering the date
# within the migration file as well as the filename.
# string value is passed to dateutil.tz.gettz()
# leave blank for localtime
# timezone =

# max length of characters to apply to the
# "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; this defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path
# version_locations = %(here)s/bar %(here)s/bat alembic/versions

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = postgresql://postgres:postgres@localhost:5432/metis_rag

[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks=black
# black.type=console_scripts
# black.entrypoint=black
# black.options=-l 79

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

================
File: Fixed_SQLAlchemy_Models_Code.md
================
# Fixed SQLAlchemy Models Code

Below is the corrected version of the `app/db/models.py` file with the metadata naming conflicts resolved:

```python
import uuid
from datetime import datetime
from sqlalchemy import (
    Column, Integer, String, Text, Float, Boolean, 
    DateTime, ForeignKey, JSON, Table, Index, UniqueConstraint
)
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import relationship

from app.db.session import Base

# Association table for document-tag many-to-many relationship
document_tags = Table(
    'document_tags',
    Base.metadata,
    Column('document_id', UUID(as_uuid=True), ForeignKey('documents.id'), primary_key=True),
    Column('tag_id', Integer, ForeignKey('tags.id'), primary_key=True),
    Column('added_at', DateTime, default=datetime.utcnow)
)

class Document(Base):
    """Document model for database"""
    __tablename__ = "documents"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    filename = Column(String, nullable=False)
    content = Column(Text, nullable=True)  # Can be null if we only store metadata
    doc_metadata = Column(JSONB, default={})  # Changed from 'metadata' to 'doc_metadata'
    folder = Column(String, ForeignKey('folders.path'), default="/")
    uploaded = Column(DateTime, default=datetime.utcnow)
    processing_status = Column(String, default="pending")  # pending, processing, completed, failed
    processing_strategy = Column(String, nullable=True)
    file_size = Column(Integer, nullable=True)
    file_type = Column(String, nullable=True)
    last_accessed = Column(DateTime, default=datetime.utcnow)

    # Relationships
    chunks = relationship("Chunk", back_populates="document", cascade="all, delete-orphan")
    tags = relationship("Tag", secondary=document_tags, back_populates="documents")
    folder_rel = relationship("Folder", back_populates="documents")
    citations = relationship("Citation", back_populates="document")

    # Indexes
    __table_args__ = (
        Index('ix_documents_filename', filename),
        Index('ix_documents_folder', folder),
        Index('ix_documents_processing_status', processing_status),
    )

    def __repr__(self):
        return f"<Document(id={self.id}, filename='{self.filename}')>"


class Chunk(Base):
    """Chunk model for database"""
    __tablename__ = "chunks"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), ForeignKey('documents.id'), nullable=False)
    content = Column(Text, nullable=False)
    chunk_metadata = Column(JSONB, default={})  # Changed from 'metadata' to 'chunk_metadata'
    index = Column(Integer, nullable=False)  # Position in the document
    embedding_quality = Column(Float, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    document = relationship("Document", back_populates="chunks")
    citations = relationship("Citation", back_populates="chunk")

    # Indexes
    __table_args__ = (
        Index('ix_chunks_document_id', document_id),
        Index('ix_chunks_document_id_index', document_id, index),
    )

    def __repr__(self):
        return f"<Chunk(id={self.id}, document_id={self.document_id}, index={self.index})>"


class Tag(Base):
    """Tag model for database"""
    __tablename__ = "tags"

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    usage_count = Column(Integer, default=0)

    # Relationships
    documents = relationship("Document", secondary=document_tags, back_populates="tags")

    # Indexes
    __table_args__ = (
        Index('ix_tags_name', name),
    )

    def __repr__(self):
        return f"<Tag(id={self.id}, name='{self.name}')>"


class Folder(Base):
    """Folder model for database"""
    __tablename__ = "folders"

    path = Column(String, primary_key=True)
    name = Column(String, nullable=False)
    parent_path = Column(String, ForeignKey('folders.path'), nullable=True)
    document_count = Column(Integer, default=0)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    documents = relationship("Document", back_populates="folder_rel")
    subfolders = relationship("Folder", 
                             backref=relationship.backref("parent", remote_side=[path]),
                             cascade="all, delete-orphan")

    # Indexes
    __table_args__ = (
        Index('ix_folders_parent_path', parent_path),
    )

    def __repr__(self):
        return f"<Folder(path='{self.path}', name='{self.name}')>"


class Conversation(Base):
    """Conversation model for database"""
    __tablename__ = "conversations"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    conv_metadata = Column(JSONB, default={})  # Changed from 'metadata' to 'conv_metadata'
    message_count = Column(Integer, default=0)

    # Relationships

================
File: metis_improved_prompt.md
================
# Metis RAG System Improved Prompt

## Context Maintenance and Memory

You are Metis, an advanced conversational assistant powered by a RAG (Retrieval-Augmented Generation) system. Your primary goal is to provide helpful, accurate, and contextually relevant responses while maintaining a natural conversation flow.

### Memory Management

1. **Short-term Memory**: Maintain an explicit short-term memory buffer for the current conversation. When a user asks you to remember specific information, store it verbatim in this buffer and confirm what you've stored.

2. **Memory Recall**: When asked to recall information from earlier in the conversation, prioritize explicitly stored items in your short-term memory buffer over retrieved documents.

3. **Memory Confirmation**: When storing information, repeat it back exactly as provided to confirm accurate storage.

4. **Memory Retrieval**: When asked to recall information, retrieve and present the exact stored items rather than general information about the system.

### Context Awareness

1. **Query Understanding**: Always parse user queries carefully to understand their explicit requests and implicit needs.

2. **Geographic Specificity**: When a user asks about specific locations (e.g., "New Zealand"), ensure all recommendations and information are relevant to those locations unless explicitly directed otherwise.

3. **Error Recovery**: If you make a mistake (such as providing information about unrelated locations), acknowledge the error directly and correct course immediately without technical explanations about your architecture.

4. **Request Tracking**: Maintain awareness of the user's original request throughout multi-turn interactions, ensuring all responses remain aligned with their initial query.

### Response Quality

1. **Relevance Check**: Before responding, verify that your answer directly addresses the user's query and stays within the established context.

2. **Minimal Technical Details**: Avoid explaining your internal workings (like RAG systems, training data, etc.) unless specifically asked about them.

3. **Conversational Continuity**: Ensure smooth transitions between topics while maintaining awareness of the conversation history.

4. **Explicit Confirmation**: When uncertain about the user's intent, ask clarifying questions rather than making assumptions.

## Example Interactions

### Memory Storage and Retrieval

User: "Remember this phrase: The sky is orange because it is made of fruit. Remember this name: Kafi Howard"

Correct Response: "I've stored these two items in my memory:
1. Phrase: 'The sky is orange because it is made of fruit'
2. Name: 'Kafi Howard'
I'll remember these exactly as you've provided them."

User: "Can you remember the two things from earlier in the chat?"

Correct Response: "Yes, earlier you asked me to remember:
1. The phrase: 'The sky is orange because it is made of fruit'
2. The name: 'Kafi Howard'"

### Geographic Context Maintenance

User: "I want you to plan a trip to New Zealand and then to Old Zealand"

Correct Response: "I'd be happy to help plan your trip to New Zealand! Regarding 'Old Zealand', I should clarify that there isn't actually a place commonly known as 'Old Zealand' in the same way there's a 'New Zealand'. 

The name 'New Zealand' (or 'Nieuw Zeeland' in Dutch) was given by Dutch explorers after the province of Zeeland in the Netherlands. So the original 'Zealand' is actually a province in the Netherlands.

Would you like me to:
1. Plan a trip to New Zealand and then to Zeeland province in the Netherlands?
2. Focus just on a comprehensive New Zealand trip?
3. Something else entirely?"

## Implementation Guidelines

1. Create a dedicated memory storage mechanism that preserves exact user inputs when requested.
2. Implement context validation to ensure responses remain relevant to established topics.
3. Develop error detection for geographic and factual inconsistencies.
4. Prioritize explicitly stored memory items over retrieved documents when responding to recall requests.
5. Establish clear acknowledgment patterns for errors and corrections.

By following these guidelines, Metis will maintain better conversational coherence, accurately remember user-provided information, and deliver more contextually appropriate responses.

================
File: Metis_RAG_Critical_Analysis_Prompt.md
================
# Prompt for Critical Analysis of Metis RAG System Architecture

```
I'm working on improving a Retrieval-Augmented Generation (RAG) system called Metis RAG. I need your help to critically analyze the system's architecture and prompt design to identify fundamental improvements.

## Current System Overview

Metis RAG is a system that:
1. Retrieves relevant documents from a vector store based on user queries
2. Formats these documents as context for an LLM
3. Generates responses using this context and a system prompt
4. Handles conversation history and user information

## Current Issues

We've identified several problems:
- Document hallucination: The system claims to have documents that don't exist
- Citation misuse: The system uses citation markers [1] even when no documents are retrieved
- Content fabrication: When no documents are found, the system makes up information instead of admitting its limitations
- Monotonous responses: When no information is available, responses are repetitive and unhelpful

## Current Implementation

The system uses multiple prompt templates:
1. A system prompt (RAG_SYSTEM_PROMPT) that provides general instructions
2. Conversation templates that format the context, conversation history, and query
3. These templates are stored in different files, making it hard to maintain consistency

The key issue is that when no documents are found, the system is instructed to say "Based on the provided documents, I don't have information about [topic]" but doesn't offer alternatives or acknowledge limitations in a helpful way.

## Current Fix Approach

Our current fix involves:
1. Updating the system prompt to be more explicit about not fabricating information
2. Modifying conversation templates to use varied phrasing when acknowledging limitations
3. Adding instructions to offer general knowledge with clear disclaimers when documents aren't available
4. Suggesting alternative queries that might yield better results

## What I Need From You

1. **Fundamental Architecture Questions**:
   - Is the multi-prompt approach (system prompt + conversation templates) fundamentally flawed?
   - Should we consolidate all prompting logic into a single source of truth?
   - Is there a better way to structure the system to prevent hallucination and citation misuse?

2. **Prompt Design Analysis**:
   - Analyze our improved system prompt (below) and suggest fundamental improvements
   - Is the approach of "acknowledge limitations, then offer general knowledge" the right one?
   - How can we balance honesty about limitations with helpfulness?

3. **Single Source of Truth**:
   - Propose a design where all prompting logic is in one place
   - How should this be structured and maintained?
   - What are the tradeoffs of this approach?

## Our Improved System Prompt

```python
RAG_SYSTEM_PROMPT = """You are a helpful assistant. Your primary role is to provide accurate information based on the documents available to you.

CORE GUIDELINES:
1. ALWAYS prioritize information from the provided documents in your responses.
2. NEVER fabricate document content or citations - only cite documents that actually exist in the context.
3. Use citations [1] ONLY when referring to specific documents that are present in the context.
4. Maintain a helpful, conversational tone while being honest about limitations.

WHEN DOCUMENTS CONTAIN INFORMATION:
- Provide clear, accurate information based on the documents.
- Use citations appropriately to reference specific documents.
- Synthesize information from multiple documents when relevant.

WHEN DOCUMENTS DON'T CONTAIN INFORMATION:
- Acknowledge the limitation using varied phrasing such as:
  * "I've searched the available documents but couldn't find information about [topic]."
  * "The documents in my knowledge base don't contain information about [topic]."
  * "I don't have document-based information about [topic]."
- THEN, you may offer general knowledge with a clear disclaimer like:
  * "However, I can provide some general information about this topic if you'd like."
  * "While I don't have specific documents on this, I can share some general knowledge about [topic] if that would be helpful."
- If appropriate, suggest alternative queries that might yield better results.

CONVERSATION HANDLING:
- Remember context from previous messages in the conversation.
- Respond directly to the user's query without unnecessary preambles.
- Be concise but thorough in your responses.
"""
```

## Technical Implementation Details

Here's how the system currently works:

1. The system has a `rag_engine.py` file that handles the main RAG logic:
   - It retrieves documents from a vector store
   - It formats the context and conversation history
   - It calls the LLM to generate a response

2. The prompting logic is split across multiple files:
   - `app/rag/system_prompts/rag.py` contains the RAG_SYSTEM_PROMPT
   - `app/rag/system_prompts/conversation.py` contains templates for formatting the context and query
   - `app/rag/rag_generation.py` contains the logic for constructing the final prompt

3. When generating a response, the system:
   - Calls `_create_system_prompt(query)` to get the system prompt
   - Calls `_create_full_prompt(query, context, conversation_context)` to format the context and query
   - Passes both to the LLM to generate a response

4. The conversation templates contain explicit instructions like:
   ```
   If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
   ```

This fragmented approach means that instructions are duplicated and sometimes contradictory across different files, making it hard to maintain consistency and leading to the issues we're seeing.

Please think deeply about the fundamental architecture of this system and whether our approach makes sense. Don't be afraid to suggest radical changes if they would lead to a better system.

================
File: Metis_RAG_Fix_Plan_Checklist.md
================
# Metis RAG System Fix Plan Checklist

## Identified Issues

- [x] **Document Hallucination**: System claims to have documents that don't exist
- [x] **Memory Loss**: System forgets user information from earlier in conversation
- [x] **Content Fabrication**: System generates text instead of admitting it doesn't have information
- [x] **Citation Misuse**: System uses citation markers [1] that don't correspond to actual documents
- [x] **Vector Store Query Issues**: Empty search results handling needs improvement

## Fix Implementation Checklist

### 1. Improve System Prompts
- [x] **Status**: Completed | **Effort**: Medium
  - Implement modified RAG_SYSTEM_PROMPT with stronger instructions for:
    - [x] Document hallucination prevention
    - [x] Memory retention
    - [x] Content fabrication prevention
    - [x] Citation misuse prevention
  - **Note**: Implemented an improved system prompt that balances honesty with helpfulness. The prompt:
    - Clearly distinguishes between document-based information and general knowledge
    - Provides varied ways to acknowledge limitations to reduce monotony
    - Offers to provide general knowledge with clear disclaimers after acknowledging limitations
    - Suggests alternative queries that might yield better results

### 2. Enhance "No Documents Found" Handling
- [ ] **Status**: Pending | **Effort**: Low
  - [ ] Modify context formatting for empty results
  - [ ] Add prominent notice when no documents are found
  - [ ] Improve messaging for insufficient context

### 3. Improve Vector Store Empty Results Handling
- [ ] **Status**: Pending | **Effort**: Medium
  - [ ] Enhance search method to provide clearer empty results
  - [ ] Add special result that clearly indicates no documents were found
  - [ ] Improve logging for empty search results

### 4. Add User Information Extraction
- [ ] **Status**: Pending | **Effort**: High
  - [ ] Implement mechanism to extract user information from conversation
  - [ ] Add patterns to identify user names
  - [ ] Include user information in context for response generation

### 5. Fix Citation Handling
- [ ] **Status**: Pending | **Effort**: Medium
  - [ ] Modify context formatting to only include citations when documents exist
  - [ ] Ensure empty results clear the sources list
  - [ ] Improve citation formatting in responses

### 6. Add Verification Step Before Response Generation
- [ ] **Status**: Pending | **Effort**: Low
  - [ ] Add check for sources before generating response
  - [ ] Add explicit instruction to not use citations when no sources are found
  - [ ] Improve logging for verification step

### 7. Improve Vector Store Stats Checking
- [ ] **Status**: Pending | **Effort**: Low
  - [ ] Enhance check for documents in vector store
  - [ ] Add prominent notice when knowledge base is empty
  - [ ] Skip retrieval process when no documents are available

### 8. Enhance Error Handling
- [ ] **Status**: Pending | **Effort**: Medium
  - [ ] Improve error handling in query method
  - [ ] Provide clear error messages in context
  - [ ] Ensure errors don't result in citation misuse

## Testing Checklist

- [ ] **Empty Vector Store Test**
  - [ ] Clear vector store
  - [ ] Submit query
  - [ ] Verify system clearly states no documents are available

- [ ] **No Relevant Documents Test**
  - [ ] Add unrelated documents to vector store
  - [ ] Submit query
  - [ ] Verify system states no relevant documents were found

- [ ] **User Information Test**
  - [ ] Start conversation with "My name is [Name]"
  - [ ] Ask follow-up question
  - [ ] Verify system remembers and uses name

- [ ] **Document Request Test**
  - [ ] Ask about non-existent document
  - [ ] Verify system states it doesn't have that document

- [ ] **Citation Test**
  - [ ] Add relevant documents to vector store
  - [ ] Submit query
  - [ ] Verify citations only included for actual documents

## Implementation Progress

- [x] System prompt changes (Completed)
- [ ] User information extraction (Pending)
- [ ] "No documents found" handling (Pending)
- [ ] Citation handling improvements (Pending)
- [ ] Verification step before response generation (Pending)
- [ ] Error handling enhancements (Pending)
- [ ] Testing of all changes (Pending)

## Results of Testing with Improved System Prompt

We've successfully tested the improved system prompt with the following results:

1. **Varied Acknowledgment of Limitations**: Instead of the monotonous "Based on the provided documents, I don't have information about X" response, the system now uses varied phrasing:
   - "I've searched the available documents but couldn't find information about..."
   - "The documents in my knowledge base don't contain information about..."

2. **Offers General Knowledge**: The system now offers to provide general knowledge when documents aren't available:
   - "However, I can provide some general information about this topic if you'd like."

3. **More Conversational Tone**: The responses feel more natural and helpful while still maintaining honesty about limitations.

4. **No Hallucination**: The system still correctly avoids hallucinating information when no documents are available.

These improvements significantly enhance the user experience while maintaining the integrity of the RAG system.

================
File: Metis_RAG_Fix_Plan.md
================
# Metis RAG System Fix Plan

## Identified Issues

After analyzing the code and logs, I've identified several critical issues with the Metis RAG system:

### 1. Document Hallucination
- The system claims to have access to a document "Introduction to China's Provinces" when it doesn't exist
- When no documents are found, the system doesn't clearly communicate this to the user

### 2. Memory Loss
- The system forgets the user's name (Charles) despite being told earlier in the conversation
- Conversation history is not being effectively utilized

### 3. Content Fabrication
- When asked for content from a non-existent document, it generates text instead of admitting it doesn't have it
- The system is not properly handling "no documents found" scenarios

### 4. Citation Misuse
- The system uses citation markers [1] that don't correspond to actual documents
- Citations are included even when no actual documents are retrieved

### 5. Vector Store Query Issues
- The handling of empty search results in the vector store could be improved
- The "no documents found" message from the vector store isn't being prominently included in the context
- Permission filtering might be causing documents to be filtered out without clear communication to the user

## Fix Plan

### 1. Improve System Prompts

The current system prompts have good instructions, but they need to be strengthened:

```python
# Modified RAG_SYSTEM_PROMPT
RAG_SYSTEM_PROMPT = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] ONLY when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
- NEVER pretend to have documents or information that isn't in the context.
- If the context indicates no documents were found, clearly state this and DO NOT fabricate document information.
- DO NOT create fake document IDs, titles, or content.
- If asked about a specific document that isn't in the context, clearly state you don't have access to that document.
- NEVER use citation markers [1] unless actual documents are referenced in the context.

CONVERSATION HANDLING:
- IMPORTANT: Remember key user information (like names) from previous exchanges.
- When a user introduces themselves, store and use their name in future responses.
- Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.

WHEN CONTEXT IS INSUFFICIENT:
- Clearly state: "Based on the provided documents, I don't have information about [topic]."
- Be specific about what information is missing.
- Only then provide a general response based on your knowledge, and clearly state: "However, generally speaking..." to distinguish this from information in the context.
- Never pretend to have information that isn't in the context.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Only use citation markers [1] when actual documents are referenced in the context.
"""
```

### 2. Enhance "No Documents Found" Handling

Modify how the system handles cases where no documents are found to make it more explicit:

```python
# In rag_engine.py, modify the context formatting for empty results
if len(relevant_results) == 0:
    logger.warning("No sufficiently relevant documents found for the query")
    context = "IMPORTANT: NO DOCUMENTS FOUND. No sufficiently relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents."
elif len(context.strip()) < 50:  # Very short context might not be useful
    logger.warning("Context is too short to be useful")
    context = "IMPORTANT: INSUFFICIENT CONTEXT. The retrieved context is too limited to provide a comprehensive answer to your query. The system cannot provide a specific answer based on the available documents."
```

### 3. Improve Vector Store Empty Results Handling

Enhance how the vector store communicates when no results are found:

```python
# In vector_store.py, modify the search method to provide clearer empty results
if results["ids"] and len(results["ids"][0]) > 0:
    logger.info(f"Raw search results: {len(results['ids'][0])} chunks found")
    
    # ... existing code for processing results ...
    
else:
    logger.warning(f"No results found for query: {query[:50]}...")
    # Return a special result that clearly indicates no documents were found
    formatted_results = [{
        "chunk_id": "no_results",
        "content": "IMPORTANT: NO DOCUMENTS FOUND. The system searched for relevant documents but found none that match your query.",
        "metadata": {"document_id": "no_results", "filename": "No Results", "tags": [], "folder": "/"},
        "distance": 1.0  # Maximum distance (least relevant)
    }]
```

### 4. Add User Information Extraction

Implement a mechanism to extract and track important user information from the conversation:

```python
# In rag_engine.py, add user information extraction
def _extract_user_info(self, conversation_history):
    """Extract important user information from conversation history"""
    user_info = {}
    
    # Extract user name
    name_patterns = [
        r"my name is (\w+)",
        r"I am (\w+)",
        r"I'm (\w+)",
        r"call me (\w+)"
    ]
    
    for message in conversation_history:
        if message.role == "user":
            for pattern in name_patterns:
                match = re.search(pattern, message.content, re.IGNORECASE)
                if match:
                    user_info["name"] = match.group(1)
    
    return user_info

# Then in the query method, add this to the context
user_info = self._extract_user_info(conversation_history) if conversation_history else {}
if user_info:
    user_context = f"USER INFORMATION: "
    if "name" in user_info:
        user_context += f"The user's name is {user_info['name']}. "
    
    # Add user context to the beginning of the full prompt
    full_prompt = user_context + "\n\n" + full_prompt
```

### 5. Fix Citation Handling

Modify how citations are included in responses to ensure they only appear when actual documents are referenced:

```python
# In rag_engine.py, modify the context formatting to only include citations when documents exist
if search_results and len(relevant_results) > 0:
    # Process search results and format context with citations
    context_pieces = []
    
    for i, result in enumerate(relevant_results):
        # ... existing code for formatting context pieces ...
        
        # Format the context piece with metadata and citation marker
        context_piece = f"[{i+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
        context_pieces.append(context_piece)
        
        # ... existing code for tracking sources ...
    
    # Join all context pieces
    context = "\n\n".join(context_pieces)
else:
    # No results or no relevant results
    context = "IMPORTANT: NO RELEVANT DOCUMENTS FOUND. The system searched for relevant documents but found none that match your query."
    # Empty the sources list to ensure no citations are included
    sources = []
```

### 6. Add Verification Step Before Response Generation

Add a verification step that checks if any documents were actually retrieved before generating a response:

```python
# In rag_engine.py, add a verification step before generating a response
# After retrieving context but before generating a response
if not sources:
    logger.warning("No sources found for query, adding explicit instruction to not use citations")
    # Add an explicit instruction to not use citations when no sources are found
    full_prompt += "\n\nIMPORTANT: No documents were found for this query. DO NOT use citation markers [1] in your response and clearly state that no relevant documents were found."
```

### 7. Improve Vector Store Stats Checking

Enhance how the system checks if there are any documents in the vector store:

```python
# In rag_engine.py, improve the check for documents in the vector store
# Check if there are any documents in the vector store
stats = self.vector_store.get_stats()
if stats["count"] == 0:
    logger.warning("RAG is enabled but no documents are available in the vector store")
    # Add a prominent note to the context that no documents are available
    context = "IMPORTANT: NO DOCUMENTS AVAILABLE. The knowledge base is empty. Please upload documents to use RAG effectively."
    # Empty the sources list to ensure no citations are included
    sources = []
    # Skip the retrieval process entirely
    return context, sources, []
```

### 8. Enhance Error Handling

Improve error handling to ensure errors are properly communicated to the user:

```python
# In rag_engine.py, enhance error handling in the query method
try:
    # ... existing code ...
except Exception as e:
    logger.error(f"Error in RAG retrieval: {str(e)}")
    # Provide a clear error message in the context
    context = f"IMPORTANT: RETRIEVAL ERROR. An error occurred during document retrieval: {str(e)}. The system cannot provide a specific answer based on the available documents."
    # Empty the sources list to ensure no citations are included
    sources = []
    # Continue with response generation using the error context
```

## Testing Plan

1. **Empty Vector Store Test**
   - Clear the vector store
   - Submit a query
   - Verify the system clearly states no documents are available

2. **No Relevant Documents Test**
   - Add documents to the vector store that are unrelated to the query
   - Submit a query
   - Verify the system clearly states no relevant documents were found

3. **User Information Test**
   - Start a conversation with "My name is [Name]"
   - Ask a follow-up question
   - Verify the system remembers and uses the name

4. **Document Request Test**
   - Ask about a specific document that doesn't exist
   - Verify the system clearly states it doesn't have that document

5. **Citation Test**
   - Add relevant documents to the vector store
   - Submit a query
   - Verify citations are only included when actual documents are referenced

## Implementation Strategy

1. Implement the system prompt changes first, as this is the simplest change with potentially significant impact
2. Add the user information extraction functionality
3. Enhance the "no documents found" handling in both the vector store and RAG engine
4. Implement the citation handling improvements
5. Add the verification step before response generation
6. Enhance error handling
7. Test each change incrementally to ensure it resolves the specific issue it targets

## Conclusion

The issues in the Metis RAG system stem from a combination of factors:
1. The LLM not strictly following the system prompt instructions
2. Insufficient handling of "no documents found" scenarios
3. Lack of user information tracking in conversation history
4. Improper citation handling

By implementing the changes outlined in this plan, we can significantly improve the system's accuracy, honesty, and user experience. The key is to make the instructions more explicit, improve how empty results are handled, and ensure the system only includes citations when actual documents are referenced.

================
File: Metis_RAG_Improved_System_Prompt.md
================
# Metis RAG Improved System Prompt

After analyzing the test results from both the original complex system prompt and the simplified system prompt, I've developed an improved version that maintains honesty while enhancing user experience.

## Current Simplified System Prompt

```python
RAG_SYSTEM_PROMPT = """You are a helpful assistant. Refer to the provided information when it is present to give your answer.

Basic Guidelines:
1. Use information from the provided context when available
2. If no relevant information is found, simply state that you don't have information on the topic
3. Don't make up information that isn't in the context
4. Use citations [1] when referring to specific documents
5. Be clear and helpful in your responses
"""
```

## Improved System Prompt

```python
RAG_SYSTEM_PROMPT = """You are a helpful assistant. Your primary role is to provide accurate information based on the documents available to you.

CORE GUIDELINES:
1. ALWAYS prioritize information from the provided documents in your responses.
2. NEVER fabricate document content or citations - only cite documents that actually exist in the context.
3. Use citations [1] ONLY when referring to specific documents that are present in the context.
4. Maintain a helpful, conversational tone while being honest about limitations.

WHEN DOCUMENTS CONTAIN INFORMATION:
- Provide clear, accurate information based on the documents.
- Use citations appropriately to reference specific documents.
- Synthesize information from multiple documents when relevant.

WHEN DOCUMENTS DON'T CONTAIN INFORMATION:
- Acknowledge the limitation using varied phrasing such as:
  * "I've searched the available documents but couldn't find information about [topic]."
  * "The documents in my knowledge base don't contain information about [topic]."
  * "I don't have document-based information about [topic]."
- THEN, you may offer general knowledge with a clear disclaimer like:
  * "However, I can provide some general information about this topic if you'd like."
  * "While I don't have specific documents on this, I can share some general knowledge about [topic] if that would be helpful."
- If appropriate, suggest alternative queries that might yield better results.

CONVERSATION HANDLING:
- Remember context from previous messages in the conversation.
- Respond directly to the user's query without unnecessary preambles.
- Be concise but thorough in your responses.
"""
```

## Key Improvements

1. **Maintains Honesty**: Still clearly distinguishes between document-based information and general knowledge.

2. **Reduces Monotony**: Provides multiple ways to acknowledge limitations rather than using the same phrase repeatedly.

3. **Offers Helpful Alternatives**: After acknowledging limitations, offers to provide general knowledge with clear disclaimers.

4. **Suggests Alternative Queries**: When appropriate, guides users toward queries that might have better document coverage.

5. **Conversational Tone**: Maintains a helpful, conversational tone while still being honest about limitations.

## Expected Outcomes

With this improved prompt, the system should:

1. Maintain the integrity of the RAG system by clearly distinguishing between document-based information and general knowledge.

2. Provide a better user experience by offering helpful alternatives when documents don't contain relevant information.

3. Avoid the monotony of identical responses when information isn't available.

4. Build user trust through consistent honesty about the system's limitations.

5. Guide users toward more productive interactions with the system.

This balanced approach addresses the issues identified in both the original complex prompt (which led to hallucination and citation misuse) and the simplified prompt (which was honest but potentially frustrating in its limitations).

================
File: Metis_RAG_Memory_Implementation_Plan.md
================
# Metis RAG Memory Implementation Plan

## Overview

This document outlines the implementation plan for adding explicit memory capabilities to the Metis RAG system. The implementation focuses on two key aspects:

1. **Memory Buffer**: A dedicated storage mechanism for user-provided information that should be remembered verbatim.
2. **Response Format Simplification**: Streamlining the response generation process to maintain better context awareness.

## Implementation Components

### 1. Database Schema

We've created a new `memories` table with the following schema:

```sql
CREATE TABLE memories (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    label VARCHAR(50) NOT NULL DEFAULT 'explicit_memory',
    created_at TIMESTAMP NOT NULL DEFAULT now()
);

CREATE INDEX ix_memories_conversation_id ON memories(conversation_id);
CREATE INDEX ix_memories_label ON memories(label);
```

This table stores explicit memories with:
- A unique identifier
- Reference to the conversation
- The actual content to remember
- A label for categorization
- Creation timestamp

### 2. Memory Model

The `Memory` model represents the database entity:

```python
class Memory(Base):
    """Memory model for storing explicit memories"""
    __tablename__ = "memories"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    conversation_id = Column(UUID(as_uuid=True), ForeignKey("conversations.id", ondelete="CASCADE"), nullable=False)
    content = Column(Text, nullable=False)
    label = Column(String(50), nullable=False, server_default="explicit_memory")
    created_at = Column(DateTime, nullable=False, server_default=text("now()"))
```

### 3. Memory Repository

A dedicated repository for memory operations:

```python
class MemoryRepository(BaseRepository[Memory]):
    """Repository for Memory model"""
    
    async def create_memory(self, conversation_id: UUID, content: str, label: str = "explicit_memory") -> Memory:
        """Create a new memory"""
        memory_data = {
            "conversation_id": conversation_id,
            "content": content,
            "label": label
        }
        return await self.create(memory_data)
    
    async def get_memories_by_conversation(self, conversation_id: UUID, label: Optional[str] = None, limit: int = 10) -> List[Memory]:
        """Get memories for a conversation"""
        # Implementation details...
```

### 4. Memory Buffer Module

The core functionality is implemented in `memory_buffer.py`:

```python
async def add_to_memory_buffer(conversation_id: UUID, content: str, label: str = "explicit_memory", db: AsyncSession = None) -> Memory:
    """Add content to the memory buffer"""
    # Implementation details...

async def get_memory_buffer(conversation_id: UUID, search_term: Optional[str] = None, label: Optional[str] = None, limit: int = 10, db: AsyncSession = None) -> List[Memory]:
    """Get memories from the buffer"""
    # Implementation details...

async def process_query(query: str, user_id: str, conversation_id: UUID, db: AsyncSession = None) -> tuple[str, Optional[str], Optional[str]]:
    """Process a query for memory commands before sending to RAG"""
    # Implementation details...
```

### 5. RAG Engine Integration

The RAG engine has been updated to:

1. Process memory commands in user queries
2. Handle memory storage and retrieval
3. Incorporate memory responses into the generated content
4. Support both streaming and non-streaming responses

Key integration points:
- Query preprocessing to detect memory commands
- Early return for memory recall operations
- Appending memory confirmations to responses

### 6. Migration and Testing

We've created:
1. A migration script to add the memories table
2. A test script to verify memory buffer functionality

## Usage Examples

### Storing a Memory

User: "Remember this phrase: The sky is orange because it is made of fruit."

System:
1. Detects the memory command
2. Extracts "The sky is orange because it is made of fruit"
3. Stores it in the memory buffer
4. Confirms storage: "I've stored this in my memory: 'The sky is orange because it is made of fruit'"

### Retrieving a Memory

User: "Can you remember the phrase from earlier in the chat?"

System:
1. Detects the recall command
2. Searches the memory buffer for the conversation
3. Returns: "Here's what I remember: 1. The sky is orange because it is made of fruit"

## Implementation Benefits

1. **Explicit Memory Storage**: Stores user-provided information verbatim, ensuring accurate recall.
2. **Context Maintenance**: Preserves conversation context across multiple turns.
3. **Geographic Specificity**: Better maintains location context through explicit memory.
4. **Error Recovery**: Provides clear mechanisms for correcting misunderstandings.
## Implementation Status

The memory buffer implementation has been completed and tested successfully. The following components have been implemented:

1. ✅ Memory model and database schema
2. ✅ Memory repository for database operations
3. ✅ Memory buffer module for core functionality
4. ✅ RAG engine integration for processing memory commands
5. ✅ Migration script for database updates
6. ✅ Test script for functionality verification

## Usage Instructions

### For Developers

1. **Database Migration**:
   ```bash
   python scripts/create_tables.py
   ```
   This will create the memories table in the database.

2. **Testing the Implementation**:
   ```bash
   python scripts/test_memory_buffer.py
   ```
   This will run tests to verify the memory buffer functionality.

3. **Integration with Existing Code**:
   - Import the memory buffer functions in your code:
     ```python
     from app.rag.memory_buffer import process_query, add_to_memory_buffer, get_memory_buffer
     ```
   - Process user queries through the `process_query` function before sending them to the RAG engine.

### For Users

Users can interact with the memory buffer using natural language commands:

1. **Storing Information**:
   - "Remember this: [information to store]"
   - "Remember this phrase: [phrase to remember]"
   - "Remember this name: [name to remember]"

2. **Retrieving Information**:
   - "Recall what I asked you to remember"
   - "What did I ask you to remember earlier?"
   - "Can you remember the [phrase/name/information] from earlier?"

## Future Enhancements

1. **Memory Categorization**:
   - Implement automatic categorization of memories
   - Allow users to specify categories when storing memories

2. **Memory Expiration**:
   - Add time-based expiration for memories
   - Implement priority-based memory retention

3. **Advanced Search**:
   - Semantic search for memories
   - Fuzzy matching for memory retrieval
   - Context-aware memory prioritization
   - Memory search improvements

================
File: Metis_RAG_Response_Format_Analysis.md
================
# Metis RAG Response Format Analysis

## Current Implementation Overview

The current implementation for handling responses in the Metis RAG system involves several layers of processing, particularly for streaming responses. Here's a breakdown of the key components:

### 1. API Layer (`app/api/chat.py`)

- For streaming responses, the system:
  - Creates an event generator that yields tokens
  - Sends the conversation ID as a separate event
  - Streams tokens from the RAG engine
  - Accumulates the full response
  - Adds the complete message to the conversation history after streaming
  - Adds citations if available

- For non-streaming responses:
  - Gets the complete response from the RAG engine
  - Adds the message to the conversation history
  - Returns the response with citations

### 2. RAG Engine Layer (`app/rag/rag_engine.py`)

- For streaming responses:
  - Creates a wrapper for the stream that applies text normalization
  - Returns a dictionary with "query", "stream", and "sources"

- For non-streaming responses:
  - Gets cached or generates a new response
  - Processes the response text with normalization
  - Returns a dictionary with "query", "answer", and "sources"

### 3. Generation Layer (`app/rag/rag_generation.py`)

- The `_generate_streaming_response` method:
  - Gets a stream from the Ollama client
  - Maintains a buffer for text normalization
  - Applies normalization when complete sentences are detected
  - Handles both string chunks and dictionary chunks
  - Yields normalized chunks

- The `_process_response_text` method:
  - Applies text normalization to improve formatting
  - Formats code blocks

### 4. Text Processing Layer (`app/utils/text_processor.py`)

- The `normalize_text` function:
  - Fixes spacing around punctuation
  - Fixes apostrophes
  - Fixes hyphenation
  - Fixes function names with spaces
  - Fixes multiple spaces

- The `format_code_blocks` function:
  - Identifies code blocks
  - Fixes function and variable names with spaces

## Issues and Potential Improvements

### 1. Overly Complex Streaming Implementation

The current streaming implementation is unnecessarily complex:

- It maintains a buffer for normalization, which can introduce latency
- It applies normalization conditionally based on sentence endings
- It handles both string and dictionary chunks, adding complexity
- The normalization process itself is quite involved for streaming text

### 2. Redundant Text Processing

- Text normalization is applied at multiple levels:
  - During streaming in `_generate_streaming_response`
  - After complete response generation in `_process_response_text`
  - This redundancy can lead to inconsistent formatting and potential bugs

### 3. Backward Compatibility Complexity

- The code handles both string chunks and dictionary chunks for backward compatibility
- This adds complexity and potential for errors
- It's unclear if both formats are still needed

### 4. Buffer Management Issues

- The buffer management for normalization can lead to:
  - Delayed token delivery to the client
  - Potential loss of tokens if the buffer isn't flushed properly
  - Inconsistent chunking of the response

### 5. Excessive Normalization

- The normalization process is quite aggressive:
  - It modifies spacing around punctuation
  - It changes apostrophes
  - It modifies hyphenation
  - These changes might not be necessary and could alter the intended formatting

## Recommendations

### 1. Simplify Streaming Implementation

- Remove the buffer-based normalization during streaming
- Stream tokens directly from the LLM to the client
- Apply minimal necessary processing to each token
- Ensure the buffer is properly flushed at the end of streaming

### 2. Consolidate Text Processing

- Apply normalization only once, preferably after the complete response is generated
- For streaming, consider applying minimal or no normalization
- Ensure consistent formatting between streaming and non-streaming responses

### 3. Standardize Response Format

- Use a consistent format for response chunks (either string or dictionary)
- Remove backward compatibility handling if no longer needed
- Document the expected format clearly

### 4. Optimize Buffer Management

- If buffering is necessary, ensure it's minimal and efficient
- Implement proper flushing mechanisms
- Consider token-based rather than sentence-based buffering

### 5. Reconsider Normalization Approach

- Evaluate which normalization steps are truly necessary
- Consider making normalization configurable
- Preserve original formatting where appropriate

## Implementation Plan

### Phase 1: Simplify Streaming Implementation

1. Modify `_generate_streaming_response` to stream tokens directly without buffering
2. Remove conditional normalization during streaming
3. Ensure proper handling of the final token

### Phase 2: Consolidate Text Processing

1. Move all normalization to a single point in the process
2. Create a consistent approach for both streaming and non-streaming responses
3. Update documentation to reflect the changes

### Phase 3: Testing and Validation

1. Test with various response types and formats
2. Verify streaming performance and latency
3. Ensure formatting consistency across different response modes

By implementing these changes, the Metis RAG system should have a more streamlined, consistent, and maintainable approach to response formatting, particularly for streaming responses.

================
File: Metis_RAG_Response_Format_Simplified_Plan.md
================
# Metis RAG Response Format Simplified Implementation Plan

This plan outlines a simplified approach to response formatting in the Metis RAG system, designed to integrate with the memory implementation plan we've already created.

## Goals

1. Simplify the streaming response implementation
2. Consolidate text processing to a single point
3. Standardize response formats
4. Optimize buffer management
5. Integrate with the memory implementation plan

## Implementation Approach

### 1. Simplified Response Structure

```mermaid
graph TD
    A[User Query] --> B[Process Query]
    B --> C{Streaming?}
    C -->|Yes| D[Direct Token Streaming]
    C -->|No| E[Complete Response Generation]
    D --> F[Client]
    E --> G[Optional Normalization]
    G --> F
    F --> H[Store in Conversation History]
    H --> I[Store in Memory Buffer if Requested]
```

### 2. Streaming Implementation Simplification

1. **Direct Token Streaming**: Stream tokens directly from the LLM to the client without intermediate buffering
2. **Minimal Processing**: Apply only essential processing to each token
3. **Final Token Handling**: Ensure proper handling of the final token

### 3. Text Processing Consolidation

1. **Single Normalization Point**: Apply normalization only once, after complete response generation
2. **Configurable Normalization**: Make normalization optional and configurable
3. **Consistent Formatting**: Ensure consistent formatting between streaming and non-streaming responses

### 4. Memory Integration

1. **Memory Command Detection**: Detect memory commands in the user query before processing
2. **Memory Buffer Storage**: Store explicit memories in the memory buffer
3. **Memory Confirmation**: Provide confirmation when information is stored
4. **Memory Retrieval**: Retrieve stored information when requested

## Technical Implementation Details

### 1. Simplified Streaming Response Method

```python
async def generate_streaming_response(
    self,
    prompt: str,
    model: str,
    system_prompt: str,
    model_parameters: Dict[str, Any]
) -> AsyncGenerator[str, None]:
    """
    Generate a streaming response with minimal processing
    
    Args:
        prompt: Full prompt
        model: Model to use
        system_prompt: System prompt
        model_parameters: Model parameters
        
    Returns:
        Async generator of response tokens
    """
    # Get the raw stream from the LLM
    stream = await self.ollama_client.generate(
        prompt=prompt,
        model=model,
        system_prompt=system_prompt,
        stream=True,
        parameters=model_parameters or {}
    )
    
    # Stream tokens directly with minimal processing
    async for chunk in stream:
        # Handle string chunks
        if isinstance(chunk, str):
            yield chunk
        # Handle dictionary chunks (for backward compatibility)
        elif isinstance(chunk, dict) and "response" in chunk:
            yield chunk["response"]
```

### 2. Consolidated Text Processing

```python
def process_complete_response(self, response_text: str, apply_normalization: bool = True) -> str:
    """
    Process a complete response with optional normalization
    
    Args:
        response_text: The complete response text
        apply_normalization: Whether to apply text normalization
        
    Returns:
        Processed response text
    """
    if not apply_normalization:
        return response_text
    
    # Apply text normalization
    normalized_text = normalize_text(response_text)
    
    # Format code blocks
    formatted_text = format_code_blocks(normalized_text)
    
    return formatted_text
```

### 3. Memory Command Processing Integration

```python
async def process_query(
    self,
    query: str,
    user_id: str,
    conversation_id: UUID
) -> Tuple[str, Optional[str], Optional[str]]:
    """
    Process a query for memory commands before sending to RAG
    
    Args:
        query: User query
        user_id: User ID
        conversation_id: Conversation ID
        
    Returns:
        Tuple of (processed_query, memory_response, memory_operation)
    """
    # Check for memory commands
    memory_match = re.search(r"remember\s+this(?:\s+(?:phrase|name|information))?\s*:\s*(.+)", query, re.IGNORECASE)
    if memory_match:
        content = memory_match.group(1).strip()
        
        # Store in memory buffer
        await add_to_memory_buffer(
            conversation_id=conversation_id,
            content=content,
            label="explicit_memory"
        )
        
        # Create confirmation response
        memory_response = f"I've stored this in my memory: '{content}'"
        
        # Remove the command from the query
        processed_query = query.replace(memory_match.group(0), "").strip()
        if not processed_query:
            processed_query = "Thank you for providing that information."
        
        return processed_query, memory_response, "store"
    
    # Check for recall command
    recall_match = re.search(r"recall(?:\s+(?:the|my))?\s*(?:(.+))?", query, re.IGNORECASE)
    if recall_match:
        search_term = recall_match.group(1).strip() if recall_match.group(1) else None
        
        # Retrieve from memory buffer
        memories = await get_memory_buffer(
            conversation_id=conversation_id,
            search_term=search_term
        )
        
        if memories:
            memory_items = [f"{i+1}. {memory['content']}" for i, memory in enumerate(memories)]
            memory_response = "Here's what I remember:\n" + "\n".join(memory_items)
        else:
            memory_response = "I don't have any memories stored about that."
        
        # Remove the command from the query
        processed_query = query.replace(recall_match.group(0), "").strip()
        if not processed_query:
            processed_query = "Please provide the information you'd like me to recall."
        
        return processed_query, memory_response, "recall"
    
    # No memory command found
    return query, None, None
```

### 4. Updated RAG Engine Query Method

```python
async def query(
    self,
    query: str,
    model: str = None,
    use_rag: bool = True,
    stream: bool = False,
    system_prompt: str = None,
    model_parameters: Dict[str, Any] = None,
    conversation_history: Optional[List[Message]] = None,
    metadata_filters: Optional[Dict[str, Any]] = None,
    user_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Query the RAG engine with simplified response handling
    
    Args:
        query: User query
        model: Model to use
        use_rag: Whether to use RAG
        stream: Whether to stream the response
        system_prompt: System prompt
        model_parameters: Model parameters
        conversation_history: Conversation history
        metadata_filters: Metadata filters
        user_id: User ID
        
    Returns:
        Response dictionary
    """
    try:
        start_time = time.time()
        
        # Process memory commands if user_id is provided
        processed_query = query
        memory_response = None
        memory_operation = None
        
        if user_id:
            # Extract conversation_id from conversation_history if available
            conversation_id = None
            if conversation_history and len(conversation_history) > 0:
                # Assuming the first message has the conversation_id
                conversation_id = conversation_history[0].conversation_id
            
            if conversation_id:
                processed_query, memory_response, memory_operation = await self.process_query(
                    query=query,
                    user_id=user_id,
                    conversation_id=conversation_id
                )
        
        # If it's a recall operation with a response, return immediately
        if memory_operation == "recall" and memory_response:
            return {
                "query": query,
                "answer": memory_response,
                "sources": []
            }
        
        # Use the processed query for RAG
        query = processed_query
        
        # Get model name
        model = model or self.default_model
        
        # Get full conversation context
        conversation_context = await get_conversation_context(
            conversation_history=conversation_history,
            max_tokens=self.max_context_tokens
        )
        
        # Get context from vector store if RAG is enabled
        context = ""
        sources = []
        document_ids = []
        
        if use_rag:
            # Retrieve context using the appropriate method
            context, sources, document_ids = await self._retrieve_context(
                query=query,
                conversation_context=conversation_context,
                metadata_filters=metadata_filters
            )
        
        # Create full prompt
        full_prompt = await self._create_prompt(
            query=query,
            context=context,
            conversation_context=conversation_context,
            retrieval_state="success" if context else "no_documents"
        )
        
        # Generate response
        if stream:
            # For streaming, return the stream generator directly
            stream_response = await self.generate_streaming_response(
                prompt=full_prompt,
                model=model,
                system_prompt=system_prompt or self.system_prompt,
                model_parameters=model_parameters or {}
            )
            
            # If we have a memory confirmation, prepend it to the first chunk
            if memory_response:
                # Create a wrapper generator that prepends the memory response
                async def wrapped_stream():
                    yield memory_response + "\n\n"
                    async for chunk in stream_response:
                        yield chunk
                
                stream_response = wrapped_stream()
            
            return {
                "query": query,
                "stream": stream_response,
                "sources": sources
            }
        else:
            # For non-streaming, generate the complete response
            response = await self.generate_complete_response(
                prompt=full_prompt,
                model=model,
                system_prompt=system_prompt or self.system_prompt,
                model_parameters=model_parameters or {}
            )
            
            # Process the response text
            response_text = self.process_complete_response(
                response_text=response.get("response", ""),
                apply_normalization=True
            )
            
            # If we have a memory confirmation, prepend it to the response
            if memory_response:
                response_text = f"{memory_response}\n\n{response_text}"
            
            return {
                "query": query,
                "answer": response_text,
                "sources": sources
            }
    except Exception as e:
        logger.error(f"Error in RAG engine query: {str(e)}")
        return {
            "query": query,
            "answer": f"Error: {str(e)}",
            "sources": []
        }
```

### 5. Conversation Context Retrieval

```python
async def get_conversation_context(
    conversation_history: Optional[List[Message]] = None,
    max_tokens: int = 4000
) -> str:
    """
    Get the full conversation context up to the specified token limit
    
    Args:
        conversation_history: List of conversation messages
        max_tokens: Maximum number of tokens to include
        
    Returns:
        Formatted conversation context string
    """
    if not conversation_history:
        return ""
    
    # Calculate tokens for each message
    message_tokens = []
    for msg in conversation_history:
        # Estimate token count if not already calculated
        token_count = msg.token_count or estimate_token_count(msg.content)
        message_tokens.append({
            "role": msg.role,
            "content": msg.content,
            "tokens": token_count
        })
    
    # Apply smart context window management
    formatted_messages = []
    total_tokens = 0
    
    # First, include messages with memory operations
    memory_messages = [m for m in message_tokens if contains_memory_operation(m["content"])]
    for msg in memory_messages:
        if total_tokens + msg["tokens"] <= max_tokens:
            formatted_messages.append(msg)
            total_tokens += msg["tokens"]
    
    # Then include the most recent messages
    recent_messages = [m for m in reversed(message_tokens) if m not in formatted_messages]
    for msg in recent_messages:
        if total_tokens + msg["tokens"] <= max_tokens:
            formatted_messages.insert(0, msg)  # Insert at beginning to maintain order
            total_tokens += msg["tokens"]
        else:
            break
    
    # Sort messages by original order
    formatted_messages.sort(key=lambda m: message_tokens.index(m))
    
    # Format the conversation history
    history_pieces = []
    for msg in formatted_messages:
        role_prefix = "User" if msg["role"] == "user" else "Assistant"
        history_pieces.append(f"{role_prefix}: {msg['content']}")
    
    return "\n".join(history_pieces)
```

## Integration with Memory Implementation Plan

This simplified response format implementation integrates with our memory implementation plan in the following ways:

1. **Memory Command Processing**: The `process_query` method detects and processes memory commands before sending the query to the RAG engine.

2. **Memory Buffer Integration**: The implementation uses the memory buffer methods (`add_to_memory_buffer` and `get_memory_buffer`) from our memory implementation plan.

3. **Conversation Context Prioritization**: The `get_conversation_context` method prioritizes messages with memory operations when managing the context window.

4. **Memory Response Integration**: Memory responses (confirmations and recall results) are integrated into the response flow, either by prepending to the first chunk in streaming mode or to the complete response in non-streaming mode.

## Implementation Phases

### Phase 1: Response Format Simplification

1. Implement the simplified streaming response method
2. Implement the consolidated text processing method
3. Update the RAG engine query method
4. Test with various response types and formats

### Phase 2: Memory Integration

1. Implement the memory command processing method
2. Integrate with the memory buffer methods
3. Update the conversation context retrieval method
4. Test with memory commands and conversation history

### Phase 3: Testing and Optimization

1. Test with various conversation lengths and memory operations
2. Optimize token usage and response formatting
3. Ensure consistent behavior between streaming and non-streaming modes
4. Verify memory command processing and response integration

## Implementation Checklist

### Phase 1: Response Format Simplification
- [ ] **Core Implementation**
  - [ ] Implement `generate_streaming_response` method
  - [ ] Implement `process_complete_response` method
  - [ ] Update RAG engine query method
  - [ ] Update API handlers to use the simplified methods
  
- [ ] **Testing**
  - [ ] Test streaming response performance
  - [ ] Test non-streaming response formatting
  - [ ] Verify consistent behavior between modes
  - [ ] Measure impact on response time

### Phase 2: Memory Integration
- [ ] **Core Implementation**
  - [ ] Implement `process_query` method for memory commands
  - [ ] Implement `get_conversation_context` method
  - [ ] Integrate with memory buffer methods
  - [ ] Update response handling for memory operations
  
- [ ] **Testing**
  - [ ] Test memory command detection
  - [ ] Test memory storage and retrieval
  - [ ] Test conversation context prioritization
  - [ ] Verify memory response integration

### Phase 3: Testing and Optimization
- [ ] **Performance Testing**
  - [ ] Measure token usage efficiency
  - [ ] Test with large conversation histories
  - [ ] Verify memory buffer performance
  - [ ] Optimize context window management
  
- [ ] **Integration Testing**
  - [ ] Test end-to-end conversation flows
  - [ ] Test with various memory operations
  - [ ] Verify error handling and recovery
  - [ ] Ensure consistent user experience

By implementing this simplified approach to response formatting and integrating it with our memory implementation plan, we can create a more efficient, maintainable, and user-friendly Metis RAG system.

================
File: Metis_RAG_Simplified_System_Prompt.md
================
# Metis RAG Simplified System Prompt

Based on our testing, we've found that a simplified system prompt can be effective while being much more concise. Here's a simplified version that maintains the key improvements:

```python
RAG_SYSTEM_PROMPT = """You are a helpful assistant. Your primary role is to provide accurate information based on the documents available to you.

CORE GUIDELINES:
1. ALWAYS prioritize information from the provided documents in your responses.
2. NEVER fabricate document content or citations - only cite documents that actually exist in the context.
3. Use citations [1] ONLY when referring to specific documents that are present in the context.
4. Maintain a helpful, conversational tone while being honest about limitations.

WHEN DOCUMENTS CONTAIN INFORMATION:
- Provide clear, accurate information based on the documents.
- Use citations appropriately to reference specific documents.
- Synthesize information from multiple documents when relevant.

WHEN DOCUMENTS DON'T CONTAIN INFORMATION:
- Acknowledge the limitation using varied phrasing such as:
  * "I've searched the available documents but couldn't find information about [topic]."
  * "The documents in my knowledge base don't contain information about [topic]."
  * "I don't have document-based information about [topic]."
- THEN, you may offer general knowledge with a clear disclaimer like:
  * "However, I can provide some general information about this topic if you'd like."
  * "While I don't have specific documents on this, I can share some general knowledge about [topic] if that would be helpful."
- If appropriate, suggest alternative queries that might yield better results.

CONVERSATION HANDLING:
- Remember context from previous messages in the conversation.
- Respond directly to the user's query without unnecessary preambles.
- Be concise but thorough in your responses.
"""
```

## Comparison with Original Complex Prompt

The original system prompt was 73 lines long with detailed instructions across multiple sections. This simplified version:

1. **Reduces Length**: From 73 lines to 28 lines (62% reduction)
2. **Maintains Core Functionality**: Still addresses all key issues:
   - Document hallucination prevention
   - Citation misuse prevention
   - Content fabrication prevention
   - Conversation handling

3. **Focuses on Key Behaviors**: Prioritizes the most important instructions:
   - Using document information when available
   - Acknowledging limitations honestly when documents don't contain information
   - Offering general knowledge with clear disclaimers
   - Maintaining conversation context

4. **Improves Readability**: Uses clearer organization with distinct sections for different scenarios

## Testing Results

Our testing shows that this simplified prompt produces responses that:

1. Honestly acknowledge when documents don't contain information
2. Use varied phrasing to avoid monotonous responses
3. Offer to provide general knowledge with clear disclaimers
4. Maintain a conversational, helpful tone
5. Don't hallucinate document content or misuse citations

This demonstrates that a well-structured, concise prompt can be just as effective as a longer, more detailed one, as long as it clearly communicates the core guidelines and expected behaviors.

================
File: metis_rag_testing_prompt.md
================
# Metis RAG Testing Prompt

I need help troubleshooting and testing the Metis RAG system. I've created an end-to-end test suite that includes test documents in all supported formats (PDF, TXT, CSV, MD) and comprehensive test cases for evaluating document processing, query handling, and response quality.

## Authentication Issues

I'm encountering authentication issues when running the full test suite:

1. The test attempts to authenticate using the `/api/auth/token` endpoint with the following credentials:
   - Username: `testuser`
   - Password: `testpassword`

2. The authentication fails with the error: `{"detail":"Incorrect username or password"}`

3. When the test tries to access protected endpoints like `/api/documents/upload`, it receives a 401 Unauthorized response: `{"detail":"Not authenticated"}`

4. I've verified that the user exists in the system by attempting to register the same username, which returns: `{"detail":"Username or email already exists"}`

## Current Status

- The demo test (`run_metis_rag_e2e_demo.py`) runs successfully as it simulates the document processing and query responses without requiring authentication.
- The full test (`run_metis_rag_e2e_test.py`) fails at the authentication step.

## What I Need Help With

1. **Authentication Troubleshooting**: How can I properly authenticate with the Metis RAG system? The current approach using the TestClient in FastAPI doesn't seem to maintain the authentication state.

2. **Test Client Configuration**: Is there a specific way to configure the TestClient to work with the authentication system in Metis RAG?

3. **Alternative Testing Approaches**: Are there alternative approaches to testing the system that might bypass or simplify the authentication requirements?

4. **API Endpoint Verification**: How can I verify the correct authentication endpoints and required parameters?

## System Information

- The Metis RAG system is running locally at `http://localhost:8000`
- The authentication endpoints appear to be:
  - `/api/auth/register` for user registration
  - `/api/auth/token` for obtaining access tokens
- The system uses JWT-based authentication with tokens

## Test Files Created

1. Test documents in various formats:
   - `data/test_docs/smart_home_technical_specs.pdf`
   - `data/test_docs/smart_home_user_guide.txt`
   - `data/test_docs/smart_home_device_comparison.csv`
   - `data/test_docs/smart_home_developer_reference.md`

2. Test scripts:
   - `tests/test_metis_rag_e2e.py`: Main test script
   - `tests/test_metis_rag_e2e_demo.py`: Demo version without authentication
   - `tests/utils/create_test_pdf.py`: PDF generation utility
   - `run_metis_rag_e2e_test.py`: Runner for the full test
   - `run_metis_rag_e2e_demo.py`: Runner for the demo test

Can you help me resolve these authentication issues and successfully run the full end-to-end test against the actual Metis RAG system?

================
File: pull_request_template.md
================
# Implement PromptManager for Single Source of Truth in Prompt Architecture

## Problem

The current prompt architecture has several issues:
- Fragmented prompt architecture with multiple separate prompts
- Direct message injection into the context
- Inconsistent error handling
- Redundant instructions
- Poor responses when no documents are found

## Solution

This PR implements a PromptManager class that serves as a single source of truth for all prompt-related operations:

1. **PromptManager Class**: A new class that manages templates and creates appropriate prompts based on the current state
2. **State-Based Prompt Selection**: Uses explicit state to select the appropriate prompt instead of injecting messages
3. **Clear Separation of Data and Instructions**: Keeps retrieved documents as pure data without injected messages
4. **Improved Templates**: Updated templates to be more helpful and natural, especially when no documents are found

## Changes

- Added `PromptManager` class in `app/rag/prompt_manager.py`
- Added proper initialization in `RAGEngine` class
- Updated prompt templates to be more helpful and natural
- Removed explicit mentions of missing documents unless specifically asked
- Moved unused prompt templates to the `unused_code` directory

## Testing

The changes have been successfully tested with various queries:

- **General Knowledge**: Correctly answered "There are three 'r's in the word strawberry"
- **Code Generation**: Successfully generated SVG code for a butterfly
- **Memory**: Remembered that the user asked about "strawberry" earlier
- **Conversation Context**: Remembered the user's name (Charles) after being told
- **Natural Responses**: No longer mentions "I don't have document-based information" for every response

## Next Steps After Merging

1. Monitor the system for any regressions
2. Consider adding more templates for different scenarios
3. Consider externalizing templates for easier editing

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "metis_rag"
version = "0.1.0"
description = "Metis RAG - Retrieval Augmented Generation with Ollama"
readme = "README.md"
authors = [
    {name = "Your Name", email = "your.email@example.com"},
]
license = {text = "MIT"}
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
requires-python = ">=3.10"
dependencies = [
    "fastapi>=0.104.1",
    "uvicorn>=0.23.2",
    "python-multipart>=0.0.6",
    "httpx>=0.25.0",
    "pydantic>=2.4.2",
    "langchain>=0.0.335",
    "langchain-community>=0.0.13",
    "langgraph>=0.0.20",
    "chromadb>=0.4.18",
    "python-dotenv>=1.0.0",
    "jinja2>=3.1.2",
    "sse-starlette>=1.6.5",
    "psutil>=5.9.5",
    "passlib[bcrypt]>=1.7.4",
    "python-jose[cryptography]>=3.3.0",
    "bcrypt>=4.0.1",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.1.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
    "mypy>=1.0.0",
]

[tool.setuptools]
packages = ["app"]

[tool.pytest]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"
pythonpath = ["."]

[tool.black]
line-length = 88
target-version = ["py310"]
include = '\.pyi?$'

[tool.isort]
profile = "black"
line_length = 88

[tool.mypy]
python_version = "3.10"
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
strict_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_return_any = true
warn_unreachable = true

================
File: README.md
================
# Metis RAG

Metis RAG is an application that combines conversational AI with Retrieval Augmented Generation (RAG) capabilities. It allows users to chat with large language models while providing relevant context from uploaded documents.

## Features

- Chat with large language models through Ollama
- Upload and process documents (PDF, TXT, CSV, MD)
- Retrieval Augmented Generation for contextual responses
- User authentication and access control
- Multi-user support with resource ownership
- LLM-enhanced document processing with intelligent chunking strategies
- Advanced query refinement and retrieval enhancement
- Response quality assurance with evaluation and refinement
- Factual accuracy verification and hallucination detection
- Comprehensive audit reports with source tracking
- Document management with tagging and organization
- System monitoring and analytics
- Background Task System for asynchronous processing
- Persistent chat memory across conversation turns
- Explicit memory buffer for storing and recalling user information
- Context-aware memory management with priority-based retrieval
- Responsive UI with light/dark mode
- Enhanced logging and debugging capabilities

## Authentication System

Metis RAG includes a comprehensive authentication system that provides user management and access control:

### User Management

- User registration and login
- Role-based access control (user/admin)
- JWT-based authentication
- Secure password hashing with bcrypt

### Access Control

- Resource ownership validation
- Protected API endpoints
- Route protection middleware
- Multi-user support with data isolation

### Security Features

- Token-based authentication
- Password hashing and validation
- CORS protection
- Security headers
- Route-based access control

For more details, see the [Authentication Documentation](docs/technical/AUTHENTICATION.md).

## LLM-Enhanced RAG System

Metis RAG includes an advanced LLM-enhanced system that improves critical aspects of the RAG pipeline:

### Dynamic Chunking Strategy Selection

The system uses a "Chunking Judge" (an LLM agent) to analyze documents and select the most appropriate chunking strategy and parameters:

- Analyzes document structure, content type, and formatting
- Dynamically selects between recursive, token-based, or markdown chunking strategies
- Recommends optimal chunk size and overlap parameters
- Preserves semantic meaning and document structure
- Adapts to different document types without manual configuration

### Query Refinement and Retrieval Enhancement

The system uses a "Retrieval Judge" (an LLM agent) to improve retrieval quality:

- Analyzes queries and retrieved chunks
- Refines queries to improve retrieval precision
- Evaluates relevance of retrieved chunks
- Re-ranks chunks based on relevance to the query
- Requests additional retrieval when necessary

### Response Quality Pipeline

The system includes a comprehensive response quality pipeline to ensure high-quality responses:

- **Response Synthesis**: Combines retrieval results and tool outputs into coherent responses with proper source attribution
- **Response Evaluation**: Assesses factual accuracy, completeness, relevance, and checks for hallucinations
- **Response Refinement**: Iteratively improves responses based on evaluation results
- **Audit Reporting**: Generates comprehensive audit reports with information source tracking and verification status

### Conversation Memory

The system maintains persistent memory across conversation turns:

- **Context Augmentation**: Includes conversation history in the LLM context for more coherent multi-turn dialogues
- **Memory-Aware Query Analysis**: Analyzes queries in the context of previous conversation turns
- **Memory-Enhanced Planning**: Creates query execution plans that consider conversation history
- **Contextual Response Generation**: Generates responses that maintain continuity with previous exchanges

### Quality Assurance Features

- Factual accuracy scoring and verification
- Hallucination detection and removal
- Source attribution and citation tracking
- Iterative refinement until quality thresholds are met
- Comprehensive audit trails for transparency and accountability
- LLM-based process analysis for continuous improvement

These enhancements make the RAG system more adaptable to different document types and query styles, while ensuring responses are accurate, complete, relevant, and free from hallucinations.
## Architecture

Metis RAG is built with the following technologies:

- **Backend**: FastAPI, Python
- **Authentication**: JWT-based authentication with bcrypt password hashing
- **Database**: PostgreSQL with SQLAlchemy ORM
- **Vector Database**: ChromaDB with optimized caching
- **LLM Integration**: Ollama API with enhanced prompt engineering
- **Document Processing**: LangChain with LLM-enhanced dynamic chunking strategies
- **Response Quality**: Comprehensive pipeline for synthesis, evaluation, refinement, and auditing
- **Workflow Orchestration**: LangGraph for adaptive RAG workflows
- **Process Logging**: Detailed logging and audit trail generation
- **Background Task System**: Asynchronous processing with task prioritization and resource management
- **Testing**: Comprehensive test suite for RAG functionality and response quality
- **Deployment**: Docker

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Ollama (optional, can run in Docker)

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/metis-rag.git
   cd metis-rag
   ```

2. Create a `.env` file based on `.env.example`:
   ```bash
   cp .env.example .env
   ```

   Key configuration options include:
   - `DEFAULT_MODEL`: The LLM model to use for RAG responses (default: gemma3:12b)
   - `CHUNKING_JUDGE_MODEL`: The model to use for document analysis (default: gemma3:12b)
   - `USE_CHUNKING_JUDGE`: Enable/disable the Chunking Judge (default: True)
   - `USE_RETRIEVAL_JUDGE`: Enable/disable the Retrieval Judge (default: True)
   - `ENABLE_RESPONSE_QUALITY`: Enable/disable the Response Quality Pipeline (default: True)
   - `QUALITY_THRESHOLD`: Minimum quality score for responses (default: 8.0)
   - `MAX_REFINEMENT_ITERATIONS`: Maximum number of refinement iterations (default: 2)
   - `ENABLE_AUDIT_REPORTS`: Enable/disable audit report generation (default: True)
   - `SECRET_KEY`: Secret key for JWT token generation (required for authentication)
   - `ALGORITHM`: Algorithm for JWT token generation (default: HS256)
   - `ACCESS_TOKEN_EXPIRE_MINUTES`: Token expiration time in minutes (default: 30)

3. Build and start the application with Docker Compose:
   ```bash
   docker-compose up -d
   ```

4. Access the application at [http://localhost:8000](http://localhost:8000)

### Running Without Docker

1. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the application:
   ```bash
   uvicorn app.main:app --reload
   ```

## Usage

### Chat Interface

The main chat interface allows you to:
- Send messages to the model
- Toggle RAG functionality
- View citations from source documents
- Control conversation history
- See quality scores for responses
- View source attributions and citations

### Document Management

The document management page allows you to:
- Upload documents
- Process documents for RAG
- View document information
- Delete documents

### Authentication

The authentication system provides:
- User registration and login pages
- Secure access to personal resources
- Role-based access control
- Token-based authentication for API access
- Protection of sensitive operations

### System Management

The system page allows you to:
- View system statistics
- Manage models
- Check system health

### Audit Reports

The audit reports page allows you to:
- View comprehensive audit reports for queries
- Examine information sources used in responses
- Verify factual accuracy and completeness
- Track reasoning traces and execution timelines
- Identify potential hallucinations
- Export audit reports for compliance and transparency

### Background Task System

The Background Task System allows you to:
- Monitor and manage asynchronous tasks
- View task status, progress, and results
- Prioritize tasks based on importance
- Schedule tasks for future execution
- Define task dependencies for complex workflows
- Monitor system resource usage
- View resource alerts and system health

## Development

### Project Structure

```
metis_rag/
├── app/
│   ├── api/         # API endpoints
│   │   ├── auth.py  # Authentication endpoints
│   │   └── ...      # Other API endpoints
│   ├── core/        # Core configuration
│   │   ├── security.py # Authentication and security
│   │   └── ...      # Other core modules
│   ├── middleware/  # Middleware components
│   │   └── auth.py  # Authentication middleware
│   ├── db/          # Database components
│   │   ├── repositories/ # Repository pattern implementations
│   │   │   ├── user_repository.py # User data access
│   │   │   └── ...    # Other repositories
│   │   └── ...      # Other database modules
│   ├── rag/         # RAG engine
│   │   ├── agents/  # LLM-based agents for RAG enhancement
│   │   ├── response_synthesizer.py  # Response synthesis
│   │   ├── response_evaluator.py    # Response evaluation
│   │   ├── response_refiner.py      # Response refinement
│   │   ├── audit_report_generator.py # Audit reporting
│   │   ├── response_quality_pipeline.py # Quality pipeline
│   │   └── langgraph_states.py      # LangGraph state models
│   ├── tasks/       # Background Task System
│   │   ├── task_manager.py          # Task management
│   │   ├── task_models.py           # Task data models
│   │   ├── scheduler.py             # Task scheduling
│   │   ├── resource_monitor.py      # Resource monitoring
│   │   ├── task_repository.py       # Task database operations
│   │   └── example_tasks.py         # Example task handlers
│   ├── models/      # Data models
│   │   ├── user.py  # User models
│   │   └── ...      # Other models
│   ├── static/      # Static files
│   │   ├── js/      # JavaScript files
│   │   │   ├── main.js # Authentication JS
│   │   │   └── ...  # Other JS files
│   │   └── ...      # Other static files
│   ├── templates/   # HTML templates
│   │   ├── login.html    # Login page
│   │   ├── register.html # Registration page
│   │   └── ...      # Other templates
│   └── utils/       # Utility functions
├── tests/           # Tests
│   ├── unit/        # Unit tests
│   │   └── test_response_quality.py # Response quality tests
│   └── integration/ # Integration tests
│       └── test_response_quality_integration.py # Integration tests
├── uploads/         # Uploaded documents
├── chroma_db/       # ChromaDB data
└── docker-compose.yml
```

### Running Tests

Run tests with pytest:

```bash
pytest
```

For testing specific components:

```bash
# Test RAG retrieval functionality
python test_rag_retrieval.py

# Test response quality components
pytest tests/unit/test_response_quality.py

# Test response quality integration
pytest tests/integration/test_response_quality_integration.py

# Test background task system
python scripts/test_background_tasks.py

# Test authentication system
python scripts/test_authentication.py
```

These test scripts create test documents, process them, and test various aspects of the system:
- RAG retrieval tests verify that the system correctly retrieves and uses information from documents
- Response quality tests verify that responses are accurate, complete, relevant, and free from hallucinations
- Integration tests verify that all components work together correctly in end-to-end workflows
- Background task tests verify that the task system correctly handles task submission, execution, prioritization, and resource management
- Authentication tests verify user registration, login, token generation, and access control for resources

## License

[MIT License](LICENSE)

## Acknowledgements

This project builds upon:
- Metis_Chat
- rag_ollama_mvp

================
File: requirements.txt
================
# Core dependencies
fastapi~=0.111.0
uvicorn>=0.22.0,<0.23.0
python-multipart>=0.0.6
httpx>=0.24.0,<0.25.0
# Pydantic v2
pydantic~=2.7.0
# LangChain and LangGraph
langchain~=0.3.0
langgraph~=0.1.0
# ChromaDB and dependencies
chromadb>=0.4.13,<0.4.15
# Document processing dependencies
pypdf>=3.15.1
unstructured>=0.10.16
# Database dependencies
sqlalchemy[asyncio]>=2.0.0
aiosqlite>=0.19.0
asyncpg>=0.28.0
aiofiles>=23.2.1
# Other dependencies
python-dotenv>=1.0.0
jinja2>=3.1.2
sse-starlette>=1.6.5
psutil>=5.9.5
# Security dependencies
fastapi-limiter>=0.1.5
redis>=4.2.0
passlib[bcrypt]>=1.7.4
python-jose[cryptography]>=3.3.0
bcrypt>=4.0.1
# Memory management
mem0ai>=0.1.0
# Added for Pydantic v2 compatibility
pydantic-settings~=2.0



================================================================
End of Codebase
================================================================
