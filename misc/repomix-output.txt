This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
app/
  api/
    __init__.py
    analytics.py
    chat.py
    documents.py
    system.py
  core/
    __init__.py
    config.py
    logging.py
    security.py
  models/
    __init__.py
    chat.py
    document.py
    system.py
  rag/
    agents/
      __init__.py
      chunking_judge.py
      langgraph_rag_agent.py
      retrieval_judge.py
    chunkers/
      __init__.py
      semantic_chunker.py
    __init__.py
    document_processor.py
    ollama_client.py
    rag_engine.py
    vector_store.py
  static/
    css/
      document-manager.css
      fonts.css
      styles.css
    js/
      chat.js
      document-manager.js
      main.js
      test_models.js
  templates/
    analytics.html
    base.html
    chat.html
    documents.html
    system.html
    test_models.html
  utils/
    __init__.py
    file_utils.py
    text_utils.py
  main.py
scripts/
  setup.sh
test_docs/
  quarterly_report.txt
  rag_test_results.json
  technical_documentation.md
test_quality_docs/
  product_specifications.csv
  quarterly_report.txt
  technical_documentation.md
tests/
  integration/
    test_api.py
    test_chunking_judge_integration.py
    test_langgraph_rag_integration.py
    test_retrieval_judge_integration.py
    test_semantic_chunker_integration.py
  retrieval_judge/
    data/
      performance_test_document.md
      product_specifications.md
      quarterly_report.txt
      technical_documentation.md
      test_document.md
      timing_test_document.md
    results/
      chunk_evaluation_results.json
      context_optimization_results.json
      judge_edge_case_analysis.json
      performance_analysis_results.json
      performance_analysis.json
      query_analysis_results.json
      query_refinement_results.json
      single_query_result.json
      timing_analysis_results.json
      timing_analysis.json
    analyze_retrieval_judge_results.py
    IMPLEMENTATION_NOTES.md
    README.md
    run_tests.py
    run_tests.sh
    test_judge_edge_cases.py
    test_performance_analysis.py
    test_retrieval_judge_comparison.py
    test_single_query.py
    test_timing_analysis.py
  unit/
    test_chunking_judge.py
    test_rag_engine.py
    test_retrieval_judge.py
    test_semantic_chunker.py
  conftest.py
  test_chunking_judge_phase1.py
  test_chunking_judge_real.py
  test_edge_cases.py
  test_file_handling.py
  test_performance.py
  test_rag_quality.py
venv_py310/
  bin/
    activate
    activate.csh
    activate.fish
    Activate.ps1
    chardetect
    chroma
    coloredlogs
    deep
    distro
    dotenv
    f2py
    fastapi
    filetype
    httpx
    huggingface-cli
    humanfriendly
    isympy
    jsondiff
    jsonpatch
    jsonpointer
    langchain-server
    langsmith
    markdown-it
    nltk
    normalizer
    onnxruntime_test
    oxmsg
    pip
    pip3
    pip3.10
    py.test
    pygmentize
    pytest
    tqdm
    typer
    uvicorn
    watchfiles
    wheel
  share/
    man/
      man1/
        isympy.1
  pyvenv.cfg
.env.example
.gitignore
chunking_judge_real_results.json
chunking_judge_test_results.json
clear_cache.py
clear_database.py
demo_presentation.py
demo_tests.py
docker-compose.yml
Dockerfile
generate_pdf.py
generate_test_data.py
llm_enhanced_rag_implementation_plan_updated.md
llm_enhanced_rag_implementation_plan.md
mem0_integration_plan.md
Metis_RAG_Implementation_Steps.md
Metis_RAG_Improvement_Plan.md
Metis_RAG_Setup_Plan.md
Metis_RAG_Technical_Demo.html
Metis_RAG_Technical_Demo.md
metis_rag_test_report.html
metis_rag_test_report.json
pyproject.toml
README.md
reprocess_documents.py
requirements.txt
run_tests.py
technical_documentation.md
test_citation_results.json
test_data.csv
test_document.txt
test_models.html
test_multi_doc_results.json
test_quality_results.json
test_rag_retrieval.py
test_response_time_results.json
TESTING.md

================================================================
Files
================================================================

================
File: app/api/__init__.py
================
from app.api.chat import router as chat_router
from app.api.documents import router as documents_router
from app.api.system import router as system_router

================
File: app/api/analytics.py
================
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import APIRouter, HTTPException, Query
import json

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.analytics")

# In-memory analytics store (replace with database in production)
query_analytics = []
document_usage = {}

@router.post("/record_query")
async def record_query(query_data: Dict[str, Any]):
    """
    Record a query for analytics
    """
    try:
        # Add timestamp if not provided
        if "timestamp" not in query_data:
            query_data["timestamp"] = datetime.now().isoformat()
        
        # Add to query analytics
        query_analytics.append({
            "query": query_data.get("query", ""),
            "model": query_data.get("model", ""),
            "use_rag": query_data.get("use_rag", False),
            "timestamp": query_data.get("timestamp"),
            "response_time_ms": query_data.get("response_time_ms", 0),
            "document_ids": query_data.get("document_ids", []),
            "token_count": query_data.get("token_count", 0)
        })
        
        # Update document usage
        for doc_id in query_data.get("document_ids", []):
            if doc_id in document_usage:
                document_usage[doc_id]["usage_count"] += 1
                document_usage[doc_id]["last_used"] = query_data.get("timestamp")
            else:
                document_usage[doc_id] = {
                    "usage_count": 1,
                    "last_used": query_data.get("timestamp")
                }
        
        # Limit the size of query_analytics to prevent memory issues
        if len(query_analytics) > 1000:
            query_analytics.pop(0)
        
        return {"success": True, "message": "Query recorded for analytics"}
    except Exception as e:
        logger.error(f"Error recording query analytics: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error recording query analytics: {str(e)}")

@router.get("/query_stats")
async def get_query_stats(
    time_period: Optional[str] = Query("all", description="Time period for stats (all, day, week, month)")
):
    """
    Get query statistics
    """
    try:
        # Filter queries by time period if specified
        filtered_queries = filter_by_time_period(query_analytics, time_period)
        
        if not filtered_queries:
            return {
                "query_count": 0,
                "avg_response_time": 0,
                "rag_usage_percent": 0,
                "time_period": time_period
            }
        
        query_count = len(filtered_queries)
        rag_count = sum(1 for q in filtered_queries if q.get("use_rag"))
        avg_response_time = sum(q.get("response_time_ms", 0) for q in filtered_queries) / query_count
        avg_token_count = sum(q.get("token_count", 0) for q in filtered_queries) / query_count
        
        # Get most common queries
        query_frequency = {}
        for q in filtered_queries:
            query_text = q.get("query", "").lower()
            if query_text in query_frequency:
                query_frequency[query_text] += 1
            else:
                query_frequency[query_text] = 1
        
        most_common_queries = sorted(
            [{"query": k, "count": v} for k, v in query_frequency.items()],
            key=lambda x: x["count"],
            reverse=True
        )[:10]  # Top 10 most common queries
        
        return {
            "query_count": query_count,
            "avg_response_time_ms": avg_response_time,
            "avg_token_count": avg_token_count,
            "rag_usage_percent": (rag_count / query_count) * 100 if query_count > 0 else 0,
            "most_common_queries": most_common_queries,
            "recent_queries": filtered_queries[-10:],  # Last 10 queries
            "time_period": time_period
        }
    except Exception as e:
        logger.error(f"Error getting query stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting query stats: {str(e)}")

@router.get("/document_usage")
async def get_document_usage(
    time_period: Optional[str] = Query("all", description="Time period for stats (all, day, week, month)")
):
    """
    Get document usage statistics
    """
    try:
        # Filter document usage by time period if specified
        filtered_usage = {}
        
        if time_period == "all":
            filtered_usage = document_usage
        else:
            # Get cutoff date
            cutoff = get_cutoff_date(time_period)
            
            # Filter document usage
            for doc_id, usage in document_usage.items():
                last_used = usage.get("last_used")
                if last_used and datetime.fromisoformat(last_used) >= cutoff:
                    filtered_usage[doc_id] = usage
        
        # Sort by usage count
        most_used = sorted(
            [{"id": k, **v} for k, v in filtered_usage.items()],
            key=lambda x: x["usage_count"],
            reverse=True
        )[:10]  # Top 10 most used documents
        
        return {
            "document_count": len(filtered_usage),
            "most_used": most_used,
            "time_period": time_period
        }
    except Exception as e:
        logger.error(f"Error getting document usage stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting document usage stats: {str(e)}")

@router.get("/system_stats")
async def get_system_stats():
    """
    Get system statistics
    """
    try:
        from app.rag.vector_store import VectorStore
        
        # Get vector store stats
        vector_store = VectorStore()
        vector_stats = vector_store.get_stats()
        
        # Get query stats
        query_count = len(query_analytics)
        
        # Get document stats
        document_count = len(document_usage)
        
        return {
            "vector_store": vector_stats,
            "query_count": query_count,
            "document_count": document_count,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting system stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting system stats: {str(e)}")

def filter_by_time_period(data_list, time_period):
    """
    Filter a list of dictionaries by time period
    """
    if time_period == "all" or not data_list:
        return data_list
    
    # Get cutoff date
    cutoff = get_cutoff_date(time_period)
    
    # Filter data
    return [
        item for item in data_list 
        if "timestamp" in item and datetime.fromisoformat(item["timestamp"]) >= cutoff
    ]

def get_cutoff_date(time_period):
    """
    Get cutoff date for time period
    """
    now = datetime.now()
    
    if time_period == "day":
        from datetime import timedelta
        return now - timedelta(days=1)
    elif time_period == "week":
        from datetime import timedelta
        return now - timedelta(days=7)
    elif time_period == "month":
        from datetime import timedelta
        return now - timedelta(days=30)
    else:
        # Default to all time
        from datetime import timedelta
        return now - timedelta(days=365 * 10)  # 10 years ago

================
File: app/api/chat.py
================
import logging
from typing import Dict, List, Optional, Any
from fastapi import APIRouter, HTTPException, Depends, Response
from sse_starlette.sse import EventSourceResponse

from app.models.chat import (
    ChatQuery,
    ChatResponse,
    Conversation,
    Message,
    Citation
)
from app.rag.rag_engine import RAGEngine
from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent
from app.utils.text_utils import extract_citations
from app.core.config import DEFAULT_MODEL, USE_LANGGRAPH_RAG

# Create router
router = APIRouter()

# In-memory conversation store (replace with database in production)
conversations: Dict[str, Conversation] = {}

# Logger
logger = logging.getLogger("app.api.chat")

# RAG engine instance
rag_engine = RAGEngine()

# LangGraph RAG Agent instance
langgraph_rag_agent = LangGraphRAGAgent() if USE_LANGGRAPH_RAG else None
if USE_LANGGRAPH_RAG:
    logger.info("LangGraph RAG Agent is enabled")
else:
    logger.info("LangGraph RAG Agent is disabled")

# Maximum number of messages to include in conversation history
MAX_HISTORY_MESSAGES = 25

@router.post("/query", response_model=ChatResponse)
async def query_chat(query: ChatQuery):
    """
    Send a chat query and get a response
    """
    try:
        # Get or create conversation
        conversation_id = query.conversation_id
        if conversation_id and conversation_id in conversations:
            conversation = conversations[conversation_id]
        else:
            conversation = Conversation()
            conversations[conversation.id] = conversation
            conversation_id = conversation.id
        
        # Add user message to conversation
        user_message = Message(content=query.message, role="user")
        conversation.add_message(user_message)
        
        # Get model name
        model = query.model or DEFAULT_MODEL
        
        # Query RAG engine
        if query.stream:
            # For streaming, return an EventSourceResponse
            logger.info(f"Streaming response for conversation {conversation_id}")
            
            async def event_generator():
                full_response = ""
                
                # Get conversation history
                conversation_history = conversation.messages[-MAX_HISTORY_MESSAGES:] if len(conversation.messages) > 0 else []
                
                # Extract metadata filters if provided
                metadata_filters = query.metadata_filters if hasattr(query, 'metadata_filters') else None
                
                # Get RAG response
                rag_response = await rag_engine.query(
                    query=query.message,
                    model=model,
                    use_rag=query.use_rag,
                    stream=True,
                    model_parameters=query.model_parameters,
                    conversation_history=conversation_history,
                    metadata_filters=metadata_filters
                )
                
                # Get sources (with safety check)
                sources = rag_response.get("sources")
                if sources is None:
                    logger.warning("No sources returned from RAG engine")
                    sources = []
                
                # Stream the response
                async for token in rag_response["stream"]:
                    full_response += token
                    yield token
                
                # Add assistant message to conversation
                assistant_message = Message(
                    content=full_response,
                    role="assistant",
                    citations=sources
                )
                conversation.add_message(assistant_message)
            
            return EventSourceResponse(event_generator())
        else:
            # For non-streaming, return the complete response
            logger.info(f"Generating response for conversation {conversation_id}")
            
            # Get conversation history
            conversation_history = conversation.messages[-MAX_HISTORY_MESSAGES:] if len(conversation.messages) > 0 else []
            
            # Extract metadata filters if provided
            metadata_filters = query.metadata_filters if hasattr(query, 'metadata_filters') else None
            
            # Get RAG response
            rag_response = await rag_engine.query(
                query=query.message,
                model=model,
                use_rag=query.use_rag,
                stream=False,
                model_parameters=query.model_parameters,
                conversation_history=conversation_history,
                metadata_filters=metadata_filters
            )
            
            # Get response and sources
            response_text = rag_response.get("answer", "")
            sources = rag_response.get("sources")
            if sources is None:
                logger.warning("No sources returned from RAG engine")
                sources = []
            
            # Add assistant message to conversation
            assistant_message = Message(
                content=response_text,
                role="assistant",
                citations=sources
            )
            conversation.add_message(assistant_message)
            
            # Return response
            return ChatResponse(
                message=response_text,
                conversation_id=conversation_id,
                citations=sources
            )
    except Exception as e:
        logger.error(f"Error generating chat response: {str(e)}")
        
        # Create a user-friendly error message
        error_message = "Sorry, there was an error processing your request."
        
        # Check if it's a future date query
        if "conversation_id" in locals() and conversation_id in conversations:
            conversation = conversations[conversation_id]
            if conversation.messages and conversation.messages[-1].role == "user":
                user_query = conversation.messages[-1].content.lower()
                
                # Check for future year patterns
                import re
                current_year = 2025  # Hardcoded for now, could use datetime.now().year
                year_match = re.search(r'\b(20\d\d|19\d\d)\b', user_query)
                
                if year_match and int(year_match.group(1)) > current_year:
                    error_message = f"I cannot provide information about events in {year_match.group(1)} as it's in the future. The current year is {current_year}."
                elif re.search(r'what will happen|what is going to happen|predict the future|future events|in the future', user_query):
                    error_message = "I cannot predict future events or provide information about what will happen in the future."
        
        # Return a 200 response with the error message instead of raising an exception
        # This allows the frontend to display the message properly
        return ChatResponse(
            message=error_message,
            conversation_id=conversation_id if "conversation_id" in locals() else None,
            citations=None
        )

@router.get("/history")
async def get_history(conversation_id: str):
    """
    Get conversation history
    """
    if conversation_id not in conversations:
        raise HTTPException(status_code=404, detail=f"Conversation {conversation_id} not found")
    
    return conversations[conversation_id]

@router.post("/save")
async def save_conversation(conversation_id: str):
    """
    Save a conversation
    """
    if conversation_id not in conversations:
        raise HTTPException(status_code=404, detail=f"Conversation {conversation_id} not found")
    
    # In a real application, this would save to a database
    # For now, we just return success
    return {"success": True, "message": f"Conversation {conversation_id} saved"}

@router.delete("/clear")
async def clear_conversation(conversation_id: Optional[str] = None):
    """
    Clear a conversation or all conversations
    """
    if conversation_id:
        if conversation_id not in conversations:
            raise HTTPException(status_code=404, detail=f"Conversation {conversation_id} not found")
        
        # Clear specific conversation
        del conversations[conversation_id]
        return {"success": True, "message": f"Conversation {conversation_id} cleared"}
    else:
        # Clear all conversations
        conversations.clear()
        return {"success": True, "message": "All conversations cleared"}

@router.post("/langgraph_query", response_model=ChatResponse)
async def langgraph_query_chat(query: ChatQuery):
    """
    Send a chat query to the LangGraph RAG Agent and get a response
    """
    if not USE_LANGGRAPH_RAG or not langgraph_rag_agent:
        raise HTTPException(status_code=400, detail="LangGraph RAG Agent is not enabled")
    
    try:
        # Get or create conversation
        conversation_id = query.conversation_id
        if conversation_id and conversation_id in conversations:
            conversation = conversations[conversation_id]
        else:
            conversation = Conversation()
            conversations[conversation.id] = conversation
            conversation_id = conversation.id
        
        # Add user message to conversation
        user_message = Message(content=query.message, role="user")
        conversation.add_message(user_message)
        
        # Get model name
        model = query.model or DEFAULT_MODEL
        
        # Format conversation history
        conversation_context = None
        if len(conversation.messages) > 1:  # Only include history if there's more than just the current message
            # Get the last few messages (up to 5) to keep context manageable, but exclude the most recent user message
            recent_history = conversation.messages[:-1]
            if len(recent_history) > 5:
                recent_history = recent_history[-5:]
            
            # Format the conversation history
            history_pieces = []
            for msg in recent_history:
                role_prefix = "User" if msg.role == "user" else "Assistant"
                history_pieces.append(f"{role_prefix}: {msg.content}")
            
            conversation_context = "\n".join(history_pieces)
            logger.info(f"Including conversation history with {len(recent_history)} messages")
        else:
            logger.info("No previous conversation history to include")
        
        # Extract metadata filters if provided
        metadata_filters = query.metadata_filters if hasattr(query, 'metadata_filters') else None
        
        # Query LangGraph RAG Agent
        if query.stream:
            # For streaming, return an EventSourceResponse
            logger.info(f"Streaming response for conversation {conversation_id} using LangGraph RAG Agent")
            
            async def event_generator():
                full_response = ""
                
                # Get LangGraph RAG response
                langgraph_response = await langgraph_rag_agent.query(
                    query=query.message,
                    model=model,
                    system_prompt=None,  # Use default system prompt
                    stream=True,
                    model_parameters=query.model_parameters,
                    conversation_context=conversation_context,
                    metadata_filters=metadata_filters
                )
                
                # Get sources (with safety check)
                sources = langgraph_response.get("sources")
                if sources is None:
                    logger.warning("No sources returned from LangGraph RAG Agent")
                    sources = []
                
                # Stream the response
                async for token in langgraph_response["stream"]:
                    full_response += token
                    yield token
                
                # Add assistant message to conversation
                assistant_message = Message(
                    content=full_response,
                    role="assistant",
                    citations=sources
                )
                conversation.add_message(assistant_message)
            
            return EventSourceResponse(event_generator())
        else:
            # For non-streaming, return the complete response
            logger.info(f"Generating response for conversation {conversation_id} using LangGraph RAG Agent")
            
            # Get LangGraph RAG response
            langgraph_response = await langgraph_rag_agent.query(
                query=query.message,
                model=model,
                system_prompt=None,  # Use default system prompt
                stream=False,
                model_parameters=query.model_parameters,
                conversation_context=conversation_context,
                metadata_filters=metadata_filters
            )
            
            # Get response and sources
            response_text = langgraph_response.get("answer", "")
            sources = langgraph_response.get("sources")
            if sources is None:
                logger.warning("No sources returned from LangGraph RAG Agent")
                sources = []
            
            # Add assistant message to conversation
            assistant_message = Message(
                content=response_text,
                role="assistant",
                citations=sources
            )
            conversation.add_message(assistant_message)
            
            # Return response
            return ChatResponse(
                message=response_text,
                conversation_id=conversation_id,
                citations=sources
            )
    except Exception as e:
        logger.error(f"Error generating chat response with LangGraph RAG Agent: {str(e)}")
        
        # Create a user-friendly error message
        error_message = "Sorry, there was an error processing your request with the LangGraph RAG Agent."
        
        # Return a 200 response with the error message instead of raising an exception
        return ChatResponse(
            message=error_message,
            conversation_id=conversation_id if "conversation_id" in locals() else None,
            citations=None
        )

================
File: app/api/documents.py
================
import logging
import os
from typing import List, Dict, Any, Optional, Set
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks, Query

from app.models.document import (
    Document, DocumentInfo, DocumentProcessRequest,
    TagUpdateRequest, FolderUpdateRequest, DocumentFilterRequest
)
from app.rag.document_processor import DocumentProcessor
from app.rag.vector_store import VectorStore
from app.utils.file_utils import validate_file, save_upload_file, delete_document_files
from app.core.config import UPLOAD_DIR, CHUNK_SIZE, CHUNK_OVERLAP

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.documents")

# Document processor
document_processor = DocumentProcessor()

# Vector store
vector_store = VectorStore()

# In-memory document store (replace with database in production)
documents: Dict[str, Document] = {}

# In-memory tag store (replace with database in production)
all_tags: Set[str] = set()

# In-memory folder store (replace with database in production)
all_folders: Set[str] = {"/"}  # Root folder always exists

@router.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    tags: str = Form(""),
    folder: str = Form("/")
):
    """
    Upload a document
    """
    try:
        # Validate file
        if not validate_file(file):
            raise HTTPException(status_code=400, detail=f"File type not allowed: {file.filename}")
        
        # Parse tags
        tag_list = [tag.strip() for tag in tags.split(",")] if tags else []
        
        # Update all_tags
        all_tags.update(tag_list)
        
        # Validate folder
        if not folder.startswith("/"):
            folder = "/" + folder
        
        # Update all_folders
        all_folders.add(folder)
        
        # Create document
        document = Document(
            filename=file.filename,
            content="",
            tags=tag_list,
            folder=folder
        )
        
        # Save file
        await save_upload_file(file, document.id)
        
        # Store document
        documents[document.id] = document
        
        return {
            "success": True,
            "message": f"Document {file.filename} uploaded successfully",
            "document_id": document.id
        }
    except Exception as e:
        logger.error(f"Error uploading document: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error uploading document: {str(e)}")

@router.get("/list", response_model=List[DocumentInfo])
async def list_documents(
    tags: Optional[List[str]] = Query(None),
    folder: Optional[str] = Query(None),
):
    """
    List all documents with optional filtering
    """
    try:
        # Filter documents
        filtered_docs = documents.values()
        
        # Filter by tags if provided
        if tags:
            filtered_docs = [doc for doc in filtered_docs if any(tag in doc.tags for tag in tags)]
        
        # Filter by folder if provided
        if folder:
            filtered_docs = [doc for doc in filtered_docs if doc.folder == folder]
        
        # Convert documents to DocumentInfo
        document_infos = [
            DocumentInfo(
                id=doc.id,
                filename=doc.filename,
                chunk_count=len(doc.chunks),
                metadata=doc.metadata,
                tags=doc.tags,
                folder=doc.folder,
                uploaded=doc.uploaded
            )
            for doc in filtered_docs
        ]
        
        return document_infos
    except Exception as e:
        logger.error(f"Error listing documents: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error listing documents: {str(e)}")

@router.post("/filter", response_model=List[DocumentInfo])
async def filter_documents(filter_request: DocumentFilterRequest):
    """
    Filter documents by tags, folder, and metadata
    """
    try:
        filtered_docs = documents.values()
        
        # Filter by tags if provided
        if filter_request.tags:
            filtered_docs = [doc for doc in filtered_docs if any(tag in doc.tags for tag in filter_request.tags)]
        
        # Filter by folder if provided
        if filter_request.folder:
            filtered_docs = [doc for doc in filtered_docs if doc.folder == filter_request.folder]
        
        # Filter by metadata if provided
        if filter_request.metadata:
            filtered_docs = [
                doc for doc in filtered_docs
                if all(
                    key in doc.metadata and doc.metadata[key] == value
                    for key, value in filter_request.metadata.items()
                )
            ]
        
        # Convert documents to DocumentInfo
        document_infos = [
            DocumentInfo(
                id=doc.id,
                filename=doc.filename,
                chunk_count=len(doc.chunks),
                metadata=doc.metadata,
                tags=doc.tags,
                folder=doc.folder,
                uploaded=doc.uploaded
            )
            for doc in filtered_docs
        ]
        
        return document_infos
    except Exception as e:
        logger.error(f"Error filtering documents: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error filtering documents: {str(e)}")

@router.get("/tags")
async def get_all_tags():
    """
    Get all tags used in the system
    """
    try:
        return {"tags": sorted(list(all_tags))}
    except Exception as e:
        logger.error(f"Error getting tags: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting tags: {str(e)}")

@router.get("/folders")
async def get_all_folders():
    """
    Get all folders used in the system
    """
    try:
        return {"folders": sorted(list(all_folders))}
    except Exception as e:
        logger.error(f"Error getting folders: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting folders: {str(e)}")

@router.get("/{document_id}")
async def get_document(document_id: str):
    """
    Get a document by ID
    """
    if document_id not in documents:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    return documents[document_id]

@router.put("/{document_id}/tags")
async def update_document_tags(document_id: str, tag_request: TagUpdateRequest):
    """
    Update document tags
    """
    if document_id not in documents:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    try:
        # Update document tags
        documents[document_id].tags = tag_request.tags
        
        # Update all_tags
        all_tags.update(tag_request.tags)
        
        # Update vector store metadata - convert tags list to string for ChromaDB
        await vector_store.update_document_metadata(
            document_id,
            {
                "tags": ",".join(tag_request.tags) if tag_request.tags else "",
                "tags_list": tag_request.tags  # Keep original list for internal use
            }
        )
        
        return {
            "success": True,
            "message": f"Tags updated for document {document_id}",
            "tags": tag_request.tags
        }
    except Exception as e:
        logger.error(f"Error updating document tags: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error updating document tags: {str(e)}")

@router.put("/{document_id}/folder")
async def update_document_folder(document_id: str, folder_request: FolderUpdateRequest):
    """
    Update document folder
    """
    if document_id not in documents:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    try:
        # Validate folder
        folder = folder_request.folder
        if not folder.startswith("/"):
            folder = "/" + folder
        
        # Update document folder
        documents[document_id].folder = folder
        
        # Update all_folders
        all_folders.add(folder)
        
        # Update vector store metadata
        await vector_store.update_document_metadata(
            document_id,
            {"folder": folder}
        )
        
        return {
            "success": True,
            "message": f"Folder updated for document {document_id}",
            "folder": folder
        }
    except Exception as e:
        logger.error(f"Error updating document folder: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error updating document folder: {str(e)}")

@router.delete("/{document_id}")
async def delete_document(document_id: str):
    """
    Delete a document
    """
    if document_id not in documents:
        raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    
    try:
        # Delete from vector store
        vector_store.delete_document(document_id)
        
        # Delete document files
        delete_document_files(document_id)
        
        # Remove from in-memory store
        del documents[document_id]
        
        # Rebuild all_tags from remaining documents
        rebuild_tags_and_folders()
        
        return {"success": True, "message": f"Document {document_id} deleted"}
    except Exception as e:
        logger.error(f"Error deleting document: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting document: {str(e)}")

def rebuild_tags_and_folders():
    """
    Rebuild all_tags and all_folders from remaining documents
    """
    global all_tags, all_folders
    
    # Reset tags and folders
    all_tags = set()
    all_folders = {"/"}  # Root folder always exists
    
    # Rebuild from documents
    for doc in documents.values():
        all_tags.update(doc.tags)
        all_folders.add(doc.folder)

async def process_document_task(
    document_ids: List[str],
    force_reprocess: bool = False,
    chunking_strategy: str = "recursive",
    chunk_size: Optional[int] = None,
    chunk_overlap: Optional[int] = None
):
    """
    Background task to process documents with configurable chunking strategy
    """
    # Create a document processor with the specified parameters
    processor = DocumentProcessor(
        chunk_size=chunk_size or CHUNK_SIZE,
        chunk_overlap=chunk_overlap or CHUNK_OVERLAP,
        chunking_strategy=chunking_strategy
    )
    
    for document_id in document_ids:
        if document_id not in documents:
            logger.warning(f"Document {document_id} not found, skipping processing")
            continue
        
        document = documents[document_id]
        
        try:
            # Skip if already processed and not forced to reprocess
            if document.chunks and not force_reprocess:
                logger.info(f"Document {document_id} already processed, skipping")
                continue
            
            # Process document with the configured processor
            processed_document = await processor.process_document(document)
            
            # Add to vector store
            await vector_store.add_document(processed_document)
            
            # Update document
            documents[document_id] = processed_document
            
            logger.info(f"Document {document_id} processed successfully with {chunking_strategy} chunking strategy")
        except Exception as e:
            logger.error(f"Error processing document {document_id}: {str(e)}")

@router.post("/process")
async def process_documents(
    request: DocumentProcessRequest,
    background_tasks: BackgroundTasks
):
    """
    Process documents with configurable chunking strategy
    """
    try:
        # Validate document IDs
        invalid_ids = [doc_id for doc_id in request.document_ids if doc_id not in documents]
        if invalid_ids:
            raise HTTPException(
                status_code=404,
                detail=f"Documents not found: {', '.join(invalid_ids)}"
            )
        
        # Log chunking strategy
        logger.info(f"Processing documents with chunking strategy: {request.chunking_strategy}")
        
        # Queue background task with chunking parameters
        background_tasks.add_task(
            process_document_task,
            request.document_ids,
            request.force_reprocess,
            request.chunking_strategy,
            request.chunk_size,
            request.chunk_overlap
        )
        
        return {
            "success": True,
            "message": f"Processing started for {len(request.document_ids)} documents"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing documents: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing documents: {str(e)}")

================
File: app/api/system.py
================
import logging
import os
import platform
import psutil
from typing import List, Dict, Any
from fastapi import APIRouter, HTTPException

from app.models.system import SystemStats, ModelInfo, HealthCheck
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore
from app.api.documents import documents
from app.core.config import API_V1_STR

# Create router
router = APIRouter()

# Logger
logger = logging.getLogger("app.api.system")

# Vector store
vector_store = VectorStore()

@router.get("/stats", response_model=SystemStats)
async def get_stats():
    """
    Get system statistics
    """
    try:
        # Get Ollama client
        async with OllamaClient() as ollama_client:
            # Get available models
            models = await ollama_client.list_models()
            model_names = [model["name"] for model in models]
        
        # Get vector store stats
        vector_store_stats = vector_store.get_stats()
        
        # Calculate total chunks
        total_chunks = sum(len(doc.chunks) for doc in documents.values())
        
        return SystemStats(
            documents_count=len(documents),
            total_chunks=total_chunks,
            vector_store_size=vector_store_stats["count"],
            available_models=model_names
        )
    except Exception as e:
        logger.error(f"Error getting system stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting system stats: {str(e)}")

@router.get("/models", response_model=List[ModelInfo])
async def get_models():
    """
    Get available models
    """
    try:
        # Get Ollama client
        async with OllamaClient() as ollama_client:
            # Get available models
            models = await ollama_client.list_models()
        
        # Convert to ModelInfo
        model_infos = [
            ModelInfo(
                name=model["name"],
                size=model.get("size"),
                modified_at=model.get("modified_at"),
                description=model.get("description", f"Ollama model: {model['name']}")
            )
            for model in models
        ]
        
        return model_infos
    except Exception as e:
        logger.error(f"Error getting models: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting models: {str(e)}")

@router.get("/health", response_model=HealthCheck)
async def health_check():
    """
    Health check endpoint
    """
    try:
        # Check Ollama
        ollama_status = "healthy"
        try:
            async with OllamaClient() as ollama_client:
                await ollama_client.list_models()
        except Exception as e:
            logger.error(f"Ollama health check failed: {str(e)}")
            ollama_status = "unhealthy"
        
        # Check vector DB
        vector_db_status = "healthy"
        try:
            vector_store.get_stats()
        except Exception as e:
            logger.error(f"Vector DB health check failed: {str(e)}")
            vector_db_status = "unhealthy"
        
        return HealthCheck(
            status="healthy" if ollama_status == "healthy" and vector_db_status == "healthy" else "unhealthy",
            ollama_status=ollama_status,
            vector_db_status=vector_db_status,
            api_version="v1"
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return HealthCheck(
            status="unhealthy",
            ollama_status="unknown",
            vector_db_status="unknown",
            api_version="v1"
        )

================
File: app/core/__init__.py
================
from app.core.config import *
from app.core.security import setup_security
from app.core.logging import setup_logging, get_logger

================
File: app/core/config.py
================
import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Base directory
BASE_DIR = Path(__file__).resolve().parent.parent.parent

# API settings
API_V1_STR = "/api"
PROJECT_NAME = "Metis RAG"

# Ollama settings
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "gemma3:12b")
DEFAULT_EMBEDDING_MODEL = os.getenv("DEFAULT_EMBEDDING_MODEL", "nomic-embed-text")

# LLM Judge settings
CHUNKING_JUDGE_MODEL = os.getenv("CHUNKING_JUDGE_MODEL", "gemma3:12b")
RETRIEVAL_JUDGE_MODEL = os.getenv("RETRIEVAL_JUDGE_MODEL", "gemma3:12b")
USE_CHUNKING_JUDGE = os.getenv("USE_CHUNKING_JUDGE", "True").lower() == "true"
USE_RETRIEVAL_JUDGE = os.getenv("USE_RETRIEVAL_JUDGE", "True").lower() == "true"

# LangGraph RAG Agent settings
LANGGRAPH_RAG_MODEL = os.getenv("LANGGRAPH_RAG_MODEL", "gemma3:12b")
USE_LANGGRAPH_RAG = os.getenv("USE_LANGGRAPH_RAG", "True").lower() == "true"

# Document settings
UPLOAD_DIR = os.getenv("UPLOAD_DIR", str(BASE_DIR / "uploads"))
CHROMA_DB_DIR = os.getenv("CHROMA_DB_DIR", str(BASE_DIR / "chroma_db"))
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))

# Security settings
CORS_ORIGINS = os.getenv("CORS_ORIGINS", "*").split(",")

# Make sure upload directory exists
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(CHROMA_DB_DIR, exist_ok=True)

================
File: app/core/logging.py
================
import logging
import sys
from typing import Any, Dict, List

# Configure logging
def setup_logging() -> None:
    """
    Configure logging for the application
    """
    logging_config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "default": {
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                "datefmt": "%Y-%m-%d %H:%M:%S",
            },
        },
        "handlers": {
            "console": {
                "level": "INFO",
                "class": "logging.StreamHandler",
                "formatter": "default",
                "stream": sys.stdout,
            },
        },
        "loggers": {
            "app": {"handlers": ["console"], "level": "INFO", "propagate": False},
            "uvicorn": {"handlers": ["console"], "level": "INFO", "propagate": False},
            "fastapi": {"handlers": ["console"], "level": "INFO", "propagate": False},
        },
    }
    
    logging.config.dictConfig(logging_config)

def get_logger(name: str) -> logging.Logger:
    """
    Get a logger instance
    """
    return logging.getLogger(name)

================
File: app/core/security.py
================
from fastapi import FastAPI, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from app.core.config import CORS_ORIGINS

def setup_security(app: FastAPI) -> None:
    """
    Setup security middleware for the application
    """
    # Setup CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=CORS_ORIGINS,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        response: Response = await call_next(request)
        
        # Add security headers
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        
        # Basic Content Security Policy
        response.headers["Content-Security-Policy"] = (
            "default-src 'self'; "
            "script-src 'self'; "
            "style-src 'self'; "
            "img-src 'self' data:; "
            "connect-src 'self';"
        )
        
        return response

================
File: app/models/__init__.py
================
from app.models.chat import Citation, Message, Conversation, ChatQuery, ChatResponse
from app.models.document import Chunk, Document, DocumentInfo, DocumentProcessRequest
from app.models.system import SystemStats, ModelInfo, HealthCheck

================
File: app/models/chat.py
================
from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field
from datetime import datetime
import uuid


class Citation(BaseModel):
    """Citation for a message"""
    document_id: str
    chunk_id: str
    relevance_score: float
    excerpt: str

    class Config:
        arbitrary_types_allowed = True


class Message(BaseModel):
    """Chat message"""
    content: str
    role: str = "user"  # "user" or "assistant"
    citations: Optional[List[Citation]] = None
    timestamp: datetime = Field(default_factory=datetime.now)

    class Config:
        arbitrary_types_allowed = True


class Conversation(BaseModel):
    """Chat conversation"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    messages: List[Message] = []
    metadata: Dict[str, Any] = {}
    created: datetime = Field(default_factory=datetime.now)
    updated: datetime = Field(default_factory=datetime.now)
    
    def add_message(self, message: Message) -> None:
        """Add a message to the conversation"""
        self.messages.append(message)
        self.updated = datetime.now()

    class Config:
        arbitrary_types_allowed = True


class ChatQuery(BaseModel):
    """Chat query model"""
    message: str
    conversation_id: Optional[str] = None
    model: Optional[str] = None
    use_rag: bool = True
    stream: bool = True
    model_parameters: Dict[str, Any] = {}
    metadata_filters: Optional[Dict[str, Any]] = None

    class Config:
        arbitrary_types_allowed = True


class ChatResponse(BaseModel):
    """Chat response model"""
    message: str
    conversation_id: str
    citations: Optional[List[Citation]] = None

    class Config:
        arbitrary_types_allowed = True

================
File: app/models/document.py
================
from typing import List, Dict, Optional, Any, Set
from pydantic import BaseModel, Field
from datetime import datetime
import uuid


class Chunk(BaseModel):
    """Document chunk model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: str
    metadata: Dict[str, Any] = {}
    embedding: Optional[List[float]] = None

    class Config:
        arbitrary_types_allowed = True


class Document(BaseModel):
    """Document model"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    filename: str
    content: str
    chunks: List[Chunk] = []
    metadata: Dict[str, Any] = {}
    tags: List[str] = []
    folder: str = "/"  # Root folder by default
    uploaded: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True


class DocumentInfo(BaseModel):
    """Document information (without content)"""
    id: str
    filename: str
    chunk_count: int
    metadata: Dict[str, Any]
    tags: List[str]
    folder: str
    uploaded: datetime

    class Config:
        arbitrary_types_allowed = True


class DocumentProcessRequest(BaseModel):
    """Document processing request"""
    document_ids: List[str]
    force_reprocess: bool = False
    chunking_strategy: Optional[str] = "recursive"
    chunk_size: Optional[int] = None
    chunk_overlap: Optional[int] = None

    class Config:
        arbitrary_types_allowed = True


class TagUpdateRequest(BaseModel):
    """Request to update document tags"""
    tags: List[str]

    class Config:
        arbitrary_types_allowed = True


class FolderUpdateRequest(BaseModel):
    """Request to update document folder"""
    folder: str

    class Config:
        arbitrary_types_allowed = True


class DocumentFilterRequest(BaseModel):
    """Request to filter documents"""
    tags: Optional[List[str]] = None
    folder: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

    class Config:
        arbitrary_types_allowed = True

================
File: app/models/system.py
================
from typing import List, Dict, Optional, Any
from pydantic import BaseModel


class SystemStats(BaseModel):
    """System statistics model"""
    documents_count: int
    total_chunks: int
    vector_store_size: Optional[int] = None
    available_models: List[str]

    class Config:
        arbitrary_types_allowed = True


class ModelInfo(BaseModel):
    """Model information"""
    name: str
    size: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None
    description: Optional[str] = None
    modified_at: Optional[str] = None
    
    class Config:
        arbitrary_types_allowed = True
    
    
class HealthCheck(BaseModel):
    """Health check model"""
    status: str
    ollama_status: str
    vector_db_status: str
    api_version: str
    
    class Config:
        arbitrary_types_allowed = True

================
File: app/rag/agents/__init__.py
================
"""
LLM-based agents for enhancing the RAG pipeline.
"""

from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.agents.retrieval_judge import RetrievalJudge

__all__ = ["ChunkingJudge", "RetrievalJudge"]

================
File: app/rag/agents/chunking_judge.py
================
"""
Chunking Judge - LLM-based agent that analyzes documents and recommends optimal chunking strategies
"""
import logging
import json
import re
from typing import Dict, Any, List, Optional

from app.models.document import Document
from app.rag.ollama_client import OllamaClient
from app.core.config import CHUNKING_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.chunking_judge")

class ChunkingJudge:
    """
    LLM-based agent that analyzes documents and recommends optimal chunking strategies
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = CHUNKING_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_document(self, document: Document) -> Dict[str, Any]:
        """
        Analyze a document and recommend the best chunking strategy and parameters
        
        Returns:
            Dict with keys:
            - strategy: The recommended chunking strategy
            - parameters: Dict of parameters for the chosen strategy
            - justification: Explanation of the recommendation
        """
        # Extract a sample of the document content (to avoid exceeding context window)
        content_sample = self._extract_representative_sample(document.content, document.filename)
        
        # Create prompt for the LLM
        prompt = self._create_analysis_prompt(document.filename, content_sample)
        
        # Get recommendation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        recommendation = self._parse_recommendation(response.get("response", ""))
        
        logger.info(f"Chunking Judge recommended strategy '{recommendation['strategy']}' for document {document.filename}")
        
        return recommendation
    
    def _extract_representative_sample(self, content: str, filename: str, max_length: int = 5000) -> str:
        """
        Extract a representative sample of the document content
        
        This function prioritizes:
        1. Headers (especially for markdown files)
        2. Introduction and conclusion sections
        3. A mix of content from throughout the document
        """
        if len(content) <= max_length:
            return content
        
        # Check if it's a markdown file
        is_markdown = filename.lower().endswith(('.md', '.markdown'))
        
        # For markdown files, prioritize headers
        if is_markdown:
            # Extract headers
            header_pattern = r'^(#{1,6}\s+.+)$'
            headers = re.findall(header_pattern, content, re.MULTILINE)
            
            # If we have headers, include them in the sample
            if headers:
                # Take all headers (they're usually short)
                headers_text = "\n".join(headers)
                
                # Calculate remaining space
                remaining_space = max_length - len(headers_text) - 100  # 100 chars buffer
                
                # Divide remaining space between intro, middle, and conclusion
                section_size = remaining_space // 3
                
                # Get intro, middle, and conclusion
                intro = content[:section_size]
                middle_start = (len(content) - section_size) // 2
                middle = content[middle_start:middle_start + section_size]
                conclusion = content[-section_size:]
                
                return f"{headers_text}\n\n--- DOCUMENT SAMPLE ---\n\nINTRO:\n{intro}\n\n[...]\n\nMIDDLE SECTION:\n{middle}\n\n[...]\n\nCONCLUSION:\n{conclusion}"
        
        # For non-markdown files or markdown files without headers
        # Take larger portions from the beginning and end (intro/conclusion)
        intro_size = max_length * 2 // 5  # 40% for intro
        conclusion_size = max_length * 2 // 5  # 40% for conclusion
        middle_size = max_length - intro_size - conclusion_size  # 20% for middle
        
        intro = content[:intro_size]
        middle_start = (len(content) - middle_size) // 2
        middle = content[middle_start:middle_start + middle_size]
        conclusion = content[-conclusion_size:]
        
        return f"INTRO:\n{intro}\n\n[...]\n\nMIDDLE SECTION:\n{middle}\n\n[...]\n\nCONCLUSION:\n{conclusion}"
    
    def _create_analysis_prompt(self, filename: str, content_sample: str) -> str:
        """Create a prompt for the LLM to analyze the document"""
        return f"""You are a document analysis expert. Your task is to analyze the following document and recommend the best chunking strategy for a RAG (Retrieval Augmented Generation) system.

Available Strategies:
- recursive: Splits text recursively by characters. Good for general text with natural separators.
- token: Splits text by tokens. Good for preserving semantic units in technical content.
- markdown: Splits markdown documents by headers. Good for structured documents with clear sections.
- semantic: Uses LLM to identify natural semantic boundaries in text. Best for preserving meaning and context in complex documents.

Document Filename: {filename}

Document Sample:
{content_sample}

Analyze the document structure, content type, and formatting. Consider:
1. Is this a structured document with clear sections or headers?
2. Does it contain code, tables, or other special formatting?
3. What's the typical paragraph and sentence length?
4. Are there natural breaks in the content?
5. Would semantic chunking be more appropriate than fixed-size chunking?

Here are some examples of good chunking strategy recommendations:

Example 1:
Document: technical_documentation.md
Recommendation: 
{{
    "strategy": "markdown",
    "parameters": {{
        "chunk_size": 1000,
        "chunk_overlap": 100
    }},
    "justification": "This is a markdown document with clear header structure. Using markdown chunking will preserve the semantic structure of the document and ensure that related content stays together."
}}

Example 2:
Document: research_paper.txt
Recommendation:
{{
    "strategy": "recursive",
    "parameters": {{
        "chunk_size": 1500,
        "chunk_overlap": 200
    }},
    "justification": "This document has long paragraphs with complex sentences. A larger chunk size with significant overlap will help preserve context and ensure that related concepts aren't split across chunks."
}}

Example 3:
Document: code_examples.py
Recommendation:
{{
    "strategy": "token",
    "parameters": {{
        "chunk_size": 500,
        "chunk_overlap": 50
    }},
    "justification": "This document contains code snippets where preserving token-level semantics is important. Token-based chunking will ensure that code blocks remain coherent."
}}

Example 4:
Document: research_paper.pdf
Recommendation:
{{
    "strategy": "semantic",
    "parameters": {{
        "chunk_size": 1500,
        "chunk_overlap": 200
    }},
    "justification": "This research paper contains complex concepts and arguments that span multiple paragraphs. Semantic chunking will identify natural boundaries in the text based on meaning rather than arbitrary character counts, preserving the logical flow and context of the arguments."
}}

Output your recommendation in JSON format:
{{
    "strategy": "...",  // One of: recursive, token, markdown
    "parameters": {{
        "chunk_size": ...,  // Recommended chunk size (characters or tokens)
        "chunk_overlap": ...  // Recommended overlap size
    }},
    "justification": "..." // Explanation of your reasoning
}}
"""
    
    def _parse_recommendation(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM response to extract the recommendation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                recommendation = json.loads(json_str)
                
                # Validate the recommendation
                if "strategy" not in recommendation:
                    raise ValueError("Missing 'strategy' in recommendation")
                
                # Validate strategy is one of the allowed values
                allowed_strategies = ["recursive", "token", "markdown", "semantic"]
                if recommendation["strategy"] not in allowed_strategies:
                    logger.warning(f"Invalid strategy '{recommendation['strategy']}', falling back to recursive")
                    recommendation["strategy"] = "recursive"
                
                # Set defaults if missing
                if "parameters" not in recommendation:
                    recommendation["parameters"] = {}
                if "chunk_size" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_size"] = 500
                if "chunk_overlap" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_overlap"] = 50
                
                return recommendation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing chunking recommendation: {str(e)}")
            # Return default recommendation
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 500,
                    "chunk_overlap": 50
                },
                "justification": "Failed to parse LLM recommendation, using default strategy."
            }

================
File: app/rag/agents/langgraph_rag_agent.py
================
"""
LangGraph RAG Agent - Orchestrates the RAG process using a state machine
"""
import logging
import json
import re
from typing import Dict, Any, List, Optional, TypedDict, Annotated, Sequence, cast, Tuple
from enum import Enum

from langchain.schema import Document as LangchainDocument
from langgraph.graph import StateGraph, END
# langgraph 0.0.20 doesn't have ToolNode in prebuilt or MemorySaver

from app.models.document import Document, Chunk
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.chunkers.semantic_chunker import SemanticChunker
from app.core.config import CHUNKING_JUDGE_MODEL, RETRIEVAL_JUDGE_MODEL, DEFAULT_MODEL

logger = logging.getLogger("app.rag.agents.langgraph_rag_agent")

# Define state types for the LangGraph state machine
class QueryAnalysisState(TypedDict):
    """State for query analysis"""
    query: str
    conversation_context: Optional[str]
    complexity: Optional[str]
    parameters: Optional[Dict[str, Any]]
    justification: Optional[str]

class RetrievalState(TypedDict):
    """State for retrieval"""
    query: str
    refined_query: Optional[str]
    conversation_context: Optional[str]
    parameters: Dict[str, Any]
    chunks: List[Dict[str, Any]]
    needs_refinement: bool
    relevance_scores: Optional[Dict[str, float]]

class GenerationState(TypedDict):
    """State for generation"""
    query: str
    conversation_context: Optional[str]
    context: str
    sources: List[Dict[str, Any]]
    document_ids: List[str]
    answer: Optional[str]

class RAGState(TypedDict):
    """Combined state for the RAG process"""
    query: str
    conversation_context: Optional[str]
    metadata_filters: Optional[Dict[str, Any]]
    model: str
    system_prompt: Optional[str]
    stream: bool
    model_parameters: Optional[Dict[str, Any]]
    query_analysis: Optional[QueryAnalysisState]
    retrieval: Optional[RetrievalState]
    generation: Optional[GenerationState]
    final_response: Optional[Dict[str, Any]]

class RAGStage(str, Enum):
    """Stages in the RAG process"""
    QUERY_ANALYSIS = "analyze_query_node"
    RETRIEVAL = "retrieve_chunks_node"
    QUERY_REFINEMENT = "refine_query_node"
    CONTEXT_OPTIMIZATION = "optimize_context_node"
    GENERATION = "generate_response_node"
    COMPLETE = "finalize_response_node"

class LangGraphRAGAgent:
    """
    LangGraph-based agent that orchestrates the RAG process using a state machine
    
    This agent integrates:
    - Chunking Judge for document analysis and chunking strategy selection
    - Semantic Chunker for intelligent text splitting
    - Retrieval Judge for query refinement and context optimization
    
    The state machine follows these stages:
    1. Query Analysis: Analyze the query to determine complexity and retrieval parameters
    2. Retrieval: Retrieve relevant chunks from the vector store
    3. Query Refinement: Refine the query if needed based on initial retrieval
    4. Context Optimization: Optimize the context assembly for generation
    5. Generation: Generate the final response using the optimized context
    """
    
    def __init__(
        self,
        vector_store: Optional[VectorStore] = None,
        ollama_client: Optional[OllamaClient] = None,
        chunking_judge: Optional[ChunkingJudge] = None,
        retrieval_judge: Optional[RetrievalJudge] = None,
        semantic_chunker: Optional[SemanticChunker] = None
    ):
        """
        Initialize the LangGraphRAGAgent
        
        Args:
            vector_store: Vector store for retrieval
            ollama_client: Client for LLM interactions
            chunking_judge: Judge for document analysis and chunking strategy selection
            retrieval_judge: Judge for query refinement and context optimization
            semantic_chunker: Chunker for intelligent text splitting
        """
        self.vector_store = vector_store or VectorStore()
        self.ollama_client = ollama_client or OllamaClient()
        self.chunking_judge = chunking_judge or ChunkingJudge(ollama_client=self.ollama_client)
        self.retrieval_judge = retrieval_judge or RetrievalJudge(ollama_client=self.ollama_client)
        self.semantic_chunker = semantic_chunker or SemanticChunker(ollama_client=self.ollama_client)
        
        # Initialize and compile the state graph
        self.graph = self._build_graph()
        self.app = self.graph.compile()
        
        logger.info("LangGraphRAGAgent initialized with state machine")
    
    def _build_graph(self) -> StateGraph:
        """
        Build the state graph for the RAG process
        
        Returns:
            StateGraph: The state graph for the RAG process
        """
        # Create the state graph
        graph = StateGraph(RAGState)
        
        # Add nodes for each stage
        graph.add_node(RAGStage.QUERY_ANALYSIS, self._analyze_query)
        graph.add_node(RAGStage.RETRIEVAL, self._retrieve_chunks)
        graph.add_node(RAGStage.QUERY_REFINEMENT, self._refine_query)
        graph.add_node(RAGStage.CONTEXT_OPTIMIZATION, self._optimize_context)
        graph.add_node(RAGStage.GENERATION, self._generate_response)
        graph.add_node(RAGStage.COMPLETE, self._finalize_response)
        
        # Define the edges between nodes
        # Start with query analysis
        graph.add_edge(RAGStage.QUERY_ANALYSIS, RAGStage.RETRIEVAL)
        
        # After retrieval, decide whether to refine the query or optimize the context
        graph.add_conditional_edges(
            RAGStage.RETRIEVAL,
            self._needs_refinement,
            {
                True: RAGStage.QUERY_REFINEMENT,
                False: RAGStage.CONTEXT_OPTIMIZATION
            }
        )
        
        # After query refinement, go back to retrieval with the refined query
        graph.add_edge(RAGStage.QUERY_REFINEMENT, RAGStage.RETRIEVAL)
        
        # After context optimization, proceed to generation
        graph.add_edge(RAGStage.CONTEXT_OPTIMIZATION, RAGStage.GENERATION)
        
        # After generation, complete the process
        graph.add_edge(RAGStage.GENERATION, RAGStage.COMPLETE)
        
        # After completion, end the process
        graph.add_edge(RAGStage.COMPLETE, END)
        
        # Set the entry point
        graph.set_entry_point(RAGStage.QUERY_ANALYSIS)
        
        return graph
    
    async def query(
        self,
        query: str,
        model: str = DEFAULT_MODEL,
        system_prompt: Optional[str] = None,
        stream: bool = False,
        model_parameters: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Query the RAG agent with the state machine
        
        Args:
            query: The user query
            model: The model to use for generation
            system_prompt: Optional system prompt for generation
            stream: Whether to stream the response
            model_parameters: Optional parameters for the model
            conversation_context: Optional conversation context
            metadata_filters: Optional filters for retrieval
            
        Returns:
            Dict with keys:
            - query: The original query
            - answer: The generated answer (if not streaming)
            - stream: The response stream (if streaming)
            - sources: List of sources used in the response
        """
        # Initialize the state
        initial_state: RAGState = {
            "query": query,
            "conversation_context": conversation_context,
            "metadata_filters": metadata_filters,
            "model": model,
            "system_prompt": system_prompt,
            "stream": stream,
            "model_parameters": model_parameters or {},
            "query_analysis": None,
            "retrieval": None,
            "generation": None,
            "final_response": None
        }
        
        logger.info(f"Starting RAG query with LangGraph: {query[:50]}...")
        # Run the state machine
        # In langgraph 0.0.20, we need to use the compiled app with ainvoke
        result = await self.app.ainvoke(initial_state)
        
        # Return the final response
        return result["final_response"]
    
    async def _analyze_query(self, state: RAGState) -> RAGState:
        """
        Analyze the query to determine complexity and retrieval parameters
        
        Args:
            state: The current state
            
        Returns:
            Updated state with query analysis
        """
        logger.info(f"Analyzing query: {state['query'][:50]}...")
        
        # Use the retrieval judge to analyze the query
        query_analysis = await self.retrieval_judge.analyze_query(state["query"])
        
        # Update the state with the query analysis
        state["query_analysis"] = {
            "query": state["query"],
            "conversation_context": state["conversation_context"],
            "complexity": query_analysis.get("complexity"),
            "parameters": query_analysis.get("parameters"),
            "justification": query_analysis.get("justification")
        }
        
        logger.info(f"Query complexity: {query_analysis.get('complexity', 'unknown')}")
        
        return state
    
    async def _retrieve_chunks(self, state: RAGState) -> RAGState:
        """
        Retrieve relevant chunks from the vector store
        
        Args:
            state: The current state
            
        Returns:
            Updated state with retrieved chunks
        """
        # Get query and parameters from the state
        query = state["query"]
        query_analysis = state["query_analysis"]
        retrieval_state = state.get("retrieval", None)
        
        # If we have a refined query from a previous iteration, use it
        if retrieval_state and retrieval_state.get("refined_query"):
            query = retrieval_state["refined_query"]
            logger.info(f"Using refined query: {query[:50]}...")
        
        # Get recommended parameters from query analysis
        parameters = query_analysis["parameters"] if query_analysis else {}
        recommended_k = parameters.get("k", 10)
        
        # Combine the query with conversation context if available
        search_query = query
        if state["conversation_context"]:
            search_query = f"{query} {state['conversation_context'][-200:]}"
        
        logger.info(f"Retrieving chunks for query: {search_query[:50]}...")
        
        # Retrieve chunks from the vector store
        search_results = await self.vector_store.search(
            query=search_query,
            top_k=max(15, recommended_k + 5),  # Get a few extra for filtering
            filter_criteria=state["metadata_filters"]
        )
        
        logger.info(f"Retrieved {len(search_results)} chunks from vector store")
        
        # Evaluate chunks with the retrieval judge
        evaluation = await self.retrieval_judge.evaluate_chunks(query, search_results)
        
        # Extract relevance scores and refinement decision
        relevance_scores = evaluation.get("relevance_scores", {})
        needs_refinement = evaluation.get("needs_refinement", False)
        
        logger.info(f"Chunk evaluation complete, needs_refinement={needs_refinement}")
        
        # Update the state with retrieval results
        state["retrieval"] = {
            "query": query,
            "refined_query": retrieval_state["refined_query"] if retrieval_state else None,
            "conversation_context": state["conversation_context"],
            "parameters": parameters,
            "chunks": search_results,
            "needs_refinement": needs_refinement,
            "relevance_scores": relevance_scores
        }
        
        return state
    
    def _needs_refinement(self, state: RAGState) -> bool:
        """
        Determine if query refinement is needed based on retrieval results
        
        Args:
            state: The current state
            
        Returns:
            True if refinement is needed, False otherwise
        """
        retrieval = state["retrieval"]
        
        # If we've already refined the query once, don't refine again
        if retrieval and retrieval.get("refined_query"):
            logger.info("Query already refined, skipping further refinement")
            return False
        
        # Otherwise, use the needs_refinement flag from the retrieval judge
        needs_refinement = retrieval["needs_refinement"] if retrieval else False
        logger.info(f"Query refinement needed: {needs_refinement}")
        
        return needs_refinement
    
    async def _refine_query(self, state: RAGState) -> RAGState:
        """
        Refine the query based on initial retrieval results
        
        Args:
            state: The current state
            
        Returns:
            Updated state with refined query
        """
        retrieval = state["retrieval"]
        
        logger.info(f"Refining query: {retrieval['query'][:50]}...")
        
        # Refine the query using the retrieval judge
        refined_query = await self.retrieval_judge.refine_query(
            retrieval["query"], 
            retrieval["chunks"]
        )
        
        logger.info(f"Refined query: {refined_query[:50]}...")
        
        # Update the state with the refined query
        retrieval["refined_query"] = refined_query
        state["retrieval"] = retrieval
        
        return state
    
    async def _optimize_context(self, state: RAGState) -> RAGState:
        """
        Optimize the context assembly for generation
        
        Args:
            state: The current state
            
        Returns:
            Updated state with optimized context
        """
        retrieval = state["retrieval"]
        query = retrieval["refined_query"] or retrieval["query"]
        chunks = retrieval["chunks"]
        relevance_scores = retrieval["relevance_scores"] or {}
        parameters = retrieval["parameters"]
        
        logger.info(f"Optimizing context for query: {query[:50]}...")
        
        # Filter chunks based on relevance scores
        relevance_threshold = parameters.get("threshold", 0.4)
        apply_reranking = parameters.get("reranking", True)
        
        relevant_results = []
        for result in chunks:
            # Skip results with None content
            if "content" not in result or result["content"] is None:
                continue
            
            chunk_id = result["chunk_id"]
            
            # Get relevance score from evaluation or calculate from distance
            if chunk_id in relevance_scores:
                relevance_score = relevance_scores[chunk_id]
            else:
                # Calculate relevance score (lower distance = higher relevance)
                relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
            
            # Only include chunks that are sufficiently relevant
            if relevance_score >= relevance_threshold:
                # Add relevance score to result for sorting
                result["relevance_score"] = relevance_score
                relevant_results.append(result)
        
        # Sort by relevance score if reranking is enabled
        if apply_reranking:
            relevant_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        logger.info(f"Found {len(relevant_results)} relevant chunks after filtering")
        
        # Optimize context assembly if we have enough chunks
        if len(relevant_results) > 3 and apply_reranking:
            logger.info("Optimizing context assembly with Retrieval Judge")
            optimized_results = await self.retrieval_judge.optimize_context(query, relevant_results)
            if optimized_results:
                relevant_results = optimized_results
                logger.info(f"Context optimized to {len(relevant_results)} chunks")
        
        # Format context with source information
        context_pieces = []
        sources = []
        document_ids = []
        
        for i, result in enumerate(relevant_results):
            # Extract metadata for better context
            metadata = result["metadata"]
            filename = metadata.get("filename", "Unknown")
            tags = metadata.get("tags", [])
            folder = metadata.get("folder", "/")
            
            # Format the context piece with metadata
            context_piece = f"[{i+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
            context_pieces.append(context_piece)
            
            # Track the source for citation
            doc_id = metadata["document_id"]
            document_ids.append(doc_id)
            
            # Get relevance score (either from judge or distance)
            relevance_score = result.get("relevance_score", 1.0 - (result["distance"] if result["distance"] is not None else 0))
            
            sources.append({
                "document_id": doc_id,
                "chunk_id": result["chunk_id"],
                "relevance_score": relevance_score,
                "excerpt": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                "filename": filename,
                "tags": tags,
                "folder": folder
            })
        
        # Join all context pieces
        context = "\n\n".join(context_pieces)
        
        # Check if we have enough relevant context
        if len(relevant_results) == 0:
            logger.warning("No sufficiently relevant documents found for the query")
            context = "Note: No sufficiently relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents."
        elif len(context.strip()) < 50:  # Very short context might not be useful
            logger.warning("Context is too short to be useful")
            context = "Note: The retrieved context is too limited to provide a comprehensive answer to your query. The system cannot provide a specific answer based on the available documents."
        
        # Update the state with generation information
        state["generation"] = {
            "query": query,
            "conversation_context": state["conversation_context"],
            "context": context,
            "sources": sources,
            "document_ids": document_ids,
            "answer": None
        }
        
        return state
    
    async def _generate_response(self, state: RAGState) -> RAGState:
        """
        Generate the final response using the optimized context
        
        Args:
            state: The current state
            
        Returns:
            Updated state with generated response
        """
        generation = state["generation"]
        query = generation["query"]
        context = generation["context"]
        conversation_context = generation["conversation_context"]
        
        logger.info(f"Generating response for query: {query[:50]}...")
        
        # Create full prompt with context and conversation history
        if conversation_context:
            full_prompt = f"""Context:
{context}

Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
        else:
            full_prompt = f"""Context:
{context}

User Question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. This is a new conversation with no previous history - treat it as such.
9. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
        
        # Create system prompt if not provided
        system_prompt = state["system_prompt"]
        if not system_prompt:
            system_prompt = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
- Do not use your general knowledge unless the context is insufficient, and clearly indicate when you're doing so.
- Analyze the context carefully to find the most relevant information for the user's question.
- If multiple sources provide different information, synthesize them and explain any discrepancies.
- If the context includes metadata like filenames, tags, or folders, use this to understand the source and relevance of the information.

WHEN CONTEXT IS INSUFFICIENT:
- Clearly state: "Based on the provided documents, I don't have information about [topic]."
- Be specific about what information is missing.
- Only then provide a general response based on your knowledge, and clearly state: "However, generally speaking..." to distinguish this from information in the context.
- Never pretend to have information that isn't in the context.

CONVERSATION HANDLING:
- IMPORTANT: Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.
- If no conversation history is provided, treat the query as a new, standalone question.
- Only maintain continuity with previous exchanges when conversation history is explicitly provided.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Always cite your sources with numbers in square brackets [1] when using information from the context.
"""
        
        # Generate response
        if state["stream"]:
            # For streaming, just return the stream response
            logger.info(f"Generating streaming response with model: {state['model']}")
            stream_response = await self.ollama_client.generate(
                prompt=full_prompt,
                model=state["model"],
                system_prompt=system_prompt,
                stream=True,
                parameters=state["model_parameters"] or {}
            )
            
            # Update the state with the stream response
            generation["stream_response"] = stream_response
        else:
            # For non-streaming, get the complete response
            logger.info(f"Generating non-streaming response with model: {state['model']}")
            response = await self.ollama_client.generate(
                prompt=full_prompt,
                model=state["model"],
                system_prompt=system_prompt,
                stream=False,
                parameters=state["model_parameters"] or {}
            )
            
            # Check if there was an error in the response
            if "error" in response:
                error_message = response.get("error", "Unknown error")
                logger.warning(f"Model returned an error: {error_message}")
                response_text = f"Error: {error_message}"
            else:
                # Get response text
                response_text = response.get("response", "")
            
            logger.info(f"Response length: {len(response_text)} characters")
            
            # Update the state with the generated answer
            generation["answer"] = response_text
        
        # Update the state with the generation information
        state["generation"] = generation
        
        return state
    
    async def _finalize_response(self, state: RAGState) -> RAGState:
        """
        Finalize the response for return to the user
        
        Args:
            state: The current state
            
        Returns:
            Updated state with final response
        """
        generation = state["generation"]
        
        # Create the final response
        if state["stream"]:
            final_response = {
                "query": state["query"],
                "stream": generation.get("stream_response"),
                "sources": generation["sources"]
            }
        else:
            final_response = {
                "query": state["query"],
                "answer": generation["answer"],
                "sources": generation["sources"]
            }
        
        # Update the state with the final response
        state["final_response"] = final_response
        
        logger.info("RAG process complete")
        
        return state

================
File: app/rag/agents/retrieval_judge.py
================
"""
Retrieval Judge - LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
"""
import logging
import json
import re
from typing import Dict, Any, List, Optional

from app.models.document import Chunk
from app.rag.ollama_client import OllamaClient
from app.core.config import RETRIEVAL_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.retrieval_judge")

class RetrievalJudge:
    """
    LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = RETRIEVAL_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Analyze a query and recommend retrieval parameters
        
        Returns:
            Dict with keys:
            - complexity: The assessed complexity of the query (simple, moderate, complex)
            - parameters: Dict of recommended retrieval parameters (k, threshold, etc.)
            - justification: Explanation of the recommendation
        """
        # Create prompt for the LLM
        prompt = self._create_query_analysis_prompt(query)
        
        # Get recommendation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        analysis = self._parse_query_analysis(response.get("response", ""))
        
        logger.info(f"Retrieval Judge analyzed query complexity as '{analysis.get('complexity', 'unknown')}' with k={analysis.get('parameters', {}).get('k', 'default')}")
        
        return analysis
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Evaluate retrieved chunks for relevance to the query
        
        Args:
            query: The user query
            chunks: List of chunks from the vector store search results
            
        Returns:
            Dict with keys:
            - relevance_scores: Dict mapping chunk IDs to relevance scores (0-1)
            - needs_refinement: Boolean indicating if query refinement is needed
            - justification: Explanation of the evaluation
        """
        # Extract a sample of chunks to avoid exceeding context window
        chunks_sample = self._extract_chunks_sample(chunks)
        
        # Create prompt for the LLM
        prompt = self._create_chunks_evaluation_prompt(query, chunks_sample)
        
        # Get evaluation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        evaluation = self._parse_chunks_evaluation(response.get("response", ""), chunks)
        
        logger.info(f"Retrieval Judge evaluated {len(chunks)} chunks, needs_refinement={evaluation.get('needs_refinement', False)}")
        
        return evaluation
    
    async def refine_query(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """
        Refine a query based on retrieved chunks to improve retrieval precision
        
        Args:
            query: The original user query
            chunks: List of chunks from the initial retrieval
            
        Returns:
            Refined query string
        """
        # Extract a sample of chunks to avoid exceeding context window
        chunks_sample = self._extract_chunks_sample(chunks)
        
        # Create prompt for the LLM
        prompt = self._create_query_refinement_prompt(query, chunks_sample)
        
        # Get refined query from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        refined_query = self._parse_refined_query(response.get("response", ""), query)
        
        logger.info(f"Retrieval Judge refined query from '{query}' to '{refined_query}'")
        
        return refined_query
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Optimize the assembly of chunks into a context for the LLM
        
        Args:
            query: The user query
            chunks: List of chunks from the vector store search results
            
        Returns:
            Reordered and filtered list of chunks optimized for context assembly
        """
        # If we have too few chunks, no need to optimize
        if len(chunks) <= 3:
            logger.info(f"Too few chunks ({len(chunks)}) to optimize, returning as is")
            return chunks
        
        # Extract a sample of chunks to avoid exceeding context window
        chunks_sample = self._extract_chunks_sample(chunks)
        
        # Create prompt for the LLM
        prompt = self._create_context_optimization_prompt(query, chunks_sample)
        
        # Get optimization from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        optimized_chunks = self._parse_context_optimization(response.get("response", ""), chunks)
        
        logger.info(f"Retrieval Judge optimized context from {len(chunks)} to {len(optimized_chunks)} chunks")
        
        return optimized_chunks
    
    def _extract_chunks_sample(self, chunks: List[Dict[str, Any]], max_chunks: int = 5, max_length: int = 5000) -> List[Dict[str, Any]]:
        """
        Extract a representative sample of chunks to avoid exceeding context window
        
        Args:
            chunks: List of chunks from the vector store search results
            max_chunks: Maximum number of chunks to include
            max_length: Maximum total length of chunk content
            
        Returns:
            List of sample chunks with truncated content if necessary
        """
        if not chunks:
            return []
        
        # Sort chunks by distance (if available) to prioritize most relevant chunks
        sorted_chunks = sorted(
            chunks, 
            key=lambda x: x.get("distance", 1.0) if x.get("distance") is not None else 1.0
        )
        
        # Take top chunks
        sample_chunks = sorted_chunks[:max_chunks]
        
        # Calculate total content length
        total_length = sum(len(chunk.get("content", "")) for chunk in sample_chunks)
        
        # If total length exceeds max_length, truncate each chunk proportionally
        if total_length > max_length:
            # Calculate scaling factor
            scale_factor = max_length / total_length
            
            # Truncate each chunk
            for chunk in sample_chunks:
                content = chunk.get("content", "")
                max_chunk_length = int(len(content) * scale_factor)
                if len(content) > max_chunk_length:
                    chunk["content"] = content[:max_chunk_length] + "..."
        
        return sample_chunks
    
    def _create_query_analysis_prompt(self, query: str) -> str:
        """Create a prompt for the LLM to analyze the query"""
        return f"""You are a query analysis expert for a RAG (Retrieval Augmented Generation) system. Your task is to analyze the following user query and recommend optimal retrieval parameters.

User Query: {query}

Analyze the query complexity, specificity, and intent. Consider:
1. Is this a simple factual question or a complex analytical query?
2. Does it require specific knowledge from documents or general knowledge?
3. Is it ambiguous or clear in its intent?
4. Does it contain multiple sub-questions or a single focused question?
5. Would it benefit from a broader or narrower retrieval approach?

Based on your analysis, recommend retrieval parameters:
- k: Number of chunks to retrieve (5-15)
- threshold: Relevance threshold for filtering (0.0-1.0)
- reranking: Whether to apply reranking (true/false)

Output your analysis in JSON format:
{{
    "complexity": "...",  // One of: simple, moderate, complex
    "parameters": {{
        "k": ...,  // Recommended number of chunks to retrieve
        "threshold": ...,  // Recommended relevance threshold
        "reranking": ...  // Whether to apply reranking (true/false)
    }},
    "justification": "..." // Explanation of your reasoning
}}
"""
    
    def _create_chunks_evaluation_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Create a prompt for the LLM to evaluate retrieved chunks"""
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            chunks_text += f"[{i+1}] Source: {filename}\n{content}\n\n"
        
        return f"""You are a relevance evaluation expert for a RAG (Retrieval Augmented Generation) system. Your task is to evaluate the relevance of retrieved chunks to the user's query.

User Query: {query}

Retrieved Chunks:
{chunks_text}

Evaluate each chunk's relevance to the query on a scale of 0.0 to 1.0, where:
- 1.0: Directly answers the query with high precision
- 0.7-0.9: Contains information highly relevant to the query
- 0.4-0.6: Contains information somewhat relevant to the query
- 0.1-0.3: Contains information tangentially related to the query
- 0.0: Contains no information relevant to the query

Also determine if the query needs refinement based on the retrieved chunks:
- If the chunks are all low relevance, the query might need refinement
- If the chunks contain relevant information but are too broad, the query might need refinement
- If the chunks contain contradictory information, the query might need refinement

Output your evaluation in JSON format:
{{
    "relevance_scores": {{
        "1": 0.8,  // Relevance score for chunk 1
        "2": 0.5,  // Relevance score for chunk 2
        ...
    }},
    "needs_refinement": true/false,  // Whether the query needs refinement
    "justification": "..." // Explanation of your evaluation
}}
"""
    
    def _create_query_refinement_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Create a prompt for the LLM to refine the query"""
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            chunks_text += f"[{i+1}] Source: {filename}\n{content}\n\n"
        
        return f"""You are a query refinement expert for a RAG (Retrieval Augmented Generation) system. Your task is to refine the user's query based on the initially retrieved chunks to improve retrieval precision.

Original User Query: {query}

Initially Retrieved Chunks:
{chunks_text}

Analyze the query and the retrieved chunks to identify:
1. Ambiguities in the original query that could be clarified
2. Missing specific terms that would improve retrieval
3. Domain-specific terminology from the chunks that could be incorporated
4. Potential reformulations that would better match the document content

Create a refined query that:
- Maintains the original intent of the user's question
- Adds specificity based on the retrieved chunks
- Incorporates relevant terminology from the documents
- Is formulated to maximize the chance of retrieving more relevant chunks

Output your refined query as plain text without any JSON formatting or explanations. The output should be ONLY the refined query text that can be directly used for retrieval.
"""
    
    def _create_context_optimization_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Create a prompt for the LLM to optimize context assembly"""
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            chunks_text += f"[{i+1}] Source: {filename}\n{content}\n\n"
        
        return f"""You are a context optimization expert for a RAG (Retrieval Augmented Generation) system. Your task is to optimize the assembly of retrieved chunks into a context for the LLM.

User Query: {query}

Retrieved Chunks:
{chunks_text}

Analyze the chunks and determine the optimal order and selection for providing context to the LLM. Consider:
1. Relevance to the query
2. Information completeness
3. Logical flow of information
4. Removal of redundant information
5. Inclusion of diverse perspectives if available

Output your optimization in JSON format:
{{
    "optimized_order": [3, 1, 5, ...],  // Chunk numbers in optimal order
    "excluded_chunks": [2, 4, ...],  // Chunk numbers to exclude (if any)
    "justification": "..." // Explanation of your optimization
}}
"""
    
    def _parse_query_analysis(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM response to extract the query analysis"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                analysis = json.loads(json_str)
                
                # Validate the analysis
                if "complexity" not in analysis:
                    raise ValueError("Missing 'complexity' in analysis")
                
                # Validate complexity is one of the allowed values
                allowed_complexity = ["simple", "moderate", "complex"]
                if analysis["complexity"] not in allowed_complexity:
                    logger.warning(f"Invalid complexity '{analysis['complexity']}', falling back to 'moderate'")
                    analysis["complexity"] = "moderate"
                
                # Set defaults if missing
                if "parameters" not in analysis:
                    analysis["parameters"] = {}
                if "k" not in analysis["parameters"]:
                    analysis["parameters"]["k"] = 10
                if "threshold" not in analysis["parameters"]:
                    analysis["parameters"]["threshold"] = 0.4
                if "reranking" not in analysis["parameters"]:
                    analysis["parameters"]["reranking"] = True
                
                return analysis
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing query analysis: {str(e)}")
            # Return default analysis
            return {
                "complexity": "moderate",
                "parameters": {
                    "k": 10,
                    "threshold": 0.4,
                    "reranking": True
                },
                "justification": "Failed to parse LLM recommendation, using default parameters."
            }
    
    def _parse_chunks_evaluation(self, response_text: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Parse the LLM response to extract the chunks evaluation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                evaluation = json.loads(json_str)
                
                # Validate the evaluation
                if "relevance_scores" not in evaluation:
                    raise ValueError("Missing 'relevance_scores' in evaluation")
                
                # Convert string keys to integers if needed
                relevance_scores = {}
                for key, value in evaluation["relevance_scores"].items():
                    # Convert key to int if it's a string representation of an int
                    try:
                        idx = int(key)
                        # Map the score to the actual chunk ID
                        if 1 <= idx <= len(chunks):
                            chunk_id = chunks[idx-1].get("chunk_id")
                            relevance_scores[chunk_id] = value
                    except (ValueError, IndexError):
                        # If key can't be converted to int or is out of range, use as is
                        relevance_scores[key] = value
                
                evaluation["relevance_scores"] = relevance_scores
                
                # Set defaults if missing
                if "needs_refinement" not in evaluation:
                    evaluation["needs_refinement"] = False
                
                return evaluation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing chunks evaluation: {str(e)}")
            # Return default evaluation
            default_scores = {}
            for chunk in chunks:
                chunk_id = chunk.get("chunk_id")
                if chunk_id:
                    default_scores[chunk_id] = 0.5
            
            return {
                "relevance_scores": default_scores,
                "needs_refinement": False,
                "justification": "Failed to parse LLM evaluation, using default relevance scores."
            }
    
    def _parse_refined_query(self, response_text: str, original_query: str) -> str:
        """Parse the LLM response to extract the refined query"""
        try:
            # Clean up the response text
            refined_query = response_text.strip()
            
            # If the response is empty or too short, return the original query
            if not refined_query or len(refined_query) < 5:
                logger.warning("Refined query is empty or too short, using original query")
                return original_query
            
            # If the response is too long (likely includes explanations), try to extract just the query
            if len(refined_query) > len(original_query) * 3:
                # Look for patterns that might indicate the actual query
                query_patterns = [
                    r'(?:refined query|new query|improved query)[:\s]+(.+?)(?:\n|$)',
                    r'(?:query|q)[:\s]+(.+?)(?:\n|$)',
                    r'"(.+?)"'
                ]
                
                for pattern in query_patterns:
                    match = re.search(pattern, refined_query, re.IGNORECASE)
                    if match:
                        extracted_query = match.group(1).strip()
                        if extracted_query and len(extracted_query) >= 5:
                            logger.info(f"Extracted refined query using pattern: {pattern}")
                            return extracted_query
            
            return refined_query
        except Exception as e:
            logger.error(f"Error parsing refined query: {str(e)}")
            return original_query
    
    def _parse_context_optimization(self, response_text: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Parse the LLM response to extract the context optimization"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                optimization = json.loads(json_str)
                
                # Validate the optimization
                if "optimized_order" not in optimization:
                    raise ValueError("Missing 'optimized_order' in optimization")
                
                # Get the optimized order
                optimized_order = optimization["optimized_order"]
                
                # Get excluded chunks
                excluded_chunks = optimization.get("excluded_chunks", [])
                
                # Create the optimized chunks list
                optimized_chunks = []
                for idx in optimized_order:
                    # Convert to 0-based index if needed
                    try:
                        idx_0based = int(idx) - 1
                        if 0 <= idx_0based < len(chunks) and idx_0based not in excluded_chunks:
                            optimized_chunks.append(chunks[idx_0based])
                    except (ValueError, IndexError):
                        logger.warning(f"Invalid chunk index in optimized order: {idx}")
                
                # If no valid chunks were found, return the original chunks
                if not optimized_chunks:
                    logger.warning("No valid chunks in optimized order, returning original chunks")
                    return chunks
                
                return optimized_chunks
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing context optimization: {str(e)}")
            # Return the original chunks
            return chunks

================
File: app/rag/chunkers/__init__.py
================
"""
Chunkers module for RAG system
"""

================
File: app/rag/chunkers/semantic_chunker.py
================
"""
Semantic Chunker - LLM-based chunker that splits text based on semantic boundaries
"""
import logging
import json
import re
from typing import List, Dict, Any, Optional, Tuple

from langchain.schema import Document as LangchainDocument
from langchain.text_splitter import TextSplitter

from app.rag.ollama_client import OllamaClient
from app.core.config import CHUNKING_JUDGE_MODEL

logger = logging.getLogger("app.rag.chunkers.semantic_chunker")

class SemanticChunker(TextSplitter):
    """
    LLM-based chunker that splits text based on semantic boundaries rather than
    just character or token counts.
    
    This chunker uses an LLM to identify natural semantic boundaries in text,
    ensuring that chunks maintain coherent meaning and context.
    """
    
    def __init__(
        self,
        ollama_client: Optional[OllamaClient] = None,
        model: str = CHUNKING_JUDGE_MODEL,
        chunk_size: int = 1500,
        chunk_overlap: int = 200,
        max_llm_context_length: int = 8000,
        cache_enabled: bool = True
    ):
        """
        Initialize the SemanticChunker.
        
        Args:
            ollama_client: Optional OllamaClient instance
            model: LLM model to use for semantic analysis
            chunk_size: Target size for chunks (in characters)
            chunk_overlap: Target overlap between chunks (in characters)
            max_llm_context_length: Maximum context length for LLM input
            cache_enabled: Whether to cache chunking results
        """
        # Initialize with default separator to satisfy TextSplitter requirements
        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.max_llm_context_length = max_llm_context_length
        self.cache_enabled = cache_enabled
        self.cache = {}  # Simple in-memory cache
    
    def split_text(self, text: str) -> List[str]:
        """
        Split text based on semantic boundaries.
        
        This method overrides the TextSplitter.split_text method to use
        LLM-based semantic chunking instead of simple character-based splitting.
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        # Check cache first if enabled
        if self.cache_enabled and text in self.cache:
            logger.info("Using cached semantic chunks")
            return self.cache[text]
        
        # If text is short enough, return it as a single chunk
        if len(text) <= self.chunk_size:
            return [text]
        
        # For longer texts, use semantic chunking
        chunks = self._semantic_chunking(text)
        
        # Cache the result if enabled
        if self.cache_enabled:
            self.cache[text] = chunks
        
        return chunks
    
    async def split_text_async(self, text: str) -> List[str]:
        """
        Asynchronous version of split_text.
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        # Check cache first if enabled
        if self.cache_enabled and text in self.cache:
            logger.info("Using cached semantic chunks")
            return self.cache[text]
        
        # If text is short enough, return it as a single chunk
        if len(text) <= self.chunk_size:
            return [text]
        
        # For longer texts, use semantic chunking
        chunks = await self._semantic_chunking_async(text)
        
        # Cache the result if enabled
        if self.cache_enabled:
            self.cache[text] = chunks
        
        return chunks
    
    def _semantic_chunking(self, text: str) -> List[str]:
        """
        Synchronous wrapper for _semantic_chunking_async.
        
        This is needed because TextSplitter.split_text is synchronous.
        In practice, this will be less efficient than the async version
        because it blocks on the LLM call.
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        import asyncio
        try:
            # Try to get the current event loop
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If we're already in an event loop, create a new one in a thread
                logger.warning("Running async semantic chunking in a new event loop")
                return asyncio.run(self._semantic_chunking_async(text))
            else:
                # If no event loop is running, use the current one
                return loop.run_until_complete(self._semantic_chunking_async(text))
        except RuntimeError:
            # If no event loop is available, create a new one
            logger.warning("No event loop available, creating a new one")
            return asyncio.run(self._semantic_chunking_async(text))
    
    async def _semantic_chunking_async(self, text: str) -> List[str]:
        """
        Split text based on semantic boundaries using LLM.
        
        For long texts, this method:
        1. Divides the text into sections that fit within the LLM context window
        2. Processes each section to identify semantic boundaries
        3. Combines the results, ensuring proper handling of section boundaries
        
        Args:
            text: The text to split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        # If text is too long for a single LLM call, process it in sections
        if len(text) > self.max_llm_context_length:
            return await self._process_long_text(text)
        
        # For text that fits in a single LLM call, process directly
        return await self._identify_semantic_boundaries(text)
    
    async def _process_long_text(self, text: str) -> List[str]:
        """
        Process a long text by dividing it into sections and processing each section.
        
        Args:
            text: The long text to process
            
        Returns:
            List of semantically chunked text
        """
        # Calculate section size, leaving room for prompt and instructions
        section_size = self.max_llm_context_length - 2000
        
        # Divide text into overlapping sections
        sections = []
        for i in range(0, len(text), section_size - self.chunk_overlap):
            section_start = max(0, i)
            section_end = min(len(text), i + section_size)
            sections.append(text[section_start:section_end])
        
        logger.info(f"Processing long text in {len(sections)} sections")
        
        # Process each section
        all_chunks = []
        for i, section in enumerate(sections):
            logger.info(f"Processing section {i+1}/{len(sections)}")
            section_chunks = await self._identify_semantic_boundaries(section)
            
            # For all but the first section, check if the first chunk should be merged
            # with the last chunk of the previous section
            if i > 0 and all_chunks and section_chunks:
                # If the first chunk of this section is small, it might be a continuation
                if len(section_chunks[0]) < self.chunk_size / 2:
                    # Merge with the last chunk of the previous section
                    merged_chunk = all_chunks[-1] + section_chunks[0]
                    # If the merged chunk is still reasonable in size, use it
                    if len(merged_chunk) <= self.chunk_size * 1.5:
                        all_chunks[-1] = merged_chunk
                        section_chunks = section_chunks[1:]
            
            all_chunks.extend(section_chunks)
        
        return all_chunks
    
    async def _identify_semantic_boundaries(self, text: str) -> List[str]:
        """
        Use LLM to identify semantic boundaries in text and split accordingly.
        
        Args:
            text: The text to analyze and split
            
        Returns:
            List of text chunks split at semantic boundaries
        """
        prompt = self._create_chunking_prompt(text)
        
        try:
            # Get boundaries from LLM
            response = await self.ollama_client.generate(
                prompt=prompt,
                model=self.model,
                stream=False
            )
            
            # Parse the response
            boundaries = self._parse_boundaries(response.get("response", ""), text)
            
            # If parsing fails or no boundaries are found, fall back to simple chunking
            if not boundaries:
                logger.warning("Failed to identify semantic boundaries, falling back to simple chunking")
                return self._fallback_chunking(text)
            
            # Create chunks based on identified boundaries
            chunks = self._create_chunks_from_boundaries(text, boundaries)
            
            logger.info(f"Created {len(chunks)} semantic chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"Error in semantic chunking: {str(e)}")
            # Fall back to simple chunking on error
            return self._fallback_chunking(text)
    
    def _create_chunking_prompt(self, text: str) -> str:
        """
        Create a prompt for the LLM to identify semantic boundaries.
        
        Args:
            text: The text to analyze
            
        Returns:
            Prompt for the LLM
        """
        return f"""You are an expert in natural language understanding and text analysis. Your task is to identify natural semantic boundaries in the following text. These boundaries should:

1. Respect the semantic structure of the content
2. Create coherent, self-contained chunks that maintain context
3. Occur at natural transitions between topics, ideas, or sections
4. Result in chunks that are approximately {self.chunk_size} characters in length (but prioritize semantic coherence over exact size)

Text to analyze:
```
{text}
```

Analyze the text and identify the character positions where natural semantic boundaries occur. Consider:
- Paragraph breaks that signal topic transitions
- Section boundaries
- Shifts in subject matter or perspective
- Transitions between different types of content (e.g., explanation to example)
- Natural pauses in the flow of information

Output ONLY a JSON array of character positions where chunks should be split, like this:
[500, 1050, 1600, 2200]

These positions should indicate the character index where each new chunk should begin (except the first chunk, which starts at position 0).

Important: Focus on semantic coherence rather than exact chunk size. It's better to have slightly uneven chunks that maintain semantic integrity than equal-sized chunks that break mid-thought.
"""
    
    def _parse_boundaries(self, response_text: str, original_text: str) -> List[int]:
        """
        Parse the LLM response to extract boundary positions.
        
        Args:
            response_text: The LLM response text
            original_text: The original text being chunked
            
        Returns:
            List of character positions for chunk boundaries
        """
        try:
            # Extract JSON array from response
            json_match = re.search(r'\[[\d\s,]+\]', response_text)
            if not json_match:
                logger.warning("Could not find boundary array in LLM response")
                return []
            
            boundaries = json.loads(json_match.group(0))
            
            # Validate boundaries
            if not isinstance(boundaries, list):
                logger.warning(f"Invalid boundary format: {boundaries}")
                return []
            
            # Filter out invalid boundaries
            valid_boundaries = [b for b in boundaries if isinstance(b, (int, float)) and 0 < b < len(original_text)]
            valid_boundaries = sorted(valid_boundaries)
            
            # Add the start position (0) if not present
            if valid_boundaries and valid_boundaries[0] > 0:
                valid_boundaries = [0] + valid_boundaries
            
            logger.info(f"Identified {len(valid_boundaries)} semantic boundaries")
            return valid_boundaries
            
        except Exception as e:
            logger.error(f"Error parsing semantic boundaries: {str(e)}")
            return []
    
    def _create_chunks_from_boundaries(self, text: str, boundaries: List[int]) -> List[str]:
        """
        Create text chunks based on identified boundaries.
        
        Args:
            text: The original text
            boundaries: List of character positions for chunk boundaries
            
        Returns:
            List of text chunks
        """
        chunks = []
        
        # Ensure the text starts at the first boundary
        if not boundaries or boundaries[0] != 0:
            boundaries = [0] + boundaries
        
        # Add the end of the text as the final boundary
        boundaries.append(len(text))
        
        # Create chunks based on boundaries
        for i in range(len(boundaries) - 1):
            start = boundaries[i]
            end = boundaries[i + 1]
            
            # Skip empty chunks
            if start == end:
                continue
                
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
        
        # Apply overlap if specified
        if self.chunk_overlap > 0:
            chunks = self._apply_overlap(chunks)
        
        return chunks
    
    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        """
        Apply overlap between chunks to maintain context.
        
        Args:
            chunks: List of text chunks without overlap
            
        Returns:
            List of text chunks with overlap applied
        """
        if not chunks or len(chunks) < 2:
            return chunks
            
        result = [chunks[0]]
        
        for i in range(1, len(chunks)):
            prev_chunk = chunks[i-1]
            current_chunk = chunks[i]
            
            # Calculate overlap size (in characters)
            overlap_size = min(self.chunk_overlap, len(prev_chunk) // 2)
            
            # If previous chunk is too small, don't apply overlap
            if len(prev_chunk) <= overlap_size * 2:
                result.append(current_chunk)
                continue
                
            # Get the overlap text from the end of the previous chunk
            overlap_text = prev_chunk[-overlap_size:]
            
            # Add the overlap to the beginning of the current chunk
            result.append(overlap_text + current_chunk)
            
        return result
    
    def _fallback_chunking(self, text: str) -> List[str]:
        """
        Fallback method for chunking when LLM-based chunking fails.
        
        Args:
            text: The text to chunk
            
        Returns:
            List of text chunks
        """
        logger.info("Using fallback chunking method")
        
        # Split by paragraphs first
        paragraphs = re.split(r'\n\s*\n', text)
        
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # If adding this paragraph would exceed the chunk size,
            # save the current chunk and start a new one
            if len(current_chunk) + len(paragraph) > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                # Otherwise, add the paragraph to the current chunk
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
        
        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # Apply overlap if specified
        if self.chunk_overlap > 0:
            chunks = self._apply_overlap(chunks)
        
        return chunks

================
File: app/rag/__init__.py
================
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.rag.vector_store import VectorStore
from app.rag.rag_engine import RAGEngine

================
File: app/rag/document_processor.py
================
import os
import logging
from typing import List, Dict, Any, Optional, Literal
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter,
    TokenTextSplitter
)
from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader, UnstructuredMarkdownLoader
from langchain.schema import Document as LangchainDocument

from app.core.config import UPLOAD_DIR, CHUNK_SIZE, CHUNK_OVERLAP, USE_CHUNKING_JUDGE
from app.models.document import Document, Chunk
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.chunkers.semantic_chunker import SemanticChunker

logger = logging.getLogger("app.rag.document_processor")

class DocumentProcessor:
    """
    Process documents for RAG with support for multiple chunking strategies
    """
    def __init__(
        self,
        chunk_size: int = CHUNK_SIZE,
        chunk_overlap: int = CHUNK_OVERLAP,
        chunking_strategy: str = "recursive"
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunking_strategy = chunking_strategy
        self.loader_map = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.csv': CSVLoader,
            '.md': UnstructuredMarkdownLoader,
        }
        self.text_splitter = self._get_text_splitter()
    
    def _get_text_splitter(self, file_ext=None):
        """Get the appropriate text splitter based on chunking strategy and file type"""
        logger.info(f"Using chunking strategy: {self.chunking_strategy} for file type: {file_ext}")
        
        # If we have a chunking analysis in document metadata, log it
        if hasattr(self, 'document') and self.document and 'chunking_analysis' in self.document.metadata:
            logger.info(f"Chunking analysis: {self.document.metadata['chunking_analysis']['justification']}")
        
        # Text file handling - use paragraph-based splitting for more natural chunks
        if file_ext == ".txt":
            # Use a larger chunk size for text files to preserve more context
            larger_chunk_size = self.chunk_size * 3  # Increase from 500 to 1500
            logger.info(f"Using paragraph-based splitting for text file with increased chunk size {larger_chunk_size}")
            return RecursiveCharacterTextSplitter(
                chunk_size=larger_chunk_size,
                chunk_overlap=self.chunk_overlap * 2,  # Increase overlap as well
                separators=["\n\n", "\n", ".", " ", ""]
            )
        
        # PDF-specific handling
        if file_ext == ".pdf":
            logger.info(f"Using PDF-specific splitting with chunk size {self.chunk_size}")
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=["\n\n", "\n", ".", " ", ""]
            )
        
        # Markdown-specific handling
        if file_ext == ".md" and self.chunking_strategy == "markdown":
            logger.info("Using header-based splitting for markdown")
            # First split by headers
            header_splitter = MarkdownHeaderTextSplitter(
                headers_to_split_on=[
                    ("#", "header1"),
                    ("##", "header2"),
                    ("###", "header3"),
                    ("####", "header4"),
                ]
            )
            # Then apply recursive splitting to each section
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
        
        # CSV-specific handling
        if file_ext == ".csv":
            logger.info(f"Using larger chunks for CSV with chunk size {self.chunk_size}")
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size * 2,  # Double chunk size for CSVs
                chunk_overlap=self.chunk_overlap
            )
        
        # Standard strategies
        if self.chunking_strategy == "recursive":
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            )
        elif self.chunking_strategy == "token":
            return TokenTextSplitter(
                chunk_size=self.chunk_size // 4,  # Adjust for tokens vs characters
                chunk_overlap=self.chunk_overlap // 4
            )
        elif self.chunking_strategy == "markdown":
            return MarkdownHeaderTextSplitter(
                headers_to_split_on=[
                    ("#", "header1"),
                    ("##", "header2"),
                    ("###", "header3"),
                    ("####", "header4"),
                ]
            )
        elif self.chunking_strategy == "semantic":
            logger.info(f"Using semantic chunking with chunk size {self.chunk_size}")
            return SemanticChunker(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
        else:
            logger.warning(f"Unknown chunking strategy: {self.chunking_strategy}, falling back to recursive")
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=["\n\n", "\n", ".", " ", ""]
            )
    
    async def process_document(self, document: Document) -> Document:
        """
        Process a document by splitting it into chunks
        """
        try:
            logger.info(f"Processing document: {document.filename}")
            
            # Get the document path
            file_path = os.path.join(UPLOAD_DIR, document.id, document.filename)
            
            # Get file extension for specialized handling
            _, ext = os.path.splitext(file_path.lower())
            
            # Use Chunking Judge if enabled
            if USE_CHUNKING_JUDGE:
                logger.info(f"Using Chunking Judge to analyze document: {document.filename}")
                chunking_judge = ChunkingJudge()
                analysis_result = await chunking_judge.analyze_document(document)
                
                # Update chunking strategy and parameters
                self.chunking_strategy = analysis_result["strategy"]
                if "parameters" in analysis_result and "chunk_size" in analysis_result["parameters"]:
                    self.chunk_size = analysis_result["parameters"]["chunk_size"]
                if "parameters" in analysis_result and "chunk_overlap" in analysis_result["parameters"]:
                    self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
                
                # Store the chunking analysis in document metadata
                document.metadata["chunking_analysis"] = analysis_result
                
                logger.info(f"Chunking Judge recommendation: strategy={self.chunking_strategy}, " +
                           f"chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
            else:
                logger.info(f"Chunking Judge disabled, using default strategy: {self.chunking_strategy}")
            
            # Get appropriate text splitter for this file type
            self.text_splitter = self._get_text_splitter(ext)
            
            # Extract text from the document based on file type
            docs = await self._load_document(file_path)
            
            # Split the document into chunks
            chunks = self._split_document(docs)
            
            # Update the document with chunks
            document.chunks = []
            for i, chunk in enumerate(chunks):
                # Start with the chunk's existing metadata
                metadata = dict(chunk.metadata) if chunk.metadata else {}
                
                # Add document metadata
                metadata.update({
                    "document_id": document.id,
                    "index": i,
                    "folder": document.folder
                })
                
                # Handle tags specially - store as string for ChromaDB compatibility
                if document.tags:
                    metadata["tags_list"] = document.tags  # Keep original list for internal use
                    metadata["tags"] = ",".join(document.tags)  # String version for ChromaDB
                else:
                    metadata["tags"] = ""
                    metadata["tags_list"] = []
                
                # Create the chunk with processed metadata
                document.chunks.append(
                    Chunk(
                        content=chunk.page_content,
                        metadata=metadata
                    )
                )
            
            logger.info(f"Document processed into {len(document.chunks)} chunks")
            
            return document
        except Exception as e:
            logger.error(f"Error processing document {document.filename}: {str(e)}")
            raise
    
    async def _load_document(self, file_path: str) -> List[LangchainDocument]:
        """
        Load a document based on its file type with improved error handling
        """
        _, ext = os.path.splitext(file_path.lower())
        
        try:
            if ext == ".pdf":
                try:
                    loader = PyPDFLoader(file_path)
                    return loader.load()
                except Exception as pdf_error:
                    logger.warning(f"Error using PyPDFLoader: {str(pdf_error)}. Falling back to manual loading.")
                    # Try loading as text, but ignore decoding errors
                    with open(file_path, 'rb') as f:
                        content = f.read().decode('utf-8', errors='ignore')
                    return [LangchainDocument(page_content=content, metadata={"source": file_path})]
            elif ext == ".csv":
                loader = CSVLoader(file_path)
                return loader.load()
            elif ext == ".md":
                try:
                    loader = UnstructuredMarkdownLoader(file_path)
                    return loader.load()
                except Exception as md_error:
                    logger.warning(f"Error using UnstructuredMarkdownLoader: {str(md_error)}. Falling back to manual loading.")
                    # Try loading as text, but ignore decoding errors
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    return [LangchainDocument(page_content=content, metadata={"source": file_path})]
            else:
                # Default to text loader for txt and other files
                loader = TextLoader(file_path)
                return loader.load()
        except Exception as e:
            logger.error(f"Error loading document {file_path}: {str(e)}")
            # Create a simple document with file content to avoid complete failure
            try:
                with open(file_path, 'rb') as f:
                    content = f.read().decode('utf-8', errors='ignore')
                return [LangchainDocument(page_content=content, metadata={"source": file_path})]
            except Exception as fallback_error:
                logger.error(f"Failed to load {file_path} even with fallback: {str(fallback_error)}")
                raise
    
    def _split_document(self, docs: List[LangchainDocument]) -> List[LangchainDocument]:
        """
        Split a document into chunks with a limit on total chunks
        """
        try:
            # Split the document using the configured text splitter
            chunks = self.text_splitter.split_documents(docs)
            
            # Log the original number of chunks
            logger.info(f"Document initially split into {len(chunks)} chunks")
            
            # Limit the maximum number of chunks per document to prevent excessive chunking
            MAX_CHUNKS = 30  # Reduced from 50 to 30 to prevent excessive chunking
            
            if len(chunks) > MAX_CHUNKS:
                logger.warning(f"Document produced {len(chunks)} chunks, limiting to {MAX_CHUNKS}")
                
                # Option 1: Simply truncate to the first MAX_CHUNKS
                # return chunks[:MAX_CHUNKS]
                
                # Option 2: Take evenly distributed chunks to maintain coverage of the document
                step = len(chunks) // MAX_CHUNKS
                limited_chunks = [chunks[i] for i in range(0, len(chunks), step)][:MAX_CHUNKS]
                
                # Ensure we have exactly MAX_CHUNKS or fewer
                return limited_chunks[:MAX_CHUNKS]
            
            return chunks
        except Exception as e:
            logger.error(f"Error splitting document: {str(e)}")
            raise
    
    def extract_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        Extract metadata from a document
        """
        try:
            _, ext = os.path.splitext(file_path.lower())
            file_stats = os.stat(file_path)
            
            metadata = {
                "file_size": file_stats.st_size,
                "file_type": ext[1:] if ext else "unknown",
                "created_at": file_stats.st_ctime,
                "modified_at": file_stats.st_mtime
            }
            
            # Add file type-specific metadata extraction here
            if ext == ".pdf":
                try:
                    # Extract PDF-specific metadata if possible
                    import pypdf
                    with open(file_path, 'rb') as f:
                        pdf = pypdf.PdfReader(f)
                        if pdf.metadata:
                            for key, value in pdf.metadata.items():
                                if key and value:
                                    # Clean up the key name
                                    clean_key = key.strip('/').lower()
                                    metadata[clean_key] = str(value)
                except Exception as e:
                    logger.warning(f"Error extracting PDF metadata: {str(e)}")
            
            return metadata
        except Exception as e:
            logger.error(f"Error extracting metadata from {file_path}: {str(e)}")
            return {}

================
File: app/rag/ollama_client.py
================
import httpx
import json
import logging
import time
import asyncio
from typing import Dict, List, Any, Optional, Generator, Tuple, Union
from sse_starlette.sse import EventSourceResponse

from app.core.config import OLLAMA_BASE_URL, DEFAULT_MODEL

logger = logging.getLogger("app.rag.ollama_client")

class OllamaClient:
    """
    Client for interacting with Ollama API
    """
    def __init__(self, base_url: str = OLLAMA_BASE_URL, timeout: int = 60):
        self.base_url = base_url
        self.timeout = timeout
        self.client = httpx.AsyncClient(timeout=timeout)
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """
        List available models
        """
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = await self.client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                return response.json().get("models", [])
            except Exception as e:
                logger.error(f"Error listing models (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise
    
    async def generate(
        self,
        prompt: str,
        model: str = DEFAULT_MODEL,
        system_prompt: Optional[str] = None,
        stream: bool = True,
        parameters: Dict[str, Any] = None
    ) -> Union[Dict[str, Any], Generator[str, None, None]]:
        """
        Generate a response from the model
        """
        if parameters is None:
            parameters = {}
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            **parameters
        }
        
        if system_prompt:
            payload["system"] = system_prompt
            
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                if stream:
                    return await self._stream_response(payload)
                else:
                    response = await self.client.post(
                        f"{self.base_url}/api/generate",
                        json=payload
                    )
                    response.raise_for_status()
                    response_data = response.json()
                    
                    # Check if the response contains an error message from the model
                    if 'error' in response_data:
                        logger.warning(f"Model returned an error: {response_data['error']}")
                        # Return the error message instead of raising an exception
                        return {
                            "response": f"I'm unable to answer that question. {response_data['error']}",
                            "error": response_data['error']
                        }
                    
                    return response_data
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP error generating response (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    # Return a user-friendly error message
                    return {
                        "response": "I'm unable to answer that question right now. There was an issue connecting to the language model.",
                        "error": str(e)
                    }
            except Exception as e:
                logger.error(f"Error generating response (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    # Return a user-friendly error message instead of raising
                    return {
                        "response": "I'm unable to process your request right now. There might be an issue with the language model or your question.",
                        "error": str(e)
                    }
    
    async def _stream_response(self, payload: Dict[str, Any]):
        """
        Stream response from the model
        """
        async def event_generator():
            try:
                async with httpx.AsyncClient(timeout=120) as client:
                    async with client.stream(
                        "POST",
                        f"{self.base_url}/api/generate",
                        json=payload,
                        timeout=120
                    ) as response:
                        response.raise_for_status()
                        async for line in response.aiter_lines():
                            if line:
                                try:
                                    data = json.loads(line)
                                    
                                    # Check if the response contains an error message
                                    if 'error' in data:
                                        logger.warning(f"Model returned an error in stream: {data['error']}")
                                        yield f"I'm unable to answer that question. {data['error']}"
                                        break
                                    
                                    yield data.get("response", "")
                                    if data.get("done", False):
                                        break
                                except json.JSONDecodeError:
                                    logger.error(f"Error decoding JSON: {line}")
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP error in streaming response: {str(e)}")
                yield "I'm unable to answer that question right now. There was an issue connecting to the language model."
            except Exception as e:
                logger.error(f"Error in streaming response: {str(e)}")
                yield "I'm unable to process your request right now. There might be an issue with the language model or your question."
        
        return event_generator()  # Return the generator directly
    
    async def create_embedding(
        self, 
        text: str, 
        model: str = DEFAULT_MODEL
    ) -> List[float]:
        """
        Create an embedding for the given text
        """
        payload = {
            "model": model,
            "prompt": text
        }
        
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = await self.client.post(
                    f"{self.base_url}/api/embeddings",
                    json=payload
                )
                response.raise_for_status()
                return response.json().get("embedding", [])
            except Exception as e:
                logger.error(f"Error creating embedding (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise

================
File: app/rag/rag_engine.py
================
import logging
import time
import asyncio
import httpx
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from app.core.config import DEFAULT_MODEL, USE_RETRIEVAL_JUDGE
from app.models.chat import Citation, Message
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge

logger = logging.getLogger("app.rag.rag_engine")

class RAGEngine:
    """
    RAG (Retrieval Augmented Generation) Engine
    """
    def __init__(
        self,
        vector_store: Optional[VectorStore] = None,
        ollama_client: Optional[OllamaClient] = None,
        retrieval_judge: Optional[RetrievalJudge] = None
    ):
        self.vector_store = vector_store or VectorStore()
        self.ollama_client = ollama_client or OllamaClient()
        self.retrieval_judge = retrieval_judge if retrieval_judge is not None else (
            RetrievalJudge(ollama_client=self.ollama_client) if USE_RETRIEVAL_JUDGE else None
        )
        
        if self.retrieval_judge:
            logger.info("Retrieval Judge is enabled")
        else:
            logger.info("Retrieval Judge is disabled")
    
    async def query(
        self,
        query: str,
        model: str = DEFAULT_MODEL,
        use_rag: bool = True,
        top_k: int = 10,
        system_prompt: Optional[str] = None,
        stream: bool = False,
        model_parameters: Dict[str, Any] = None,
        conversation_history: Optional[List[Message]] = None,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Query the RAG engine with optional conversation history and metadata filtering
        """
        start_time = time.time()
        document_ids = []
        
        try:
            logger.info(f"RAG query: {query[:50]}...")
            
            # Get context from vector store if RAG is enabled
            context = ""
            sources = []
            
            # Format conversation history if provided
            conversation_context = ""
            if conversation_history and len(conversation_history) > 1:  # Only include history if there's more than just the current message
                # Get the last few messages (up to 5) to keep context manageable, but exclude the most recent user message
                # which is the current query and shouldn't be treated as history
                recent_history = conversation_history[:-1]
                if len(recent_history) > 5:
                    recent_history = recent_history[-5:]
                
                # Format the conversation history
                history_pieces = []
                for msg in recent_history:
                    role_prefix = "User" if msg.role == "user" else "Assistant"
                    history_pieces.append(f"{role_prefix}: {msg.content}")
                
                conversation_context = "\n".join(history_pieces)
                logger.info(f"Including conversation history with {len(recent_history)} messages")
            else:
                logger.info("No previous conversation history to include")
            
            if use_rag:
                # Use enhanced retrieval if Retrieval Judge is enabled
                if self.retrieval_judge:
                    logger.info("Using enhanced retrieval with Retrieval Judge")
                    context, sources, document_ids = await self._enhanced_retrieval(
                        query=query,
                        conversation_context=conversation_context,
                        top_k=top_k,
                        metadata_filters=metadata_filters
                    )
                else:
                    # Use standard retrieval
                    logger.info("Using standard retrieval (Retrieval Judge disabled)")
                    # Check if there are any documents in the vector store
                    stats = self.vector_store.get_stats()
                    if stats["count"] == 0:
                        logger.warning("RAG is enabled but no documents are available in the vector store")
                        # Add a note to the context that no documents are available
                        context = "Note: No documents are available for retrieval. Please upload documents to use RAG effectively."
                    else:
                        # Combine the current query with conversation context for better retrieval
                        search_query = query
                        if conversation_context:
                            # For retrieval, we focus more on the current query but include
                            # some context from the conversation to improve relevance
                            search_query = f"{query} {conversation_context[-200:]}"
                        
                        # Log the search query
                        logger.info(f"Searching with query: {search_query[:100]}...")
                        
                        # Use a higher fixed value for top_k to get more potential matches, then filter by relevance
                        search_results = await self.vector_store.search(
                            query=search_query,
                            top_k=15,  # Fixed value to retrieve more chunks (document has 24 chunks total)
                            filter_criteria=metadata_filters
                        )
                        
                        if search_results:
                            # Log the number of results
                            logger.info(f"Retrieved {len(search_results)} chunks from vector store")
                            
                            # Format context with source information
                            context_pieces = []
                            relevant_results = []
                            
                            # Set a relevance threshold - only include chunks with relevance score above this
                            relevance_threshold = 0.4  # Lower threshold to ensure we include more relevant chunks
                            
                            for i, result in enumerate(search_results):
                                # Skip results with None content
                                if "content" not in result or result["content"] is None:
                                    logger.warning(f"Skipping result {i+1} due to missing or None content")
                                    continue
                                    
                                # Extract metadata for better context
                                metadata = result["metadata"]
                                filename = metadata.get("filename", "Unknown")
                                tags = metadata.get("tags", [])
                                folder = metadata.get("folder", "/")
                                
                                # Calculate relevance score (lower distance = higher relevance)
                                relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
                                
                                # Log the relevance score for debugging
                                logger.debug(f"Chunk {i+1} relevance score: {relevance_score:.4f}")
                                
                                # Only include chunks that are sufficiently relevant
                                if relevance_score >= relevance_threshold:
                                    # Format the context piece with metadata
                                    context_piece = f"[{len(relevant_results)+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
                                    context_pieces.append(context_piece)
                                    
                                    # Track the source for citation
                                    doc_id = metadata["document_id"]
                                    document_ids.append(doc_id)
                                    
                                    sources.append({
                                        "document_id": doc_id,
                                        "chunk_id": result["chunk_id"],
                                        "relevance_score": relevance_score,
                                        "excerpt": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                                        "filename": filename,
                                        "tags": tags,
                                        "folder": folder
                                    })
                                    
                                    relevant_results.append(result)
                                else:
                                    logger.info(f"Skipping chunk {i+1} with low relevance score: {relevance_score:.4f}")
                            
                            # Join all context pieces
                            context = "\n\n".join(context_pieces)
                            
                            # Log how many chunks were filtered out due to low relevance
                            logger.info(f"Using {len(relevant_results)} of {len(search_results)} chunks after relevance filtering")
                            
                            # Log the total context length
                            logger.info(f"Total context length: {len(context)} characters")
                            
                            # Check if we have enough relevant context
                            if len(relevant_results) == 0:
                                logger.warning("No sufficiently relevant documents found for the query")
                                context = "Note: No sufficiently relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents."
                            elif len(context.strip()) < 50:  # Very short context might not be useful
                                logger.warning("Context is too short to be useful")
                                context = "Note: The retrieved context is too limited to provide a comprehensive answer to your query. The system cannot provide a specific answer based on the available documents."
                        else:
                            logger.warning("No relevant documents found for the query")
                            context = "Note: No relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents."
            
            # Create full prompt with context and conversation history
            full_prompt = query
            
            # If we have conversation history, include it
            if conversation_context:
                full_prompt = f"""Previous conversation:
{conversation_context}

User's new question: {query}"""
            else:
                # No conversation history, just use the query directly
                full_prompt = f"""User Question: {query}"""
            
            # If we have context from RAG, include it
            if context:
                # Construct the prompt differently based on whether we have conversation history
                if conversation_context:
                    full_prompt = f"""Context:
{context}

Previous conversation:
{conversation_context}

User's new question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
                else:
                    full_prompt = f"""Context:
{context}

User Question: {query}

IMPORTANT INSTRUCTIONS:
1. ONLY use information that is explicitly stated in the provided context above.
2. When using information from the context, ALWAYS reference your sources with the number in square brackets, like [1] or [2].
3. If the context contains the answer, provide it clearly and concisely.
4. If the context doesn't contain the answer, explicitly state: "Based on the provided documents, I don't have information about [topic]."
5. NEVER make up or hallucinate information that is not in the context.
6. If you're unsure about something, be honest about your uncertainty.
7. Organize your answer in a clear, structured way.
8. This is a new conversation with no previous history - treat it as such.
9. If you need to use your general knowledge because the context is insufficient, clearly indicate this by stating: "However, generally speaking..."
"""
            
            # Create system prompt if not provided
            if not system_prompt:
                system_prompt = """You are a helpful assistant that provides accurate, factual responses based on the Metis RAG system.

ROLE AND CAPABILITIES:
- You have access to a Retrieval-Augmented Generation (RAG) system that can retrieve relevant documents to answer questions.
- Your primary function is to use the retrieved context to provide accurate, well-informed answers.
- You can cite sources using the numbers in square brackets like [1] or [2] when they are provided in the context.

STRICT GUIDELINES FOR USING CONTEXT:
- ONLY use information that is explicitly stated in the provided context.
- NEVER make up or hallucinate information that is not in the context.
- If the context doesn't contain the answer, explicitly state that the information is not available in the provided documents.
- Do not use your general knowledge unless the context is insufficient, and clearly indicate when you're doing so.
- Analyze the context carefully to find the most relevant information for the user's question.
- If multiple sources provide different information, synthesize them and explain any discrepancies.
- If the context includes metadata like filenames, tags, or folders, use this to understand the source and relevance of the information.

WHEN CONTEXT IS INSUFFICIENT:
- Clearly state: "Based on the provided documents, I don't have information about [topic]."
- Be specific about what information is missing.
- Only then provide a general response based on your knowledge, and clearly state: "However, generally speaking..." to distinguish this from information in the context.
- Never pretend to have information that isn't in the context.

CONVERSATION HANDLING:
- IMPORTANT: Only refer to previous conversations if they are explicitly provided in the conversation history.
- NEVER fabricate or hallucinate previous exchanges that weren't actually provided.
- If no conversation history is provided, treat the query as a new, standalone question.
- Only maintain continuity with previous exchanges when conversation history is explicitly provided.

RESPONSE STYLE:
- Be clear, direct, and helpful.
- Structure your responses logically.
- Use appropriate formatting to enhance readability.
- Maintain a consistent, professional tone throughout the conversation.
- For new conversations with no history, start fresh without referring to non-existent previous exchanges.
- DO NOT start your responses with phrases like "I've retrieved relevant context" or similar preambles.
- Answer questions directly without mentioning the retrieval process.
- Always cite your sources with numbers in square brackets [1] when using information from the context.
"""
            # Log the prompt and system prompt for debugging
            logger.debug(f"System prompt: {system_prompt[:200]}...")
            logger.debug(f"Full prompt: {full_prompt[:200]}...")
            
            # Generate response
            if stream:
                # For streaming, just return the stream response
                logger.info(f"Generating streaming response with model: {model}")
                stream_response = await self.ollama_client.generate(
                    prompt=full_prompt,
                    model=model,
                    system_prompt=system_prompt,
                    stream=True,
                    parameters=model_parameters or {}
                )
                
                # Record analytics asynchronously
                response_time_ms = (time.time() - start_time) * 1000
                logger.info(f"Response time: {response_time_ms:.2f}ms")
                
                # Use await instead of async for with the coroutine
                await self._record_analytics(
                    query=query,
                    model=model,
                    use_rag=use_rag,
                    response_time_ms=response_time_ms,
                    document_ids=document_ids,
                    token_count=len(query.split())  # Approximate token count
                )
                
                return {
                    "query": query,
                    "stream": stream_response,
                    "sources": [Citation(**source) for source in sources] if sources else []
                }
            else:
                # For non-streaming, get the complete response
                logger.info(f"Generating non-streaming response with model: {model}")
                response = await self.ollama_client.generate(
                    prompt=full_prompt,
                    model=model,
                    system_prompt=system_prompt,
                    stream=False,
                    parameters=model_parameters or {}
                )
                
                # Calculate response time
                response_time_ms = (time.time() - start_time) * 1000
                logger.info(f"Response time: {response_time_ms:.2f}ms")
                
                # Check if there was an error in the response
                if "error" in response:
                    error_message = response.get("error", "Unknown error")
                    logger.warning(f"Model returned an error: {error_message}")
                    response_text = response.get("response", f"Error: {error_message}")
                else:
                    # Get response text
                    response_text = response.get("response", "")
                
                logger.info(f"Response length: {len(response_text)} characters")
                
                # Log a preview of the response
                if response_text:
                    logger.debug(f"Response preview: {response_text[:100]}...")
                
                # Record analytics asynchronously
                await self._record_analytics(
                    query=query,
                    model=model,
                    use_rag=use_rag,
                    response_time_ms=response_time_ms,
                    document_ids=document_ids,
                    token_count=len(query.split()) + len(response_text.split())  # Approximate token count
                )
                
                return {
                    "query": query,
                    "answer": response_text,
                    "sources": [Citation(**source) for source in sources] if sources else []
                }
        except Exception as e:
            logger.error(f"Error querying RAG engine: {str(e)}")
            raise
    
    async def _enhanced_retrieval(
        self,
        query: str,
        conversation_context: str = "",
        top_k: int = 10,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, List[Dict[str, Any]], List[str]]:
        """
        Enhanced retrieval using the Retrieval Judge
        
        Args:
            query: The user query
            conversation_context: Optional conversation history context
            top_k: Number of chunks to retrieve
            metadata_filters: Optional filters for retrieval
            
        Returns:
            Tuple of (context, sources, document_ids)
        """
        document_ids = []
        sources = []
        context = ""
        
        try:
            # Check if there are any documents in the vector store
            stats = self.vector_store.get_stats()
            if stats["count"] == 0:
                logger.warning("RAG is enabled but no documents are available in the vector store")
                return "Note: No documents are available for retrieval. Please upload documents to use RAG effectively.", [], []
            
            # Step 1: Analyze the query using the Retrieval Judge
            logger.info("Analyzing query with Retrieval Judge")
            query_analysis = await self.retrieval_judge.analyze_query(query)
            
            # Extract recommended parameters
            recommended_k = query_analysis.get("parameters", {}).get("k", top_k)
            relevance_threshold = query_analysis.get("parameters", {}).get("threshold", 0.4)
            apply_reranking = query_analysis.get("parameters", {}).get("reranking", True)
            
            logger.info(f"Query complexity: {query_analysis.get('complexity', 'unknown')}")
            logger.info(f"Recommended parameters: k={recommended_k}, threshold={relevance_threshold}, reranking={apply_reranking}")
            
            # Combine the current query with conversation context for better retrieval
            search_query = query
            if conversation_context:
                # For retrieval, we focus more on the current query but include
                # some context from the conversation to improve relevance
                search_query = f"{query} {conversation_context[-200:]}"
            
            # Log the search query
            logger.info(f"Searching with query: {search_query[:100]}...")
            
            # Step 2: Initial retrieval with recommended parameters
            search_results = await self.vector_store.search(
                query=search_query,
                top_k=max(15, recommended_k + 5),  # Get a few extra for filtering
                filter_criteria=metadata_filters
            )
            
            if not search_results:
                logger.warning("No relevant documents found for the query")
                return "Note: No relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents.", [], []
            
            # Log the number of results
            logger.info(f"Retrieved {len(search_results)} chunks from vector store")
            
            # Step 3: Evaluate chunks with the Retrieval Judge
            logger.info("Evaluating chunks with Retrieval Judge")
            evaluation = await self.retrieval_judge.evaluate_chunks(query, search_results)
            
            # Extract relevance scores and refinement decision
            relevance_scores = evaluation.get("relevance_scores", {})
            needs_refinement = evaluation.get("needs_refinement", False)
            
            logger.info(f"Chunk evaluation complete, needs_refinement={needs_refinement}")
            
            # Step 4: Refine query if needed and perform additional retrieval
            if needs_refinement:
                logger.info("Refining query based on initial retrieval")
                refined_query = await self.retrieval_judge.refine_query(query, search_results)
                
                logger.info(f"Refined query: {refined_query}")
                
                # Perform additional retrieval with refined query
                additional_results = await self.vector_store.search(
                    query=refined_query,
                    top_k=recommended_k,
                    filter_criteria=metadata_filters
                )
                
                if additional_results:
                    logger.info(f"Retrieved {len(additional_results)} additional chunks with refined query")
                    
                    # Combine results, avoiding duplicates
                    existing_chunk_ids = {result["chunk_id"] for result in search_results}
                    for result in additional_results:
                        if result["chunk_id"] not in existing_chunk_ids:
                            search_results.append(result)
                    
                    # Re-evaluate all chunks
                    logger.info("Re-evaluating all chunks after query refinement")
                    evaluation = await self.retrieval_judge.evaluate_chunks(refined_query, search_results)
                    relevance_scores = evaluation.get("relevance_scores", {})
            
            # Step 5: Filter and re-rank chunks based on relevance scores
            relevant_results = []
            
            for result in search_results:
                # Skip results with None content
                if "content" not in result or result["content"] is None:
                    continue
                
                chunk_id = result["chunk_id"]
                
                # Get relevance score from evaluation or calculate from distance
                if chunk_id in relevance_scores:
                    relevance_score = relevance_scores[chunk_id]
                else:
                    # Calculate relevance score (lower distance = higher relevance)
                    relevance_score = 1.0 - (result["distance"] if result["distance"] is not None else 0)
                
                # Only include chunks that are sufficiently relevant
                if relevance_score >= relevance_threshold:
                    # Add relevance score to result for sorting
                    result["relevance_score"] = relevance_score
                    relevant_results.append(result)
            
            # Sort by relevance score if reranking is enabled
            if apply_reranking:
                relevant_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
            
            # Step 6: Optimize context assembly if we have enough chunks
            if len(relevant_results) > 3 and apply_reranking:
                logger.info("Optimizing context assembly with Retrieval Judge")
                optimized_results = await self.retrieval_judge.optimize_context(query, relevant_results)
                if optimized_results:
                    relevant_results = optimized_results
            
            # Step 7: Format context with source information
            context_pieces = []
            
            for i, result in enumerate(relevant_results):
                # Extract metadata for better context
                metadata = result["metadata"]
                filename = metadata.get("filename", "Unknown")
                tags = metadata.get("tags", [])
                folder = metadata.get("folder", "/")
                
                # Format the context piece with metadata
                context_piece = f"[{i+1}] Source: {filename}, Tags: {tags}, Folder: {folder}\n\n{result['content']}"
                context_pieces.append(context_piece)
                
                # Track the source for citation
                doc_id = metadata["document_id"]
                document_ids.append(doc_id)
                
                # Get relevance score (either from judge or distance)
                relevance_score = result.get("relevance_score", 1.0 - (result["distance"] if result["distance"] is not None else 0))
                
                sources.append({
                    "document_id": doc_id,
                    "chunk_id": result["chunk_id"],
                    "relevance_score": relevance_score,
                    "excerpt": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                    "filename": filename,
                    "tags": tags,
                    "folder": folder
                })
            
            # Join all context pieces
            context = "\n\n".join(context_pieces)
            
            # Log how many chunks were used
            logger.info(f"Using {len(relevant_results)} chunks after Retrieval Judge optimization")
            
            # Log the total context length
            logger.info(f"Total context length: {len(context)} characters")
            
            # Check if we have enough relevant context
            if len(relevant_results) == 0:
                logger.warning("No sufficiently relevant documents found for the query")
                context = "Note: No sufficiently relevant documents found in the knowledge base for your query. The system cannot provide a specific answer based on the available documents."
            elif len(context.strip()) < 50:  # Very short context might not be useful
                logger.warning("Context is too short to be useful")
                context = "Note: The retrieved context is too limited to provide a comprehensive answer to your query. The system cannot provide a specific answer based on the available documents."
            
            return context, sources, document_ids
            
        except Exception as e:
            logger.error(f"Error in enhanced retrieval: {str(e)}")
            # Return empty context in case of error
            return "Note: An error occurred during retrieval. The system cannot provide a specific answer based on the available documents.", [], []
    
    async def _record_analytics(
        self,
        query: str,
        model: str,
        use_rag: bool,
        response_time_ms: float,
        document_ids: List[str],
        token_count: int
    ) -> None:
        """
        Record query analytics asynchronously
        """
        try:
            # Prepare analytics data
            analytics_data = {
                "query": query,
                "model": model,
                "use_rag": use_rag,
                "timestamp": datetime.now().isoformat(),
                "response_time_ms": response_time_ms,
                "document_ids": document_ids,
                "token_count": token_count
            }
            
            # Send analytics data to the API
            async with httpx.AsyncClient() as client:
                await client.post(
                    "http://localhost:8000/api/analytics/record_query",
                    json=analytics_data,
                    timeout=5.0
                )
            
            logger.debug(f"Recorded analytics for query: {query[:30]}...")
        except Exception as e:
            # Don't let analytics errors affect the main functionality
            logger.error(f"Error recording analytics: {str(e)}")

================
File: app/rag/vector_store.py
================
import logging
import os
import json
import time
from typing import List, Dict, Any, Optional, Tuple
import chromadb
from chromadb.config import Settings

from app.core.config import CHROMA_DB_DIR, DEFAULT_EMBEDDING_MODEL
from app.models.document import Document, Chunk
from app.rag.ollama_client import OllamaClient

logger = logging.getLogger("app.rag.vector_store")

class VectorStore:
    """
    Vector store for document embeddings using ChromaDB with caching for performance
    """
    def __init__(
        self,
        persist_directory: str = CHROMA_DB_DIR,
        embedding_model: str = DEFAULT_EMBEDDING_MODEL,
        enable_cache: bool = True,
        cache_ttl: int = 3600  # 1 hour in seconds
    ):
        self.persist_directory = persist_directory
        self.embedding_model = embedding_model
        self.ollama_client = None
        
        # Cache settings
        self.enable_cache = enable_cache
        self.cache_ttl = cache_ttl
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.max_cache_size = 1000
        
        # Initialize ChromaDB
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        
        # Create or get the collection
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
        
        logger.info(f"Vector store initialized with collection 'documents', caching {'enabled' if enable_cache else 'disabled'}")
    
    async def add_document(self, document: Document) -> None:
        """
        Add a document to the vector store with batch embedding
        """
        try:
            logger.info(f"Adding document {document.id} to vector store")
            
            # Make sure we have an Ollama client
            if self.ollama_client is None:
                self.ollama_client = OllamaClient()
            
            # Prepare chunks for batch processing
            chunks_to_embed = [chunk for chunk in document.chunks if not chunk.embedding]
            chunk_contents = [chunk.content for chunk in chunks_to_embed]
            
            # Create embeddings in batch if possible
            if chunk_contents:
                try:
                    # Batch embedding
                    embeddings = await self._batch_create_embeddings(chunk_contents)
                    
                    # Assign embeddings to chunks
                    for i, chunk in enumerate(chunks_to_embed):
                        chunk.embedding = embeddings[i]
                except Exception as batch_error:
                    logger.warning(f"Batch embedding failed: {str(batch_error)}. Falling back to sequential embedding.")
                    # Fall back to sequential embedding
                    for chunk in chunks_to_embed:
                        chunk.embedding = await self.ollama_client.create_embedding(
                            text=chunk.content,
                            model=self.embedding_model
                        )
            
            # Add chunks to the collection
            for chunk in document.chunks:
                if not chunk.embedding:
                    logger.warning(f"Chunk {chunk.id} has no embedding, skipping")
                    continue
                    
                # Prepare metadata - convert any lists to strings to satisfy ChromaDB requirements
                metadata = {
                    "document_id": document.id,
                    "chunk_index": chunk.metadata.get("index", 0),
                    "filename": document.filename,
                    "tags": ",".join(document.tags) if document.tags else "",
                    "folder": document.folder
                }
                
                # Add any additional metadata from the chunk
                for key, value in chunk.metadata.items():
                    # Convert lists to strings if present
                    if isinstance(value, list):
                        metadata[key] = ",".join(str(item) for item in value)
                    else:
                        metadata[key] = value
                
                self.collection.add(
                    ids=[chunk.id],
                    embeddings=[chunk.embedding],
                    documents=[chunk.content],
                    metadatas=[metadata]
                )
            
            # Clear the cache to ensure we're using the latest embeddings
            self.clear_cache()
            
            logger.info(f"Added {len(document.chunks)} chunks to vector store for document {document.id}")
        except Exception as e:
            logger.error(f"Error adding document {document.id} to vector store: {str(e)}")
            raise
    
    async def _batch_create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Create embeddings for a list of texts (batched)"""
        try:
            # Attempt to use Langchain's embed_documents for batch embedding
            from langchain.embeddings import OllamaEmbeddings
            embeddings_model = OllamaEmbeddings(model=self.embedding_model)
            return embeddings_model.embed_documents(texts)
        except (ImportError, NotImplementedError) as e:
            # Handle the case where the provider doesn't support batch embedding
            logger.warning(f"Batch embedding not supported: {str(e)}. Falling back to sequential embedding.")
            embeddings = []
            for text in texts:
                embedding = await self.ollama_client.create_embedding(text=text, model=self.embedding_model)
                embeddings.append(embedding)
            return embeddings
    
    async def update_document_metadata(self, document_id: str, metadata_update: Dict[str, Any]) -> None:
        """
        Update metadata for all chunks of a document
        """
        try:
            logger.info(f"Updating metadata for document {document_id}")
            
            # Get all chunks for the document
            results = self.collection.get(
                where={"document_id": document_id}
            )
            
            if not results["ids"]:
                logger.warning(f"No chunks found for document {document_id}")
                return
            
            # Update each chunk's metadata
            for i, chunk_id in enumerate(results["ids"]):
                # Get current metadata
                current_metadata = results["metadatas"][i]
                
                # Update with new metadata
                updated_metadata = {**current_metadata, **metadata_update}
                
                # Update in collection
                self.collection.update(
                    ids=[chunk_id],
                    metadatas=[updated_metadata]
                )
            
            logger.info(f"Updated metadata for {len(results['ids'])} chunks of document {document_id}")
        except Exception as e:
            logger.error(f"Error updating metadata for document {document_id}: {str(e)}")
            raise
    
    async def search(
        self,
        query: str,
        top_k: int = 5,
        filter_criteria: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for documents similar to the query with caching for performance
        """
        try:
            # Check cache if enabled
            if self.enable_cache:
                cache_key = self._create_cache_key(query, top_k, filter_criteria)
                cached_result = self._get_from_cache(cache_key)
                
                if cached_result:
                    self.cache_hits += 1
                    logger.info(f"Cache hit for query: {query[:50]}... (hits: {self.cache_hits}, misses: {self.cache_misses})")
                    return cached_result
                
                self.cache_misses += 1
                logger.info(f"Cache miss for query: {query[:50]}... (hits: {self.cache_hits}, misses: {self.cache_misses})")
            
            logger.info(f"Searching for documents similar to query: {query[:50]}...")
            
            # Make sure we have an Ollama client
            if self.ollama_client is None:
                self.ollama_client = OllamaClient()
            
            # Create query embedding
            query_embedding = await self.ollama_client.create_embedding(
                text=query,
                model=self.embedding_model
            )
            
            # Log the query embedding for debugging
            logger.debug(f"Query embedding (first 5 values): {query_embedding[:5]}")
            
            # Log filter criteria if present
            if filter_criteria:
                logger.info(f"Applying filter criteria: {filter_criteria}")
            
            # Search for similar documents
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=filter_criteria
            )
            
            # Format results
            formatted_results = []
            if results["ids"] and len(results["ids"][0]) > 0:
                logger.info(f"Raw search results: {len(results['ids'][0])} chunks found")
                
                for i in range(len(results["ids"][0])):
                    chunk_id = results["ids"][0][i]
                    content = results["documents"][0][i]
                    metadata = results["metadatas"][0][i]
                    distance = results["distances"][0][i] if "distances" in results else None
                    
                    # Log each result for debugging
                    logger.debug(f"Result {i+1}:")
                    logger.debug(f"  Chunk ID: {chunk_id}")
                    logger.debug(f"  Distance: {distance}")
                    logger.debug(f"  Metadata: {metadata}")
                    logger.debug(f"  Content preview: {content[:100] if content is not None else 'None'}...")
                    
                    # Skip adding None content to results or provide a default value
                    if content is not None:
                        formatted_results.append({
                            "chunk_id": chunk_id,
                            "content": content,
                            "metadata": metadata,
                            "distance": distance
                        })
                    else:
                        logger.warning(f"Skipping result with chunk_id {chunk_id} due to None content")
            else:
                logger.warning(f"No results found for query: {query[:50]}...")
            
            logger.info(f"Found {len(formatted_results)} similar documents")
            
            # Log document IDs for easier tracking
            if formatted_results:
                doc_ids = set(result["metadata"]["document_id"] for result in formatted_results)
                logger.info(f"Documents retrieved: {doc_ids}")
            
            # Cache results if enabled
            if self.enable_cache:
                self._add_to_cache(cache_key, formatted_results)
            
            return formatted_results
        except Exception as e:
            logger.error(f"Error searching for documents: {str(e)}")
            raise
    
    def _create_cache_key(self, query: str, top_k: int, filter_criteria: Optional[Dict[str, Any]]) -> str:
        """Create a cache key from the search parameters"""
        # Normalize the query by removing extra whitespace and lowercasing
        normalized_query = " ".join(query.lower().split())
        
        # Convert filter criteria to a stable string representation
        filter_str = json.dumps(filter_criteria or {}, sort_keys=True)
        
        # Combine parameters into a cache key
        return f"{normalized_query}:{top_k}:{filter_str}"
    
    def _get_from_cache(self, key: str) -> Optional[List[Dict[str, Any]]]:
        """Get results from cache if they exist and are not expired"""
        if key in self.cache:
            entry = self.cache[key]
            if time.time() - entry["timestamp"] < self.cache_ttl:
                return entry["results"]
            else:
                # Expired, remove from cache
                del self.cache[key]
        return None
    
    def _add_to_cache(self, key: str, results: List[Dict[str, Any]]) -> None:
        """Add results to cache"""
        self.cache[key] = {
            "results": results,
            "timestamp": time.time()
        }
        
        # Prune cache if it gets too large
        if len(self.cache) > self.max_cache_size:
            self._prune_cache()
    
    def _prune_cache(self) -> None:
        """Remove oldest entries from cache"""
        # Sort by timestamp and keep the newest entries
        sorted_cache = sorted(
            self.cache.items(),
            key=lambda x: x[1]["timestamp"],
            reverse=True
        )
        
        # Keep only half of the max cache size
        keep_count = self.max_cache_size // 2
        self.cache = dict(sorted_cache[:keep_count])
        logger.info(f"Pruned cache to {keep_count} entries")
    
    async def search_by_tags(
        self,
        query: str,
        tags: List[str],
        top_k: int = 5,
        additional_filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for documents with specific tags
        """
        try:
            logger.info(f"Searching for documents with tags {tags} similar to query: {query[:50]}...")
            
            # Prepare filter criteria
            filter_criteria = additional_filters or {}
            
            # Add tag filter
            if tags:
                # Since tags are now stored as comma-separated strings, we need to use $contains
                # to check if any of the requested tags are in the document's tags string
                tag_conditions = []
                for tag in tags:
                    # For each tag, create a condition that checks if it's in the tags string
                    # We add commas to ensure we match whole tags, not substrings
                    tag_conditions.append({"$contains": tag})
                
                # Use $or to match any of the tag conditions
                if "tags" not in filter_criteria:
                    filter_criteria["tags"] = {"$or": tag_conditions}
                else:
                    # If there's already a tags filter, combine with it
                    existing_filter = filter_criteria["tags"]
                    filter_criteria["tags"] = {"$and": [existing_filter, {"$or": tag_conditions}]}
            
            # Perform search with filters
            return await self.search(query, top_k, filter_criteria)
        except Exception as e:
            logger.error(f"Error searching for documents by tags: {str(e)}")
            raise
    
    async def search_by_folder(
        self,
        query: str,
        folder: str,
        top_k: int = 5,
        additional_filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for documents in a specific folder
        """
        try:
            logger.info(f"Searching for documents in folder {folder} similar to query: {query[:50]}...")
            
            # Prepare filter criteria
            filter_criteria = additional_filters or {}
            
            # Add folder filter
            if folder:
                filter_criteria["folder"] = folder
            
            # Perform search with filters
            return await self.search(query, top_k, filter_criteria)
        except Exception as e:
            logger.error(f"Error searching for documents by folder: {str(e)}")
            raise
    
    def delete_document(self, document_id: str) -> None:
        """
        Delete a document from the vector store
        """
        try:
            logger.info(f"Deleting document {document_id} from vector store")
            
            # Delete chunks with the given document_id
            self.collection.delete(
                where={"document_id": document_id}
            )
            
            logger.info(f"Deleted document {document_id} from vector store")
        except Exception as e:
            logger.error(f"Error deleting document {document_id} from vector store: {str(e)}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the vector store
        """
        try:
            count = self.collection.count()
            stats = {
                "count": count,
                "embeddings_model": self.embedding_model
            }
            
            # Add cache stats if enabled
            if self.enable_cache:
                cache_stats = self.get_cache_stats()
                stats.update(cache_stats)
            
            return stats
        except Exception as e:
            logger.error(f"Error getting vector store stats: {str(e)}")
            raise
    
    def clear_cache(self) -> None:
        """
        Clear the cache to ensure fresh results
        """
        if self.enable_cache:
            self.cache = {}
            logger.info("Vector store cache cleared")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the cache
        """
        if not self.enable_cache:
            return {"cache_enabled": False}
        
        total_requests = self.cache_hits + self.cache_misses
        hit_ratio = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            "cache_enabled": True,
            "cache_size": len(self.cache),
            "cache_max_size": self.max_cache_size,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_hit_ratio": hit_ratio,
            "cache_ttl_seconds": self.cache_ttl
        }

================
File: app/static/css/document-manager.css
================
/* Document Management Sidebar Styles */
.document-section {
    margin-top: 20px;
    border-top: 1px solid var(--border-color);
    padding-top: 15px;
    overflow: hidden;
    max-height: 42px; /* Height of the header */
    transition: max-height 0.3s ease-out;
}

.document-section.expanded {
    max-height: 500px; /* Adjust based on expected content */
    overflow-y: auto;
}

.document-section-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 15px;
    cursor: pointer;
}

.document-section-title {
    font-weight: 600;
    font-size: 1rem;
    display: flex;
    align-items: center;
    gap: 8px;
    color: var(--accent-color);
}

.document-count {
    background-color: var(--accent-color);
    color: white;
    border-radius: 10px;
    padding: 2px 6px;
    font-size: 0.7rem;
    margin-left: 5px;
    transition: all 0.3s;
}

.document-count.has-documents {
    animation: pulse 1.5s infinite;
}

@keyframes pulse {
    0% { transform: scale(1); }
    50% { transform: scale(1.1); }
    100% { transform: scale(1); }
}

.document-list {
    margin-top: 15px;
    max-height: 300px;
    overflow-y: auto;
}

.sidebar-document-item {
    background-color: var(--input-bg);
    border-radius: 4px;
    padding: 8px 10px;
    margin-bottom: 8px;
    font-size: 0.9rem;
    border: 1px solid var(--border-color);
}

.doc-header {
    display: flex;
    align-items: center;
    margin-bottom: 5px;
}

.doc-select-label {
    display: flex;
    align-items: center;
    gap: 5px;
    cursor: pointer;
    width: 100%;
    margin: 0;
}

.doc-title {
    font-weight: 500;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

.doc-meta {
    display: flex;
    justify-content: space-between;
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-bottom: 5px;
}

/* Document Tags */
.doc-tags {
    display: flex;
    flex-wrap: wrap;
    gap: 4px;
    margin-bottom: 5px;
}

.doc-tag {
    background-color: var(--primary-color);
    color: white;
    padding: 2px 6px;
    border-radius: 10px;
    font-size: 0.7rem;
    white-space: nowrap;
}

.light-mode .doc-tag {
    color: white;
}

/* Document Folder */
.doc-folder {
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-bottom: 5px;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

.doc-folder i {
    margin-right: 4px;
}

.doc-actions {
    display: flex;
    justify-content: flex-end;
    gap: 5px;
}

.doc-action {
    background-color: transparent;
    color: var(--muted-color);
    border: none;
    padding: 2px 5px;
    border-radius: 3px;
    cursor: pointer;
    font-size: 0.8rem;
}

.doc-action:hover {
    background-color: var(--border-color);
    color: var(--text-color);
}

.process-btn:hover {
    color: var(--accent-color);
}

.delete-btn:hover {
    color: var(--error-color);
}

.edit-btn:hover {
    color: var(--secondary-color);
}

.document-upload {
    margin-top: 15px;
    margin-bottom: 15px;
}

.upload-form {
    display: flex;
    flex-direction: column;
    gap: 10px;
}

.upload-input {
    display: flex;
    align-items: center;
}

.upload-input input[type="file"] {
    flex: 1;
    font-size: 0.8rem;
}

.upload-button {
    padding: 6px 12px;
    font-size: 0.8rem;
}

.progress-bar {
    height: 6px;
    background-color: var(--border-color);
    border-radius: 3px;
    margin-top: 5px;
    display: none;
}

.progress-bar-fill {
    height: 100%;
    background-color: var(--accent-color);
    border-radius: 3px;
    width: 0;
    transition: width 0.3s;
}

.batch-actions {
    display: flex;
    gap: 8px;
    margin-top: 10px;
}

.batch-actions button {
    flex: 1;
    padding: 6px 10px;
    font-size: 0.8rem;
}

.document-loading,
.document-empty,
.document-error {
    text-align: center;
    padding: 10px;
    color: var(--muted-color);
    font-size: 0.9rem;
}

.document-error {
    color: var(--error-color);
}

/* Tag Management */
.tag-input-container {
    position: relative;
    margin-bottom: 10px;
}

.tag-input {
    width: 100%;
    padding-right: 30px;
}

.tag-suggestions {
    position: absolute;
    top: 100%;
    left: 0;
    right: 0;
    background-color: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 4px;
    max-height: 150px;
    overflow-y: auto;
    z-index: 10;
    display: none;
}

.tag-suggestion-item {
    padding: 5px 10px;
    cursor: pointer;
}

.tag-suggestion-item:hover {
    background-color: var(--border-color);
}

.tag-list {
    display: flex;
    flex-wrap: wrap;
    gap: 5px;
    margin-top: 10px;
}

.tag-item {
    background-color: var(--primary-color);
    color: white;
    padding: 3px 8px;
    border-radius: 12px;
    font-size: 0.8rem;
    display: flex;
    align-items: center;
    gap: 5px;
}

.light-mode .tag-item {
    color: white;
}

.tag-remove {
    cursor: pointer;
    font-weight: bold;
}

/* Folder Selection */
.folder-select {
    width: 100%;
    margin-bottom: 10px;
}

.folder-path {
    font-size: 0.8rem;
    color: var(--muted-color);
    margin-top: 5px;
}

/* Filter Panel */
.filter-panel {
    margin-bottom: 15px;
    padding: 10px;
    border: 1px solid var(--border-color);
    border-radius: 4px;
    background-color: var(--input-bg);
}

.filter-title {
    font-weight: bold;
    margin-bottom: 8px;
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.filter-toggle {
    background: none;
    border: none;
    color: var(--text-color);
    cursor: pointer;
    padding: 0;
}

.filter-content {
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.filter-content.show {
    max-height: 300px;
    overflow-y: auto;
}

.filter-section {
    margin-bottom: 10px;
}

.filter-section-title {
    font-weight: 500;
    margin-bottom: 5px;
    font-size: 0.9rem;
}

.filter-tags, .filter-folders {
    display: flex;
    flex-wrap: wrap;
    gap: 5px;
    margin-bottom: 10px;
}

.filter-tag, .filter-folder {
    background-color: var(--border-color);
    color: var(--text-color);
    padding: 3px 8px;
    border-radius: 12px;
    font-size: 0.8rem;
    cursor: pointer;
}

.filter-tag.active, .filter-folder.active {
    background-color: var(--primary-color);
    color: white;
}

.light-mode .filter-tag.active,
.light-mode .filter-folder.active {
    color: white;
}

.filter-actions {
    display: flex;
    justify-content: space-between;
    margin-top: 10px;
}

.filter-actions button {
    padding: 4px 8px;
    font-size: 0.8rem;
}

/* Document Edit Modal */
.modal {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-color: rgba(0, 0, 0, 0.5);
    z-index: 1000;
    align-items: center;
    justify-content: center;
}

.modal-content {
    background-color: var(--card-bg);
    border-radius: 8px;
    padding: 20px;
    width: 90%;
    max-width: 500px;
    max-height: 90vh;
    overflow-y: auto;
}

.modal-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 15px;
}

.modal-title {
    font-weight: bold;
    font-size: 1.2rem;
}

.modal-close {
    background: none;
    border: none;
    font-size: 1.5rem;
    cursor: pointer;
    color: var(--text-color);
}

.modal-body {
    margin-bottom: 20px;
}

.modal-footer {
    display: flex;
    justify-content: flex-end;
    gap: 10px;
}

/* Responsive adjustments */
@media (max-width: 768px) {
    .document-section.expanded {
        max-height: 300px;
    }
    
    .modal-content {
        width: 95%;
    }
    
    /* Improve touch targets for mobile */
    .doc-action {
        padding: 8px 10px;
        margin: 2px;
    }
    
    .filter-tag, .filter-folder {
        padding: 8px 12px;
        margin-bottom: 8px;
    }
    
    /* Adjust document items for better mobile experience */
    .sidebar-document-item {
        padding: 12px;
    }
    
    /* Make checkboxes easier to tap */
    input[type="checkbox"] {
        width: 20px;
        height: 20px;
    }
    
    /* Improve form elements */
    input[type="text"],
    input[type="file"],
    select,
    button {
        min-height: 44px; /* Apple's recommended minimum touch target size */
        font-size: 16px; /* Prevents iOS zoom on focus */
    }
    
    /* Add pull-to-refresh indicator */
    .pull-to-refresh {
        height: 50px;
        display: flex;
        align-items: center;
        justify-content: center;
        color: var(--muted-color);
        font-size: 0.9rem;
    }
    
    .pull-to-refresh .spinner {
        margin-right: 10px;
    }
    
    /* Add swipe actions for document items */
    .sidebar-document-item {
        position: relative;
        transition: transform 0.3s;
    }
    
    .sidebar-document-item.show-actions {
        transform: translateX(-80px);
    }
    
    .swipe-actions {
        position: absolute;
        right: -80px;
        top: 0;
        bottom: 0;
        width: 80px;
        display: flex;
        align-items: center;
        justify-content: center;
    }
    
    .swipe-action {
        width: 40px;
        height: 40px;
        border-radius: 50%;
        background-color: var(--error-color);
        color: white;
        display: flex;
        align-items: center;
        justify-content: center;
    }
}

================
File: app/static/css/fonts.css
================
@font-face {
    font-family: 'Azonix';
    src: url('../fonts/Azonix.woff2') format('woff2'),
         url('../fonts/Azonix.woff') format('woff'),
         url('../fonts/Azonix.ttf') format('truetype');
    font-weight: normal;
    font-style: normal;
    font-display: swap;
}

/* Fallback font definition */
.azonix-font {
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
    letter-spacing: 1px;
}

================
File: app/static/css/styles.css
================
/* Base styles */
:root {
    --primary-color: #1a5d1a;
    --secondary-color: #2e8b57;
    --accent-color: #50c878;
    --text-color: #f0f0f0;
    --bg-color: #121212;
    --card-bg: #1e1e1e;
    --border-color: #333;
    --input-bg: #2a2a2a;
    --hover-color: #3a7a5d;
    --muted-color: #888;
    --sidebar-width: 280px;
    --success-color: #2ecc71;
    --error-color: #e74c3c;
    --warning-color: #f39c12;
    --info-color: #3498db;
    --chat-user-bg: #1e1e1e;
    --chat-bot-bg: var(--secondary-color);
    --chat-container-bg: var(--card-bg);
    --source-bg: rgba(80, 200, 120, 0.2);
    --shadow-sm: 0 2px 4px rgba(0, 0, 0, 0.1);
    --shadow-md: 0 4px 8px rgba(0, 0, 0, 0.2);
    --shadow-lg: 0 8px 16px rgba(0, 0, 0, 0.3);
    --border-radius-sm: 4px;
    --border-radius-md: 8px;
    --border-radius-lg: 12px;
    --ginkgo-green: #50c878;
    --ginkgo-dark: #121212;
}

.light-mode {
    --primary-color: #1a5d1a;
    --secondary-color: #2e8b57;
    --accent-color: #50c878;
    --text-color: #333;
    --bg-color: #f5f5f5;
    --card-bg: #ffffff;
    --border-color: #ddd;
    --input-bg: #f9f9f9;
    --hover-color: #3a7a5d;
    --muted-color: #777;
    --chat-user-bg: #f0f0f0;
    --chat-bot-bg: rgba(46, 139, 87, 0.1);
    --chat-container-bg: var(--card-bg);
    --source-bg: rgba(80, 200, 120, 0.1);
    --shadow-sm: 0 2px 4px rgba(0, 0, 0, 0.05);
    --shadow-md: 0 4px 8px rgba(0, 0, 0, 0.1);
    --shadow-lg: 0 8px 16px rgba(0, 0, 0, 0.15);
    --ginkgo-green: #2e8b57;
    --ginkgo-dark: #333;
}

* {
    box-sizing: border-box;
    transition: background-color 0.3s, color 0.3s, box-shadow 0.3s, transform 0.2s;
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    margin: 0;
    padding: 0;
    background-color: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
    height: 100vh;
    overflow: hidden;
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
}

.app-container {
    display: flex;
    height: 100vh;
    overflow: hidden;
    background-image: linear-gradient(to bottom right, rgba(26, 93, 26, 0.05), rgba(80, 200, 120, 0.05));
}

/* Sidebar Styles */
.sidebar {
    width: var(--sidebar-width);
    background-color: var(--card-bg);
    border-right: 1px solid var(--border-color);
    padding: 20px;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    box-shadow: var(--shadow-md);
    z-index: 10;
}

.sidebar-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 20px;
    padding-bottom: 12px;
    border-bottom: 1px solid var(--border-color);
}

.sidebar-content {
    flex: 1;
    overflow-y: auto;
    padding-right: 5px;
}

/* Custom scrollbar for sidebar */
.sidebar-content::-webkit-scrollbar {
    width: 6px;
}

.sidebar-content::-webkit-scrollbar-track {
    background: transparent;
}

.sidebar-content::-webkit-scrollbar-thumb {
    background-color: var(--border-color);
    border-radius: 10px;
}

/* Main Content Styles */
.main-content {
    flex: 1;
    display: flex;
    flex-direction: column;
    padding: 24px;
    overflow: hidden;
}

.chat-area {
    flex: 1;
    display: flex;
    flex-direction: column;
    overflow: hidden;
}

.chat-container {
    flex: 1;
    background-color: var(--chat-container-bg);
    border-radius: var(--border-radius-md);
    padding: 24px;
    margin-bottom: 24px;
    box-shadow: var(--shadow-md);
    overflow-y: auto;
    scroll-behavior: smooth;
}

/* Custom scrollbar for chat container */
.chat-container::-webkit-scrollbar {
    width: 6px;
}

.chat-container::-webkit-scrollbar-track {
    background: transparent;
}

.chat-container::-webkit-scrollbar-thumb {
    background-color: var(--border-color);
    border-radius: 10px;
}

.input-area {
    background-color: var(--card-bg);
    border-radius: var(--border-radius-md);
    padding: 24px;
    box-shadow: var(--shadow-md);
}

h1 {
    color: var(--ginkgo-green);
    margin: 0;
    font-size: 1.8rem;
    flex: 1;
    margin-right: 10px;
    font-weight: normal;
    letter-spacing: 1px;
    display: flex;
    align-items: center;
    gap: 8px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    text-transform: uppercase;
}

.theme-toggle {
    background: var(--ginkgo-dark);
    color: var(--ginkgo-green);
    border: 1px solid var(--ginkgo-green);
    padding: 6px 10px;
    border-radius: 20px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 4px;
    font-size: 0.8rem;
    transition: all 0.2s ease;
    box-shadow: var(--shadow-sm);
}

.theme-toggle:hover {
    background: var(--ginkgo-green);
    color: var(--ginkgo-dark);
    transform: translateY(-1px);
    box-shadow: var(--shadow-md);
}

.theme-toggle:active {
    transform: translateY(0);
}

/* Message Styles */
.message {
    margin-bottom: 20px;
    padding: 14px 18px;
    border-radius: var(--border-radius-md);
    position: relative;
    animation: messageAppear 0.4s forwards;
    box-shadow: var(--shadow-sm);
    line-height: 1.5;
    max-width: 85%;
}

@keyframes messageAppear {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

.user-message {
    background-color: var(--chat-user-bg);
    color: var(--ginkgo-green);
    margin-left: auto;
    margin-right: 10px;
    border-bottom-right-radius: 4px;
    align-self: flex-end;
    border: 1px solid rgba(0, 255, 0, 0.3);
}

.bot-message {
    background-color: rgba(0, 255, 0, 0.05);
    color: white;
    margin-left: 10px;
    margin-right: auto;
    border-bottom-left-radius: 4px;
    white-space: pre-wrap;
    align-self: flex-start;
    border-left: 3px solid var(--ginkgo-green);
}

.light-mode .user-message,
.light-mode .bot-message {
    color: var(--text-color);
}

.message-header {
    font-weight: normal;
    margin-bottom: 8px;
    font-size: 0.9rem;
    opacity: 0.9;
    color: var(--ginkgo-green);
    text-transform: uppercase;
    letter-spacing: 1px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.copy-button {
    position: absolute;
    top: 10px;
    right: 10px;
    background-color: rgba(255, 255, 255, 0.2);
    color: white;
    border: none;
    border-radius: var(--border-radius-sm);
    padding: 4px 10px;
    font-size: 0.8rem;
    cursor: pointer;
    opacity: 0;
    transition: opacity 0.2s ease;
}

.message:hover .copy-button {
    opacity: 1;
}

.copy-button:hover {
    background-color: rgba(255, 255, 255, 0.3);
    transform: translateY(-1px);
}

.copy-button:active {
    transform: translateY(0);
}

/* Form Elements */
.form-group {
    margin-bottom: 18px;
}

label {
    display: block;
    margin-bottom: 6px;
    font-weight: normal;
    font-size: 0.9rem;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    letter-spacing: 0.5px;
    text-transform: uppercase;
    color: var(--ginkgo-green);
}

.param-description {
    font-size: 0.85rem;
    color: var(--muted-color);
    margin-top: 3px;
    margin-bottom: 6px;
    line-height: 1.4;
}

.tooltip {
    display: inline-block;
    margin-left: 5px;
    color: var(--ginkgo-green);
    cursor: help;
    position: relative;
}

.tooltip .tooltip-text {
    visibility: hidden;
    width: 280px;
    background-color: var(--card-bg);
    color: var(--text-color);
    text-align: left;
    border-radius: var(--border-radius-md);
    padding: 12px;
    position: absolute;
    z-index: 100;
    bottom: 125%;
    left: 50%;
    margin-left: -140px;
    opacity: 0;
    transition: opacity 0.3s, transform 0.3s;
    box-shadow: var(--shadow-md);
    border: 1px solid var(--border-color);
    font-weight: normal;
    font-size: 0.9rem;
    transform: translateY(5px);
    line-height: 1.5;
}

.tooltip:hover .tooltip-text {
    visibility: visible;
    opacity: 1;
    transform: translateY(0);
}

input[type="text"],
input[type="number"],
textarea,
select {
    width: 100%;
    padding: 12px;
    border: 1px solid var(--border-color);
    border-radius: var(--border-radius-sm);
    background-color: var(--input-bg);
    color: var(--text-color);
    font-family: inherit;
    font-size: 0.95rem;
    transition: border-color 0.3s, box-shadow 0.3s;
}

input[type="text"]:focus,
input[type="number"]:focus,
textarea:focus,
select:focus {
    outline: none;
    border-color: var(--accent-color);
    box-shadow: 0 0 0 2px rgba(80, 200, 120, 0.2);
}

textarea {
    resize: vertical;
    min-height: 100px;
    line-height: 1.5;
}

.checkbox-container {
    display: flex;
    align-items: center;
    gap: 10px;
    padding: 5px 0;
}

input[type="checkbox"] {
    width: 18px;
    height: 18px;
    cursor: pointer;
}

button {
    background-color: var(--ginkgo-green);
    color: var(--ginkgo-dark);
    padding: 10px 20px;
    border: none;
    border-radius: var(--border-radius-sm);
    cursor: pointer;
    font-size: 0.9rem;
    font-weight: normal;
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 8px;
    transition: all 0.2s ease;
    box-shadow: var(--shadow-sm);
    text-transform: uppercase;
    letter-spacing: 1px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

button:hover {
    background-color: var(--hover-color);
    transform: translateY(-1px);
    box-shadow: var(--shadow-md);
}

button:active {
    transform: translateY(0);
}

button.secondary {
    background-color: transparent;
    color: var(--ginkgo-green);
    border: 1px solid var(--ginkgo-green);
}

button.secondary:hover {
    background-color: rgba(0, 255, 0, 0.1);
}

button.danger {
    background-color: transparent;
    color: #ff3333;
    border: 1px solid #ff3333;
}

button.danger:hover {
    background-color: rgba(255, 51, 51, 0.1);
}

/* Advanced Options */
.advanced-options {
    margin-top: 20px;
    border-top: 1px solid var(--border-color);
    padding-top: 15px;
}

.advanced-toggle {
    background: none;
    border: none;
    color: var(--ginkgo-green);
    cursor: pointer;
    padding: 0;
    font-size: 0.9rem;
    display: flex;
    align-items: center;
    gap: 5px;
    font-family: 'Azonix', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    letter-spacing: 0.5px;
    text-transform: uppercase;
}

.advanced-content {
    display: none;
    margin-top: 15px;
}

.advanced-content.show {
    display: block;
}

.parameter-grid {
    display: grid;
    grid-template-columns: 1fr;
    gap: 15px;
}

/* Action Buttons */
.action-buttons, .batch-actions, .filter-actions {
    display: flex;
    gap: 10px;
    margin-top: 20px;
}

.action-buttons button, .batch-actions button, .filter-actions button {
    flex: 1;
}

/* Token Usage Indicator */
.token-usage {
    position: fixed;
    bottom: 15px;
    right: 15px;
    background-color: var(--card-bg);
    padding: 10px 15px;
    border-radius: var(--border-radius-md);
    font-size: 0.85rem;
    box-shadow: var(--shadow-md);
    display: none;
    border: 1px solid var(--border-color);
    z-index: 100;
    transition: opacity 0.3s, transform 0.3s;
    transform: translateY(0);
}

.token-usage:hover {
    transform: translateY(-2px);
}

.token-usage-title {
    font-weight: 600;
    margin-bottom: 6px;
    display: flex;
    align-items: center;
    gap: 6px;
}

.token-usage-bar {
    width: 100%;
    height: 8px;
    background-color: var(--border-color);
    border-radius: 4px;
    margin-bottom: 6px;
    overflow: hidden;
}

#token-usage-fill {
    height: 100%;
    width: 0%;
    background-color: var(--ginkgo-green);
    border-radius: 4px;
    transition: width 0.5s ease-out;
}

/* Source Citation Styles */
.sources-section {
    margin-top: 0.75rem;
    font-size: 0.85rem;
    color: var(--text-color);
    opacity: 0.9;
    padding-top: 0.5rem;
    border-top: 1px solid rgba(255, 255, 255, 0.1);
}

.source-item {
    display: inline-block;
    background-color: var(--source-bg);
    margin-right: 0.5rem;
    margin-bottom: 0.5rem;
    padding: 0.3rem 0.6rem;
    border-radius: var(--border-radius-sm);
    font-size: 0.8rem;
    transition: transform 0.2s;
}

.source-item:hover {
    transform: translateY(-1px);
}

/* Loading Indicator */
.loading {
    display: none;
    margin-top: 25px;
    text-align: center;
    color: var(--ginkgo-green);
    font-weight: 500;
}

.loading.show {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 10px;
}

.spinner {
    display: inline-block;
    width: 24px;
    height: 24px;
    border: 3px solid rgba(0, 255, 0, 0.1);
    border-radius: 50%;
    border-top-color: var(--ginkgo-green);
    animation: spin 1s ease-in-out infinite;
    vertical-align: middle;
}

@keyframes spin {
    to { transform: rotate(360deg); }
}

/* Utility Classes */
.text-center {
    text-align: center;
}

.mb-1 {
    margin-bottom: 0.5rem;
}

.mb-2 {
    margin-bottom: 1rem;
}

.mb-3 {
    margin-bottom: 1.5rem;
}

.success {
    color: var(--success-color);
}

.error {
    color: var(--error-color);
}

.warning {
    color: var(--warning-color);
}

.info {
    color: var(--info-color);
}

/* Responsive Styles */
@media (max-width: 768px) {
    .app-container {
        flex-direction: column;
    }
    
    .sidebar {
        width: 100%;
        height: auto;
        max-height: 40vh;
        border-right: none;
        border-bottom: 1px solid var(--border-color);
        padding: 15px;
    }
    
    .main-content {
        height: 60vh;
        padding: 15px;
    }
    
    .parameter-grid {
        grid-template-columns: 1fr;
    }
    
    .message {
        max-width: 90%;
        padding: 12px 15px;
    }
    
    h1 {
        font-size: 1.5rem;
    }
    
    .chat-container {
        padding: 15px;
        margin-bottom: 15px;
    }
    
    .input-area {
        padding: 15px;
    }
    
    .token-usage {
        bottom: 10px;
        right: 10px;
        padding: 8px 12px;
        font-size: 0.8rem;
    }
}

@media (max-width: 480px) {
    .message {
        max-width: 95%;
    }
    
    .copy-button {
        padding: 3px 6px;
        font-size: 0.7rem;
    }
    
    .sidebar-header {
        flex-direction: column;
        align-items: flex-start;
        gap: 10px;
    }
    
    .theme-toggle {
        align-self: flex-end;
    }
}

================
File: app/static/js/chat.js
================
// Chat functionality
document.addEventListener('DOMContentLoaded', function() {
    // Elements
    const chatContainer = document.getElementById('chat-container');
    const userInput = document.getElementById('user-input');
    const sendButton = document.getElementById('send-button');
    const clearButton = document.getElementById('clear-chat');
    const saveButton = document.getElementById('save-chat');
    const modelSelect = document.getElementById('model');
    const ragToggle = document.getElementById('rag-toggle');
    const streamToggle = document.getElementById('stream-toggle');
    const loadingIndicator = document.getElementById('loading');
    const maxResults = document.getElementById('max-results');
    const temperature = document.getElementById('temperature');
    const metadataFilters = document.getElementById('metadata-filters');
    
    // Toggle RAG parameters visibility
    if (ragToggle) {
        ragToggle.addEventListener('change', function() {
            const ragParams = document.querySelectorAll('.rag-param');
            ragParams.forEach(param => {
                param.style.display = this.checked ? 'block' : 'none';
            });
        });
    }
    
    // Load available models
    if (modelSelect) {
        console.log('Loading models...');
        fetch('/api/system/models')
            .then(response => {
                console.log('Response status:', response.status);
                return response.json();
            })
            .then(models => {
                console.log('Models fetched:', models);
                console.log('Number of models:', models.length);
                
                // Clear the dropdown
                modelSelect.innerHTML = '';
                
                // Add models to dropdown
                models.forEach(model => {
                    const option = document.createElement('option');
                    option.value = model.name;
                    option.textContent = model.name;
                    modelSelect.appendChild(option);
                    console.log('Added model to dropdown:', model.name);
                });
            })
            .catch(error => {
                console.error('Error loading models:', error);
            });
    }
    
    // Send message
    function sendMessage() {
        const message = userInput.value.trim();
        if (!message) return;
        
        // Log the selected model
        console.log('Selected model for this message:', modelSelect.value);
        
        // Add user message to conversation
        addMessage('user', message);
        
        // Clear input
        userInput.value = '';
        
        // Show loading indicator
        loadingIndicator.style.display = 'block';
        
        // Prepare query
        const query = {
            message: message,
            model: modelSelect.value,
            use_rag: ragToggle.checked,
            model_parameters: {
                temperature: parseFloat(temperature.value),
                max_results: ragToggle.checked ? parseInt(maxResults.value) : 0
            }
        };
        
        // Log the query being sent
        console.log('Sending query with model:', query.model);
        
        // Parse metadata filters if provided
        if (ragToggle.checked && metadataFilters.value.trim()) {
            try {
                query.metadata_filters = JSON.parse(metadataFilters.value);
            } catch (e) {
                console.error('Invalid metadata filter JSON:', e);
                addMessage('assistant', 'Error: Invalid metadata filter format. Please use valid JSON.');
                loadingIndicator.style.display = 'none';
                return;
            }
        }
        // Use streaming based on the toggle
        query.stream = streamToggle.checked;
        
        // Create message element for assistant response
        const messageDiv = document.createElement('div');
        messageDiv.className = 'message assistant-message';
        
        // Add header
        const headerDiv = document.createElement('div');
        headerDiv.className = 'message-header';
        headerDiv.textContent = 'Metis:';
        messageDiv.appendChild(headerDiv);
        
        // Add content div for streaming response
        const contentDiv = document.createElement('div');
        contentDiv.id = 'streaming-response';
        messageDiv.appendChild(contentDiv);
        
        // Add to chat container
        chatContainer.appendChild(messageDiv);
        
        // Scroll to bottom
        chatContainer.scrollTop = chatContainer.scrollHeight;
        
        // Send to API
        fetch('/api/chat/query', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(query)
        })
        .then(response => {
            if (!response.ok) {
                throw new Error('Network response was not ok');
            }
            
            if (query.stream) {
                // Handle streaming response
                // Create a new reader for the response
                const reader = response.body.getReader();
                let decoder = new TextDecoder();
                let fullResponse = '';
                
                // Function to process the stream
                function processStream() {
                    return reader.read().then(({ done, value }) => {
                        if (done) {
                            // Hide loading indicator when done
                            loadingIndicator.style.display = 'none';
                            return;
                        }
                        
                        // Decode the chunk and append to the response
                        const chunk = decoder.decode(value, { stream: true });
                        
                        // Process the chunk (which may contain multiple SSE events)
                        const lines = chunk.split('\n');
                        for (const line of lines) {
                            if (line.startsWith('data:')) {
                                const data = line.substring(5).trim();
                                if (data) {
                                    try {
                                        // Try to parse as JSON (for newer format)
                                        try {
                                            const jsonData = JSON.parse(data);
                                            if (jsonData.chunk) {
                                                fullResponse += jsonData.chunk;
                                            } else {
                                                fullResponse += data;
                                            }
                                        } catch (e) {
                                            // If not JSON, just append the data (older format)
                                            // Check if we need to add a space before the new token
                                            const needsSpace = fullResponse.length > 0 &&
                                                              !fullResponse.endsWith(' ') &&
                                                              !fullResponse.endsWith('\n') &&
                                                              !fullResponse.endsWith('.') &&
                                                              !fullResponse.endsWith(',') &&
                                                              !fullResponse.endsWith('!') &&
                                                              !fullResponse.endsWith('?') &&
                                                              !fullResponse.endsWith(':') &&
                                                              !fullResponse.endsWith(';') &&
                                                              !fullResponse.endsWith('(') &&
                                                              !data.startsWith(' ') &&
                                                              !data.startsWith('\n') &&
                                                              !data.startsWith('.') &&
                                                              !data.startsWith(',') &&
                                                              !data.startsWith('!') &&
                                                              !data.startsWith('?') &&
                                                              !data.startsWith(':') &&
                                                              !data.startsWith(';') &&
                                                              !data.startsWith(')');
                                            
                                            if (needsSpace) {
                                                fullResponse += ' ';
                                            }
                                            
                                            fullResponse += data;
                                        }
                                    } catch (e) {
                                        console.error('Error processing chunk:', e);
                                    }
                                }
                            }
                        }
                        
                        // Update the content div with the current response
                        contentDiv.textContent = fullResponse;
                        
                        // Scroll to bottom
                        chatContainer.scrollTop = chatContainer.scrollHeight;
                        
                        // Continue reading
                        return processStream();
                    }).catch(error => {
                        console.error('Error reading stream:', error);
                        loadingIndicator.style.display = 'none';
                    });
                }
                
                // Start processing the stream
                return processStream();
            } else {
                // Handle non-streaming response
                return response.json().then(data => {
                    // Hide loading indicator
                    loadingIndicator.style.display = 'none';
                    
                    // Display the response
                    contentDiv.textContent = data.message;
                    
                    // Scroll to bottom
                    chatContainer.scrollTop = chatContainer.scrollHeight;
                    
                    // Add citations if available
                    if (data.citations && data.citations.length > 0) {
                        const citationsDiv = document.createElement('div');
                        citationsDiv.className = 'sources-section';
                        citationsDiv.innerHTML = '<strong>Sources:</strong> ';
                        
                        data.citations.forEach(citation => {
                            const sourceSpan = document.createElement('span');
                            sourceSpan.className = 'source-item';
                            sourceSpan.textContent = citation.document_id;
                            sourceSpan.title = citation.excerpt;
                            citationsDiv.appendChild(sourceSpan);
                        });
                        
                        messageDiv.appendChild(citationsDiv);
                    }
                });
            }
        })
        .catch(error => {
            console.error('Error:', error);
            
            // Add error message with more details
            contentDiv.textContent = 'Sorry, there was an error processing your request. ';
            
            // Check if the query is about future events
            const currentYear = new Date().getFullYear();
            const queryLower = message.toLowerCase();
            const yearMatch = queryLower.match(/\b(20\d\d|19\d\d)\b/);
            
            if (yearMatch && parseInt(yearMatch[1]) > currentYear) {
                contentDiv.textContent = `I cannot provide information about events in ${yearMatch[1]} as it's in the future. ` +
                    `The current year is ${currentYear}. I can only provide information about past or current events.`;
            }
            // Check if the query is about speculative future events
            else if (/what will happen|what is going to happen|predict the future|future events|in the future/.test(queryLower)) {
                contentDiv.textContent = "I cannot predict future events or provide information about what will happen in the future. " +
                    "I can only provide information about past or current events based on available data.";
            }
            // Add suggestion based on RAG status
            else if (ragToggle.checked) {
                contentDiv.textContent += 'This might be because there are no documents available for RAG. ' +
                    'Try uploading some documents or disabling the RAG feature.';
            }
            // Add suggestion based on streaming status
            else if (streamToggle.checked) {
                contentDiv.textContent += 'You might try disabling streaming mode for better error handling. ';
            }
            else {
                contentDiv.textContent += 'Please try again later or with different parameters.';
            }
            
            // Hide loading indicator
            loadingIndicator.style.display = 'none';
        });
    }
    
    // Clear chat
    if (clearButton) {
        clearButton.addEventListener('click', function() {
            if (confirm('Are you sure you want to clear the chat history?')) {
                clearConversation();
                chatContainer.innerHTML = '';
                
                // Add welcome message
                const welcomeMessage = document.createElement('div');
                welcomeMessage.className = 'message bot-message';
                welcomeMessage.innerHTML = `
                    <div class="message-header">Metis:</div>
                    Hello! I'm your Metis RAG assistant. Ask me anything about your uploaded documents or chat with me directly.
                `;
                chatContainer.appendChild(welcomeMessage);
            }
        });
    }
    
    // Add message to chat
    function addMessage(role, content, citations = null) {
        // Create message element
        const messageDiv = document.createElement('div');
        messageDiv.className = `message ${role}-message`;
        
        // Add header
        const headerDiv = document.createElement('div');
        headerDiv.className = 'message-header';
        headerDiv.textContent = role === 'user' ? 'You:' : 'Metis:';
        messageDiv.appendChild(headerDiv);
        
        // Add content
        const contentDiv = document.createElement('div');
        contentDiv.textContent = content;
        messageDiv.appendChild(contentDiv);
        
        // Add citations if available
        if (citations && citations.length > 0) {
            const citationsDiv = document.createElement('div');
            citationsDiv.className = 'sources-section';
            citationsDiv.innerHTML = '<strong>Sources:</strong> ';
            
            citations.forEach(citation => {
                const sourceSpan = document.createElement('span');
                sourceSpan.className = 'source-item';
                sourceSpan.textContent = citation.document_id;
                sourceSpan.title = citation.excerpt;
                citationsDiv.appendChild(sourceSpan);
            });
            
            messageDiv.appendChild(citationsDiv);
        }
        
        // Add to chat container
        chatContainer.appendChild(messageDiv);
        
        // Scroll to bottom
        chatContainer.scrollTop = chatContainer.scrollHeight;
    }
    
    // Clear conversation
    function clearConversation() {
        // Clear conversation from local storage or API
        fetch('/api/chat/clear', {
            method: 'DELETE'
        })
        .then(response => response.json())
        .then(data => {
            console.log('Conversation cleared:', data);
            // Use the globally exposed clearConversation function from main.js
            // This ensures proper clearing of localStorage and updating the UI
            if (window.clearConversation) {
                window.clearConversation();
            } else {
                // Fallback if the global function isn't available
                localStorage.removeItem('metis_conversation');
                console.warn('window.clearConversation not found, using fallback clear method');
            }
        })
        .catch(error => {
            console.error('Error clearing conversation:', error);
        });
    }
    
    // Event listeners
    if (sendButton) {
        sendButton.addEventListener('click', sendMessage);
    }
    
    if (userInput) {
        userInput.addEventListener('keydown', function(e) {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendMessage();
            }
        });
    }
});

================
File: app/static/js/document-manager.js
================
// Document Management Functionality
class DocumentManager {
    constructor() {
        this.documents = [];
        this.selectedDocuments = [];
        this.isExpanded = false;
        this.isUploading = false;
        this.allTags = [];
        this.allFolders = ['/'];
        this.activeFilters = {
            tags: [],
            folder: null
        };
        
        // Initialize elements after DOM is loaded
        document.addEventListener('DOMContentLoaded', () => this.initialize());
    }
    
    initialize() {
        // Get elements
        this.docSection = document.getElementById('document-section');
        this.docList = document.getElementById('document-list');
        this.uploadForm = document.getElementById('doc-upload-form');
        this.documentFile = document.getElementById('document-file');
        this.uploadProgress = document.getElementById('upload-progress');
        this.uploadProgressFill = document.getElementById('upload-progress-fill');
        this.toggleBtn = document.getElementById('toggle-documents');
        this.processSelectedBtn = document.getElementById('process-selected-btn');
        this.deleteSelectedBtn = document.getElementById('delete-selected-btn');
        this.documentCount = document.getElementById('document-count');
        this.tagInput = document.getElementById('doc-tags');
        this.folderSelect = document.getElementById('doc-folder');
        this.filterPanel = document.getElementById('filter-panel');
        this.editModal = document.getElementById('document-edit-modal');
        
        // Mobile detection
        this.isMobile = window.innerWidth <= 768;
        
        if (!this.docSection) return;
        
        // Set up event listeners
        this.setupEventListeners();
        
        // Load tags and folders
        this.loadTagsAndFolders();
        
        // Load documents
        this.loadDocuments();
        
        // Set up mobile-specific features
        if (this.isMobile) {
            this.setupMobileSupport();
        }
        
        // Listen for window resize to adjust mobile features
        window.addEventListener('resize', () => {
            const wasMobile = this.isMobile;
            this.isMobile = window.innerWidth <= 768;
            
            // If switching between mobile and desktop
            if (wasMobile !== this.isMobile) {
                if (this.isMobile) {
                    this.setupMobileSupport();
                } else {
                    this.removeMobileSupport();
                }
            }
        });
    }
    
    setupEventListeners() {
        // Toggle document section
        if (this.toggleBtn) {
            this.toggleBtn.addEventListener('click', () => this.toggleDocumentSection());
        }
        
        // Upload form
        if (this.uploadForm) {
            this.uploadForm.addEventListener('submit', (e) => this.handleUpload(e));
        }
        
        // Process selected documents
        if (this.processSelectedBtn) {
            this.processSelectedBtn.addEventListener('click', () => this.processSelected());
        }
        
        // Delete selected documents
        if (this.deleteSelectedBtn) {
            this.deleteSelectedBtn.addEventListener('click', () => this.deleteSelected());
        }
        
        // Filter toggle
        const filterToggle = document.getElementById('filter-toggle');
        if (filterToggle) {
            filterToggle.addEventListener('click', () => {
                const filterContent = document.getElementById('filter-content');
                filterContent.classList.toggle('show');
                const icon = filterToggle.querySelector('i');
                if (icon) {
                    icon.className = filterContent.classList.contains('show') ?
                        'fas fa-chevron-up' : 'fas fa-chevron-down';
                }
            });
        }
        
        // Apply filters button
        const applyFiltersBtn = document.getElementById('apply-filters');
        if (applyFiltersBtn) {
            applyFiltersBtn.addEventListener('click', () => this.applyFilters());
        }
        
        // Clear filters button
        const clearFiltersBtn = document.getElementById('clear-filters');
        if (clearFiltersBtn) {
            clearFiltersBtn.addEventListener('click', () => this.clearFilters());
        }
        
        // Modal close button
        const modalClose = document.querySelector('.modal-close');
        if (modalClose) {
            modalClose.addEventListener('click', () => {
                if (this.editModal) {
                    this.editModal.style.display = 'none';
                }
            });
        }
        
        // Save changes button in modal
        const saveChangesBtn = document.getElementById('save-changes');
        if (saveChangesBtn) {
            saveChangesBtn.addEventListener('click', () => this.saveDocumentChanges());
        }
        
        // Tag input for suggestions
        if (this.tagInput) {
            this.tagInput.addEventListener('input', () => this.showTagSuggestions());
            this.tagInput.addEventListener('keydown', (e) => {
                if (e.key === 'Enter' && this.tagInput.value.trim()) {
                    e.preventDefault();
                    this.addTag(this.tagInput.value.trim());
                    this.tagInput.value = '';
                }
            });
        }
    }
    
    loadTagsAndFolders() {
        // Load all tags
        fetch('/api/documents/tags')
            .then(response => response.json())
            .then(data => {
                this.allTags = data.tags || [];
                this.renderTagFilters();
            })
            .catch(error => {
                console.error('Error loading tags:', error);
            });
        
        // Load all folders
        fetch('/api/documents/folders')
            .then(response => response.json())
            .then(data => {
                this.allFolders = data.folders || ['/'];
                this.renderFolderFilters();
                this.populateFolderSelect();
            })
            .catch(error => {
                console.error('Error loading folders:', error);
            });
    }
    
    renderTagFilters() {
        const filterTagsContainer = document.getElementById('filter-tags');
        if (!filterTagsContainer || !this.allTags.length) return;
        
        filterTagsContainer.innerHTML = '';
        
        this.allTags.forEach(tag => {
            const tagEl = document.createElement('div');
            tagEl.className = 'filter-tag';
            tagEl.textContent = tag;
            tagEl.dataset.tag = tag;
            
            if (this.activeFilters.tags.includes(tag)) {
                tagEl.classList.add('active');
            }
            
            tagEl.addEventListener('click', () => {
                tagEl.classList.toggle('active');
            });
            
            filterTagsContainer.appendChild(tagEl);
        });
    }
    
    renderFolderFilters() {
        const filterFoldersContainer = document.getElementById('filter-folders');
        if (!filterFoldersContainer || !this.allFolders.length) return;
        
        filterFoldersContainer.innerHTML = '';
        
        this.allFolders.forEach(folder => {
            const folderEl = document.createElement('div');
            folderEl.className = 'filter-folder';
            folderEl.textContent = folder === '/' ? 'Root' : folder.replace('/', '');
            folderEl.dataset.folder = folder;
            
            if (this.activeFilters.folder === folder) {
                folderEl.classList.add('active');
            }
            
            folderEl.addEventListener('click', () => {
                // Deactivate all folders
                document.querySelectorAll('.filter-folder').forEach(el => {
                    el.classList.remove('active');
                });
                
                // Activate this folder
                folderEl.classList.add('active');
            });
            
            filterFoldersContainer.appendChild(folderEl);
        });
    }
    
    populateFolderSelect() {
        if (!this.folderSelect || !this.allFolders.length) return;
        
        this.folderSelect.innerHTML = '';
        
        this.allFolders.forEach(folder => {
            const option = document.createElement('option');
            option.value = folder;
            option.textContent = folder === '/' ? 'Root' : folder.replace('/', '');
            this.folderSelect.appendChild(option);
        });
    }
    
    applyFilters() {
        // Get selected tags
        const selectedTags = Array.from(document.querySelectorAll('.filter-tag.active'))
            .map(el => el.dataset.tag);
        
        // Get selected folder
        const selectedFolder = document.querySelector('.filter-folder.active')?.dataset.folder;
        
        this.activeFilters = {
            tags: selectedTags,
            folder: selectedFolder
        };
        
        // Load filtered documents
        this.loadFilteredDocuments();
        
        // Close filter panel
        const filterContent = document.getElementById('filter-content');
        if (filterContent) {
            filterContent.classList.remove('show');
        }
        
        const filterToggle = document.getElementById('filter-toggle');
        if (filterToggle) {
            const icon = filterToggle.querySelector('i');
            if (icon) {
                icon.className = 'fas fa-chevron-down';
            }
        }
    }
    
    clearFilters() {
        // Clear active filters
        document.querySelectorAll('.filter-tag.active, .filter-folder.active').forEach(el => {
            el.classList.remove('active');
        });
        
        this.activeFilters = {
            tags: [],
            folder: null
        };
        
        // Load all documents
        this.loadDocuments();
    }
    
    loadFilteredDocuments() {
        if (!this.docList) return;
        
        // Show loading indicator
        this.docList.innerHTML = '<div class="document-loading">Loading documents...</div>';
        
        // Prepare filter request
        const filterRequest = {
            tags: this.activeFilters.tags.length > 0 ? this.activeFilters.tags : null,
            folder: this.activeFilters.folder
        };
        
        fetch('/api/documents/filter', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(filterRequest)
        })
            .then(response => response.json())
            .then(documents => {
                this.documents = documents;
                this.renderDocuments();
            })
            .catch(error => {
                console.error('Error loading filtered documents:', error);
                this.docList.innerHTML = '<div class="document-error">Error loading documents</div>';
            });
    }
    
    toggleDocumentSection() {
        if (!this.docSection) return;
        
        this.isExpanded = !this.isExpanded;
        this.docSection.classList.toggle('expanded', this.isExpanded);
        
        // Update toggle button icon and text
        if (this.toggleBtn) {
            const icon = this.toggleBtn.querySelector('i');
            if (icon) {
                icon.className = this.isExpanded ? 'fas fa-chevron-up' : 'fas fa-chevron-down';
            }
        }
        
        // Load documents if expanding and not already loaded
        if (this.isExpanded && this.documents.length === 0) {
            this.loadDocuments();
        }
    }
    
    loadDocuments() {
        if (!this.docList) return Promise.resolve();
        
        // Show loading indicator
        this.docList.innerHTML = '<div class="document-loading">Loading documents...</div>';
        
        // Return the promise for chaining
        return fetch('/api/documents/list')
            .then(response => response.json())
            .then(documents => {
                this.documents = documents;
                this.renderDocuments();
                return documents;
            })
            .catch(error => {
                console.error('Error loading documents:', error);
                this.docList.innerHTML = '<div class="document-error">Error loading documents</div>';
                return [];
            });
    }
    
    renderDocuments() {
        if (!this.docList) return;
        
        this.docList.innerHTML = '';
        
        if (this.documents.length === 0) {
            this.docList.innerHTML = '<div class="document-empty">No documents found</div>';
            this.updateDocumentCount(0);
            return;
        }
        
        this.documents.forEach(doc => {
            const docEl = this.createDocumentElement(doc);
            this.docList.appendChild(docEl);
        });
        
        this.updateDocumentCount(this.documents.length);
        this.updateBatchButtons();
    }
    
    createDocumentElement(doc) {
        const docEl = document.createElement('div');
        docEl.className = 'sidebar-document-item';
        docEl.dataset.id = doc.id;
        
        const date = new Date(doc.uploaded);
        const formattedDate = date.toLocaleDateString();
        
        // Create tags HTML
        const tagsHtml = doc.tags && doc.tags.length > 0
            ? `<div class="doc-tags">${doc.tags.map(tag => `<span class="doc-tag">${tag}</span>`).join('')}</div>`
            : '';
        
        // Create folder HTML
        const folderHtml = doc.folder
            ? `<div class="doc-folder"><i class="fas fa-folder"></i> ${doc.folder === '/' ? 'Root' : doc.folder.replace('/', '')}</div>`
            : '';
        
        docEl.innerHTML = `
            <div class="doc-header">
                <label class="doc-select-label">
                    <input type="checkbox" class="doc-select" data-id="${doc.id}">
                    <span class="doc-title" title="${doc.filename}">${this.truncateFilename(doc.filename)}</span>
                </label>
            </div>
            ${tagsHtml}
            ${folderHtml}
            <div class="doc-meta">
                <span class="doc-chunks">${doc.chunk_count} chunks</span>
                <span class="doc-date">${formattedDate}</span>
            </div>
            <div class="doc-actions">
                <button class="doc-action edit-btn" data-id="${doc.id}" title="Edit Document">
                    <i class="fas fa-edit"></i>
                </button>
                <button class="doc-action process-btn" data-id="${doc.id}" title="Process Document">
                    <i class="fas fa-sync-alt"></i>
                </button>
                <button class="doc-action delete-btn" data-id="${doc.id}" title="Delete Document">
                    <i class="fas fa-trash"></i>
                </button>
            </div>
            ${this.isMobile ? `
            <div class="swipe-actions">
                <button class="swipe-action delete-action" title="Delete Document">
                    <i class="fas fa-trash"></i>
                </button>
            </div>` : ''}
        `;
        
        // Add event listeners
        const checkbox = docEl.querySelector('.doc-select');
        checkbox.addEventListener('change', () => {
            if (checkbox.checked) {
                this.selectedDocuments.push(doc.id);
            } else {
                const index = this.selectedDocuments.indexOf(doc.id);
                if (index !== -1) {
                    this.selectedDocuments.splice(index, 1);
                }
            }
            this.updateBatchButtons();
        });
        
        const editBtn = docEl.querySelector('.edit-btn');
        editBtn.addEventListener('click', () => {
            this.openEditModal(doc);
        });
        
        const processBtn = docEl.querySelector('.process-btn');
        processBtn.addEventListener('click', () => {
            this.processDocuments([doc.id]);
        });
        
        const deleteBtn = docEl.querySelector('.delete-btn');
        deleteBtn.addEventListener('click', () => {
            this.deleteDocument(doc.id);
        });
        
        return docEl;
    }
    
    openEditModal(doc) {
        if (!this.editModal) return;
        
        // Set document ID
        this.editModal.dataset.documentId = doc.id;
        
        // Set document title
        const modalTitle = this.editModal.querySelector('.modal-title');
        if (modalTitle) {
            modalTitle.textContent = `Edit: ${this.truncateFilename(doc.filename)}`;
        }
        
        // Clear existing tags
        const tagList = document.getElementById('edit-tag-list');
        if (tagList) {
            tagList.innerHTML = '';
            
            // Add current tags
            if (doc.tags && doc.tags.length > 0) {
                doc.tags.forEach(tag => {
                    const tagEl = document.createElement('div');
                    tagEl.className = 'tag-item';
                    tagEl.innerHTML = `
                        ${tag}
                        <span class="tag-remove" data-tag="${tag}">&times;</span>
                    `;
                    
                    const removeBtn = tagEl.querySelector('.tag-remove');
                    removeBtn.addEventListener('click', () => {
                        tagEl.remove();
                    });
                    
                    tagList.appendChild(tagEl);
                });
            }
        }
        
        // Set folder
        const folderSelect = document.getElementById('edit-folder');
        if (folderSelect) {
            // Populate folder options
            folderSelect.innerHTML = '';
            this.allFolders.forEach(folder => {
                const option = document.createElement('option');
                option.value = folder;
                option.textContent = folder === '/' ? 'Root' : folder.replace('/', '');
                option.selected = folder === doc.folder;
                folderSelect.appendChild(option);
            });
        }
        
        // Show modal
        this.editModal.style.display = 'flex';
        
        // Set up tag input
        const tagInput = document.getElementById('edit-tag-input');
        if (tagInput) {
            tagInput.value = '';
            tagInput.focus();
            
            // Remove existing event listeners
            const newTagInput = tagInput.cloneNode(true);
            tagInput.parentNode.replaceChild(newTagInput, tagInput);
            
            // Add event listeners
            newTagInput.addEventListener('keydown', (e) => {
                if (e.key === 'Enter' && newTagInput.value.trim()) {
                    e.preventDefault();
                    this.addTagToModal(newTagInput.value.trim());
                    newTagInput.value = '';
                }
            });
            
            // Tag suggestions
            newTagInput.addEventListener('input', () => {
                this.showTagSuggestionsInModal(newTagInput.value);
            });
        }
    }
    
    addTagToModal(tag) {
        const tagList = document.getElementById('edit-tag-list');
        if (!tagList) return;
        
        // Check if tag already exists
        const existingTags = Array.from(tagList.querySelectorAll('.tag-item'))
            .map(el => el.textContent.trim().replace('', ''));
        
        if (existingTags.includes(tag)) return;
        
        // Add tag
        const tagEl = document.createElement('div');
        tagEl.className = 'tag-item';
        tagEl.innerHTML = `
            ${tag}
            <span class="tag-remove" data-tag="${tag}">&times;</span>
        `;
        
        const removeBtn = tagEl.querySelector('.tag-remove');
        removeBtn.addEventListener('click', () => {
            tagEl.remove();
        });
        
        tagList.appendChild(tagEl);
    }
    
    showTagSuggestionsInModal(input) {
        const suggestionsContainer = document.getElementById('tag-suggestions');
        if (!suggestionsContainer || !input) {
            if (suggestionsContainer) {
                suggestionsContainer.style.display = 'none';
            }
            return;
        }
        
        // Filter tags
        const matchingTags = this.allTags.filter(tag =>
            tag.toLowerCase().includes(input.toLowerCase()) &&
            !Array.from(document.getElementById('edit-tag-list').querySelectorAll('.tag-item'))
                .map(el => el.textContent.trim().replace('', ''))
                .includes(tag)
        );
        
        if (matchingTags.length === 0) {
            suggestionsContainer.style.display = 'none';
            return;
        }
        
        // Render suggestions
        suggestionsContainer.innerHTML = '';
        matchingTags.forEach(tag => {
            const suggestionEl = document.createElement('div');
            suggestionEl.className = 'tag-suggestion-item';
            suggestionEl.textContent = tag;
            suggestionEl.addEventListener('click', () => {
                this.addTagToModal(tag);
                document.getElementById('edit-tag-input').value = '';
                suggestionsContainer.style.display = 'none';
            });
            
            suggestionsContainer.appendChild(suggestionEl);
        });
        
        suggestionsContainer.style.display = 'block';
    }
    
    saveDocumentChanges() {
        const documentId = this.editModal.dataset.documentId;
        if (!documentId) return;
        
        // Get tags
        const tags = Array.from(document.getElementById('edit-tag-list').querySelectorAll('.tag-item'))
            .map(el => el.textContent.trim().replace('', ''));
        
        // Get folder
        const folder = document.getElementById('edit-folder').value;
        
        // Update tags
        this.updateDocumentTags(documentId, tags);
        
        // Update folder
        this.updateDocumentFolder(documentId, folder);
        
        // Close modal
        this.editModal.style.display = 'none';
    }
    
    updateDocumentTags(documentId, tags) {
        fetch(`/api/documents/${documentId}/tags`, {
            method: 'PUT',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ tags })
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification('Tags updated successfully');
                this.loadDocuments();
                this.loadTagsAndFolders();
            } else {
                showNotification('Error updating tags: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error updating tags:', error);
            showNotification('Error updating tags', 'warning');
        });
    }
    
    updateDocumentFolder(documentId, folder) {
        fetch(`/api/documents/${documentId}/folder`, {
            method: 'PUT',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ folder })
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification('Folder updated successfully');
                this.loadDocuments();
                this.loadTagsAndFolders();
            } else {
                showNotification('Error updating folder: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error updating folder:', error);
            showNotification('Error updating folder', 'warning');
        });
    }
    
    truncateFilename(filename) {
        if (filename.length > 20) {
            return filename.substring(0, 17) + '...';
        }
        return filename;
    }
    
    handleUpload(e) {
        e.preventDefault();
        
        if (this.isUploading) return;
        
        if (!this.documentFile || !this.documentFile.files[0]) {
            showNotification('Please select a file to upload', 'warning');
            return;
        }
        
        const file = this.documentFile.files[0];
        const formData = new FormData();
        formData.append('file', file);
        
        // Add tags if provided
        if (this.tagInput && this.tagInput.value.trim()) {
            formData.append('tags', this.tagInput.value.trim());
        }
        
        // Add folder if provided
        if (this.folderSelect && this.folderSelect.value) {
            formData.append('folder', this.folderSelect.value);
        }
        
        // Reset progress bar
        if (this.uploadProgressFill) {
            this.uploadProgressFill.style.width = '0%';
        }
        
        // Show progress bar
        if (this.uploadProgress) {
            this.uploadProgress.style.display = 'block';
        }
        
        this.isUploading = true;
        
        // Upload file
        const xhr = new XMLHttpRequest();
        xhr.open('POST', '/api/documents/upload', true);
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable && this.uploadProgressFill) {
                const percentComplete = (e.loaded / e.total) * 100;
                this.uploadProgressFill.style.width = percentComplete + '%';
            }
        };
        
        xhr.onload = () => {
            this.isUploading = false;
            
            if (xhr.status === 200) {
                const response = JSON.parse(xhr.responseText);
                if (response.success) {
                    showNotification('Document uploaded successfully!');
                    if (this.documentFile) {
                        this.documentFile.value = '';
                    }
                    if (this.tagInput) {
                        this.tagInput.value = '';
                    }
                    this.loadDocuments();
                    this.loadTagsAndFolders();
                    
                    // Process the document
                    this.processDocuments([response.document_id]);
                } else {
                    showNotification('Error uploading document: ' + response.message, 'warning');
                }
            } else {
                showNotification('Error uploading document', 'warning');
            }
            
            // Hide progress bar after a delay
            setTimeout(() => {
                if (this.uploadProgress) {
                    this.uploadProgress.style.display = 'none';
                }
            }, 1000);
        };
        
        xhr.onerror = () => {
            this.isUploading = false;
            showNotification('Error uploading document', 'warning');
            
            // Hide progress bar
            if (this.uploadProgress) {
                this.uploadProgress.style.display = 'none';
            }
        };
        
        xhr.send(formData);
    }
    
    processDocuments(documentIds) {
        // Get chunking strategy options if available
        const chunkingStrategy = document.getElementById('chunking-strategy')?.value || 'recursive';
        const chunkSize = document.getElementById('chunk-size')?.value || null;
        const chunkOverlap = document.getElementById('chunk-overlap')?.value || null;
        
        const request = {
            document_ids: documentIds,
            force_reprocess: false,
            chunking_strategy: chunkingStrategy,
            chunk_size: chunkSize ? parseInt(chunkSize) : null,
            chunk_overlap: chunkOverlap ? parseInt(chunkOverlap) : null
        };
        
        // Log the processing request
        console.log('Processing documents with options:', request);
        
        fetch('/api/documents/process', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(request)
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification(`Processing started for ${documentIds.length} document(s) with ${chunkingStrategy} chunking strategy`);
            } else {
                showNotification('Error processing documents: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error processing documents:', error);
            showNotification('Error processing documents', 'warning');
        });
    }
    
    deleteDocument(documentId) {
        if (!confirm('Are you sure you want to delete this document?')) {
            return;
        }
        
        fetch(`/api/documents/${documentId}`, {
            method: 'DELETE'
        })
        .then(response => response.json())
        .then(data => {
            if (data.success) {
                showNotification('Document deleted successfully');
                
                // Remove from selected documents
                const index = this.selectedDocuments.indexOf(documentId);
                if (index !== -1) {
                    this.selectedDocuments.splice(index, 1);
                }
                
                this.loadDocuments();
                this.loadTagsAndFolders();
            } else {
                showNotification('Error deleting document: ' + data.message, 'warning');
            }
        })
        .catch(error => {
            console.error('Error deleting document:', error);
            showNotification('Error deleting document', 'warning');
        });
    }
    
    processSelected() {
        if (this.selectedDocuments.length === 0) return;
        this.processDocuments([...this.selectedDocuments]);
    }
    
    deleteSelected() {
        if (this.selectedDocuments.length === 0) return;
        
        if (!confirm('Are you sure you want to delete ' + this.selectedDocuments.length + ' document(s)?')) {
            return;
        }
        
        const promises = this.selectedDocuments.map(id => {
            return fetch(`/api/documents/${id}`, {
                method: 'DELETE'
            }).then(response => response.json());
        });
        
        Promise.all(promises)
            .then(() => {
                showNotification('Documents deleted successfully');
                this.selectedDocuments = [];
                this.loadDocuments();
                this.loadTagsAndFolders();
            })
            .catch(error => {
                console.error('Error deleting documents:', error);
                showNotification('Error deleting documents', 'warning');
            });
    }
    
    updateBatchButtons() {
        const hasSelected = this.selectedDocuments.length > 0;
        
        if (this.processSelectedBtn) {
            this.processSelectedBtn.disabled = !hasSelected;
        }
        
        if (this.deleteSelectedBtn) {
            this.deleteSelectedBtn.disabled = !hasSelected;
        }
    }
    
    updateDocumentCount(count) {
        if (this.documentCount) {
            this.documentCount.textContent = count;
            
            // If there are documents and the section is not expanded, add a visual indicator
            if (count > 0 && !this.isExpanded) {
                this.documentCount.classList.add('has-documents');
            } else {
                this.documentCount.classList.remove('has-documents');
            }
        }
    }
    
    setupMobileSupport() {
        // Add pull-to-refresh functionality
        this.setupPullToRefresh();
        
        // Add swipe gestures for document items
        this.setupSwipeGestures();
    }
    
    removeMobileSupport() {
        // Remove mobile-specific event listeners
        if (this.docList) {
            this.docList.removeEventListener('touchstart', this.touchStartHandler);
            this.docList.removeEventListener('touchmove', this.touchMoveHandler);
            this.docList.removeEventListener('touchend', this.touchEndHandler);
        }
        
        // Remove pull-to-refresh indicator if it exists
        const refreshIndicator = document.querySelector('.pull-to-refresh');
        if (refreshIndicator) {
            refreshIndicator.parentNode.removeChild(refreshIndicator);
        }
    }
    
    setupPullToRefresh() {
        if (!this.docList) return;
        
        // Create pull-to-refresh indicator
        const refreshIndicator = document.createElement('div');
        refreshIndicator.className = 'pull-to-refresh';
        refreshIndicator.style.display = 'none';
        refreshIndicator.innerHTML = '<span class="spinner"></span> Pull to refresh';
        
        // Insert before document list
        this.docList.parentNode.insertBefore(refreshIndicator, this.docList);
        
        // Touch event variables
        let touchStartY = 0;
        let touchEndY = 0;
        
        // Touch event handlers
        this.touchStartHandler = (e) => {
            touchStartY = e.touches[0].clientY;
        };
        
        this.touchMoveHandler = (e) => {
            touchEndY = e.touches[0].clientY;
            
            // If scrolled to top and pulling down
            if (this.docList.scrollTop === 0 && touchEndY > touchStartY) {
                refreshIndicator.style.display = 'flex';
                e.preventDefault(); // Prevent default scroll
            }
        };
        
        this.touchEndHandler = () => {
            if (refreshIndicator.style.display === 'flex') {
                refreshIndicator.innerHTML = '<span class="spinner"></span> Refreshing...';
                
                // Reload documents
                this.loadDocuments().then(() => {
                    refreshIndicator.style.display = 'none';
                });
            }
        };
        
        // Add touch event listeners
        this.docList.addEventListener('touchstart', this.touchStartHandler);
        this.docList.addEventListener('touchmove', this.touchMoveHandler);
        this.docList.addEventListener('touchend', this.touchEndHandler);
    }
    
    setupSwipeGestures() {
        // Set up swipe gestures for document items
        document.querySelectorAll('.sidebar-document-item').forEach(item => {
            // Add delete action event listener to existing swipe actions
            const deleteAction = item.querySelector('.swipe-action.delete-action');
            if (deleteAction) {
                deleteAction.addEventListener('click', () => {
                    const docId = item.dataset.id;
                    if (docId) {
                        this.deleteDocument(docId);
                    }
                });
            }
            
            // Touch variables
            let touchStartX = 0;
            let touchEndX = 0;
            
            // Touch event handlers
            item.addEventListener('touchstart', (e) => {
                touchStartX = e.touches[0].clientX;
            });
            
            item.addEventListener('touchend', (e) => {
                touchEndX = e.changedTouches[0].clientX;
                
                // Swipe left to show delete button
                if (touchStartX - touchEndX > 50) {
                    item.classList.add('show-actions');
                }
                
                // Swipe right to hide actions
                if (touchEndX - touchStartX > 50) {
                    item.classList.remove('show-actions');
                }
            });
        });
    }
}

// Initialize document manager
const documentManager = new DocumentManager();

================
File: app/static/js/main.js
================
// Theme switching functionality
document.addEventListener('DOMContentLoaded', function() {
    // Check for saved theme preference or default to 'dark'
    const savedTheme = localStorage.getItem('theme') || 'dark';
    document.documentElement.setAttribute('data-theme', savedTheme);
    
    // Update theme toggle button
    updateThemeToggle(savedTheme);
    
    // Set up theme toggle button
    const themeToggle = document.getElementById('theme-toggle');
    if (themeToggle) {
        themeToggle.addEventListener('click', toggleTheme);
    }
});

// Toggle theme function
function toggleTheme() {
    const currentTheme = document.documentElement.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    
    document.documentElement.setAttribute('data-theme', newTheme);
    localStorage.setItem('theme', newTheme);
    updateThemeToggle(newTheme);
}

// Update theme toggle button
function updateThemeToggle(theme) {
    const toggle = document.getElementById('theme-toggle');
    if (toggle) {
        toggle.innerHTML = theme === 'dark' ? '<i class="fas fa-sun"></i>' : '<i class="fas fa-moon"></i>';
        toggle.title = theme === 'dark' ? 'Switch to light mode' : 'Switch to dark mode';
    }
}

// Show notification
function showNotification(message, type = 'info') {
    // Create notification element
    const notification = document.createElement('div');
    notification.className = `notification ${type}`;
    notification.style.position = 'fixed';
    notification.style.top = '20px';
    notification.style.right = '20px';
    notification.style.backgroundColor = type === 'warning' ? '#ff9800' : 'var(--secondary-color)';
    notification.style.color = 'white';
    notification.style.padding = '10px 15px';
    notification.style.borderRadius = '4px';
    notification.style.boxShadow = '0 2px 10px rgba(0, 0, 0, 0.2)';
    notification.style.zIndex = '1000';
    notification.style.maxWidth = '300px';
    notification.textContent = message;
    
    // Add close button
    const closeBtn = document.createElement('span');
    closeBtn.innerHTML = '&times;';
    closeBtn.style.marginLeft = '10px';
    closeBtn.style.cursor = 'pointer';
    closeBtn.style.fontWeight = 'bold';
    closeBtn.onclick = function() {
        document.body.removeChild(notification);
    };
    notification.appendChild(closeBtn);
    
    // Add to body
    document.body.appendChild(notification);
    
    // Auto remove after 5 seconds
    setTimeout(() => {
        if (document.body.contains(notification)) {
            document.body.removeChild(notification);
        }
    }, 5000);
}

// Copy to clipboard function
function copyToClipboard(text) {
    navigator.clipboard.writeText(text).then(() => {
        showNotification('Copied to clipboard!');
    }).catch(err => {
        console.error('Could not copy text: ', err);
        showNotification('Failed to copy to clipboard', 'warning');
    });
}

// Conversation Management
// Make conversation globally accessible
window.conversation = {
    messages: [],
    metadata: {
        estimatedTokens: 0,
        maxTokens: 4096,
        lastUpdated: new Date().toISOString()
    }
};
// Reference for local use
let conversation = window.conversation;

// Token estimation (rough approximation - 1 token  4 characters)
function estimateTokens(text) {
    return Math.ceil(text.length / 4);
}

// Add message to conversation
function addMessage(role, content, sources = null) {
    conversation.messages.push({
        role: role,
        content: content,
        sources: sources,
        timestamp: new Date().toISOString()
    });
    
    const tokens = estimateTokens(content);
    conversation.metadata.estimatedTokens += tokens;
    conversation.metadata.lastUpdated = new Date().toISOString();
    
    saveToLocalStorage();
    updateTokenDisplay();
}

// Get formatted conversation history for prompt
function getConversationHistory() {
    let history = '';
    conversation.messages.forEach(msg => {
        const role = msg.role === 'user' ? 'User' : 'Metis';
        history += `${role}: ${msg.content}\n\n`;
    });
    return history;
}

// Get formatted conversation for Ollama
function getFormattedPrompt(newPrompt) {
    // First check if we need to trim the conversation
    trimConversationToFit(conversation.metadata.maxTokens);
    
    // Then format the conversation with the new prompt
    let formattedPrompt = '';
    
    // Add conversation history
    conversation.messages.forEach(msg => {
        const role = msg.role === 'user' ? 'User' : 'Metis';
        formattedPrompt += `${role}: ${msg.content}\n\n`;
    });
    
    // Add the new prompt
    formattedPrompt += `User: ${newPrompt}\n\nMetis:`;
    
    return formattedPrompt;
}

// Clear conversation
function clearConversation() {
    window.conversation = {
        messages: [],
        metadata: {
            estimatedTokens: 0,
            maxTokens: parseInt(document.getElementById('num_ctx')?.value || 4096),
            lastUpdated: new Date().toISOString()
        }
    };
    // Update local reference
    conversation = window.conversation;
    saveToLocalStorage();
    updateTokenDisplay();
}

// Make clearConversation globally accessible
window.clearConversation = clearConversation;

// Save conversation to localStorage
function saveToLocalStorage() {
    localStorage.setItem('metis_conversation', JSON.stringify(conversation));
}

// Load conversation from localStorage
function loadFromLocalStorage() {
    const saved = localStorage.getItem('metis_conversation');
    if (saved) {
        try {
            conversation = JSON.parse(saved);
            // Update max tokens from current form value
            const numCtxElement = document.getElementById('num_ctx');
            if (numCtxElement) {
                conversation.metadata.maxTokens = parseInt(numCtxElement.value) || 4096;
            }
            updateTokenDisplay();
            return true;
        } catch (e) {
            console.error('Error loading conversation:', e);
            return false;
        }
    }
    return false;
}

// Get total token count for conversation
function getConversationTokenCount() {
    return conversation.metadata.estimatedTokens;
}

// Trim conversation to fit within token limit
function trimConversationToFit(maxTokens) {
    // Reserve tokens for the new prompt and response (rough estimate)
    const reservedTokens = 1000;
    const availableTokens = maxTokens - reservedTokens;
    
    // If we're already under the limit, no need to trim
    if (getConversationTokenCount() <= availableTokens) {
        return;
    }
    
    // Remove oldest messages until we're under the limit
    while (getConversationTokenCount() > availableTokens && conversation.messages.length > 0) {
        const removedMsg = conversation.messages.shift();
        conversation.metadata.estimatedTokens -= estimateTokens(removedMsg.content);
    }
    
    // Update localStorage and UI
    saveToLocalStorage();
    updateTokenDisplay();
    
    // Show a notification that some messages were removed
    showNotification('Some older messages were removed to stay within the token limit.');
}

// Update token display
function updateTokenDisplay() {
    const tokenUsage = document.getElementById('token-usage');
    const tokenUsageFill = document.getElementById('token-usage-fill');
    const tokenUsageText = document.getElementById('token-usage-text');
    
    if (!tokenUsage || !tokenUsageFill || !tokenUsageText) return;
    
    const currentTokens = getConversationTokenCount();
    const maxTokens = conversation.metadata.maxTokens;
    const percentage = Math.min((currentTokens / maxTokens) * 100, 100);
    
    tokenUsageFill.style.width = `${percentage}%`;
    tokenUsageText.textContent = `${currentTokens} / ${maxTokens} tokens`;
    
    // Set color based on usage
    if (percentage > 90) {
        tokenUsageFill.style.backgroundColor = '#d32f2f'; // Red for high usage
    } else if (percentage > 70) {
        tokenUsageFill.style.backgroundColor = '#ff9800'; // Orange for medium usage
    } else {
        tokenUsageFill.style.backgroundColor = 'var(--accent-color)'; // Default color
    }
    
    // Show the token usage indicator if we have messages
    if (conversation.messages.length > 0) {
        tokenUsage.style.display = 'block';
    } else {
        tokenUsage.style.display = 'none';
    }
}

// Initialize conversation
function initConversation() {
    // Try to load from localStorage
    if (!loadFromLocalStorage()) {
        // If no saved conversation, initialize a new one
        clearConversation();
    }
    
    // Render the conversation if there's a chat container
    const chatContainer = document.getElementById('chat-container');
    if (chatContainer) {
        renderConversation();
    }
    
    // Update token display
    updateTokenDisplay();
}

// Render conversation in UI
function renderConversation() {
    const chatContainer = document.getElementById('chat-container');
    if (!chatContainer) return;
    
    chatContainer.innerHTML = '';
    
    conversation.messages.forEach(msg => {
        const messageDiv = document.createElement('div');
        messageDiv.className = `message ${msg.role === 'user' ? 'user-message' : 'bot-message'}`;
        
        const headerDiv = document.createElement('div');
        headerDiv.className = 'message-header';
        headerDiv.textContent = msg.role === 'user' ? 'You:' : 'Metis:';
        messageDiv.appendChild(headerDiv);
        
        const contentText = document.createTextNode(msg.content);
        messageDiv.appendChild(contentText);
        
        if (msg.role === 'assistant') {
            const copyButton = document.createElement('button');
            copyButton.className = 'copy-button';
            copyButton.innerHTML = '<i class="fas fa-copy"></i> Copy';
            copyButton.onclick = function() {
                copyToClipboard(msg.content);
            };
            messageDiv.appendChild(copyButton);
            
            // Add sources if available
            if (msg.sources && msg.sources.length > 0) {
                const sourcesDiv = document.createElement('div');
                sourcesDiv.className = 'sources-section';
                sourcesDiv.innerHTML = '<strong>Sources:</strong> ';
                
                msg.sources.forEach(source => {
                    const sourceSpan = document.createElement('span');
                    sourceSpan.className = 'source-item';
                    sourceSpan.textContent = source.filename || source;
                    sourcesDiv.appendChild(sourceSpan);
                });
                
                messageDiv.appendChild(sourcesDiv);
            }
        }
        
        chatContainer.appendChild(messageDiv);
    });
    
    // Scroll to bottom
    chatContainer.scrollTop = chatContainer.scrollHeight;
}

// Initialize when page loads
document.addEventListener('DOMContentLoaded', function() {
    // Initialize conversation
    initConversation();
    
    // Set up advanced options toggle if it exists
    const advancedToggle = document.getElementById('advanced-toggle');
    const advancedContent = document.getElementById('advanced-content');
    const advancedIcon = document.getElementById('advanced-icon');
    
    if (advancedToggle && advancedContent && advancedIcon) {
        // Show advanced options if they were previously shown
        if (localStorage.getItem('advancedOptions') === 'shown') {
            advancedContent.classList.add('show');
            advancedIcon.classList.replace('fa-chevron-down', 'fa-chevron-up');
        }
        
        advancedToggle.addEventListener('click', function() {
            advancedContent.classList.toggle('show');
            if (advancedContent.classList.contains('show')) {
                advancedIcon.classList.replace('fa-chevron-down', 'fa-chevron-up');
                localStorage.setItem('advancedOptions', 'shown');
            } else {
                advancedIcon.classList.replace('fa-chevron-up', 'fa-chevron-down');
                localStorage.setItem('advancedOptions', 'hidden');
            }
        });
    }
    
    // Set up context window size change listener if it exists
    const numCtxElement = document.getElementById('num_ctx');
    if (numCtxElement) {
        numCtxElement.addEventListener('change', function() {
            conversation.metadata.maxTokens = parseInt(this.value) || 4096;
            updateTokenDisplay();
            saveToLocalStorage();
        });
    }
});

================
File: app/static/js/test_models.js
================
// This script will be loaded by the browser and will log any errors
console.log('Test script loaded');

document.addEventListener('DOMContentLoaded', function() {
    console.log('DOM loaded, initializing test script');
    
    // Get elements
    const modelSelect = document.getElementById('test-model-select');
    const modelList = document.getElementById('model-list');
    
    if (modelSelect) {
        console.log('Found model select element');
    } else {
        console.error('Model select element not found');
    }
    
    if (modelList) {
        console.log('Found model list element');
    } else {
        console.error('Model list element not found');
    }
    
    // Try to fetch models
    fetch('/api/system/models')
        .then(response => {
            console.log('Response status:', response.status);
            return response.json();
        })
        .then(models => {
            console.log('Models fetched:', models);
            console.log('Number of models:', models.length);
            
            // Log each model
            models.forEach(model => {
                console.log('Model:', model.name);
            });
            
            // Update UI if elements exist
            if (modelSelect) {
                // Clear the dropdown
                modelSelect.innerHTML = '';
                
                // Add models to dropdown
                models.forEach(model => {
                    const option = document.createElement('option');
                    option.value = model.name;
                    option.textContent = model.name;
                    modelSelect.appendChild(option);
                    console.log('Added model to dropdown:', model.name);
                });
            }
            
            if (modelList) {
                // Clear the list
                modelList.innerHTML = '';
                
                // Add models to list
                models.forEach(model => {
                    const modelItem = document.createElement('div');
                    modelItem.className = 'model-item';
                    modelItem.textContent = model.name;
                    modelList.appendChild(modelItem);
                    console.log('Added model to list:', model.name);
                });
            }
        })
        .catch(error => {
            console.error('Error fetching models:', error);
            
            if (modelSelect) {
                modelSelect.innerHTML = '<option value="">Error loading models</option>';
            }
            
            if (modelList) {
                modelList.innerHTML = '<div>Error loading models: ' + error.message + '</div>';
            }
        });
});

================
File: app/templates/analytics.html
================
{% extends "base.html" %}

{% block title %}Analytics - Metis RAG{% endblock %}

{% block head %}
<style>
    .analytics-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }
    
    .analytics-section {
        margin-bottom: 30px;
        background-color: var(--card-bg);
        border-radius: 8px;
        padding: 20px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    
    .analytics-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 20px;
    }
    
    .analytics-title {
        font-size: 1.5rem;
        font-weight: 600;
        margin: 0;
    }
    
    .analytics-controls {
        display: flex;
        gap: 10px;
    }
    
    .analytics-card {
        background-color: var(--input-bg);
        border-radius: 8px;
        padding: 15px;
        margin-bottom: 15px;
    }
    
    .analytics-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 15px;
        margin-bottom: 20px;
    }
    
    .stat-card {
        background-color: var(--input-bg);
        border-radius: 8px;
        padding: 15px;
        text-align: center;
    }
    
    .stat-value {
        font-size: 2rem;
        font-weight: bold;
        margin: 10px 0;
        color: var(--accent-color);
    }
    
    .stat-label {
        font-size: 0.9rem;
        color: var(--muted-color);
    }
    
    .chart-container {
        height: 300px;
        margin-bottom: 20px;
    }
    
    .table-container {
        overflow-x: auto;
    }
    
    table {
        width: 100%;
        border-collapse: collapse;
    }
    
    th, td {
        padding: 10px;
        text-align: left;
        border-bottom: 1px solid var(--border-color);
    }
    
    th {
        background-color: var(--card-bg);
        font-weight: 600;
    }
    
    tr:hover {
        background-color: var(--border-color);
    }
    
    .loading {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 200px;
        font-size: 1.2rem;
        color: var(--muted-color);
    }
    
    .spinner {
        border: 4px solid var(--border-color);
        border-top: 4px solid var(--accent-color);
        border-radius: 50%;
        width: 30px;
        height: 30px;
        animation: spin 1s linear infinite;
        margin-right: 10px;
    }
    
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
</style>
{% endblock %}

{% block content %}
<div class="analytics-container">
    <h1>Analytics Dashboard</h1>
    
    <div class="analytics-section">
        <div class="analytics-header">
            <h2 class="analytics-title">System Overview</h2>
            <div class="analytics-controls">
                <button id="refresh-stats" class="secondary">
                    <i class="fas fa-sync-alt"></i> Refresh
                </button>
            </div>
        </div>
        
        <div id="system-stats" class="analytics-grid">
            <div class="loading">
                <div class="spinner"></div> Loading system stats...
            </div>
        </div>
    </div>
    
    <div class="analytics-section">
        <div class="analytics-header">
            <h2 class="analytics-title">Query Analytics</h2>
            <div class="analytics-controls">
                <select id="query-time-period">
                    <option value="all">All Time</option>
                    <option value="day">Last 24 Hours</option>
                    <option value="week">Last 7 Days</option>
                    <option value="month">Last 30 Days</option>
                </select>
            </div>
        </div>
        
        <div id="query-stats" class="analytics-grid">
            <div class="loading">
                <div class="spinner"></div> Loading query stats...
            </div>
        </div>
        
        <h3>Most Common Queries</h3>
        <div id="common-queries" class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Query</th>
                        <th>Count</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Will be populated by JavaScript -->
                </tbody>
            </table>
        </div>
        
        <h3>Recent Queries</h3>
        <div id="recent-queries" class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Query</th>
                        <th>Model</th>
                        <th>RAG</th>
                        <th>Response Time</th>
                        <th>Timestamp</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Will be populated by JavaScript -->
                </tbody>
            </table>
        </div>
    </div>
    
    <div class="analytics-section">
        <div class="analytics-header">
            <h2 class="analytics-title">Document Usage</h2>
            <div class="analytics-controls">
                <select id="document-time-period">
                    <option value="all">All Time</option>
                    <option value="day">Last 24 Hours</option>
                    <option value="week">Last 7 Days</option>
                    <option value="month">Last 30 Days</option>
                </select>
            </div>
        </div>
        
        <div id="document-stats" class="analytics-grid">
            <div class="loading">
                <div class="spinner"></div> Loading document stats...
            </div>
        </div>
        
        <h3>Most Used Documents</h3>
        <div id="most-used-documents" class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Document ID</th>
                        <th>Usage Count</th>
                        <th>Last Used</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Will be populated by JavaScript -->
                </tbody>
            </table>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Load initial data
        loadSystemStats();
        loadQueryStats();
        loadDocumentStats();
        
        // Set up event listeners
        document.getElementById('refresh-stats').addEventListener('click', function() {
            loadSystemStats();
            loadQueryStats();
            loadDocumentStats();
        });
        
        document.getElementById('query-time-period').addEventListener('change', function() {
            loadQueryStats();
        });
        
        document.getElementById('document-time-period').addEventListener('change', function() {
            loadDocumentStats();
        });
    });
    
    function loadSystemStats() {
        const statsContainer = document.getElementById('system-stats');
        statsContainer.innerHTML = '<div class="loading"><div class="spinner"></div> Loading system stats...</div>';
        
        fetch('/api/analytics/system_stats')
            .then(response => response.json())
            .then(data => {
                statsContainer.innerHTML = '';
                
                // Vector store stats
                const vectorStats = data.vector_store || {};
                
                // Add document count stat
                addStatCard(statsContainer, data.document_count || 0, 'Documents');
                
                // Add vector store document count
                addStatCard(statsContainer, vectorStats.count || 0, 'Vector Chunks');
                
                // Add query count stat
                addStatCard(statsContainer, data.query_count || 0, 'Total Queries');
                
                // Add embedding model
                addStatCard(statsContainer, vectorStats.embeddings_model || 'N/A', 'Embedding Model');
                
                // Add cache stats if available
                if (vectorStats.cache_enabled) {
                    addStatCard(statsContainer, vectorStats.cache_hit_ratio ? (vectorStats.cache_hit_ratio * 100).toFixed(1) + '%' : '0%', 'Cache Hit Ratio');
                    addStatCard(statsContainer, vectorStats.cache_size || 0, 'Cache Size');
                }
            })
            .catch(error => {
                console.error('Error loading system stats:', error);
                statsContainer.innerHTML = '<div class="analytics-card">Error loading system stats</div>';
            });
    }
    
    function loadQueryStats() {
        const statsContainer = document.getElementById('query-stats');
        const commonQueriesTable = document.getElementById('common-queries').querySelector('tbody');
        const recentQueriesTable = document.getElementById('recent-queries').querySelector('tbody');
        const timePeriod = document.getElementById('query-time-period').value;
        
        statsContainer.innerHTML = '<div class="loading"><div class="spinner"></div> Loading query stats...</div>';
        commonQueriesTable.innerHTML = '<tr><td colspan="2">Loading...</td></tr>';
        recentQueriesTable.innerHTML = '<tr><td colspan="5">Loading...</td></tr>';
        
        fetch(`/api/analytics/query_stats?time_period=${timePeriod}`)
            .then(response => response.json())
            .then(data => {
                statsContainer.innerHTML = '';
                
                // Add query count stat
                addStatCard(statsContainer, data.query_count || 0, 'Queries');
                
                // Add average response time
                addStatCard(statsContainer, data.avg_response_time_ms ? data.avg_response_time_ms.toFixed(0) + ' ms' : 'N/A', 'Avg Response Time');
                
                // Add average token count
                addStatCard(statsContainer, data.avg_token_count ? data.avg_token_count.toFixed(0) : 'N/A', 'Avg Token Count');
                
                // Add RAG usage percentage
                addStatCard(statsContainer, data.rag_usage_percent ? data.rag_usage_percent.toFixed(1) + '%' : '0%', 'RAG Usage');
                
                // Populate most common queries table
                commonQueriesTable.innerHTML = '';
                if (data.most_common_queries && data.most_common_queries.length > 0) {
                    data.most_common_queries.forEach(query => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${query.query}</td>
                            <td>${query.count}</td>
                        `;
                        commonQueriesTable.appendChild(row);
                    });
                } else {
                    commonQueriesTable.innerHTML = '<tr><td colspan="2">No queries found</td></tr>';
                }
                
                // Populate recent queries table
                recentQueriesTable.innerHTML = '';
                if (data.recent_queries && data.recent_queries.length > 0) {
                    data.recent_queries.forEach(query => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${query.query}</td>
                            <td>${query.model}</td>
                            <td>${query.use_rag ? 'Yes' : 'No'}</td>
                            <td>${query.response_time_ms ? query.response_time_ms.toFixed(0) + ' ms' : 'N/A'}</td>
                            <td>${formatTimestamp(query.timestamp)}</td>
                        `;
                        recentQueriesTable.appendChild(row);
                    });
                } else {
                    recentQueriesTable.innerHTML = '<tr><td colspan="5">No queries found</td></tr>';
                }
            })
            .catch(error => {
                console.error('Error loading query stats:', error);
                statsContainer.innerHTML = '<div class="analytics-card">Error loading query stats</div>';
                commonQueriesTable.innerHTML = '<tr><td colspan="2">Error loading data</td></tr>';
                recentQueriesTable.innerHTML = '<tr><td colspan="5">Error loading data</td></tr>';
            });
    }
    
    function loadDocumentStats() {
        const statsContainer = document.getElementById('document-stats');
        const documentsTable = document.getElementById('most-used-documents').querySelector('tbody');
        const timePeriod = document.getElementById('document-time-period').value;
        
        statsContainer.innerHTML = '<div class="loading"><div class="spinner"></div> Loading document stats...</div>';
        documentsTable.innerHTML = '<tr><td colspan="3">Loading...</td></tr>';
        
        fetch(`/api/analytics/document_usage?time_period=${timePeriod}`)
            .then(response => response.json())
            .then(data => {
                statsContainer.innerHTML = '';
                
                // Add document count stat
                addStatCard(statsContainer, data.document_count || 0, 'Documents Used');
                
                // Populate most used documents table
                documentsTable.innerHTML = '';
                if (data.most_used && data.most_used.length > 0) {
                    data.most_used.forEach(doc => {
                        const row = document.createElement('tr');
                        row.innerHTML = `
                            <td>${doc.id}</td>
                            <td>${doc.usage_count}</td>
                            <td>${formatTimestamp(doc.last_used)}</td>
                        `;
                        documentsTable.appendChild(row);
                    });
                } else {
                    documentsTable.innerHTML = '<tr><td colspan="3">No document usage data found</td></tr>';
                }
            })
            .catch(error => {
                console.error('Error loading document stats:', error);
                statsContainer.innerHTML = '<div class="analytics-card">Error loading document stats</div>';
                documentsTable.innerHTML = '<tr><td colspan="3">Error loading data</td></tr>';
            });
    }
    
    function addStatCard(container, value, label) {
        const card = document.createElement('div');
        card.className = 'stat-card';
        card.innerHTML = `
            <div class="stat-value">${value}</div>
            <div class="stat-label">${label}</div>
        `;
        container.appendChild(card);
    }
    
    function formatTimestamp(timestamp) {
        if (!timestamp) return 'N/A';
        
        try {
            const date = new Date(timestamp);
            return date.toLocaleString();
        } catch (e) {
            return timestamp;
        }
    }
</script>
{% endblock %}

================
File: app/templates/base.html
================
<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Security-Policy" content="default-src 'self'; style-src 'self' 'unsafe-inline' https://cdnjs.cloudflare.com https://fonts.googleapis.com; script-src 'self' 'unsafe-inline'; font-src 'self' https://cdnjs.cloudflare.com https://fonts.gstatic.com; connect-src 'self';">
    <title>{% block title %}Metis RAG{% endblock %}</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="{{ url_for('static', path='css/fonts.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', path='css/styles.css') }}">
    <meta name="description" content="Metis RAG - Retrieval Augmented Generation with Ollama">
    {% block head %}{% endblock %}
</head>
<body>
    <div class="app-container">
        <!-- Left Sidebar -->
        <div class="sidebar">
            <div class="sidebar-header">
                <h1><i class="fas fa-brain" style="font-size: 1.1em; color: var(--ginkgo-green);"></i> Metis RAG</h1>
                <button id="theme-toggle" class="theme-toggle" title="Toggle light/dark mode">
                    <i class="fas fa-sun"></i>
                </button>
            </div>
            
            <div class="sidebar-content">
                {% block sidebar %}{% endblock %}
            </div>
        </div>
        
        <!-- Right Content Area -->
        <div class="main-content">
            {% block content %}{% endblock %}
        </div>
    </div>
    
    <!-- Token usage indicator -->
    <div class="token-usage" id="token-usage">
        <div class="token-usage-title">
            <i class="fas fa-microchip"></i> Token Usage
        </div>
        <div class="token-usage-bar">
            <div id="token-usage-fill"></div>
        </div>
        <div id="token-usage-text">0 / 4096 tokens</div>
    </div>
    
    <script src="{{ url_for('static', path='js/main.js') }}"></script>
    {% block scripts %}{% endblock %}
</body>
</html>

================
File: app/templates/chat.html
================
{% extends "base.html" %}

{% block title %}Chat - Metis RAG{% endblock %}

{% block head %}
<!-- Document manager styles -->
<link rel="stylesheet" href="{{ url_for('static', path='css/document-manager.css') }}">
{% endblock %}

{% block sidebar %}
<form id="chat-form">
    <!-- Model Selection -->
    <div class="form-group">
        <label for="model">
            Select Model:
            <span class="tooltip">
                <i class="fas fa-info-circle"></i>
                <span class="tooltip-text">Choose which AI model to use. Different models have different capabilities and performance characteristics.</span>
            </span>
        </label>
        <div class="param-description">Choose the AI model to use for generating responses.</div>
        <select id="model" name="model">
            <option value="llama3" selected>Llama 3</option>
            <!-- Other models will be loaded dynamically -->
        </select>
    </div>
    
    <!-- RAG Toggle -->
    <div class="form-group checkbox-container">
        <input type="checkbox" id="rag-toggle" name="use_rag" value="true">
        <label for="rag-toggle">
            Use RAG
            <span class="tooltip">
                <i class="fas fa-info-circle"></i>
                <span class="tooltip-text">When enabled, the model will use your documents to provide more informed responses.</span>
            </span>
            <div class="param-description">Enable Retrieval Augmented Generation with your documents</div>
        </label>
    </div>
    
    <!-- Streaming Toggle -->
    <div class="form-group checkbox-container">
        <input type="checkbox" id="stream-toggle" name="use_stream" value="true" checked>
        <label for="stream-toggle">
            Use Streaming
            <span class="tooltip">
                <i class="fas fa-info-circle"></i>
                <span class="tooltip-text">When enabled, responses will be streamed in real-time. Disable for better error handling.</span>
            </span>
            <div class="param-description">Enable streaming responses for real-time feedback</div>
        </label>
    </div>
    
    <!-- Advanced Parameters -->
    <div class="advanced-options">
        <button type="button" id="advanced-toggle" class="advanced-toggle">
            <i class="fas fa-cog"></i> Advanced Parameters
            <i id="advanced-icon" class="fas fa-chevron-down"></i>
        </button>
        
        <div id="advanced-content" class="advanced-content">
            <div class="parameter-grid">
                <div class="form-group">
                    <label for="temperature">
                        Temperature:
                        <span class="tooltip">
                            <i class="fas fa-info-circle"></i>
                            <span class="tooltip-text">Controls randomness. Lower values make responses more focused and deterministic. Higher values make output more random and creative.</span>
                        </span>
                    </label>
                    <div class="param-description">Controls randomness: lower = more focused, higher = more creative (0-1)</div>
                    <input type="number" id="temperature" name="temperature" step="0.1" min="0.0" max="1.0" value="0.7">
                </div>
                
                <!-- RAG-specific parameters -->
                <div class="form-group rag-param">
                    <label for="max-results">
                        Max Results:
                        <span class="tooltip">
                            <i class="fas fa-info-circle"></i>
                            <span class="tooltip-text">The maximum number of document chunks to retrieve for context.</span>
                        </span>
                    </label>
                    <div class="param-description">Number of document chunks to retrieve (1-10)</div>
                    <input type="number" id="max-results" name="max_results" min="1" max="10" value="4">
                </div>
                
                <!-- Metadata Filtering -->
                <div class="form-group rag-param">
                    <label for="metadata-filters">
                        Metadata Filters:
                        <span class="tooltip">
                            <i class="fas fa-info-circle"></i>
                            <span class="tooltip-text">Filter documents by metadata (e.g., {"filename": "report.pdf"}).</span>
                        </span>
                    </label>
                    <div class="param-description">JSON object for filtering documents by metadata</div>
                    <textarea id="metadata-filters" name="metadata_filters" rows="2" placeholder='{"filename": "example.pdf"}'></textarea>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Action Buttons for Sidebar -->
    <div class="action-buttons" style="margin-top: 20px;">
        <button type="button" id="clear-chat" class="secondary">
            <i class="fas fa-trash"></i> Clear Chat
        </button>
        <button type="button" id="save-chat" class="secondary">
            <i class="fas fa-save"></i> Save
        </button>
    </div>
</form>

<!-- Document Management Section -->
<div id="document-section" class="document-section">
    <div id="toggle-documents" class="document-section-header">
        <div class="document-section-title">
            <i class="fas fa-file-alt"></i> Documents
            <span id="document-count" class="document-count">0</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    
    <!-- Filter Panel -->
    <div id="filter-panel" class="filter-panel">
        <div class="filter-title">
            <span><i class="fas fa-filter"></i> Filter Documents</span>
            <button id="filter-toggle" class="filter-toggle">
                <i class="fas fa-chevron-down"></i>
            </button>
        </div>
        
        <div id="filter-content" class="filter-content">
            <div class="filter-section">
                <div class="filter-section-title">Tags</div>
                <div id="filter-tags" class="filter-tags">
                    <!-- Tags will be loaded dynamically -->
                </div>
            </div>
            
            <div class="filter-section">
                <div class="filter-section-title">Folders</div>
                <div id="filter-folders" class="filter-folders">
                    <!-- Folders will be loaded dynamically -->
                </div>
            </div>
            
            <div class="filter-actions">
                <button id="apply-filters">Apply</button>
                <button id="clear-filters">Clear</button>
            </div>
        </div>
    </div>
    
    <div class="document-upload">
        <form id="doc-upload-form" class="upload-form">
            <div class="upload-input">
                <input type="file" id="document-file" accept=".pdf,.txt,.csv,.md" required>
            </div>
            
            <div class="form-group">
                <input type="text" id="doc-tags" placeholder="Tags (comma separated)">
            </div>
            
            <div class="form-group">
                <select id="doc-folder">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
            </div>
            
            <button type="submit" class="upload-button">
                <i class="fas fa-upload"></i> Upload
            </button>
            <div id="upload-progress" class="progress-bar">
                <div id="upload-progress-fill" class="progress-bar-fill"></div>
            </div>
        </form>
    </div>
    
    <div id="document-list" class="document-list">
        <!-- Documents will be loaded dynamically -->
        <div class="document-loading">Loading documents...</div>
    </div>
    
    <div class="batch-actions">
        <button id="process-selected-btn" class="secondary" disabled>
            <i class="fas fa-sync-alt"></i> Process
        </button>
        <button id="delete-selected-btn" class="danger" disabled>
            <i class="fas fa-trash"></i> Delete
        </button>
    </div>
</div>

<!-- Document Edit Modal -->
<div id="document-edit-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Edit Document</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="form-group">
                <label for="edit-tag-input">Tags</label>
                <div class="tag-input-container">
                    <input type="text" id="edit-tag-input" class="tag-input" placeholder="Add a tag and press Enter">
                    <div id="tag-suggestions" class="tag-suggestions"></div>
                </div>
                <div id="edit-tag-list" class="tag-list">
                    <!-- Tags will be added here -->
                </div>
            </div>
            
            <div class="form-group">
                <label for="edit-folder">Folder</label>
                <select id="edit-folder" class="folder-select">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
                <div class="folder-path">Current path: <span id="current-folder-path">/</span></div>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="save-changes">Save Changes</button>
        </div>
    </div>
</div>
{% endblock %}

{% block content %}
<div class="chat-area">
    <!-- Chat Container -->
    <div class="chat-container" id="chat-container">
        <div class="message bot-message">
            <div class="message-header">Metis:</div>
            Hello! I'm your Metis RAG assistant. Ask me anything about your uploaded documents or chat with me directly.
        </div>
    </div>
    
    <!-- Input Area -->
    <div class="input-area">
        <div class="form-group">
            <label for="user-input">
                Your Message:
                <span class="tooltip">
                    <i class="fas fa-info-circle"></i>
                    <span class="tooltip-text">Enter your question or instruction for the AI model. Be specific for better results.</span>
                </span>
            </label>
            <div class="param-description">Type your message or question for the AI to respond to.</div>
            <textarea id="user-input" rows="4" placeholder="Type your message here..." required></textarea>
            <div class="keyboard-shortcuts">
                Press <strong>Enter</strong> to send, <strong>Shift+Enter</strong> for new line
            </div>
        </div>
        
        <div class="submit-container">
            <button type="button" id="send-button">
                <i class="fas fa-paper-plane"></i> Send Message
            </button>
        </div>
    </div>
</div>

<div id="loading" class="loading">
    <span class="spinner"></span> Generating response...
</div>
{% endblock %}

{% block scripts %}
<script src="{{ url_for('static', path='js/document-manager.js') }}"></script>
<script src="{{ url_for('static', path='js/chat.js') }}"></script>
{% endblock %}

================
File: app/templates/documents.html
================
{% extends "base.html" %}

{% block title %}Documents - Metis RAG{% endblock %}

{% block head %}
<link rel="stylesheet" href="{{ url_for('static', path='css/document-manager.css') }}">
<style>
    .documents-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }

    .document-section {
        margin-bottom: 30px;
    }

    .document-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
        gap: 20px;
        margin-top: 20px;
    }

    .document-card {
        border: 1px solid var(--border-color);
        border-radius: 5px;
        padding: 15px;
        background-color: var(--card-bg);
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    .document-card h3 {
        margin-top: 0;
        margin-bottom: 10px;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .document-meta {
        font-size: 0.9em;
        color: var(--muted-color);
        margin-bottom: 10px;
    }

    .document-actions {
        display: flex;
        justify-content: space-between;
        margin-top: 15px;
    }

    .upload-form {
        margin-bottom: 20px;
        padding: 20px;
        border: 1px solid var(--border-color);
        border-radius: 5px;
        background-color: var(--card-bg);
    }

    .upload-input {
        margin-bottom: 15px;
    }

    .progress-bar {
        height: 10px;
        background-color: var(--border-color);
        border-radius: 5px;
        margin-top: 10px;
    }

    .progress-bar-fill {
        height: 100%;
        background-color: var(--primary-color);
        border-radius: 5px;
        width: 0;
        transition: width 0.3s;
    }

    .batch-actions {
        margin-bottom: 20px;
    }
    
    .filter-section {
        margin-bottom: 20px;
        padding: 15px;
        border: 1px solid var(--border-color);
        border-radius: 5px;
        background-color: var(--card-bg);
    }
    
    .card-tags {
        display: flex;
        flex-wrap: wrap;
        gap: 5px;
        margin-bottom: 10px;
    }
    
    .card-tag {
        background-color: var(--primary-color);
        color: white;
        padding: 3px 8px;
        border-radius: 12px;
        font-size: 0.8rem;
    }
    
    .card-folder {
        font-size: 0.9em;
        color: var(--muted-color);
        margin-bottom: 10px;
    }
    
    .card-folder i {
        margin-right: 5px;
    }
    
    .form-row {
        display: flex;
        gap: 15px;
        margin-bottom: 15px;
    }
    
    .form-group {
        flex: 1;
    }
    
    .form-group label {
        display: block;
        margin-bottom: 5px;
        font-weight: 500;
    }
    
    .form-group input,
    .form-group select {
        width: 100%;
        padding: 8px;
        border: 1px solid var(--border-color);
        border-radius: 4px;
        background-color: var(--input-bg);
        color: var(--text-color);
    }
</style>
{% endblock %}

{% block content %}
<div class="documents-container">
    <div class="document-section">
        <h2>Document Management</h2>
        
        <div class="upload-form">
            <h3>Upload Document</h3>
            <form id="upload-form">
                <div class="upload-input">
                    <input type="file" id="document-file" accept=".pdf,.txt,.csv,.md" required>
                </div>
                
                <div class="form-row">
                    <div class="form-group">
                        <label for="doc-tags">Tags (comma separated)</label>
                        <input type="text" id="doc-tags" placeholder="e.g. important, reference, work">
                    </div>
                    
                    <div class="form-group">
                        <label for="doc-folder">Folder</label>
                        <select id="doc-folder">
                            <option value="/">Root</option>
                            <!-- Folders will be loaded dynamically -->
                        </select>
                    </div>
                </div>
                
                <button type="submit">Upload Document</button>
                <div class="progress-bar" id="upload-progress">
                    <div class="progress-bar-fill" id="upload-progress-fill"></div>
                </div>
            </form>
        </div>
        
        <div class="filter-section">
            <div class="filter-title">
                <h3>Filter Documents</h3>
                <button id="filter-toggle" class="filter-toggle">
                    <i class="fas fa-chevron-down"></i>
                </button>
            </div>
            
            <div id="filter-content" class="filter-content">
                <div class="filter-section">
                    <div class="filter-section-title">Tags</div>
                    <div id="filter-tags" class="filter-tags">
                        <!-- Tags will be loaded dynamically -->
                    </div>
                </div>
                
                <div class="filter-section">
                    <div class="filter-section-title">Folders</div>
                    <div id="filter-folders" class="filter-folders">
                        <!-- Folders will be loaded dynamically -->
                    </div>
                </div>
                
                <div class="filter-actions">
                    <button id="apply-filters">Apply Filters</button>
                    <button id="clear-filters">Clear Filters</button>
                </div>
            </div>
        </div>
        
        <div class="document-section">
            <h3>Document Processing Options</h3>
            <div class="processing-options">
                <div class="form-row">
                    <div class="form-group">
                        <label for="chunking-strategy">Chunking Strategy</label>
                        <select id="chunking-strategy">
                            <option value="recursive">Recursive (Default)</option>
                            <option value="token">Token-based</option>
                            <option value="markdown">Markdown Headers</option>
                        </select>
                        <div class="param-description">Choose how documents are split into chunks</div>
                    </div>
                    
                    <div class="form-group">
                        <label for="chunk-size">Chunk Size</label>
                        <input type="number" id="chunk-size" placeholder="Default: 1000" min="100" max="4000">
                        <div class="param-description">Size of each chunk in characters</div>
                    </div>
                    
                    <div class="form-group">
                        <label for="chunk-overlap">Chunk Overlap</label>
                        <input type="number" id="chunk-overlap" placeholder="Default: 200" min="0" max="1000">
                        <div class="param-description">Overlap between chunks in characters</div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="batch-actions">
            <button id="process-selected-btn" disabled>Process Selected</button>
            <button id="delete-selected-btn" disabled>Delete Selected</button>
        </div>
        
        <div id="document-list" class="document-grid">
            <!-- Documents will be loaded dynamically -->
            <div class="document-loading">Loading documents...</div>
        </div>
    </div>
</div>

<!-- Document Edit Modal -->
<div id="document-edit-modal" class="modal">
    <div class="modal-content">
        <div class="modal-header">
            <div class="modal-title">Edit Document</div>
            <button class="modal-close">&times;</button>
        </div>
        
        <div class="modal-body">
            <div class="form-group">
                <label for="edit-tag-input">Tags</label>
                <div class="tag-input-container">
                    <input type="text" id="edit-tag-input" class="tag-input" placeholder="Add a tag and press Enter">
                    <div id="tag-suggestions" class="tag-suggestions"></div>
                </div>
                <div id="edit-tag-list" class="tag-list">
                    <!-- Tags will be added here -->
                </div>
            </div>
            
            <div class="form-group">
                <label for="edit-folder">Folder</label>
                <select id="edit-folder" class="folder-select">
                    <option value="/">Root</option>
                    <!-- Folders will be loaded dynamically -->
                </select>
                <div class="folder-path">Current path: <span id="current-folder-path">/</span></div>
            </div>
        </div>
        
        <div class="modal-footer">
            <button id="save-changes">Save Changes</button>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script src="{{ url_for('static', path='js/document-manager.js') }}"></script>
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Initialize notification function if not already defined
        if (typeof showNotification !== 'function') {
            window.showNotification = function(message, type = 'info') {
                const notification = document.createElement('div');
                notification.className = `notification ${type}`;
                notification.style.position = 'fixed';
                notification.style.top = '20px';
                notification.style.right = '20px';
                notification.style.backgroundColor = type === 'warning' ? '#ff9800' : 'var(--secondary-color)';
                notification.style.color = 'white';
                notification.style.padding = '10px 15px';
                notification.style.borderRadius = '4px';
                notification.style.boxShadow = '0 2px 10px rgba(0, 0, 0, 0.2)';
                notification.style.zIndex = '1000';
                notification.style.maxWidth = '300px';
                notification.textContent = message;
                
                // Add close button
                const closeBtn = document.createElement('span');
                closeBtn.innerHTML = '&times;';
                closeBtn.style.marginLeft = '10px';
                closeBtn.style.cursor = 'pointer';
                closeBtn.style.fontWeight = 'bold';
                closeBtn.onclick = function() {
                    document.body.removeChild(notification);
                };
                notification.appendChild(closeBtn);
                
                // Add to body
                document.body.appendChild(notification);
                
                // Auto remove after 5 seconds
                setTimeout(() => {
                    if (document.body.contains(notification)) {
                        document.body.removeChild(notification);
                    }
                }, 5000);
            };
        }
    });
</script>
{% endblock %}

================
File: app/templates/system.html
================
{% extends "base.html" %}

{% block title %}System - Metis RAG{% endblock %}

{% block head %}
<style>
    .system-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }

    .stats-section {
        margin-bottom: 30px;
    }

    .stats-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 20px;
        margin-top: 20px;
    }

    .stat-card {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 15px;
        background-color: white;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        text-align: center;
    }

    .stat-card h3 {
        margin-top: 0;
        color: var(--secondary-color);
    }

    .stat-value {
        font-size: 2em;
        font-weight: bold;
        color: var(--primary-color);
        margin: 10px 0;
    }

    .models-section {
        margin-bottom: 30px;
    }

    .model-list {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
        gap: 20px;
        margin-top: 20px;
    }

    .model-card {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 15px;
        background-color: white;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    .model-card h3 {
        margin-top: 0;
        margin-bottom: 10px;
    }

    .model-meta {
        font-size: 0.9em;
        color: #777;
        margin-bottom: 10px;
    }

    .health-section {
        margin-bottom: 30px;
    }

    .health-status {
        display: flex;
        flex-direction: column;
        gap: 15px;
        margin-top: 20px;
    }

    .health-item {
        display: flex;
        align-items: center;
        padding: 15px;
        border-radius: 5px;
        background-color: white;
        border: 1px solid #ddd;
    }

    .health-indicator {
        width: 15px;
        height: 15px;
        border-radius: 50%;
        margin-right: 15px;
    }

    .health-indicator.healthy {
        background-color: var(--success-color);
    }

    .health-indicator.unhealthy {
        background-color: var(--error-color);
    }

    .health-indicator.unknown {
        background-color: var(--warning-color);
    }

    .refresh-btn {
        margin-left: auto;
    }
</style>
{% endblock %}

{% block content %}
<div class="system-container">
    <div class="stats-section">
        <h2>System Statistics</h2>
        <div class="stats-grid" id="stats-grid">
            <div class="stat-card">
                <h3>Documents</h3>
                <div class="stat-value" id="docs-count">-</div>
            </div>
            <div class="stat-card">
                <h3>Chunks</h3>
                <div class="stat-value" id="chunks-count">-</div>
            </div>
            <div class="stat-card">
                <h3>Vector Store Entries</h3>
                <div class="stat-value" id="vectors-count">-</div>
            </div>
            <div class="stat-card">
                <h3>Available Models</h3>
                <div class="stat-value" id="models-count">-</div>
            </div>
        </div>
    </div>

    <div class="health-section">
        <h2>System Health</h2>
        <div class="health-status" id="health-status">
            <div class="health-item">
                <div class="health-indicator unknown"></div>
                <div class="health-name">Overall Health</div>
                <div class="health-details" id="health-overall">Checking...</div>
                <button class="refresh-btn" id="refresh-health-btn">Refresh</button>
            </div>
            <div class="health-item">
                <div class="health-indicator unknown"></div>
                <div class="health-name">Ollama</div>
                <div class="health-details" id="health-ollama">Checking...</div>
            </div>
            <div class="health-item">
                <div class="health-indicator unknown"></div>
                <div class="health-name">Vector DB</div>
                <div class="health-details" id="health-vectordb">Checking...</div>
            </div>
        </div>
    </div>

    <div class="models-section">
        <h2>Available Models</h2>
        <div class="model-list" id="model-list">
            <div class="model-loading">Loading models...</div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const statsGrid = document.getElementById('stats-grid');
        const modelList = document.getElementById('model-list');
        const refreshHealthBtn = document.getElementById('refresh-health-btn');
        
        // Load system stats
        loadSystemStats();
        
        // Load models
        loadModels();
        
        // Check health
        checkHealth();
        
        // Refresh health
        refreshHealthBtn.addEventListener('click', checkHealth);
        
        // Load system stats
        function loadSystemStats() {
            fetch('/api/system/stats')
                .then(response => response.json())
                .then(stats => {
                    document.getElementById('docs-count').textContent = stats.documents_count;
                    document.getElementById('chunks-count').textContent = stats.total_chunks;
                    document.getElementById('vectors-count').textContent = stats.vector_store_size || '0';
                    document.getElementById('models-count').textContent = stats.available_models.length;
                })
                .catch(error => {
                    console.error('Error loading system stats:', error);
                });
        }
        
        // Load models
        function loadModels() {
            fetch('/api/system/models')
                .then(response => response.json())
                .then(models => {
                    modelList.innerHTML = '';
                    
                    if (models.length === 0) {
                        modelList.innerHTML = '<div class="model-empty">No models found</div>';
                        return;
                    }
                    
                    models.forEach(model => {
                        const modelEl = createModelElement(model);
                        modelList.appendChild(modelEl);
                    });
                })
                .catch(error => {
                    console.error('Error loading models:', error);
                    modelList.innerHTML = '<div class="model-error">Error loading models</div>';
                });
        }
        
        // Create model element
        function createModelElement(model) {
            const modelEl = document.createElement('div');
            modelEl.className = 'model-card';
            
            const modified = model.modified_at ? new Date(model.modified_at).toLocaleDateString() : 'Unknown';
            const size = model.size ? formatBytes(model.size) : 'Unknown';
            
            modelEl.innerHTML = `
                <h3>${model.name}</h3>
                <div class="model-meta">
                    <div>Size: ${size}</div>
                    <div>Modified: ${modified}</div>
                </div>
                <div>${model.description || ''}</div>
            `;
            
            return modelEl;
        }
        
        // Check health
        function checkHealth() {
            // Reset indicators
            const indicators = document.querySelectorAll('.health-indicator');
            indicators.forEach(ind => {
                ind.className = 'health-indicator unknown';
            });
            
            document.getElementById('health-overall').textContent = 'Checking...';
            document.getElementById('health-ollama').textContent = 'Checking...';
            document.getElementById('health-vectordb').textContent = 'Checking...';
            
            // Fetch health
            fetch('/api/system/health')
                .then(response => response.json())
                .then(health => {
                    // Update overall health
                    const overallIndicator = document.querySelector('.health-item:nth-child(1) .health-indicator');
                    overallIndicator.className = `health-indicator ${health.status}`;
                    document.getElementById('health-overall').textContent = health.status.charAt(0).toUpperCase() + health.status.slice(1);
                    
                    // Update Ollama health
                    const ollamaIndicator = document.querySelector('.health-item:nth-child(2) .health-indicator');
                    ollamaIndicator.className = `health-indicator ${health.ollama_status}`;
                    document.getElementById('health-ollama').textContent = health.ollama_status.charAt(0).toUpperCase() + health.ollama_status.slice(1);
                    
                    // Update Vector DB health
                    const vectordbIndicator = document.querySelector('.health-item:nth-child(3) .health-indicator');
                    vectordbIndicator.className = `health-indicator ${health.vector_db_status}`;
                    document.getElementById('health-vectordb').textContent = health.vector_db_status.charAt(0).toUpperCase() + health.vector_db_status.slice(1);
                })
                .catch(error => {
                    console.error('Error checking health:', error);
                    document.getElementById('health-overall').textContent = 'Error checking health';
                });
        }
        
        // Format bytes
        function formatBytes(bytes, decimals = 2) {
            if (bytes === 0) return '0 Bytes';
            
            const k = 1024;
            const dm = decimals < 0 ? 0 : decimals;
            const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'];
            
            const i = Math.floor(Math.log(bytes) / Math.log(k));
            
            return parseFloat((bytes / Math.pow(k, i)).toFixed(dm)) + ' ' + sizes[i];
        }
    });
</script>
{% endblock %}

================
File: app/templates/test_models.html
================
{% extends "base.html" %}

{% block title %}Test Models - Metis RAG{% endblock %}

{% block head %}
<style>
    .test-container {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
    }
    .model-list {
        margin-top: 20px;
        border: 1px solid #ccc;
        padding: 10px;
        border-radius: 5px;
    }
    .model-item {
        padding: 5px;
        margin: 5px 0;
        background-color: #f5f5f5;
        border-radius: 3px;
    }
    .test-dropdown {
        width: 100%;
        padding: 8px;
        margin-top: 10px;
        border-radius: 4px;
        border: 1px solid #ccc;
    }
</style>
{% endblock %}

{% block content %}
<div class="test-container">
    <h1>Test Models</h1>
    
    <div>
        <h2>Model Selection Dropdown</h2>
        <p>This dropdown should be populated with all available models:</p>
        <select id="test-model-select" class="test-dropdown">
            <option value="loading">Loading models...</option>
        </select>
    </div>
    
    <div>
        <h2>Model List</h2>
        <p>This list should show all available models:</p>
        <div id="model-list" class="model-list">
            <div>Loading models...</div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script src="{{ url_for('static', path='js/test_models.js') }}"></script>
{% endblock %}

================
File: app/utils/__init__.py
================
from app.utils.file_utils import validate_file, save_upload_file, delete_document_files
from app.utils.text_utils import extract_citations, truncate_text, clean_text

================
File: app/utils/file_utils.py
================
import os
import logging
import shutil
from pathlib import Path
from typing import List, Optional, Set
from fastapi import UploadFile

from app.core.config import UPLOAD_DIR

logger = logging.getLogger("app.utils.file_utils")

# Set of allowed file extensions
ALLOWED_EXTENSIONS = {".pdf", ".txt", ".csv", ".md"}

def validate_file(file: UploadFile) -> bool:
    """
    Validate if a file is allowed
    """
    # Get file extension
    _, ext = os.path.splitext(file.filename.lower())
    
    # Check if extension is allowed
    if ext not in ALLOWED_EXTENSIONS:
        logger.warning(f"File type {ext} is not allowed")
        return False
    
    # TODO: Add more validation if needed (file size, content check, etc.)
    
    return True

async def save_upload_file(file: UploadFile, document_id: str) -> str:
    """
    Save an uploaded file to the upload directory
    """
    try:
        # Create directory for the document
        document_dir = os.path.join(UPLOAD_DIR, document_id)
        os.makedirs(document_dir, exist_ok=True)
        
        # Define file path
        file_path = os.path.join(document_dir, file.filename)
        
        # Save file
        with open(file_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        logger.info(f"File saved to {file_path}")
        return file_path
    except Exception as e:
        logger.error(f"Error saving uploaded file: {str(e)}")
        raise
    finally:
        # Make sure to close the file
        await file.close()

def delete_document_files(document_id: str) -> None:
    """
    Delete document files
    """
    try:
        # Get document directory
        document_dir = os.path.join(UPLOAD_DIR, document_id)
        
        # Check if directory exists
        if os.path.exists(document_dir):
            # Delete directory and all its contents
            shutil.rmtree(document_dir)
            logger.info(f"Deleted document files for {document_id}")
        else:
            logger.warning(f"Document directory for {document_id} does not exist")
    except Exception as e:
        logger.error(f"Error deleting document files: {str(e)}")
        raise

================
File: app/utils/text_utils.py
================
import re
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger("app.utils.text_utils")

def extract_citations(text: str) -> List[int]:
    """
    Extract citation numbers from text
    
    Example:
    "According to [1] and also mentioned in [3], the study shows..."
    Returns: [1, 3]
    """
    try:
        # Find all citations in the format [number]
        citations = re.findall(r'\[(\d+)\]', text)
        
        # Convert to integers and remove duplicates
        citation_numbers = list(set(int(c) for c in citations))
        
        # Sort by number
        citation_numbers.sort()
        
        return citation_numbers
    except Exception as e:
        logger.error(f"Error extracting citations: {str(e)}")
        return []

def truncate_text(text: str, max_length: int = 100) -> str:
    """
    Truncate text to a maximum length
    """
    if len(text) <= max_length:
        return text
    return text[:max_length] + "..."

def clean_text(text: str) -> str:
    """
    Clean text by removing extra whitespace, etc.
    """
    # Remove multiple newlines
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # Remove multiple spaces
    text = re.sub(r' {2,}', ' ', text)
    
    # Trim whitespace
    text = text.strip()
    
    return text

================
File: app/main.py
================
import logging
import os
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

from app.core.config import API_V1_STR, PROJECT_NAME
from app.core.security import setup_security
from app.core.logging import setup_logging
from app.api.chat import router as chat_router
from app.api.documents import router as documents_router
from app.api.system import router as system_router
from app.api.analytics import router as analytics_router

# Setup logging
setup_logging()
logger = logging.getLogger("app.main")

# Create FastAPI app
app = FastAPI(title=PROJECT_NAME)

# Setup security
setup_security(app)

# Mount static files
app.mount("/static", StaticFiles(directory="app/static"), name="static")

# Setup templates
templates = Jinja2Templates(directory="app/templates")

# Include API routers
app.include_router(chat_router, prefix=f"{API_V1_STR}/chat", tags=["chat"])
app.include_router(documents_router, prefix=f"{API_V1_STR}/documents", tags=["documents"])
app.include_router(system_router, prefix=f"{API_V1_STR}/system", tags=["system"])
app.include_router(analytics_router, prefix=f"{API_V1_STR}/analytics", tags=["analytics"])

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """
    Root endpoint that returns the main HTML page
    """
    return templates.TemplateResponse("chat.html", {"request": request})

@app.get("/documents", response_class=HTMLResponse)
async def documents_page(request: Request):
    """
    Documents management page
    """
    return templates.TemplateResponse("documents.html", {"request": request})

@app.get("/system", response_class=HTMLResponse)
async def system_page(request: Request):
    """
    System management page
    """
    return templates.TemplateResponse("system.html", {"request": request})

@app.get("/analytics", response_class=HTMLResponse)
async def analytics_page(request: Request):
    """
    Analytics dashboard page
    """
    return templates.TemplateResponse("analytics.html", {"request": request})

@app.get("/test-models", response_class=HTMLResponse)
async def test_models_page(request: Request):
    """
    Test models page for debugging
    """
    return templates.TemplateResponse("test_models.html", {"request": request})

@app.on_event("startup")
async def startup_event():
    """
    Actions to run on application startup
    """
    logger.info("Starting up Metis RAG application")

@app.on_event("shutdown")
async def shutdown_event():
    """
    Actions to run on application shutdown
    """
    logger.info("Shutting down Metis RAG application")

================
File: scripts/setup.sh
================
#!/bin/bash
# Setup script for Metis RAG

# Create virtual environment
echo "Creating virtual environment..."
python3 -m venv venv
source venv/bin/activate

# Install dependencies
echo "Installing dependencies..."
pip install -r requirements.txt

# Create necessary directories
echo "Creating necessary directories..."
mkdir -p uploads chroma_db

# Create .env file if it doesn't exist
if [ ! -f .env ]; then
    echo "Creating .env file..."
    cp .env.example .env
fi

echo "Setup complete! You can now run the application with:"
echo "uvicorn app.main:app --reload"

================
File: test_docs/quarterly_report.txt
================
Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs

================
File: test_docs/rag_test_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "answer": "Based on the provided context, the architecture of Metis RAG can be summarized as follows:\n\nAccording to [1] and [2], Metis RAG has a modular architecture consisting of three main components: Frontend Layer, API Layer, and RAG Engine.\n\nThe Frontend Layer is built with HTML, CSS, and JavaScript, providing an intuitive interface for document management, chat interactions, system configuration, and analytics and monitoring. This is stated in both [1] and [2].\n\nThe API Layer is implemented using FastAPI and provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval. Again, this information is present in both [1] and [2].\n\nThe RAG Engine consists of three main parts: Document Processing, Vector Store, and LLM Integration.\n\nDocument Processing handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction, as described in [1] and [2].\n\nVector Store is responsible for storing document embeddings, efficient similarity search, and metadata filtering. This information can be found in both [1] and [2].\n\nLLM Integration connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization. This is also stated in both [1] and [2].\n\nNo additional information beyond what's provided in the context is necessary to answer this question.\n\nSources:\n[1] Source: technical_documentation.md, Tags: [], Folder: /test\n[2] Source: technical_documentation.md, Tags: [], Folder: /test",
    "sources": [
      {
        "document_id": "b09f6566-22ad-4d5f-ad6e-1c8a5dda1ef8",
        "chunk_id": "f86ba6ad-8036-44b4-a647-d6044fc42cdc",
        "relevance_score": 0.6145516890095938,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "2fec744c-af0d-40da-b8b4-b22fbf53ca8c",
        "chunk_id": "465de177-2f2c-4f02-bc13-b45e30970fcb",
        "relevance_score": 0.6145516890095938,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "4cb81e59-d0cd-427d-9478-3db915be355c",
        "relevance_score": 0.5401424824966378,
        "excerpt": "Product ID: P003\nName: MetisRAG Basic\nCategory: Software\nPrice: $499\nFeatures: Basic RAG capabilitie..."
      }
    ]
  },
  {
    "query": "What was the revenue reported in Q1 2025?",
    "answer": "I'll do my best to provide an accurate and well-informed response based on the provided context.\n\nAccording to the Quarterly Business Report [1], the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year.\n\nSource: [1]\n\nPlease let me know if you'd like me to provide more information or clarification!",
    "sources": [
      {
        "document_id": "3404bc5a-fe9f-4d36-8d19-6a8a4fd8216a",
        "chunk_id": "1561ae99-4450-4d13-8460-9cf188d7d7ff",
        "relevance_score": 0.6630528548563427,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "173e7a09-66e5-4523-a883-6916e339d6a2",
        "chunk_id": "7d141d75-a409-472d-9529-74d94d89f742",
        "relevance_score": 0.6630528548563427,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "016005ef-97e5-405b-8360-ea5501fb15d6",
        "relevance_score": 0.5167652143787004,
        "excerpt": "Product ID: P001\nName: MetisRAG Enterprise\nCategory: Software\nPrice: $4999\nFeatures: Advanced RAG ca..."
      }
    ]
  },
  {
    "query": "What are the components of the RAG engine?",
    "answer": "Based on the provided context, I can answer the user's question.\n\nThe components of the RAG engine are described in the Architecture Overview section [1] and [2]. According to this information, the core RAG engine consists of three main components:\n\n* Document Processing: This component handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction.\n* Vector Store: This component is responsible for storing document embeddings, efficient similarity search, and metadata filtering.\n* LLM Integration: This component connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization.\n\nThese components work together to enable the Metis RAG system's Retrieval-Augmented Generation capabilities.",
    "sources": [
      {
        "document_id": "b09f6566-22ad-4d5f-ad6e-1c8a5dda1ef8",
        "chunk_id": "f86ba6ad-8036-44b4-a647-d6044fc42cdc",
        "relevance_score": 0.57311767290417,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "2fec744c-af0d-40da-b8b4-b22fbf53ca8c",
        "chunk_id": "465de177-2f2c-4f02-bc13-b45e30970fcb",
        "relevance_score": 0.57311767290417,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "36fe23f2-875b-4b97-a697-b531f3e063a2",
        "relevance_score": 0.5202960339765587,
        "excerpt": "Product ID: P002\nName: MetisRAG Professional\nCategory: Software\nPrice: $1999\nFeatures: Standard RAG ..."
      }
    ]
  },
  {
    "query": "What are the strategic initiatives for Q2?",
    "answer": "Based on the provided context, it appears that there is no specific information regarding strategic initiatives for Q2 (Quarter 2). The context primarily discusses the potential and current state of Quantum Process Synchronization (QPS) research, emphasizing its developmental phase.\n\nWhile we don't have direct information about strategic initiatives for Q2, we can infer from the general tone and themes in the provided context that any strategic plans for this quarter would likely involve continued research, testing, and refinement of QPS-based technologies. The fact that investors and governments worldwide are showing strong interest in quantum research suggests that there may be ongoing efforts to accelerate the development of practical applications.\n\nHowever, without more specific information, it's difficult to provide concrete details about what these strategic initiatives might entail. If you're looking for guidance on potential strategic plans or initiatives related to QPS, I would suggest exploring other sources that provide more context or insights into the research and development landscape.",
    "sources": [
      {
        "document_id": "c8f04dec-4895-4bc0-ade4-c2c1ce465f18",
        "chunk_id": "2ed5746d-8fba-4f8d-9287-553b93aec2a0",
        "relevance_score": 0.5260502976127305,
        "excerpt": "While the potential of QPS is immense, the research team emphasizes that practical applications are ..."
      },
      {
        "document_id": "94db96c8-d51e-4c2a-8bbf-5e78cf655596",
        "chunk_id": "774daf50-a734-49f4-b473-c19fc94801ee",
        "relevance_score": 0.5260502976127305,
        "excerpt": "While the potential of QPS is immense, the research team emphasizes that practical applications are ..."
      },
      {
        "document_id": "d9c026c6-5313-4a7e-8d9e-60ee34a816c9",
        "chunk_id": "b10fa892-9ae1-438f-8870-559d363db1df",
        "relevance_score": 0.5260502976127305,
        "excerpt": "While the potential of QPS is immense, the research team emphasizes that practical applications are ..."
      }
    ]
  },
  {
    "query": "How does the vector store work?",
    "answer": "Based on the provided context, specifically [1] and [2], I can provide information about the Vector Store component of the Metis RAG system.\n\nAccording to the documentation, the Vector Store is responsible for:\n\n* Storing document embeddings\n* Efficient similarity search\n* Metadata filtering\n\nThis suggests that the Vector Store plays a crucial role in enabling efficient retrieval and comparison of documents within the Metis RAG system. The vector store likely uses techniques such as latent semantic analysis (LSA) or word embeddings to create dense representations of documents, allowing for fast and accurate matching between queries and relevant documents.\n\nHowever, I must note that the context does not provide detailed information on how the Vector Store component works in practice, such as specific algorithms used or parameters for tuning its performance. For more comprehensive understanding of this component's functionality, additional documentation or expert knowledge may be necessary.\n\nIn any case, based on the provided context, it appears that the Vector Store is designed to facilitate efficient and effective document retrieval within the Metis RAG system.",
    "sources": [
      {
        "document_id": "b09f6566-22ad-4d5f-ad6e-1c8a5dda1ef8",
        "chunk_id": "f86ba6ad-8036-44b4-a647-d6044fc42cdc",
        "relevance_score": 0.5206977405774185,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "2fec744c-af0d-40da-b8b4-b22fbf53ca8c",
        "chunk_id": "465de177-2f2c-4f02-bc13-b45e30970fcb",
        "relevance_score": 0.5206977405774185,
        "excerpt": "# Metis RAG Technical Documentation\n\n## Introduction\n\nThis document provides technical documentation..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "4cb81e59-d0cd-427d-9478-3db915be355c",
        "relevance_score": 0.49338299939463015,
        "excerpt": "Product ID: P003\nName: MetisRAG Basic\nCategory: Software\nPrice: $499\nFeatures: Basic RAG capabilitie..."
      }
    ]
  },
  {
    "query": "What was the customer retention rate?",
    "answer": "Based on the provided context, the customer retention rate is mentioned in [1] and [2]. According to these sources, the customer retention rate remained strong at 94%.\n\nSince this information is present in the provided context, I will not attempt to provide an alternative answer or make any assumptions. Instead, I will reiterate the directly stated information.\n\nCustomer Retention Rate: 94% [1], [2]",
    "sources": [
      {
        "document_id": "3404bc5a-fe9f-4d36-8d19-6a8a4fd8216a",
        "chunk_id": "1561ae99-4450-4d13-8460-9cf188d7d7ff",
        "relevance_score": 0.5500964726882089,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "173e7a09-66e5-4523-a883-6916e339d6a2",
        "chunk_id": "7d141d75-a409-472d-9529-74d94d89f742",
        "relevance_score": 0.5500964726882089,
        "excerpt": "Quarterly Business Report\nExecutive Summary\nThis quarterly report provides an overview of business p..."
      },
      {
        "document_id": "23adc9d8-5baa-4d9b-a0f4-75180c467c47",
        "chunk_id": "955afe4e-e33d-4cef-abdb-f02504119b3d",
        "relevance_score": 0.4963214209080996,
        "excerpt": "Product ID: P005\nName: MetisRAG Mobile\nCategory: Mobile App\nPrice: $9.99 per month\nFeatures: iOS and..."
      }
    ]
  }
]

================
File: test_docs/technical_documentation.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: test_quality_docs/product_specifications.csv
================
Product ID,Name,Category,Price,Features,Release Date
P001,MetisRAG Enterprise,Software,$4999,Advanced RAG capabilities|Multi-document retrieval|Enterprise security,2025-01-15
P002,MetisRAG Professional,Software,$1999,Standard RAG capabilities|Document management|API access,2025-01-15
P003,MetisRAG Basic,Software,$499,Basic RAG capabilities|Limited document storage|Web interface,2025-01-15
P004,MetisRAG API,Service,$0.10 per query,Pay-per-use|REST API|Developer documentation,2025-02-01
P005,MetisRAG Mobile,Mobile App,$9.99 per month,iOS and Android|Offline capabilities|Voice interface,2025-03-15

================
File: test_quality_docs/quarterly_report.txt
================
Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs

================
File: test_quality_docs/technical_documentation.md
================
# Metis RAG Technical Documentation

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/integration/test_api.py
================
import pytest
from fastapi.testclient import TestClient
import os
import tempfile
from io import BytesIO
import uuid
from unittest.mock import AsyncMock

from app.main import app
from app.models.chat import ChatQuery
from app.models.document import DocumentProcessRequest
# Import necessary components for dependency injection and mocking
from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore

# --- Fixture for TestClient ---
@pytest.fixture
def client():
    """
    Create a TestClient instance for each test.
    This ensures that dependency overrides are correctly applied.
    """
    with TestClient(app) as c:  # Use 'with' statement for proper cleanup
        yield c


# --- Helper function for dependency injection ---
async def get_langgraph_rag_agent():
    """Dependency for injecting the LangGraphRAGAgent"""
    # In a real application, you'd create the agent here with proper dependencies
    # For testing, we'll override this with a mocked version
    return LangGraphRAGAgent()


def test_health_check(client: TestClient):  # Use the fixture
    """
    Test the health check endpoint
    """
    response = client.get("/api/system/health")
    assert response.status_code == 200
    data = response.json()
    assert "status" in data
    # Note: This might fail in CI if Ollama is not available

def test_chat_query_without_context(client: TestClient): # Use the fixture
    """
    Test chat query without RAG context
    """
    # Arrange
    query = ChatQuery(
        message="Hello, world!",
        use_rag=False,
        stream=False
    )

    # Act
    response = client.post("/api/chat/query", json=query.dict())

    # Assert
    assert response.status_code == 200
    data = response.json()
    assert "message" in data
    assert "conversation_id" in data

@pytest.mark.asyncio
async def test_langgraph_rag_chat_endpoint(client: TestClient): # Use the fixture
    """
    Test the LangGraph RAG Agent query endpoint
    """
    # Arrange: Create a mock query
    query = ChatQuery(
        message="What are the key features of the Metis RAG system?",
        use_rag=True,
        stream=False
    )

    # Mock dependencies (OllamaClient and VectorStore)
    mock_ollama_client = AsyncMock(spec=OllamaClient)
    mock_vector_store = AsyncMock(spec=VectorStore)

    # Configure mock responses (adjust as needed for your test case)
    mock_ollama_client.generate.return_value = {"response": "This is a mock LLM response."}
    mock_vector_store.search.return_value = [
        {
            "chunk_id": "chunk1",
            "content": "Metis RAG features include document management...",
            "metadata": {"document_id": "doc1", "filename": "test.txt"},
            "distance": 0.1
        }
    ]

    # Create an instance of the LangGraphRAGAgent with mocked dependencies
    rag_agent = LangGraphRAGAgent(
        ollama_client=mock_ollama_client,
        vector_store=mock_vector_store
    )

    # Override the dependency in the app (VERY IMPORTANT)
    app.dependency_overrides[get_langgraph_rag_agent] = lambda: rag_agent

    try:
        # Act: Send the request to the correct endpoint
        response = client.post("/api/chat/langgraph_query", json=query.dict())  # Using the endpoint defined in app/api/chat.py

        # Assert: Check the response
        assert response.status_code == 200
        data = response.json()
        assert "message" in data
        assert "conversation_id" in data
        assert "citations" in data

        # Assert that the mocked methods were called
        mock_ollama_client.generate.assert_called()
        mock_vector_store.search.assert_called()
    finally:
        # Reset dependency overrides (IMPORTANT for other tests)
        app.dependency_overrides = {}

# Ollama is running, so we can run this test
def test_document_upload_and_process(client: TestClient): # Use the fixture
    """
    Test document upload and processing
    """
    # Arrange
    test_content = b"This is a test document for RAG testing."
    test_file = BytesIO(test_content)
    test_file.name = "test.txt"

    # Act - Upload
    upload_response = client.post(
        "/api/documents/upload",
        files={"file": ("test.txt", test_file, "text/plain")}
    )

    # Assert - Upload
    assert upload_response.status_code == 200
    upload_data = upload_response.json()
    assert upload_data["success"] is True
    assert "document_id" in upload_data
    document_id = upload_data["document_id"]

    # Act - Process
    process_request = DocumentProcessRequest(document_ids=[document_id])
    process_response = client.post(
        "/api/documents/process",
        json=process_request.dict()
    )

    # Assert - Process
    assert process_response.status_code == 200
    process_data = process_response.json()
    assert process_data["success"] is True

    # Act - Get Document (to check chunk count)
    get_response = client.get(f"/api/documents/{document_id}")
    assert get_response.status_code == 200
    document = get_response.json()
    assert "chunks" in document
    assert len(document["chunks"]) > 0  # Check that chunks were created

    # Act - List Documents
    list_response = client.get("/api/documents/list")

    # Assert - List Documents
    assert list_response.status_code == 200
    documents = list_response.json()
    assert len(documents) >= 1
    found = any(doc["id"] == document_id for doc in documents)
    assert found is True

    # Act - Delete
    delete_response = client.delete(f"/api/documents/{document_id}")

    # Assert - Delete
    assert delete_response.status_code == 200
    delete_data = delete_response.json()
    assert delete_data["success"] is True

================
File: tests/integration/test_chunking_judge_integration.py
================
"""
Integration tests for the Chunking Judge with DocumentProcessor
"""
import pytest
import os
from unittest.mock import AsyncMock, patch, MagicMock
import tempfile
import shutil

from app.rag.document_processor import DocumentProcessor
from app.rag.agents.chunking_judge import ChunkingJudge
from app.models.document import Document
from app.core.config import UPLOAD_DIR

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "strategy": "markdown",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is a structured markdown document with clear headers."
        }
        """
    }
    return client

@pytest.fixture
def test_document():
    """Create a test document for processing"""
    return Document(
        id="test-integration-id",
        filename="test_integration.md",
        content="""# Test Document

This is a test document for integration testing.

## Section 1

Content for section 1.

## Section 2

Content for section 2.
"""
    )

@pytest.fixture
def setup_document_directory(test_document):
    """Set up a temporary directory for the test document"""
    # Create document directory
    doc_dir = os.path.join(UPLOAD_DIR, test_document.id)
    os.makedirs(doc_dir, exist_ok=True)
    
    # Write document content to file
    doc_path = os.path.join(doc_dir, test_document.filename)
    with open(doc_path, 'w') as f:
        f.write(test_document.content)
    
    yield
    
    # Clean up
    if os.path.exists(doc_dir):
        shutil.rmtree(doc_dir)

@pytest.mark.asyncio
@patch('app.core.config.USE_CHUNKING_JUDGE', True)
async def test_document_processor_with_chunking_judge(mock_ollama_client, test_document, setup_document_directory):
    """Test that the DocumentProcessor correctly uses the Chunking Judge"""
    # Patch the ChunkingJudge to use our mock
    with patch('app.rag.agents.chunking_judge.ChunkingJudge', return_value=ChunkingJudge(ollama_client=mock_ollama_client)):
        # Create document processor
        processor = DocumentProcessor()
        
        # Process document
        processed_doc = await processor.process_document(test_document)
        
        # Verify chunking analysis was added to metadata
        assert "chunking_analysis" in processed_doc.metadata
        assert processed_doc.metadata["chunking_analysis"]["strategy"] == "markdown"
        assert processed_doc.metadata["chunking_analysis"]["parameters"]["chunk_size"] == 800
        assert processed_doc.metadata["chunking_analysis"]["parameters"]["chunk_overlap"] == 100
        
        # Verify chunks were created
        assert len(processed_doc.chunks) > 0

@pytest.mark.asyncio
@patch('app.core.config.USE_CHUNKING_JUDGE', False)
async def test_document_processor_without_chunking_judge(test_document, setup_document_directory):
    """Test that the DocumentProcessor works correctly when Chunking Judge is disabled"""
    # Create document processor
    processor = DocumentProcessor()
    
    # Process document
    processed_doc = await processor.process_document(test_document)
    
    # Verify chunking analysis was not added to metadata
    assert "chunking_analysis" not in processed_doc.metadata
    
    # Verify chunks were created
    assert len(processed_doc.chunks) > 0

================
File: tests/integration/test_langgraph_rag_integration.py
================
"""
Integration test for the LangGraph RAG Agent
"""
import pytest
import logging
from typing import Dict, Any, List

from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient

# Configure logging
logger = logging.getLogger("tests.integration.test_langgraph_rag_integration")

@pytest.mark.asyncio
async def test_langgraph_rag_agent_initialization():
    """Test that the LangGraph RAG Agent can be initialized"""
    # Initialize components
    vector_store = VectorStore()
    ollama_client = OllamaClient()
    
    # Initialize LangGraph RAG Agent
    langgraph_rag_agent = LangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client
    )
    
    # Verify that the agent was initialized
    assert langgraph_rag_agent is not None
    assert langgraph_rag_agent.graph is not None
    assert langgraph_rag_agent.vector_store is not None
    assert langgraph_rag_agent.ollama_client is not None
    assert langgraph_rag_agent.chunking_judge is not None
    assert langgraph_rag_agent.retrieval_judge is not None
    assert langgraph_rag_agent.semantic_chunker is not None

@pytest.mark.asyncio
async def test_langgraph_rag_agent_query():
    """Test that the LangGraph RAG Agent can process a query"""
    # Initialize components with mock ollama client to avoid actual LLM calls
    vector_store = VectorStore()
    
    # Create a mock OllamaClient that returns predefined responses
    class MockOllamaClient:
        async def generate(self, prompt, model=None, system_prompt=None, stream=False, parameters=None):
            if stream:
                # For streaming, return an async generator
                async def mock_stream():
                    for token in ["This", " is", " a", " mock", " response", "."] if not prompt.startswith("Error") else ["Error"]:
                        yield token
                return mock_stream()
            else:
                # For non-streaming, return a dict with the response
                return {"response": "This is a mock response." if not prompt.startswith("Error") else "Error"}
    
    ollama_client = MockOllamaClient()
    
    # Initialize LangGraph RAG Agent with mock client
    langgraph_rag_agent = LangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client
    )
    
    # Test query
    query = "What are the key features of the Metis RAG system?"
    
    # Query the LangGraph RAG Agent
    response = await langgraph_rag_agent.query(
        query=query,
        stream=False
    )
    
    # Verify the response structure
    assert response is not None
    assert "query" in response
    assert response["query"] == query
    assert "answer" in response or "stream" in response
    
    # If not streaming, verify the answer
    if "answer" in response:
        assert response["answer"] == "This is a mock response."
    
    # Verify sources are included (even if empty)
    assert "sources" in response

@pytest.mark.asyncio
async def test_langgraph_rag_agent_error_handling():
    """Test that the LangGraph RAG Agent handles errors gracefully"""
    # Initialize components with mock ollama client that raises exceptions
    vector_store = VectorStore()
    
    # Create a mock OllamaClient that raises exceptions
    class ErrorOllamaClient:
        async def generate(self, prompt, model=None, system_prompt=None, stream=False, parameters=None):
            if prompt.startswith("Error"):
                raise Exception("Mock error")
            
            if stream:
                # For streaming, return an async generator
                async def mock_stream():
                    for token in ["This", " is", " a", " mock", " response", "."]:
                        yield token
                return mock_stream()
            else:
                # For non-streaming, return a dict with the response
                return {"response": "This is a mock response."}
    
    ollama_client = ErrorOllamaClient()
    
    # Initialize LangGraph RAG Agent with error client
    langgraph_rag_agent = LangGraphRAGAgent(
        vector_store=vector_store,
        ollama_client=ollama_client
    )
    
    # Test query that triggers an error
    query = "Error: This should trigger an exception"
    
    try:
        # Query the LangGraph RAG Agent
        response = await langgraph_rag_agent.query(
            query=query,
            stream=False
        )
        
        # If we get here, the agent handled the error
        assert response is not None
        assert "query" in response
        assert "error" in response or "answer" in response
    except Exception as e:
        # The test fails if an unhandled exception is raised
        assert False, f"LangGraph RAG Agent did not handle the error: {str(e)}"

================
File: tests/integration/test_retrieval_judge_integration.py
================
"""
Integration tests for the Retrieval Judge with RAG Engine
"""
import pytest
import os
import json
from unittest.mock import AsyncMock, MagicMock, patch

from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.models.document import Document, Chunk


@pytest.fixture
def mock_ollama_client():
    """Create a mock OllamaClient"""
    client = AsyncMock(spec=OllamaClient)
    
    # Mock generate method to return a valid response
    async def mock_generate(prompt, model=None, system_prompt=None, stream=False, parameters=None):
        if "analyze the query" in prompt.lower():
            return {
                "response": json.dumps({
                    "complexity": "moderate",
                    "parameters": {
                        "k": 5,
                        "threshold": 0.5,
                        "reranking": True
                    },
                    "justification": "This is a moderately complex query about machine learning."
                })
            }
        elif "evaluate the relevance" in prompt.lower():
            return {
                "response": json.dumps({
                    "relevance_scores": {
                        "1": 0.9,
                        "2": 0.7,
                        "3": 0.3
                    },
                    "needs_refinement": False,
                    "justification": "The first two chunks are highly relevant to the query."
                })
            }
        elif "refine the user's query" in prompt.lower():
            return {
                "response": "What are the key concepts and applications of machine learning?"
            }
        elif "optimize the assembly" in prompt.lower():
            return {
                "response": json.dumps({
                    "optimized_order": [1, 2],
                    "excluded_chunks": [3],
                    "justification": "Ordered chunks for logical flow and excluded less relevant chunk."
                })
            }
        else:
            return {
                "response": "This is a response from the mock LLM."
            }
    
    client.generate = mock_generate
    
    # Mock create_embedding method
    async def mock_create_embedding(text, model=None):
        # Return a simple mock embedding (10 dimensions)
        return [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    
    client.create_embedding = mock_create_embedding
    
    return client


@pytest.fixture
def mock_vector_store(mock_ollama_client):
    """Create a mock VectorStore"""
    store = AsyncMock(spec=VectorStore)
    
    # Mock get_stats method
    store.get_stats.return_value = {"count": 10, "embeddings_model": "test-model"}
    
    # Mock search method
    async def mock_search(query, top_k=5, filter_criteria=None):
        return [
            {
                "chunk_id": "chunk1",
                "content": "Machine learning is a subset of artificial intelligence that focuses on developing systems that learn from data.",
                "metadata": {
                    "document_id": "doc1",
                    "filename": "ml_basics.md",
                    "tags": "machine learning,ai",
                    "folder": "/tech"
                },
                "distance": 0.1
            },
            {
                "chunk_id": "chunk2",
                "content": "Common machine learning algorithms include linear regression, decision trees, and neural networks.",
                "metadata": {
                    "document_id": "doc1",
                    "filename": "ml_basics.md",
                    "tags": "machine learning,algorithms",
                    "folder": "/tech"
                },
                "distance": 0.2
            },
            {
                "chunk_id": "chunk3",
                "content": "Python is a popular programming language for data science and machine learning projects.",
                "metadata": {
                    "document_id": "doc2",
                    "filename": "programming.md",
                    "tags": "python,programming",
                    "folder": "/tech/programming"
                },
                "distance": 0.6
            }
        ]
    
    store.search = mock_search
    
    return store


@pytest.fixture
def retrieval_judge(mock_ollama_client):
    """Create a RetrievalJudge with a mock OllamaClient"""
    return RetrievalJudge(ollama_client=mock_ollama_client, model="test-model")


@pytest.fixture
def rag_engine(mock_vector_store, mock_ollama_client, retrieval_judge):
    """Create a RAGEngine with mock components"""
    with patch('app.rag.rag_engine.USE_RETRIEVAL_JUDGE', True):
        engine = RAGEngine(
            vector_store=mock_vector_store,
            ollama_client=mock_ollama_client,
            retrieval_judge=retrieval_judge
        )
        return engine


class TestRetrievalJudgeIntegration:
    """Integration tests for the RetrievalJudge with RAGEngine"""

    @pytest.mark.asyncio
    async def test_enhanced_retrieval(self, rag_engine):
        """Test the enhanced retrieval method"""
        # Call the enhanced retrieval method
        context, sources, document_ids = await rag_engine._enhanced_retrieval(
            query="What is machine learning?",
            top_k=5
        )
        
        # Verify the results
        assert context  # Context should not be empty
        assert "Machine learning is a subset of artificial intelligence" in context
        assert len(sources) > 0
        assert len(document_ids) > 0
        
        # Verify sources contain the expected information
        assert sources[0]["document_id"] == "doc1"
        assert sources[0]["chunk_id"] == "chunk1"
        assert sources[0]["filename"] == "ml_basics.md"
        
        # Verify document IDs were collected
        assert "doc1" in document_ids

    @pytest.mark.asyncio
    async def test_query_with_retrieval_judge(self, rag_engine):
        """Test the query method with Retrieval Judge enabled"""
        # Call the query method
        result = await rag_engine.query(
            query="What is machine learning?",
            model="test-model",
            use_rag=True,
            stream=False
        )
        
        # Verify the result
        assert "query" in result
        assert "answer" in result
        assert "sources" in result
        assert len(result["sources"]) > 0
        
        # Verify the query was processed
        assert result["query"] == "What is machine learning?"
        
        # Verify the answer is not empty
        assert result["answer"]
        
        # Verify sources are included
        assert result["sources"][0].document_id == "doc1"
        assert result["sources"][0].chunk_id == "chunk1"

    @pytest.mark.asyncio
    async def test_query_with_retrieval_judge_disabled(self, rag_engine):
        """Test the query method with Retrieval Judge disabled"""
        # Temporarily disable the Retrieval Judge
        rag_engine.retrieval_judge = None
        
        # Call the query method
        result = await rag_engine.query(
            query="What is machine learning?",
            model="test-model",
            use_rag=True,
            stream=False
        )
        
        # Verify the result
        assert "query" in result
        assert "answer" in result
        assert "sources" in result
        assert len(result["sources"]) > 0
        
        # Verify the query was processed
        assert result["query"] == "What is machine learning?"
        
        # Verify the answer is not empty
        assert result["answer"]
        
        # Verify sources are included
        assert result["sources"][0].document_id == "doc1"

    @pytest.mark.asyncio
    async def test_query_with_rag_disabled(self, rag_engine):
        """Test the query method with RAG disabled"""
        # Call the query method with use_rag=False
        result = await rag_engine.query(
            query="What is machine learning?",
            model="test-model",
            use_rag=False,
            stream=False
        )
        
        # Verify the result
        assert "query" in result
        assert "answer" in result
        assert "sources" in result
        
        # Verify no sources are included when RAG is disabled
        assert len(result["sources"]) == 0
        
        # Verify the query was processed
        assert result["query"] == "What is machine learning?"
        
        # Verify the answer is not empty
        assert result["answer"]

================
File: tests/integration/test_semantic_chunker_integration.py
================
"""
Integration tests for the Semantic Chunker
"""
import pytest
import os
from unittest.mock import AsyncMock, patch, MagicMock
import tempfile
import shutil

from app.rag.document_processor import DocumentProcessor
from app.rag.chunkers.semantic_chunker import SemanticChunker
from app.models.document import Document
from app.core.config import UPLOAD_DIR

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    # Mock for chunking judge
    client.generate.return_value = {
        "response": """
        {
            "strategy": "semantic",
            "parameters": {
                "chunk_size": 1000,
                "chunk_overlap": 150
            },
            "justification": "This document contains complex concepts that would benefit from semantic chunking to preserve meaning and context."
        }
        """
    }
    return client

@pytest.fixture
def mock_semantic_chunker():
    """Create a mock SemanticChunker for testing"""
    chunker = MagicMock(spec=SemanticChunker)
    
    # Mock the split_documents method
    def mock_split_documents(docs):
        # Create 3 chunks from each document
        result = []
        for doc in docs:
            content = doc.page_content
            chunk_size = len(content) // 3
            for i in range(3):
                start = i * chunk_size
                end = start + chunk_size if i < 2 else len(content)
                chunk_content = content[start:end]
                result.append(doc.__class__(
                    page_content=chunk_content,
                    metadata=doc.metadata.copy()
                ))
        return result
    
    chunker.split_documents.side_effect = mock_split_documents
    return chunker

@pytest.fixture
def sample_document():
    """Create a sample document for testing"""
    return Document(
        id="test-doc-id",
        filename="test_semantic.txt",
        content="""
        This is a complex document with multiple semantic sections.
        
        Section 1: Introduction
        This section introduces the main concepts and provides context.
        
        Section 2: Main Content
        This section contains the core information and detailed explanations.
        
        Section 3: Conclusion
        This section summarizes the key points and provides next steps.
        """
    )

@pytest.fixture
def temp_upload_dir():
    """Create a temporary upload directory for testing"""
    # Create a temporary directory
    temp_dir = tempfile.mkdtemp()
    
    # Create the document directory structure
    doc_dir = os.path.join(temp_dir, "test-doc-id")
    os.makedirs(doc_dir, exist_ok=True)
    
    # Yield the temp directory
    yield temp_dir
    
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.mark.asyncio
async def test_document_processor_with_semantic_chunking(mock_ollama_client, sample_document, temp_upload_dir):
    """Test that the DocumentProcessor correctly uses semantic chunking when recommended by the Chunking Judge"""
    # Create the document file
    doc_path = os.path.join(temp_upload_dir, sample_document.id, sample_document.filename)
    os.makedirs(os.path.dirname(doc_path), exist_ok=True)
    with open(doc_path, 'w') as f:
        f.write(sample_document.content)
    
    # Patch the UPLOAD_DIR config
    with patch('app.rag.document_processor.UPLOAD_DIR', temp_upload_dir):
        # Patch the USE_CHUNKING_JUDGE config to True
        with patch('app.rag.document_processor.USE_CHUNKING_JUDGE', True):
            # Patch the OllamaClient to use our mock
            with patch('app.rag.agents.chunking_judge.OllamaClient', return_value=mock_ollama_client):
                    # Create document processor
                    processor = DocumentProcessor()
                    
                    # Process the document
                    processed_doc = await processor.process_document(sample_document)
                    
                    # Verify the document was processed with semantic chunking
                    assert processed_doc.metadata["chunking_analysis"]["strategy"] == "semantic"
                    assert len(processed_doc.chunks) > 0
                    
                    # Verify the chunking parameters
                    assert processor.chunk_size == 1000
                    assert processor.chunk_overlap == 150

@pytest.mark.asyncio
async def test_semantic_chunker_with_document_processor(mock_ollama_client, mock_semantic_chunker, sample_document, temp_upload_dir):
    """Test that the SemanticChunker integrates correctly with DocumentProcessor"""
    # Create the document file
    doc_path = os.path.join(temp_upload_dir, sample_document.id, sample_document.filename)
    os.makedirs(os.path.dirname(doc_path), exist_ok=True)
    with open(doc_path, 'w') as f:
        f.write(sample_document.content)
    
    # Patch the UPLOAD_DIR config
    with patch('app.rag.document_processor.UPLOAD_DIR', temp_upload_dir):
        # Patch the USE_CHUNKING_JUDGE config to False to use direct strategy
        with patch('app.rag.document_processor.USE_CHUNKING_JUDGE', False):
                # Create document processor with semantic chunking strategy
                processor = DocumentProcessor(chunking_strategy="semantic")
                
                # Process the document
                processed_doc = await processor.process_document(sample_document)
                
                # Verify the document was processed
                assert len(processed_doc.chunks) > 0
                
                # Verify the chunking strategy
                assert processor.chunking_strategy == "semantic"

================
File: tests/retrieval_judge/data/performance_test_document.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/data/product_specifications.md
================
# Product Specifications

## System Requirements

The Metis RAG system requires the following minimum specifications:
- CPU: 4 cores, 2.5GHz or higher
- RAM: 16GB minimum, 32GB recommended
- Storage: 100GB SSD
- Operating System: Ubuntu 22.04 LTS, Windows Server 2019, or macOS 12+
- Network: 100Mbps internet connection

## API Reference

### Authentication

All API requests require authentication using JWT tokens. To obtain a token:

```
POST /api/auth/token
{
  "username": "your_username",
  "password": "your_password"
}
```

The response will include an access token valid for 24 hours.

### Document Management

#### Upload Document

```
POST /api/documents/upload
Content-Type: multipart/form-data
Authorization: Bearer <token>

Form fields:
- file: The document file
- tags: Comma-separated tags (optional)
- folder: Target folder path (optional)
```

#### List Documents

```
GET /api/documents/list
Authorization: Bearer <token>
```

Optional query parameters:
- folder: Filter by folder
- tags: Filter by tags (comma-separated)
- page: Page number (default: 1)
- limit: Items per page (default: 20)

### Chat API

#### Create Chat Session

```
POST /api/chat/sessions
Authorization: Bearer <token>
{
  "title": "Optional chat title"
}
```

#### Send Message

```
POST /api/chat/messages
Authorization: Bearer <token>
{
  "session_id": "chat_session_id",
  "content": "Your message here",
  "use_rag": true
}
```

## Performance Benchmarks

The system has been benchmarked with the following results:
- Document processing: 5 pages/second
- Vector search latency: <50ms for 10k documents
- End-to-end query response time: <2 seconds
- Maximum documents: 100,000
- Maximum vector store size: 10GB

================
File: tests/retrieval_judge/data/quarterly_report.txt
================
Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs

================
File: tests/retrieval_judge/data/technical_documentation.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/data/test_document.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/data/timing_test_document.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

================
File: tests/retrieval_judge/results/chunk_evaluation_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.8,
        "chunk2": 0.5,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.9
      },
      "needs_refinement": true,
      "justification": "The query 'How does the system work?' is quite broad. While chunk 1 (technical_documentation.md) discusses the core components of the system \u2013 vector store, document processing pipeline, and API layer \u2013 it doesn't provide a high-level explanation of the system's overall functionality. Chunk 2 is tangentially related, focusing on document processing. Chunk 3 is irrelevant. Chunk 4 is irrelevant. Chunk 5, however, directly addresses the question by describing the Retrieval Judge's role in enhancing the RAG retrieval process, which is a key aspect of how the system works.  Because the initial chunks are too detailed and don't answer the question at a high level, and because chunk 5 provides a more relevant explanation, the query needs refinement to focus on the overall architecture and operation of the RAG system."
    },
    "time": 9.267931938171387
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.95,
        "chunk2": 0.3,
        "chunk3": 0.98,
        "chunk4": 0.1,
        "chunk5": 0.2
      },
      "needs_refinement": true,
      "justification": "The query asks 'Tell me about the API'. Chunk 1 and 3 directly address this, describing the API layer's implementation and endpoints. Chunk 5 discusses the Retrieval Judge, which is related to improving retrieval but doesn't directly answer the question. Chunk 2 is about the document processing pipeline, which is a component *related* to the API but not the API itself. Chunk 4 discusses LLM integration, which is also tangential.  Because chunks 1 and 3 are highly relevant and chunks 2, 4, and 5 are not directly addressing the user's request, the query likely needs refinement to focus on the API layer specifically.  A more targeted query, such as 'Describe the API endpoints' would likely yield better results."
    },
    "time": 8.807852029800415
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.8,
        "chunk2": 0.5,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.9
      },
      "needs_refinement": false,
      "justification": "Chunk 1 directly addresses the query by explaining the core functionality of a vector store \u2013 embeddings, similarity search, and metadata filtering. This is the most relevant chunk. Chunk 2 discusses the document processing pipeline, which is related but doesn't directly answer the question about how a vector store works. Chunk 3 is entirely irrelevant. Chunk 4 is also irrelevant. Chunk 5, while discussing a Retrieval Judge, provides a mechanism *for* improving retrieval, which is relevant to understanding how a vector store is used in a RAG system, and therefore deserves a high relevance score.  Because the most relevant chunk provides a solid explanation, and the other chunks are either irrelevant or provide supporting details, no query refinement is immediately needed."
    },
    "time": 8.454214334487915
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.8,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.4
      },
      "needs_refinement": true,
      "justification": "Chunks 1 and 2 directly address the query about the document processing pipeline, detailing its stages (validation, parsing, chunking, metadata extraction). Chunk 3 is entirely irrelevant. Chunk 4 is also irrelevant. Chunk 5 discusses the Retrieval Judge, which is a component *related* to the pipeline but doesn't describe the pipeline itself.  The retrieved chunks are somewhat broad and don't provide a concise overview of the document processing pipeline.  Therefore, the query needs refinement to focus on obtaining a specific definition or explanation of the pipeline stages."
    },
    "time": 7.412783145904541
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.95,
        "chunk2": 0.3,
        "chunk3": 0.1,
        "chunk4": 0.9,
        "chunk5": 0.95
      },
      "needs_refinement": false,
      "justification": "Chunks 1, 4, and 5 directly address the query about context window optimization. Chunk 1 describes the LLM integration component's role in managing context window optimization. Chunk 4 specifically mentions the LLM integration component and its support for context window optimization. Chunk 5 details the Retrieval Judge's function of optimizing context assembly, which is a key aspect of context window management. Chunk 2 is only tangentially related as it discusses the document processing pipeline, which is a broader component of the system. Therefore, no query refinement is needed based on these retrieved chunks."
    },
    "time": 7.632442951202393
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.8,
        "chunk2": 0.5,
        "chunk3": 0.1,
        "chunk4": 0.9,
        "chunk5": 0.7
      },
      "needs_refinement": false,
      "justification": "Chunk 1 and 4 directly address the query by mentioning supported embedding models (Llama 3, Gemma, Mistral). Chunk 4 is the most relevant as it explicitly lists the models. Chunk 1 provides context about the vector store and HNSW index, which is related but not a direct answer. Chunk 2 discusses document processing, which is a prerequisite but doesn't answer the question. Chunk 3 is irrelevant. Chunk 5 discusses the Retrieval Judge, which is related to improving retrieval but doesn't list the supported embedding models.  The retrieved chunks provide sufficient information to answer the query, so refinement is not immediately needed."
    },
    "time": 7.953837871551514
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.6,
        "chunk3": 0.1,
        "chunk4": 0.0,
        "chunk5": 0.8
      },
      "needs_refinement": true,
      "justification": "Chunks 1 and 5 directly address the query's focus on 'chunking strategies' and their impact on retrieval performance. Chunk 1 specifically discusses configurable strategies within the document processing pipeline. Chunk 5 describes the Retrieval Judge, which is a system designed to evaluate and refine retrieval based on chunk relevance \u2013 indicating a need to understand the strategies themselves. Chunk 2 is tangentially related as it discusses the document processing pipeline, but doesn't directly address the strategies. Chunk 3 and 4 are completely irrelevant to the query.  Because chunks 1 and 5 provide some information, but the overall discussion is still quite broad and doesn't fully explain the different chunking strategies, a refinement of the query is needed to narrow the focus and obtain more specific answers."
    },
    "time": 8.762250185012817
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.4,
        "chunk3": 0.9,
        "chunk4": 0.1,
        "chunk5": 0.6
      },
      "needs_refinement": true,
      "justification": "Several chunks directly address aspects of the query. Chunk 1 and 3 both discuss authentication and document management endpoints (specifically the API layer). Chunk 5 discusses the Retrieval Judge which is relevant to improving the retrieval process, but doesn't directly answer the user's question. Chunk 2 is only tangentially related as it discusses the document processing pipeline, which is a component *involved* in document management but doesn't detail the authentication or endpoints.  The retrieved chunks are somewhat broad and don't provide a consolidated answer.  To improve the response, the query could be refined to focus on specific authentication methods and the available API endpoints for document management."
    },
    "time": 8.091121912002563
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.4,
        "chunk3": 0.1,
        "chunk4": 0.2,
        "chunk5": 0.95
      },
      "needs_refinement": true,
      "justification": "The query 'RAG?' refers to Retrieval Augmented Generation. Chunk 1 ('vector store', 'HNSW index') and Chunk 5 ('Retrieval Judge') directly address the core concept of RAG, specifically the retrieval component. Chunk 5 is the most relevant as it explicitly defines the Retrieval Judge, which is a key element in RAG systems. Chunk 2 is tangentially related as it describes the document processing pipeline, but doesn't directly answer the question. Chunk 3 is irrelevant. Chunk 4 is also irrelevant.  Because several chunks are low relevance and the query is broad, it suggests the user may need a more specific question, such as 'How does a Retrieval Judge work in RAG?' or 'What are the components of a RAG system?'"
    },
    "time": 8.8508939743042
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.3,
        "chunk3": 0.1,
        "chunk4": 0.1,
        "chunk5": 0.8
      },
      "needs_refinement": true,
      "justification": "The query 'Embeddings?' directly relates to the concept of vector embeddings, which are explicitly mentioned in chunk 1 and chunk 5. Chunk 1 provides a detailed explanation of the vector store's role in storing and utilizing embeddings. Chunk 5 discusses the Retrieval Judge's role in evaluating relevance, suggesting a need for more specific information about embeddings themselves. Chunks 2, 3, and 4 are tangentially related to document processing and API management, respectively, and do not directly address the user's question about embeddings.  Because chunks 2, 3, and 4 are low relevance and chunk 5 discusses evaluation of relevance, it suggests the query might need refinement to focus on the specific technical aspects of embeddings."
    },
    "time": 8.577693939208984
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "evaluation": {
      "relevance_scores": {
        "chunk1": 0.9,
        "chunk2": 0.8,
        "chunk3": 0.1,
        "chunk4": 0.2,
        "chunk5": 0.95
      },
      "needs_refinement": false,
      "justification": "The retrieved chunks largely address the user's detailed query about the Metis RAG system. Chunk 1 and 2 directly address the document processing pipeline, chunking strategies, and vector store management \u2013 key aspects of the user's question. Chunk 5 provides a detailed explanation of the Retrieval Judge, which is also a crucial component of the system. Chunk 3 is tangentially related as it discusses the API layer, and Chunk 4 discusses LLM integration, which is not directly relevant to the core aspects of the user's question.  The relevance scores reflect this.  Because the chunks cover all the requested aspects in detail, no query refinement is needed."
    },
    "time": 8.356603860855103
  }
]

================
File: tests/retrieval_judge/results/context_optimization_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 13.782373189926147
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 9.767060041427612
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 12.937751054763794
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 12.568903923034668
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 14.531967878341675
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 8.959358930587769
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 11.965378761291504
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 12.734175205230713
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 11.795776128768921
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "original_chunk_count": 5,
    "optimized_chunk_count": 1,
    "optimized_chunk_ids": [
      "chunk1"
    ],
    "time": 7.629971981048584
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "original_chunk_count": 5,
    "optimized_chunk_count": 3,
    "optimized_chunk_ids": [
      "chunk1",
      "chunk3",
      "chunk2"
    ],
    "time": 14.76965880393982
  }
]

================
File: tests/retrieval_judge/results/judge_edge_case_analysis.json
================
{
  "by_type": {
    "long": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 0,
          "complex": 1
        },
        "avg_time": 11.576659202575684
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 0.0,
        "avg_time": 8.356603860855103
      },
      "query_refinement": {
        "avg_length_change_percent": 61.26582278481012,
        "avg_time": 4.841519117355347
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 40.0,
        "avg_time": 14.76965880393982
      }
    },
    "ambiguous": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 7.90027117729187
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 1.0,
        "avg_time": 9.0378919839859
      },
      "query_refinement": {
        "avg_length_change_percent": 650.5714285714286,
        "avg_time": 1.9932677745819092
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 11.77471661567688
      }
    },
    "domain-specific": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 8.493697881698608
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 0.0,
        "avg_time": 7.793140411376953
      },
      "query_refinement": {
        "avg_length_change_percent": 138.01948051948054,
        "avg_time": 1.816763997077942
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 11.745663404464722
      }
    },
    "short": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 1,
          "moderate": 1,
          "complex": 0
        },
        "avg_time": 8.945188164710999
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 1.0,
        "avg_time": 8.714293956756592
      },
      "query_refinement": {
        "avg_length_change_percent": 887.5,
        "avg_time": 1.51704740524292
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 9.712874054908752
      }
    },
    "multi-part": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 8.343578577041626
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 1.0,
        "avg_time": 8.42668604850769
      },
      "query_refinement": {
        "avg_length_change_percent": 131.19658119658118,
        "avg_time": 2.0405884981155396
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 12.349776983261108
      }
    },
    "typo": {
      "query_analysis": {
        "complexity_distribution": {
          "simple": 0,
          "moderate": 2,
          "complex": 0
        },
        "avg_time": 7.732823133468628
      },
      "chunk_evaluation": {
        "needs_refinement_rate": 0.5,
        "avg_time": 7.933498740196228
      },
      "query_refinement": {
        "avg_length_change_percent": 70.64516129032256,
        "avg_time": 1.8848679065704346
      },
      "context_optimization": {
        "avg_chunk_reduction_percent": 80.0,
        "avg_time": 12.753327488899231
      }
    }
  },
  "overall": {
    "query_analysis": {
      "complexity_distribution": {
        "moderate": 9,
        "simple": 1,
        "complex": 1
      },
      "avg_time": 8.58252518827265
    },
    "chunk_evaluation": {
      "needs_refinement_rate": 0.6363636363636364,
      "avg_time": 8.378875103863804
    },
    "query_refinement": {
      "avg_length_change_percent": 347.0119205400397,
      "avg_time": 2.122417298230258
    },
    "context_optimization": {
      "avg_chunk_reduction_percent": 76.36363636363637,
      "avg_time": 11.949306899851019
    }
  },
  "type_performance_ranking": {
    "by_refinement_rate": [
      {
        "type": "long",
        "rate": 0.0
      },
      {
        "type": "domain-specific",
        "rate": 0.0
      },
      {
        "type": "typo",
        "rate": 0.5
      },
      {
        "type": "ambiguous",
        "rate": 1.0
      },
      {
        "type": "short",
        "rate": 1.0
      },
      {
        "type": "multi-part",
        "rate": 1.0
      }
    ],
    "by_chunk_reduction": [
      {
        "type": "ambiguous",
        "reduction": 80.0
      },
      {
        "type": "domain-specific",
        "reduction": 80.0
      },
      {
        "type": "short",
        "reduction": 80.0
      },
      {
        "type": "multi-part",
        "reduction": 80.0
      },
      {
        "type": "typo",
        "reduction": 80.0
      },
      {
        "type": "long",
        "reduction": 40.0
      }
    ]
  }
}

================
File: tests/retrieval_judge/results/performance_analysis_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "standard": {
      "total_time": 176.16782879829407,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.814018726348877,
        "ollama_generate": 154.82823419570923,
        "rag_engine_enhanced_retrieval": 155.65416193008423
      }
    },
    "judge": {
      "total_time": 17.874395847320557,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 38.319051027297974,
        "judge_analyze_query": 11.228012084960938,
        "judge_evaluate_chunks": 6.622217893600464,
        "rag_engine_enhanced_retrieval": 17.857052087783813
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 1,
      "vector_store_cache_misses": 2,
      "ollama_cache_hits": 1,
      "ollama_cache_misses": 5
    }
  },
  {
    "query": "What are the components of the RAG engine?",
    "standard": {
      "total_time": 160.53554272651672,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.8336210250854492,
        "ollama_generate": 147.0875780582428,
        "rag_engine_enhanced_retrieval": 147.9596929550171
      }
    },
    "judge": {
      "total_time": 17.655393838882446,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 30.1685574054718,
        "judge_analyze_query": 11.909356117248535,
        "judge_evaluate_chunks": 5.723083019256592,
        "rag_engine_enhanced_retrieval": 17.63789200782776
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 2,
      "vector_store_cache_misses": 4,
      "ollama_cache_hits": 2,
      "ollama_cache_misses": 10
    }
  },
  {
    "query": "How does the vector store work?",
    "standard": {
      "total_time": 175.53075098991394,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.8636059761047363,
        "ollama_generate": 160.59179711341858,
        "rag_engine_enhanced_retrieval": 161.46342086791992
      }
    },
    "judge": {
      "total_time": 18.127063035964966,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 32.13697576522827,
        "judge_analyze_query": 11.290737867355347,
        "judge_evaluate_chunks": 6.814745187759399,
        "rag_engine_enhanced_retrieval": 18.110551118850708
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 3,
      "vector_store_cache_misses": 6,
      "ollama_cache_hits": 3,
      "ollama_cache_misses": 15
    }
  },
  {
    "query": "What is the role of the LLM integration component?",
    "standard": {
      "total_time": 172.4841389656067,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.8085780143737793,
        "ollama_generate": 158.92998576164246,
        "rag_engine_enhanced_retrieval": 159.7460000514984
      }
    },
    "judge": {
      "total_time": 17.969091653823853,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 30.641315460205078,
        "judge_analyze_query": 12.501478910446167,
        "judge_evaluate_chunks": 5.444411993026733,
        "rag_engine_enhanced_retrieval": 17.951611757278442
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 4,
      "vector_store_cache_misses": 8,
      "ollama_cache_hits": 4,
      "ollama_cache_misses": 20
    }
  },
  {
    "query": "How does the document processing pipeline work?",
    "standard": {
      "total_time": 172.63411116600037,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0.873924970626831,
        "ollama_generate": 156.8555817604065,
        "rag_engine_enhanced_retrieval": 157.73775696754456
      }
    },
    "judge": {
      "total_time": 20.422394037246704,
      "sources_count": 1,
      "component_timings": {
        "vector_store_search": 0,
        "ollama_generate": 35.2523295879364,
        "judge_analyze_query": 14.513217687606812,
        "judge_evaluate_chunks": 5.88365912437439,
        "rag_engine_enhanced_retrieval": 20.402302980422974
      }
    },
    "cache_stats": {
      "vector_store_cache_hits": 5,
      "vector_store_cache_misses": 10,
      "ollama_cache_hits": 5,
      "ollama_cache_misses": 25
    }
  }
]

================
File: tests/retrieval_judge/results/performance_analysis.json
================
{
  "avg_standard_time": 171.47047452926637,
  "avg_judge_time": 18.409667682647704,
  "time_difference_percent": -89.26365152182187,
  "standard_component_averages": {
    "vector_store_search": 0.8387497425079345,
    "ollama_generate": 155.6586353778839,
    "rag_engine_enhanced_retrieval": 156.51220655441284
  },
  "judge_component_averages": {
    "vector_store_search": 0.0,
    "ollama_generate": 33.303645849227905,
    "judge_analyze_query": 12.28856053352356,
    "judge_evaluate_chunks": 6.097623443603515,
    "rag_engine_enhanced_retrieval": 18.39188199043274
  },
  "cache_statistics": {
    "vector_store_cache_hits": 15,
    "vector_store_cache_misses": 30,
    "vector_store_cache_hit_rate": 33.33333333333333,
    "ollama_cache_hits": 15,
    "ollama_cache_misses": 75,
    "ollama_cache_hit_rate": 16.666666666666664
  },
  "avg_standard_sources": 1.0,
  "avg_judge_sources": 1.0
}

================
File: tests/retrieval_judge/results/query_analysis_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the system work?\" is a moderate complexity question. It\u2019s not a simple factual question like \"What is the capital of France?\" but it doesn't require deep analytical reasoning. It necessitates understanding the overall architecture and functionality of the system, which likely involves multiple documents.  Therefore, retrieving a moderate number of chunks (k=8) is appropriate to capture a sufficient range of information. A relevance threshold of 0.6 suggests prioritizing documents that are strongly related to the system's operation, avoiding overly tangential results. Reranking is crucial here as initial retrieval might return documents discussing specific components without explicitly addressing the 'how it works' question. Reranking will help prioritize documents that provide a high-level explanation of the system's workflow."
    },
    "time": 7.263206243515015
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"Tell me about the API\" is moderately complex. It\u2019s not a simple factual question like \"What is the capital of France?\" but it\u2019s also not a complex analytical question. It clearly requires retrieval of information related to an API \u2013 likely documentation, tutorials, or explanations of its features.  The query is ambiguous as \u2018the API\u2019 could refer to many different APIs.  Therefore, a moderate retrieval approach is needed.  I recommend retrieving 8 chunks (k=8) to provide a good balance between breadth and precision. A relevance threshold of 0.6 is suggested to filter out less relevant results, prioritizing documents that discuss the API's functionality and purpose. Reranking is crucial here to further refine the results based on semantic similarity to the query, as different documents might use slightly different phrasing to describe the same API.  Without reranking, the initial retrieval might return documents that mention the API but don't directly address the user's request for information *about* it."
    },
    "time": 8.537336111068726
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the vektor store work?\" represents a moderate complexity question. It\u2019s not a simple factual recall but requires understanding the underlying mechanisms of a vektor store.  It necessitates accessing documents that explain the concepts of vector embeddings, similarity search, and potentially the architecture of vektor stores.  Because of this, a moderate number of chunks (k=8) is recommended to capture a range of explanations. A relevance threshold of 0.6 is suggested \u2013 this allows for some relevant but potentially tangential information to be retrieved, as the concept of a vektor store can be explained from various angles.  Reranking (true) is crucial here. Without reranking, the initial retrieval might return documents focused on specific implementations or use cases, rather than the core principles. Reranking will help prioritize documents that directly address the question of how a vektor store functions."
    },
    "time": 7.581515073776245
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"What is the document processing pipeline?\" is moderately complex. It\u2019s not a simple factual question like \"What is the capital of France?\" but it doesn't require deep analytical reasoning. It clearly indicates a need for information about a specific process \u2013 a document processing pipeline.  Therefore, we need to retrieve a sufficient number of relevant chunks (k=8) to cover different aspects of the pipeline. A relevance threshold of 0.6 suggests prioritizing chunks with a strong connection to the query, but allowing for some related information. Reranking is recommended to further refine the results by considering semantic similarity beyond just keyword matching.  Without more context about the domain (e.g., NLP, data science), assuming a general document processing pipeline is reasonable.  A lower k value (5-7) might miss crucial details, while a higher value (10-15) could introduce noise."
    },
    "time": 7.884131193161011
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the system handle context window optimization?\" is moderately complex. It\u2019s not a simple factual question, as it asks about a system\u2019s *handling* of a specific technical concept (context window optimization). This implies a need for understanding the system's architecture and processes, requiring retrieval of documents that explain the system's design and how it manages context.  It\u2019s not purely a general knowledge question; it demands specific knowledge about the RAG system\u2019s implementation. The query contains a single, focused question, but the \u2018how\u2019 implies a need to understand the mechanisms involved.  Therefore, a retrieval approach of 5-15 chunks is appropriate to capture the breadth of information needed. A relevance threshold of 0.6 is recommended to filter out less relevant results, focusing on documents that directly address context window optimization. Finally, reranking is recommended to further refine the retrieved chunks based on semantic similarity to the query, as simply retrieving the top k chunks might not perfectly align with the user's intent.  This will help prioritize documents that truly explain the system's approach."
    },
    "time": 9.091415882110596
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"What embedding models are supported for semantic search?\" is moderately complex. It\u2019s not a simple factual question like \"What is the capital of France?\" but it doesn't require deep analytical reasoning. It necessitates retrieving documents that discuss the various embedding models used in semantic search.  The intent is clear \u2013 the user wants a list of supported models.  Therefore, a moderate retrieval approach is appropriate.  I recommend k=8 because it balances retrieving enough relevant documents (to cover a reasonable range of models) without overwhelming the system with irrelevant results. A threshold of 0.6 is suggested to filter out documents with only a superficial connection to the query. Reranking is crucial here. Without it, the initial retrieval might return documents that mention embedding models in passing, rather than those specifically focused on their use in semantic search. Reranking will prioritize documents that directly address the query's focus."
    },
    "time": 7.895979881286621
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "This query is moderately complex as it combines a request for strategies (requiring specific knowledge) with an analytical component \u2013 how those strategies affect retrieval performance. It\u2019s not a simple factual question like \u2018What is chunking?\u2019 but it\u2019s also not a purely conceptual question.  The user wants to understand *how* chunking impacts retrieval, implying they're looking for evidence and comparisons.  Therefore, a moderate number of chunks (k=8) is appropriate to capture a range of relevant information. A relevance threshold of 0.6 suggests prioritizing results that are strongly related to both \u2018chunking strategies\u2019 and \u2018retrieval performance\u2019.  Finally, reranking is recommended (true) to further refine the results, as the initial retrieval might return documents that discuss chunking strategies but not necessarily their impact on retrieval.  The goal is to prioritize documents that directly address the query's analytical aspect."
    },
    "time": 7.855952024459839
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query \"How does the authentication work and what endpoints are available for document management?\" is moderately complex. It contains two distinct sub-questions: authentication process and document management endpoints. This suggests a need for retrieving information from multiple document types and potentially different sections within those documents.  A retrieval of 8 chunks (k=8) provides a good balance between capturing relevant information and avoiding overwhelming the generation model with irrelevant results.  A threshold of 0.6 indicates a strong relevance requirement \u2013 we want to prioritize documents that are highly related to both authentication and document management. Reranking is recommended (reranking: true) to further refine the retrieved chunks, as the initial retrieval might produce a mix of documents with varying degrees of relevance to each sub-question.  Without reranking, the generation model might struggle to synthesize information effectively from a less-relevant set of chunks.  The query isn't purely factual; it requires understanding a process (authentication) and identifying specific resources (endpoints). Therefore, a more nuanced retrieval strategy is needed beyond simply finding documents containing the keywords."
    },
    "time": 8.831205129623413
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "analysis": {
      "complexity": "simple",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query 'RAG?' is extremely simple and represents a user seeking a basic understanding of the RAG (Retrieval Augmented Generation) concept. It\u2019s not a complex analytical question requiring deep understanding or nuanced interpretation.  Therefore, a moderate number of chunks (k=8) should be retrieved to capture a broad range of relevant information. A relevance threshold of 0.6 is recommended to filter out less relevant results, ensuring the generated response focuses on the core concept. Reranking is beneficial here to prioritize documents that explicitly define or explain RAG, even if they don't perfectly match the query terms.  Without further context, a broad retrieval is appropriate, but a slight filtering is needed to avoid irrelevant noise."
    },
    "time": 6.665657043457031
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "analysis": {
      "complexity": "moderate",
      "parameters": {
        "k": 8,
        "threshold": 0.6,
        "reranking": true
      },
      "justification": "The query 'Embeddings?' is moderately complex. It\u2019s not a simple factual question like 'What is an embedding?' but it\u2019s also not a complex analytical question. The user is likely seeking a general understanding of embeddings \u2013 their definition, purpose, and perhaps different types.  Therefore, a retrieval approach that balances breadth and relevance is needed.  \n\n* **k (8):**  I've recommended 8 chunks. This provides a reasonable number of potentially relevant documents without overwhelming the generation model with too much information.  A lower value (5-7) might miss key concepts, while a higher value (10-15) could dilute the focus. \n* **threshold (0.6):** A threshold of 0.6 suggests prioritizing documents with a high degree of relevance. This allows the generation model to focus on the most pertinent information.  A lower threshold (0.4) would broaden the search, while a higher value (0.8) might exclude some useful context. \n* **reranking (true):** Reranking is crucial here.  Initial retrieval might return documents that use the word 'embeddings' in a tangential way. Reranking, using a model trained to understand semantic similarity, will help prioritize documents where 'embeddings' are central to the meaning, ensuring the generation model receives the most relevant context.  Without reranking, the generation model could be misled by less relevant results."
    },
    "time": 11.224719285964966
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "analysis": {
      "complexity": "complex",
      "parameters": {
        "k": 10,
        "threshold": 0.7,
        "reranking": true
      },
      "justification": "This query represents a complex analytical query rather than a simple factual one. The user isn't just asking for a single answer; they're seeking a deep understanding of the Metis RAG system's internal workings. The query explicitly asks about multiple aspects of the system \u2013 document processing, chunking strategies, vector store efficiency, and retrieval judge optimization. This necessitates retrieving a significant number of relevant chunks to cover all these areas.  \n\n* **k (Number of chunks to retrieve):**  I recommend 10. Given the multi-faceted nature of the query, retrieving 5-15 chunks would likely be insufficient to fully address all the user's questions.  A larger number allows for a more thorough exploration of the system's components.\n* **threshold (Relevance threshold):** I've set the threshold to 0.7. This suggests a strong emphasis on relevance. The user wants detailed explanations, so a higher threshold will prioritize chunks that are highly relevant to the core aspects of the Metis RAG system.\n* **reranking:**  `true`.  The user's query indicates a desire for optimized retrieval. The retrieval judge component of Metis RAG is specifically mentioned, implying a focus on improving relevance. Reranking will further refine the results, ensuring the most pertinent chunks are presented first.  Without reranking, the initial retrieval might return a mix of relevant and irrelevant chunks, requiring the user to sift through them."
    },
    "time": 11.576659202575684
  }
]

================
File: tests/retrieval_judge/results/query_refinement_results.json
================
[
  {
    "query": "How does the system work?",
    "type": "ambiguous",
    "description": "Very general query without specific focus",
    "refined_query": "How does the RAG system work, including document embedding storage, the document processing pipeline, and the LLM integration component?",
    "time": 1.9273748397827148
  },
  {
    "query": "Tell me about the API",
    "type": "ambiguous",
    "description": "Ambiguous query about the API without specifying which aspect",
    "refined_query": "Tell me about the API layer, specifically its implementation using FastAPI for document upload and management, chat interactions, and analytics data retrieval, including authentication with JWT tokens.",
    "time": 2.0591607093811035
  },
  {
    "query": "How does the vektor store work?",
    "type": "typo",
    "description": "Typo in 'vector'",
    "refined_query": "refinement process?",
    "time": 2.063575029373169
  },
  {
    "query": "What is the documnet procesing pipeline?",
    "type": "typo",
    "description": "Multiple typos in 'document processing'",
    "refined_query": "What is the document processing pipeline, including file validation, parsing, chunking, and metadata extraction?",
    "time": 1.7061607837677002
  },
  {
    "query": "How does the system handle context window optimization?",
    "type": "domain-specific",
    "description": "Domain-specific query about LLM context windows",
    "refined_query": "How does the LLM integration component handle context window optimization, specifically within the Retrieval Judge's query refinement process?",
    "time": 1.803995132446289
  },
  {
    "query": "What embedding models are supported for semantic search?",
    "type": "domain-specific",
    "description": "Domain-specific query about embedding models",
    "refined_query": "What embedding models are supported for semantic search, specifically considering models like Llama 3, Gemma, and Mistral?",
    "time": 1.8295328617095947
  },
  {
    "query": "What are the chunking strategies and how do they affect retrieval performance?",
    "type": "multi-part",
    "description": "Multi-part query about chunking strategies and their impact",
    "refined_query": "What are the different document chunking strategies and how do they affect retrieval performance, particularly within the context of document processing pipelines and retrieval judge optimization?",
    "time": 1.98307204246521
  },
  {
    "query": "How does the authentication work and what endpoints are available for document management?",
    "type": "multi-part",
    "description": "Multi-part query about authentication and document management endpoints",
    "refined_query": "What authentication methods are used for the document management API endpoints (specifically JWT tokens), and what document management API endpoints are available (e.g., upload, management)?",
    "time": 2.098104953765869
  },
  {
    "query": "RAG?",
    "type": "short",
    "description": "Very short query with just an acronym",
    "refined_query": "RAG system components and functionality",
    "time": 1.2879359722137451
  },
  {
    "query": "Embeddings?",
    "type": "short",
    "description": "Very short query with just a technical term",
    "refined_query": "Explain vector store embeddings and their role in RAG systems, including HNSW indexing and metadata filtering.",
    "time": 1.7461588382720947
  },
  {
    "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
    "type": "long",
    "description": "Very long query with multiple questions",
    "refined_query": "I want a detailed explanation of the Metis RAG system\u2019s architecture, focusing on its document processing pipeline, chunking strategies, vector store implementation, and the Retrieval Judge\u2019s optimization process. Specifically, I\u2019m interested in how the system handles different file formats (PDF, DOCX, TXT, Markdown), the configurable chunking strategies used for document splitting, the efficient metadata filtering within the vector store (using HNSW index and metadata like tags, folder, and document ID), and how the Retrieval Judge leverages LLM analysis to refine queries and optimize retrieval parameters for improved relevance.",
    "time": 4.841519117355347
  }
]

================
File: tests/retrieval_judge/results/single_query_result.json
================
{
  "query": "What are the components of the RAG engine and how do they work together?",
  "standard": {
    "answer": "Based on the provided documents [1] and [2], the components of the RAG engine are:\n\n* Document Processing: This pipeline handles file validation and parsing, text extraction, and Chu... (the text is cut off, but it seems to be related to processing documents)\n\nAccording to the context, these components work together as part of the RAG Engine within Metis RAG's modular architecture.\n\nNote that there might be more information about the Document Processing pipeline or other RAG engine components not included in the provided documents. However, based on the available context, this is all the information I can provide.",
    "sources_count": 2,
    "time": 184.06564712524414
  },
  "judge": {
    "answer": "Based on the provided documents [1], [2], and [3], the components of the RAG engine are:\n\n* Document Processing: The core RAG engine consists of a document processing pipeline that handles:\n\t+ File validation and parsing\n\t+ Text extraction\n\t[No further information is available in the context about how these components work together.]\n\nNote that there is no explicit information provided in the context about the specific interactions or processes between the different components of the RAG engine. However, based on the general architecture overview, it can be inferred that the RAG engine likely integrates with the frontend and API layers to facilitate document management, chat interactions, system configuration, and analytics data retrieval.\n\nSources:\n[1], [2], and [3]",
    "sources_count": 3,
    "time": 37.67006587982178
  }
}

================
File: tests/retrieval_judge/results/timing_analysis_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "standard": {
      "total_time": 171.3785719871521,
      "sources_count": 1
    },
    "judge": {
      "total_time": 36.81675601005554,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 12.479793787002563,
        "evaluate_chunks": 6.393211126327515,
        "enhanced_retrieval": 18.878051280975342
      }
    }
  },
  {
    "query": "What are the components of the RAG engine?",
    "standard": {
      "total_time": 164.1695818901062,
      "sources_count": 1
    },
    "judge": {
      "total_time": 31.121859073638916,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 12.357210874557495,
        "evaluate_chunks": 5.824731111526489,
        "enhanced_retrieval": 18.187943935394287
      }
    }
  },
  {
    "query": "How does the vector store work?",
    "standard": {
      "total_time": 167.41857886314392,
      "sources_count": 1
    },
    "judge": {
      "total_time": 31.442083835601807,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 11.610037803649902,
        "evaluate_chunks": 6.113225936889648,
        "enhanced_retrieval": 17.730960845947266
      }
    }
  },
  {
    "query": "What is the role of the LLM integration component?",
    "standard": {
      "total_time": 175.33833718299866,
      "sources_count": 1
    },
    "judge": {
      "total_time": 29.5294828414917,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 11.025973081588745,
        "evaluate_chunks": 5.471881866455078,
        "enhanced_retrieval": 16.504756927490234
      }
    }
  },
  {
    "query": "How does the document processing pipeline work?",
    "standard": {
      "total_time": 174.38789820671082,
      "sources_count": 1
    },
    "judge": {
      "total_time": 31.299037218093872,
      "sources_count": 1,
      "component_timings": {
        "analyze_query": 11.572484970092773,
        "evaluate_chunks": 5.757498025894165,
        "enhanced_retrieval": 17.335467100143433
      }
    }
  }
]

================
File: tests/retrieval_judge/results/timing_analysis.json
================
{
  "avg_standard_time": 170.53859362602233,
  "avg_judge_time": 32.041843795776366,
  "time_difference_percent": -81.21138264688543,
  "component_averages": {
    "analyze_query": 11.809100103378295,
    "evaluate_chunks": 5.912109613418579,
    "enhanced_retrieval": 17.72743601799011
  },
  "first_run": {
    "standard": 171.3785719871521,
    "judge": 36.81675601005554
  },
  "subsequent_runs": {
    "standard": 170.3285990357399,
    "judge": 30.848115742206573
  }
}

================
File: tests/retrieval_judge/analyze_retrieval_judge_results.py
================
#!/usr/bin/env python3
"""
Analysis script for Retrieval Judge comparison results.
This script:
1. Loads the results from the comparison test
2. Performs detailed analysis on the effectiveness of the Retrieval Judge
3. Generates visualizations to highlight key findings
4. Provides recommendations for improving the Retrieval Judge
"""

import os
import json
import logging
import argparse
from typing import Dict, List, Any, Optional
import matplotlib.pyplot as plt
import numpy as np
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("analyze_retrieval_judge_results")

def load_results(results_file: str) -> List[Dict[str, Any]]:
    """Load the comparison test results from file"""
    logger.info(f"Loading results from {results_file}")
    
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
        
        logger.info(f"Loaded {len(results)} test results")
        return results
    except Exception as e:
        logger.error(f"Error loading results: {str(e)}")
        return []

def load_metrics(metrics_file: str) -> Dict[str, Any]:
    """Load the analysis metrics from file"""
    logger.info(f"Loading metrics from {metrics_file}")
    
    try:
        with open(metrics_file, 'r') as f:
            metrics = json.load(f)
        
        logger.info(f"Loaded metrics for {metrics.get('total_queries', 0)} queries")
        return metrics
    except Exception as e:
        logger.error(f"Error loading metrics: {str(e)}")
        return {}

def analyze_source_relevance(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the relevance of sources retrieved by both methods"""
    logger.info("Analyzing source relevance...")
    
    relevance_analysis = {
        "avg_standard_relevance": 0,
        "avg_judge_relevance": 0,
        "relevance_improvement": 0,
        "by_complexity": {},
        "by_document": defaultdict(lambda: {"standard": 0, "judge": 0, "count": 0})
    }
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        relevance_analysis["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_relevance": 0,
            "avg_judge_relevance": 0,
            "improvement": 0
        }
    
    # Calculate average relevance scores
    total_standard_relevance = 0
    total_judge_relevance = 0
    total_sources = 0
    
    for result in results:
        complexity = result["complexity"]
        
        # Calculate average relevance for standard retrieval
        standard_sources = result["standard"]["sources"]
        standard_relevance = sum(s["relevance_score"] for s in standard_sources) if standard_sources else 0
        standard_avg_relevance = standard_relevance / len(standard_sources) if standard_sources else 0
        
        # Calculate average relevance for judge retrieval
        judge_sources = result["judge"]["sources"]
        judge_relevance = sum(s["relevance_score"] for s in judge_sources) if judge_sources else 0
        judge_avg_relevance = judge_relevance / len(judge_sources) if judge_sources else 0
        
        # Update totals
        total_standard_relevance += standard_avg_relevance
        total_judge_relevance += judge_avg_relevance
        total_sources += 1
        
        # Update complexity metrics
        if complexity in relevance_analysis["by_complexity"]:
            relevance_analysis["by_complexity"][complexity]["count"] += 1
            relevance_analysis["by_complexity"][complexity]["avg_standard_relevance"] += standard_avg_relevance
            relevance_analysis["by_complexity"][complexity]["avg_judge_relevance"] += judge_avg_relevance
        
        # Track relevance by document
        for source in standard_sources:
            doc_id = source["document_id"]
            relevance_analysis["by_document"][doc_id]["standard"] += source["relevance_score"]
            relevance_analysis["by_document"][doc_id]["count"] += 1
            
        for source in judge_sources:
            doc_id = source["document_id"]
            relevance_analysis["by_document"][doc_id]["judge"] += source["relevance_score"]
            # Don't increment count again as we're calculating averages
    
    # Calculate overall averages
    if total_sources > 0:
        relevance_analysis["avg_standard_relevance"] = total_standard_relevance / total_sources
        relevance_analysis["avg_judge_relevance"] = total_judge_relevance / total_sources
        
        # Calculate improvement percentage
        if relevance_analysis["avg_standard_relevance"] > 0:
            relevance_analysis["relevance_improvement"] = (
                (relevance_analysis["avg_judge_relevance"] - relevance_analysis["avg_standard_relevance"]) / 
                relevance_analysis["avg_standard_relevance"] * 100
            )
    
    # Calculate complexity averages and improvements
    for complexity, data in relevance_analysis["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_relevance"] /= data["count"]
            data["avg_judge_relevance"] /= data["count"]
            
            # Calculate improvement percentage
            if data["avg_standard_relevance"] > 0:
                data["improvement"] = (
                    (data["avg_judge_relevance"] - data["avg_standard_relevance"]) / 
                    data["avg_standard_relevance"] * 100
                )
    
    # Calculate document averages
    for doc_id, data in relevance_analysis["by_document"].items():
        if data["count"] > 0:
            data["avg_standard"] = data["standard"] / data["count"]
            data["avg_judge"] = data["judge"] / data["count"]
            
            # Calculate improvement percentage
            if data["avg_standard"] > 0:
                data["improvement"] = (
                    (data["avg_judge"] - data["avg_standard"]) / 
                    data["avg_standard"] * 100
                )
    
    # Convert defaultdict to regular dict for JSON serialization
    relevance_analysis["by_document"] = dict(relevance_analysis["by_document"])
    
    return relevance_analysis

def analyze_query_refinement(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the effectiveness of query refinement"""
    logger.info("Analyzing query refinement effectiveness...")
    
    # This is a bit tricky since we don't have direct access to the refined queries
    # We'll infer effectiveness by comparing source relevance for ambiguous queries
    
    refinement_analysis = {
        "ambiguous_queries": [],
        "multi_part_queries": [],
        "avg_improvement_ambiguous": 0,
        "avg_improvement_multi_part": 0
    }
    
    # Extract ambiguous and multi-part queries
    ambiguous_queries = [r for r in results if r["complexity"] == "ambiguous"]
    multi_part_queries = [r for r in results if r["complexity"] == "multi-part"]
    
    # Calculate average improvement for ambiguous queries
    total_improvement_ambiguous = 0
    for query in ambiguous_queries:
        standard_sources = query["standard"]["sources"]
        judge_sources = query["judge"]["sources"]
        
        standard_relevance = sum(s["relevance_score"] for s in standard_sources) / len(standard_sources) if standard_sources else 0
        judge_relevance = sum(s["relevance_score"] for s in judge_sources) / len(judge_sources) if judge_sources else 0
        
        improvement = ((judge_relevance - standard_relevance) / standard_relevance * 100) if standard_relevance > 0 else 0
        
        refinement_analysis["ambiguous_queries"].append({
            "query": query["query"],
            "standard_relevance": standard_relevance,
            "judge_relevance": judge_relevance,
            "improvement": improvement
        })
        
        total_improvement_ambiguous += improvement
    
    # Calculate average improvement for multi-part queries
    total_improvement_multi_part = 0
    for query in multi_part_queries:
        standard_sources = query["standard"]["sources"]
        judge_sources = query["judge"]["sources"]
        
        standard_relevance = sum(s["relevance_score"] for s in standard_sources) / len(standard_sources) if standard_sources else 0
        judge_relevance = sum(s["relevance_score"] for s in judge_sources) / len(judge_sources) if judge_sources else 0
        
        improvement = ((judge_relevance - standard_relevance) / standard_relevance * 100) if standard_relevance > 0 else 0
        
        refinement_analysis["multi_part_queries"].append({
            "query": query["query"],
            "standard_relevance": standard_relevance,
            "judge_relevance": judge_relevance,
            "improvement": improvement
        })
        
        total_improvement_multi_part += improvement
    
    # Calculate averages
    if ambiguous_queries:
        refinement_analysis["avg_improvement_ambiguous"] = total_improvement_ambiguous / len(ambiguous_queries)
    
    if multi_part_queries:
        refinement_analysis["avg_improvement_multi_part"] = total_improvement_multi_part / len(multi_part_queries)
    
    return refinement_analysis

def analyze_context_optimization(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the effectiveness of context optimization"""
    logger.info("Analyzing context optimization effectiveness...")
    
    optimization_analysis = {
        "avg_standard_sources": 0,
        "avg_judge_sources": 0,
        "source_count_difference": 0,
        "source_count_difference_percent": 0,
        "by_complexity": {}
    }
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        optimization_analysis["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_sources": 0,
            "avg_judge_sources": 0,
            "difference": 0,
            "difference_percent": 0
        }
    
    # Calculate source counts
    total_standard_sources = 0
    total_judge_sources = 0
    total_queries = len(results)
    
    for result in results:
        complexity = result["complexity"]
        standard_source_count = len(result["standard"]["sources"])
        judge_source_count = len(result["judge"]["sources"])
        
        # Update totals
        total_standard_sources += standard_source_count
        total_judge_sources += judge_source_count
        
        # Update complexity metrics
        if complexity in optimization_analysis["by_complexity"]:
            optimization_analysis["by_complexity"][complexity]["count"] += 1
            optimization_analysis["by_complexity"][complexity]["avg_standard_sources"] += standard_source_count
            optimization_analysis["by_complexity"][complexity]["avg_judge_sources"] += judge_source_count
    
    # Calculate overall averages
    if total_queries > 0:
        optimization_analysis["avg_standard_sources"] = total_standard_sources / total_queries
        optimization_analysis["avg_judge_sources"] = total_judge_sources / total_queries
        
        # Calculate difference
        optimization_analysis["source_count_difference"] = optimization_analysis["avg_judge_sources"] - optimization_analysis["avg_standard_sources"]
        
        # Calculate percentage difference
        if optimization_analysis["avg_standard_sources"] > 0:
            optimization_analysis["source_count_difference_percent"] = (
                optimization_analysis["source_count_difference"] / optimization_analysis["avg_standard_sources"] * 100
            )
    
    # Calculate complexity averages and differences
    for complexity, data in optimization_analysis["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_sources"] /= data["count"]
            data["avg_judge_sources"] /= data["count"]
            
            # Calculate difference
            data["difference"] = data["avg_judge_sources"] - data["avg_standard_sources"]
            
            # Calculate percentage difference
            if data["avg_standard_sources"] > 0:
                data["difference_percent"] = (
                    data["difference"] / data["avg_standard_sources"] * 100
                )
    
    return optimization_analysis

def analyze_performance_impact(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze the performance impact of using the Retrieval Judge"""
    logger.info("Analyzing performance impact...")
    
    performance_analysis = {
        "avg_standard_time": 0,
        "avg_judge_time": 0,
        "time_difference": 0,
        "time_difference_percent": 0,
        "by_complexity": {}
    }
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        performance_analysis["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_time": 0,
            "avg_judge_time": 0,
            "difference": 0,
            "difference_percent": 0
        }
    
    # Calculate processing times
    total_standard_time = 0
    total_judge_time = 0
    total_queries = len(results)
    
    for result in results:
        complexity = result["complexity"]
        standard_time = result["standard"]["time"]
        judge_time = result["judge"]["time"]
        
        # Update totals
        total_standard_time += standard_time
        total_judge_time += judge_time
        
        # Update complexity metrics
        if complexity in performance_analysis["by_complexity"]:
            performance_analysis["by_complexity"][complexity]["count"] += 1
            performance_analysis["by_complexity"][complexity]["avg_standard_time"] += standard_time
            performance_analysis["by_complexity"][complexity]["avg_judge_time"] += judge_time
    
    # Calculate overall averages
    if total_queries > 0:
        performance_analysis["avg_standard_time"] = total_standard_time / total_queries
        performance_analysis["avg_judge_time"] = total_judge_time / total_queries
        
        # Calculate difference
        performance_analysis["time_difference"] = performance_analysis["avg_judge_time"] - performance_analysis["avg_standard_time"]
        
        # Calculate percentage difference
        if performance_analysis["avg_standard_time"] > 0:
            performance_analysis["time_difference_percent"] = (
                performance_analysis["time_difference"] / performance_analysis["avg_standard_time"] * 100
            )
    
    # Calculate complexity averages and differences
    for complexity, data in performance_analysis["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_time"] /= data["count"]
            data["avg_judge_time"] /= data["count"]
            
            # Calculate difference
            data["difference"] = data["avg_judge_time"] - data["avg_standard_time"]
            
            # Calculate percentage difference
            if data["avg_standard_time"] > 0:
                data["difference_percent"] = (
                    data["difference"] / data["avg_standard_time"] * 100
                )
    
    return performance_analysis

def generate_visualizations(
    results: List[Dict[str, Any]], 
    relevance_analysis: Dict[str, Any],
    refinement_analysis: Dict[str, Any],
    optimization_analysis: Dict[str, Any],
    performance_analysis: Dict[str, Any]
):
    """Generate visualizations to highlight key findings"""
    logger.info("Generating visualizations...")
    
    # Create output directory for visualizations
    vis_dir = os.path.join("tests", "retrieval_judge", "visualizations")
    os.makedirs(vis_dir, exist_ok=True)
    
    # 1. Relevance improvement by complexity
    plt.figure(figsize=(10, 6))
    complexities = list(relevance_analysis["by_complexity"].keys())
    improvements = [relevance_analysis["by_complexity"][c]["improvement"] for c in complexities]
    
    plt.bar(complexities, improvements, color='skyblue')
    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)
    plt.title('Relevance Improvement by Query Complexity')
    plt.xlabel('Query Complexity')
    plt.ylabel('Improvement (%)')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'relevance_by_complexity.png'))
    plt.close()
    
    # 2. Source count comparison
    plt.figure(figsize=(10, 6))
    complexities = list(optimization_analysis["by_complexity"].keys())
    standard_sources = [optimization_analysis["by_complexity"][c]["avg_standard_sources"] for c in complexities]
    judge_sources = [optimization_analysis["by_complexity"][c]["avg_judge_sources"] for c in complexities]
    
    x = np.arange(len(complexities))
    width = 0.35
    
    plt.bar(x - width/2, standard_sources, width, label='Standard Retrieval', color='lightcoral')
    plt.bar(x + width/2, judge_sources, width, label='Judge-Enhanced Retrieval', color='lightgreen')
    
    plt.title('Average Number of Sources by Query Complexity')
    plt.xlabel('Query Complexity')
    plt.ylabel('Average Number of Sources')
    plt.xticks(x, complexities)
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'source_count_comparison.png'))
    plt.close()
    
    # 3. Processing time comparison
    plt.figure(figsize=(10, 6))
    complexities = list(performance_analysis["by_complexity"].keys())
    standard_times = [performance_analysis["by_complexity"][c]["avg_standard_time"] for c in complexities]
    judge_times = [performance_analysis["by_complexity"][c]["avg_judge_time"] for c in complexities]
    
    x = np.arange(len(complexities))
    width = 0.35
    
    plt.bar(x - width/2, standard_times, width, label='Standard Retrieval', color='lightcoral')
    plt.bar(x + width/2, judge_times, width, label='Judge-Enhanced Retrieval', color='lightgreen')
    
    plt.title('Average Processing Time by Query Complexity')
    plt.xlabel('Query Complexity')
    plt.ylabel('Average Time (seconds)')
    plt.xticks(x, complexities)
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'processing_time_comparison.png'))
    plt.close()
    
    # 4. Query refinement effectiveness
    plt.figure(figsize=(10, 6))
    query_types = ['Ambiguous', 'Multi-part']
    improvements = [
        refinement_analysis["avg_improvement_ambiguous"],
        refinement_analysis["avg_improvement_multi_part"]
    ]
    
    plt.bar(query_types, improvements, color=['orange', 'purple'])
    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)
    plt.title('Query Refinement Effectiveness')
    plt.xlabel('Query Type')
    plt.ylabel('Average Relevance Improvement (%)')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(vis_dir, 'query_refinement_effectiveness.png'))
    plt.close()
    
    logger.info(f"Visualizations saved to {os.path.abspath(vis_dir)}")

def generate_improvement_recommendations(
    relevance_analysis: Dict[str, Any],
    refinement_analysis: Dict[str, Any],
    optimization_analysis: Dict[str, Any],
    performance_analysis: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """Generate recommendations for improving the Retrieval Judge"""
    logger.info("Generating improvement recommendations...")
    
    recommendations = []
    
    # 1. Check relevance improvement
    overall_relevance_improvement = relevance_analysis.get("relevance_improvement", 0)
    if overall_relevance_improvement < 10:
        recommendations.append({
            "area": "Relevance Scoring",
            "issue": "Limited overall relevance improvement",
            "recommendation": "Enhance the relevance evaluation prompt to better distinguish between highly relevant and tangentially relevant content. Consider using a more fine-grained scoring system or incorporating domain-specific knowledge."
        })
    
    # 2. Check performance impact
    time_difference_percent = performance_analysis.get("time_difference_percent", 0)
    if time_difference_percent > 50:
        recommendations.append({
            "area": "Performance",
            "issue": "Significant processing time increase",
            "recommendation": "Optimize the judge's processing pipeline by: 1) Using a smaller, faster model for initial query analysis, 2) Implementing caching for similar queries, 3) Reducing the context size in prompts, or 4) Implementing parallel processing for independent judge operations."
        })
    
    # 3. Check query refinement effectiveness
    avg_improvement_ambiguous = refinement_analysis.get("avg_improvement_ambiguous", 0)
    if avg_improvement_ambiguous < 15:
        recommendations.append({
            "area": "Query Refinement",
            "issue": "Limited effectiveness for ambiguous queries",
            "recommendation": "Improve the query refinement prompt to better handle ambiguity by: 1) Adding examples of successful disambiguations, 2) Incorporating domain-specific terminology, 3) Implementing a clarification step that generates multiple possible interpretations before selecting the most likely one."
        })
    
    # 4. Check context optimization
    source_count_difference = optimization_analysis.get("source_count_difference", 0)
    if source_count_difference < 0:
        recommendations.append({
            "area": "Context Optimization",
            "issue": "Judge retrieves fewer sources on average",
            "recommendation": "Revise the context optimization logic to: 1) Focus more on diversity of information rather than just relevance, 2) Implement a minimum source count based on query complexity, 3) Add a post-processing step to ensure critical information isn't excluded."
        })
    
    # 5. Check complex query handling
    complex_improvement = relevance_analysis.get("by_complexity", {}).get("complex", {}).get("improvement", 0)
    if complex_improvement < 20:
        recommendations.append({
            "area": "Complex Query Handling",
            "issue": "Limited improvement for complex analytical queries",
            "recommendation": "Enhance complex query processing by: 1) Breaking down complex queries into sub-queries, 2) Implementing a multi-step retrieval process that builds context incrementally, 3) Adding a synthesis step that combines information from multiple sources."
        })
    
    # 6. Check multi-part query handling
    multi_part_improvement = relevance_analysis.get("by_complexity", {}).get("multi-part", {}).get("improvement", 0)
    if multi_part_improvement < 15:
        recommendations.append({
            "area": "Multi-part Query Handling",
            "issue": "Limited improvement for multi-part queries",
            "recommendation": "Improve multi-part query handling by: 1) Implementing a query decomposition step that identifies distinct sub-questions, 2) Retrieving information for each sub-question separately, 3) Merging the results with appropriate weighting, 4) Adding a final relevance check to ensure all parts of the query are addressed."
        })
    
    # Always recommend monitoring and feedback loop
    recommendations.append({
        "area": "Continuous Improvement",
        "issue": "Need for ongoing optimization",
        "recommendation": "Implement a feedback loop by: 1) Tracking user satisfaction with responses, 2) Logging cases where the judge significantly improves or degrades results, 3) Periodically retraining or fine-tuning the judge with examples of successful and unsuccessful retrievals."
    })
    
    return recommendations

def main():
    """Main analysis function"""
    parser = argparse.ArgumentParser(description="Analyze Retrieval Judge comparison results")
    parser.add_argument("--results", default=os.path.join("tests", "retrieval_judge", "results", "retrieval_judge_comparison_results.json"), 
                        help="Path to results JSON file")
    parser.add_argument("--metrics", default=os.path.join("tests", "retrieval_judge", "results", "retrieval_judge_metrics.json"), 
                        help="Path to metrics JSON file")
    parser.add_argument("--output", default=os.path.join("tests", "retrieval_judge", "results", "retrieval_judge_analysis_report.json"), 
                        help="Path to output analysis report")
    args = parser.parse_args()
    
    logger.info("Starting Retrieval Judge results analysis...")
    
    try:
        # Load results and metrics
        results = load_results(args.results)
        metrics = load_metrics(args.metrics)
        
        if not results:
            logger.error("No results to analyze. Please run the comparison test first.")
            return
        
        # Perform detailed analysis
        relevance_analysis = analyze_source_relevance(results)
        refinement_analysis = analyze_query_refinement(results)
        optimization_analysis = analyze_context_optimization(results)
        performance_analysis = analyze_performance_impact(results)
        
        # Generate visualizations
        generate_visualizations(
            results,
            relevance_analysis,
            refinement_analysis,
            optimization_analysis,
            performance_analysis
        )
        
        # Generate improvement recommendations
        recommendations = generate_improvement_recommendations(
            relevance_analysis,
            refinement_analysis,
            optimization_analysis,
            performance_analysis
        )
        
        # Compile analysis report
        analysis_report = {
            "summary": {
                "total_queries": len(results),
                "overall_relevance_improvement": relevance_analysis["relevance_improvement"],
                "performance_impact": performance_analysis["time_difference_percent"],
                "recommendation_count": len(recommendations)
            },
            "detailed_analysis": {
                "relevance": relevance_analysis,
                "query_refinement": refinement_analysis,
                "context_optimization": optimization_analysis,
                "performance": performance_analysis
            },
            "recommendations": recommendations
        }
        
        # Save analysis report
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        with open(args.output, "w") as f:
            json.dump(analysis_report, f, indent=2)
        
        # Print summary
        logger.info("\n=== RETRIEVAL JUDGE ANALYSIS SUMMARY ===")
        logger.info(f"Total queries analyzed: {len(results)}")
        logger.info(f"Overall relevance improvement: {relevance_analysis['relevance_improvement']:.2f}%")
        logger.info(f"Performance impact: {performance_analysis['time_difference_percent']:.2f}% increase in processing time")
        logger.info(f"Generated {len(recommendations)} improvement recommendations")
        logger.info(f"Analysis report saved to {os.path.abspath(args.output)}")
        logger.info(f"Visualizations saved to {os.path.abspath(os.path.join('tests', 'retrieval_judge', 'visualizations'))}")
        
        # Print top recommendations
        logger.info("\nTop improvement recommendations:")
        for i, rec in enumerate(recommendations[:3], 1):
            logger.info(f"{i}. {rec['area']}: {rec['recommendation']}")
        
        logger.info("\nRetrieval Judge analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Error during analysis: {str(e)}")
        raise

if __name__ == "__main__":
    main()

================
File: tests/retrieval_judge/IMPLEMENTATION_NOTES.md
================
# Retrieval Judge Implementation Notes

## Overview

The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by analyzing queries, evaluating retrieved chunks, refining queries when needed, and optimizing context assembly. This document provides technical details about the implementation and testing framework.

## Implementation Details

The Retrieval Judge is implemented in `app/rag/agents/retrieval_judge.py` and consists of four main components:

1. **Query Analysis**: Analyzes the complexity of a query and recommends optimal retrieval parameters (k, threshold, reranking).
2. **Chunk Evaluation**: Evaluates the relevance of retrieved chunks to the query and determines if query refinement is needed.
3. **Query Refinement**: Refines ambiguous or complex queries to improve retrieval precision.
4. **Context Optimization**: Reorders and filters chunks to create an optimal context for the LLM.

The judge is integrated with the RAG engine in `app/rag/rag_engine.py` through the `_enhanced_retrieval` method, which is called when the judge is enabled.

## Testing Framework

The testing framework consists of:

1. **Comparison Tests** (`test_retrieval_judge_comparison.py`): Compares standard retrieval vs. retrieval with the judge enabled using a variety of test queries.
2. **Analysis Tools** (`analyze_retrieval_judge_results.py`): Analyzes the test results and generates visualizations and recommendations.
3. **Run Script** (`run_tests.sh`): Shell script to run both the comparison tests and analysis in sequence.

### Test Methodology

The tests use a controlled environment with predefined test documents and queries of varying complexity. For each query, the test:

1. Runs the query with standard retrieval
2. Runs the query with judge-enhanced retrieval
3. Records the results, including:
   - Retrieved sources
   - Relevance scores
   - Processing time
   - Generated answers

The analysis then compares these results to evaluate the effectiveness of the judge.

### Test Queries

The test queries are designed to cover a range of complexity levels and query types:

- **Simple factual queries**: Direct questions with clear answers
- **Moderate complexity queries**: Questions requiring synthesis of information
- **Complex analytical queries**: Questions requiring deeper understanding and inference
- **Ambiguous queries**: Questions with unclear intent or multiple interpretations
- **Multi-part queries**: Questions that combine multiple distinct information needs

### Analysis Metrics

The analysis evaluates the Retrieval Judge on several dimensions:

1. **Source relevance**: How relevant are the retrieved chunks to the query?
2. **Query refinement effectiveness**: How well does the judge improve ambiguous or complex queries?
3. **Context optimization**: How effectively does the judge select and order chunks?
4. **Performance impact**: What is the processing time overhead of using the judge?

## Expected Results

Based on the implementation, we expect the Retrieval Judge to show:

1. **Improved relevance** for complex, ambiguous, and multi-part queries
2. **More focused retrieval** with fewer but more relevant chunks
3. **Better context organization** for improved response generation
4. **Some performance overhead** due to the additional LLM calls

The analysis will quantify these improvements and identify areas for further optimization.

## Future Improvements

Potential areas for improvement in the Retrieval Judge include:

1. **Performance optimization**: Reducing the overhead of LLM calls
2. **Caching**: Implementing caching for similar queries
3. **Parallel processing**: Running independent judge operations in parallel
4. **Feedback loop**: Incorporating user feedback to improve the judge over time
5. **Domain adaptation**: Fine-tuning the judge for specific domains or document types

## Running the Tests

See the `README.md` file for detailed instructions on running the tests.

## Interpreting Results

The analysis generates visualizations and recommendations that can be used to:

1. Evaluate the overall effectiveness of the Retrieval Judge
2. Identify specific query types where the judge performs well or poorly
3. Quantify the performance impact of using the judge
4. Guide future improvements to the judge implementation

The recommendations are based on the test results and suggest specific ways to improve the judge's effectiveness.

================
File: tests/retrieval_judge/README.md
================
# Retrieval Judge Testing

This directory contains tests and analysis tools for evaluating the effectiveness of the Retrieval Judge component in the Metis RAG system.

## Overview

The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by:

1. Analyzing queries to determine optimal retrieval parameters
2. Evaluating retrieved chunks for relevance
3. Refining queries when needed to improve retrieval precision
4. Optimizing context assembly for better response generation

These tests compare standard retrieval against retrieval with the judge enabled to measure the impact and effectiveness of the judge.

## Test Files

- `test_retrieval_judge_comparison.py`: Main test script that compares standard retrieval vs. retrieval with the judge enabled
- `analyze_retrieval_judge_results.py`: Analysis script that processes test results and generates visualizations and recommendations
- `run_tests.sh`: Shell script to run both the comparison test and analysis in sequence
- `run_tests.py`: Python script to run both the comparison test and analysis in sequence
- `IMPLEMENTATION_NOTES.md`: Technical details about the implementation and testing framework

## Directory Structure

- `data/`: Test documents used for retrieval testing
- `results/`: JSON output files from test runs
- `visualizations/`: Generated charts and graphs showing test results

## Running the Tests

### Prerequisites

Make sure you have the required dependencies installed:

```bash
pip install matplotlib numpy
```

### Option 1: Run the Complete Test Suite

The easiest way to run all tests is to use one of the provided runner scripts:

#### Using the Shell Script:

```bash
cd /path/to/Metis_RAG
./tests/retrieval_judge/run_tests.sh
```

#### Using the Python Script:

```bash
cd /path/to/Metis_RAG
python -m tests.retrieval_judge.run_tests
```

Both scripts will run the comparison tests and the analysis in sequence.

### Option 2: Run Tests Individually

#### Step 1: Run the Comparison Test

```bash
cd /path/to/Metis_RAG
python -m tests.retrieval_judge.test_retrieval_judge_comparison
```

This will:
- Create test documents in the `tests/retrieval_judge/data/` directory
- Process these documents and add them to a test vector store
- Run test queries with both standard retrieval and judge-enhanced retrieval
- Save the results to `tests/retrieval_judge/results/retrieval_judge_comparison_results.json`
- Save metrics to `tests/retrieval_judge/results/retrieval_judge_metrics.json`

#### Step 2: Analyze the Results

```bash
cd /path/to/Metis_RAG
python -m tests.retrieval_judge.analyze_retrieval_judge_results
```

This will:
- Load the test results and metrics
- Perform detailed analysis on the effectiveness of the Retrieval Judge
- Generate visualizations in the `tests/retrieval_judge/visualizations/` directory
- Generate recommendations for improving the Retrieval Judge
- Save the analysis report to `tests/retrieval_judge/results/retrieval_judge_analysis_report.json`

## Test Queries

The test includes queries of varying complexity:

1. **Simple factual queries**: Direct questions with clear answers in the documents
2. **Moderate complexity queries**: Questions requiring synthesis of information
3. **Complex analytical queries**: Questions requiring deeper understanding and inference
4. **Ambiguous queries**: Questions with unclear intent or multiple interpretations
5. **Multi-part queries**: Questions that combine multiple distinct information needs

## Analysis Metrics

The analysis evaluates the Retrieval Judge on several dimensions:

1. **Source relevance**: How relevant are the retrieved chunks to the query?
2. **Query refinement effectiveness**: How well does the judge improve ambiguous or complex queries?
3. **Context optimization**: How effectively does the judge select and order chunks?
4. **Performance impact**: What is the processing time overhead of using the judge?

## Customizing Tests

You can modify the test queries or add new ones by editing the `TEST_QUERIES` list in `test_retrieval_judge_comparison.py`.

To test with different documents, you can modify the document content variables (`MARKDOWN_CONTENT`, `PDF_CONTENT`, `TECHNICAL_SPECS_CONTENT`) or add new test documents.

## Interpreting Results

The analysis generates several visualizations to help interpret the results:

1. **Relevance by complexity**: Shows how the judge improves relevance across query types
2. **Source count comparison**: Compares the number of sources retrieved by each method
3. **Processing time comparison**: Shows the performance impact of using the judge
4. **Query refinement effectiveness**: Shows how well the judge handles ambiguous queries

The analysis also generates specific recommendations for improving the Retrieval Judge based on the test results.

## Implementation Details

For more information about the implementation and testing methodology, see the `IMPLEMENTATION_NOTES.md` file in this directory.

================
File: tests/retrieval_judge/run_tests.py
================
#!/usr/bin/env python3
"""
Python script to run the Retrieval Judge tests and analysis
"""

import os
import sys
import subprocess
import time
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("run_tests")

def run_command(command, description):
    """Run a command and log the output"""
    logger.info(f"Running: {description}")
    start_time = time.time()
    
    try:
        result = subprocess.run(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=True
        )
        
        logger.info(f"Command completed in {time.time() - start_time:.2f} seconds")
        logger.info(result.stdout)
        
        if result.stderr:
            logger.warning(result.stderr)
            
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Command failed with exit code {e.returncode}")
        logger.error(e.stdout)
        logger.error(e.stderr)
        return False

def main():
    """Main function to run the tests"""
    logger.info("===== Metis RAG Retrieval Judge Test Suite =====")
    
    # Get the base directory
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    os.chdir(base_dir)
    
    # Run the comparison tests
    logger.info("Step 1: Running comparison tests...")
    if not run_command(
        [sys.executable, "-m", "tests.retrieval_judge.test_retrieval_judge_comparison"],
        "Comparison tests"
    ):
        logger.error("Comparison tests failed. Exiting.")
        return 1
    
    # Run the analysis
    logger.info("Step 2: Running analysis...")
    if not run_command(
        [sys.executable, "-m", "tests.retrieval_judge.analyze_retrieval_judge_results"],
        "Analysis"
    ):
        logger.error("Analysis failed. Exiting.")
        return 1
    
    logger.info("===== Test Suite Completed Successfully =====")
    logger.info("Results are available in: tests/retrieval_judge/results/")
    logger.info("Visualizations are available in: tests/retrieval_judge/visualizations/")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

================
File: tests/retrieval_judge/run_tests.sh
================
#!/bin/bash
# Script to run the Retrieval Judge tests and analysis

echo "===== Metis RAG Retrieval Judge Test Suite ====="
echo ""

# Set the base directory
BASE_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$BASE_DIR/../.."

echo "Step 1: Running comparison tests..."
python -m tests.retrieval_judge.test_retrieval_judge_comparison
if [ $? -ne 0 ]; then
    echo "Error: Comparison tests failed."
    exit 1
fi

echo ""
echo "Step 2: Running analysis..."
python -m tests.retrieval_judge.analyze_retrieval_judge_results
if [ $? -ne 0 ]; then
    echo "Error: Analysis failed."
    exit 1
fi

echo ""
echo "===== Test Suite Completed Successfully ====="
echo "Results are available in: tests/retrieval_judge/results/"
echo "Visualizations are available in: tests/retrieval_judge/visualizations/"

================
File: tests/retrieval_judge/test_judge_edge_cases.py
================
#!/usr/bin/env python3
"""
Test script to evaluate the Retrieval Judge's handling of edge cases.
This script:
1. Tests the judge with ambiguous queries
2. Tests the judge with queries containing typos
3. Tests the judge with domain-specific queries
4. Tests the judge with multi-part queries
5. Tests the judge with very short and very long queries
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_judge_edge_cases")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test queries for different edge cases
EDGE_CASE_QUERIES = [
    # Ambiguous queries
    {
        "query": "How does the system work?",
        "type": "ambiguous",
        "description": "Very general query without specific focus"
    },
    {
        "query": "Tell me about the API",
        "type": "ambiguous",
        "description": "Ambiguous query about the API without specifying which aspect"
    },
    
    # Queries with typos
    {
        "query": "How does the vektor store work?",
        "type": "typo",
        "description": "Typo in 'vector'"
    },
    {
        "query": "What is the documnet procesing pipeline?",
        "type": "typo",
        "description": "Multiple typos in 'document processing'"
    },
    
    # Domain-specific queries
    {
        "query": "How does the system handle context window optimization?",
        "type": "domain-specific",
        "description": "Domain-specific query about LLM context windows"
    },
    {
        "query": "What embedding models are supported for semantic search?",
        "type": "domain-specific",
        "description": "Domain-specific query about embedding models"
    },
    
    # Multi-part queries
    {
        "query": "What are the chunking strategies and how do they affect retrieval performance?",
        "type": "multi-part",
        "description": "Multi-part query about chunking strategies and their impact"
    },
    {
        "query": "How does the authentication work and what endpoints are available for document management?",
        "type": "multi-part",
        "description": "Multi-part query about authentication and document management endpoints"
    },
    
    # Very short queries
    {
        "query": "RAG?",
        "type": "short",
        "description": "Very short query with just an acronym"
    },
    {
        "query": "Embeddings?",
        "type": "short",
        "description": "Very short query with just a technical term"
    },
    
    # Very long queries
    {
        "query": "I'm trying to understand how the Metis RAG system works in detail, particularly how the document processing pipeline handles different file formats, how the chunking strategies are implemented, how the vector store manages embeddings efficiently, and how the retrieval judge optimizes the retrieval process to improve the relevance of the results. Can you explain all of these aspects in detail?",
        "type": "long",
        "description": "Very long query with multiple questions"
    }
]

# Sample chunks for testing
SAMPLE_CHUNKS = [
    {
        "chunk_id": "chunk1",
        "content": "The vector store is responsible for storing document embeddings, efficient similarity search, and metadata filtering. It uses a HNSW index for approximate nearest neighbor search and supports filtering by metadata such as tags, folder, and document ID.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "vector store,embeddings,search",
            "folder": "/docs"
        },
        "distance": 0.2
    },
    {
        "chunk_id": "chunk2",
        "content": "The document processing pipeline handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction. It supports various file formats including PDF, DOCX, TXT, and Markdown.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "document processing,chunking,extraction",
            "folder": "/docs"
        },
        "distance": 0.3
    },
    {
        "chunk_id": "chunk3",
        "content": "The API layer is implemented using FastAPI and provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval. All API requests require authentication using JWT tokens.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "api,fastapi,endpoints",
            "folder": "/docs"
        },
        "distance": 0.4
    },
    {
        "chunk_id": "chunk4",
        "content": "The LLM integration component connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization. It supports various models including Llama 3, Gemma, and Mistral.",
        "metadata": {
            "document_id": "doc1",
            "filename": "technical_documentation.md",
            "tags": "llm,inference,context window",
            "folder": "/docs"
        },
        "distance": 0.5
    },
    {
        "chunk_id": "chunk5",
        "content": "The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by analyzing queries to determine optimal retrieval parameters, evaluating retrieved chunks for relevance, refining queries when needed to improve retrieval precision, and optimizing context assembly for better response generation.",
        "metadata": {
            "document_id": "doc2",
            "filename": "advanced_features.md",
            "tags": "retrieval judge,llm,optimization",
            "folder": "/docs/advanced"
        },
        "distance": 0.6
    }
]

async def test_query_analysis(retrieval_judge):
    """Test the judge's query analysis capabilities"""
    logger.info("Testing query analysis...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Analyze query
        start_time = time.time()
        analysis = await retrieval_judge.analyze_query(query)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Query complexity: {analysis.get('complexity', 'unknown')}")
        logger.info(f"Recommended parameters: {analysis.get('parameters', {})}")
        logger.info(f"Analysis time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "analysis": analysis,
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "query_analysis_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Query analysis results saved to {os.path.abspath(results_path)}")
    
    return results

async def test_chunk_evaluation(retrieval_judge):
    """Test the judge's chunk evaluation capabilities"""
    logger.info("Testing chunk evaluation...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Evaluate chunks
        start_time = time.time()
        evaluation = await retrieval_judge.evaluate_chunks(query, SAMPLE_CHUNKS)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Needs refinement: {evaluation.get('needs_refinement', False)}")
        logger.info(f"Relevance scores: {evaluation.get('relevance_scores', {})}")
        logger.info(f"Evaluation time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "evaluation": evaluation,
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "chunk_evaluation_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Chunk evaluation results saved to {os.path.abspath(results_path)}")
    
    return results

async def test_query_refinement(retrieval_judge):
    """Test the judge's query refinement capabilities"""
    logger.info("Testing query refinement...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Refine query
        start_time = time.time()
        refined_query = await retrieval_judge.refine_query(query, SAMPLE_CHUNKS)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Original query: {query}")
        logger.info(f"Refined query: {refined_query}")
        logger.info(f"Refinement time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "refined_query": refined_query,
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "query_refinement_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Query refinement results saved to {os.path.abspath(results_path)}")
    
    return results

async def test_context_optimization(retrieval_judge):
    """Test the judge's context optimization capabilities"""
    logger.info("Testing context optimization...")
    
    results = []
    
    for query_info in EDGE_CASE_QUERIES:
        query = query_info["query"]
        query_type = query_info["type"]
        description = query_info["description"]
        
        logger.info(f"\n=== Testing {query_type} query: {query} ===")
        logger.info(f"Description: {description}")
        
        # Optimize context
        start_time = time.time()
        optimized_chunks = await retrieval_judge.optimize_context(query, SAMPLE_CHUNKS)
        elapsed_time = time.time() - start_time
        
        # Log results
        logger.info(f"Original chunks: {len(SAMPLE_CHUNKS)}")
        logger.info(f"Optimized chunks: {len(optimized_chunks)}")
        logger.info(f"Optimization time: {elapsed_time:.2f}s")
        
        # Store results
        result = {
            "query": query,
            "type": query_type,
            "description": description,
            "original_chunk_count": len(SAMPLE_CHUNKS),
            "optimized_chunk_count": len(optimized_chunks),
            "optimized_chunk_ids": [chunk["chunk_id"] for chunk in optimized_chunks],
            "time": elapsed_time
        }
        
        results.append(result)
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "context_optimization_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Context optimization results saved to {os.path.abspath(results_path)}")
    
    return results

async def analyze_results(query_analysis_results, chunk_evaluation_results, query_refinement_results, context_optimization_results):
    """Analyze the test results"""
    logger.info("\n=== ANALYSIS OF RETRIEVAL JUDGE EDGE CASE HANDLING ===")
    
    # Group results by query type
    query_types = set(r["type"] for r in query_analysis_results)
    
    # Analyze each query type
    for query_type in query_types:
        logger.info(f"\n--- {query_type.upper()} QUERIES ---")
        
        # Filter results for this query type
        qa_results = [r for r in query_analysis_results if r["type"] == query_type]
        ce_results = [r for r in chunk_evaluation_results if r["type"] == query_type]
        qr_results = [r for r in query_refinement_results if r["type"] == query_type]
        co_results = [r for r in context_optimization_results if r["type"] == query_type]
        
        # Query analysis metrics
        complexities = [r["analysis"].get("complexity", "unknown") for r in qa_results]
        complexity_counts = {}
        for complexity in complexities:
            complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
        
        avg_qa_time = sum(r["time"] for r in qa_results) / len(qa_results) if qa_results else 0
        
        logger.info("Query Analysis:")
        logger.info(f"  Complexity distribution: {complexity_counts}")
        logger.info(f"  Average analysis time: {avg_qa_time:.2f}s")
        
        # Chunk evaluation metrics
        needs_refinement_count = sum(1 for r in ce_results if r["evaluation"].get("needs_refinement", False))
        avg_ce_time = sum(r["time"] for r in ce_results) / len(ce_results) if ce_results else 0
        
        logger.info("Chunk Evaluation:")
        logger.info(f"  Queries needing refinement: {needs_refinement_count}/{len(ce_results)}")
        logger.info(f"  Average evaluation time: {avg_ce_time:.2f}s")
        
        # Query refinement metrics
        avg_qr_time = sum(r["time"] for r in qr_results) / len(qr_results) if qr_results else 0
        
        # Calculate average change in query length
        original_lengths = [len(r["query"]) for r in qr_results]
        refined_lengths = [len(r["refined_query"]) for r in qr_results]
        avg_length_change = sum(refined - original for original, refined in zip(original_lengths, refined_lengths)) / len(qr_results) if qr_results else 0
        avg_length_change_percent = (sum(refined / original for original, refined in zip(original_lengths, refined_lengths) if original > 0) / len(qr_results) - 1) * 100 if qr_results else 0
        
        logger.info("Query Refinement:")
        logger.info(f"  Average refinement time: {avg_qr_time:.2f}s")
        logger.info(f"  Average change in query length: {avg_length_change:.2f} characters ({avg_length_change_percent:.2f}%)")
        
        # Context optimization metrics
        avg_co_time = sum(r["time"] for r in co_results) / len(co_results) if co_results else 0
        avg_chunk_reduction = sum(r["original_chunk_count"] - r["optimized_chunk_count"] for r in co_results) / len(co_results) if co_results else 0
        avg_chunk_reduction_percent = (1 - sum(r["optimized_chunk_count"] for r in co_results) / sum(r["original_chunk_count"] for r in co_results)) * 100 if sum(r["original_chunk_count"] for r in co_results) > 0 else 0
        
        logger.info("Context Optimization:")
        logger.info(f"  Average optimization time: {avg_co_time:.2f}s")
        logger.info(f"  Average chunk reduction: {avg_chunk_reduction:.2f} chunks ({avg_chunk_reduction_percent:.2f}%)")
    
    # Overall analysis
    logger.info("\n--- OVERALL ANALYSIS ---")
    
    # Query analysis metrics
    all_complexities = [r["analysis"].get("complexity", "unknown") for r in query_analysis_results]
    all_complexity_counts = {}
    for complexity in all_complexities:
        all_complexity_counts[complexity] = all_complexity_counts.get(complexity, 0) + 1
    
    avg_all_qa_time = sum(r["time"] for r in query_analysis_results) / len(query_analysis_results) if query_analysis_results else 0
    
    logger.info("Query Analysis:")
    logger.info(f"  Overall complexity distribution: {all_complexity_counts}")
    logger.info(f"  Overall average analysis time: {avg_all_qa_time:.2f}s")
    
    # Chunk evaluation metrics
    all_needs_refinement_count = sum(1 for r in chunk_evaluation_results if r["evaluation"].get("needs_refinement", False))
    avg_all_ce_time = sum(r["time"] for r in chunk_evaluation_results) / len(chunk_evaluation_results) if chunk_evaluation_results else 0
    
    logger.info("Chunk Evaluation:")
    logger.info(f"  Overall queries needing refinement: {all_needs_refinement_count}/{len(chunk_evaluation_results)}")
    logger.info(f"  Overall average evaluation time: {avg_all_ce_time:.2f}s")
    
    # Query refinement metrics
    avg_all_qr_time = sum(r["time"] for r in query_refinement_results) / len(query_refinement_results) if query_refinement_results else 0
    
    # Calculate overall average change in query length
    all_original_lengths = [len(r["query"]) for r in query_refinement_results]
    all_refined_lengths = [len(r["refined_query"]) for r in query_refinement_results]
    avg_all_length_change = sum(refined - original for original, refined in zip(all_original_lengths, all_refined_lengths)) / len(query_refinement_results) if query_refinement_results else 0
    avg_all_length_change_percent = (sum(refined / original for original, refined in zip(all_original_lengths, all_refined_lengths) if original > 0) / len(query_refinement_results) - 1) * 100 if query_refinement_results else 0
    
    logger.info("Query Refinement:")
    logger.info(f"  Overall average refinement time: {avg_all_qr_time:.2f}s")
    logger.info(f"  Overall average change in query length: {avg_all_length_change:.2f} characters ({avg_all_length_change_percent:.2f}%)")
    
    # Context optimization metrics
    avg_all_co_time = sum(r["time"] for r in context_optimization_results) / len(context_optimization_results) if context_optimization_results else 0
    avg_all_chunk_reduction = sum(r["original_chunk_count"] - r["optimized_chunk_count"] for r in context_optimization_results) / len(context_optimization_results) if context_optimization_results else 0
    avg_all_chunk_reduction_percent = (1 - sum(r["optimized_chunk_count"] for r in context_optimization_results) / sum(r["original_chunk_count"] for r in context_optimization_results)) * 100 if sum(r["original_chunk_count"] for r in context_optimization_results) > 0 else 0
    
    logger.info("Context Optimization:")
    logger.info(f"  Overall average optimization time: {avg_all_co_time:.2f}s")
    logger.info(f"  Overall average chunk reduction: {avg_all_chunk_reduction:.2f} chunks ({avg_all_chunk_reduction_percent:.2f}%)")
    
    # Identify query types where judge performs best/worst
    type_performance = {}
    for query_type in query_types:
        # Filter results for this query type
        qa_results = [r for r in query_analysis_results if r["type"] == query_type]
        ce_results = [r for r in chunk_evaluation_results if r["type"] == query_type]
        qr_results = [r for r in query_refinement_results if r["type"] == query_type]
        co_results = [r for r in context_optimization_results if r["type"] == query_type]
        
        # Calculate performance metrics
        needs_refinement_rate = sum(1 for r in ce_results if r["evaluation"].get("needs_refinement", False)) / len(ce_results) if ce_results else 0
        avg_length_change_percent = (sum(len(r["refined_query"]) / len(r["query"]) for r in qr_results if len(r["query"]) > 0) / len(qr_results) - 1) * 100 if qr_results else 0
        avg_chunk_reduction_percent = (1 - sum(r["optimized_chunk_count"] for r in co_results) / sum(r["original_chunk_count"] for r in co_results)) * 100 if sum(r["original_chunk_count"] for r in co_results) > 0 else 0
        
        type_performance[query_type] = {
            "needs_refinement_rate": needs_refinement_rate,
            "avg_length_change_percent": avg_length_change_percent,
            "avg_chunk_reduction_percent": avg_chunk_reduction_percent
        }
    
    # Sort by refinement rate (lower is better)
    refinement_sorted = sorted(type_performance.items(), key=lambda x: x[1]["needs_refinement_rate"])
    
    logger.info("\nQuery types by refinement rate (lower is better):")
    for query_type, metrics in refinement_sorted:
        logger.info(f"  {query_type}: {metrics['needs_refinement_rate']*100:.2f}% need refinement")
    
    # Sort by chunk reduction (higher is better)
    chunk_reduction_sorted = sorted(type_performance.items(), key=lambda x: x[1]["avg_chunk_reduction_percent"], reverse=True)
    
    logger.info("\nQuery types by chunk reduction (higher is better):")
    for query_type, metrics in chunk_reduction_sorted:
        logger.info(f"  {query_type}: {metrics['avg_chunk_reduction_percent']:.2f}% chunk reduction")
    
    # Save analysis to file
    analysis = {
        "by_type": {
            query_type: {
                "query_analysis": {
                    "complexity_distribution": {complexity: sum(1 for r in query_analysis_results if r["type"] == query_type and r["analysis"].get("complexity") == complexity) for complexity in set(all_complexities)},
                    "avg_time": sum(r["time"] for r in query_analysis_results if r["type"] == query_type) / len([r for r in query_analysis_results if r["type"] == query_type]) if [r for r in query_analysis_results if r["type"] == query_type] else 0
                },
                "chunk_evaluation": {
                    "needs_refinement_rate": sum(1 for r in chunk_evaluation_results if r["type"] == query_type and r["evaluation"].get("needs_refinement", False)) / len([r for r in chunk_evaluation_results if r["type"] == query_type]) if [r for r in chunk_evaluation_results if r["type"] == query_type] else 0,
                    "avg_time": sum(r["time"] for r in chunk_evaluation_results if r["type"] == query_type) / len([r for r in chunk_evaluation_results if r["type"] == query_type]) if [r for r in chunk_evaluation_results if r["type"] == query_type] else 0
                },
                "query_refinement": {
                    "avg_length_change_percent": (sum(len(r["refined_query"]) / len(r["query"]) for r in query_refinement_results if r["type"] == query_type and len(r["query"]) > 0) / len([r for r in query_refinement_results if r["type"] == query_type]) - 1) * 100 if [r for r in query_refinement_results if r["type"] == query_type] else 0,
                    "avg_time": sum(r["time"] for r in query_refinement_results if r["type"] == query_type) / len([r for r in query_refinement_results if r["type"] == query_type]) if [r for r in query_refinement_results if r["type"] == query_type] else 0
                },
                "context_optimization": {
                    "avg_chunk_reduction_percent": (1 - sum(r["optimized_chunk_count"] for r in context_optimization_results if r["type"] == query_type) / sum(r["original_chunk_count"] for r in context_optimization_results if r["type"] == query_type)) * 100 if sum(r["original_chunk_count"] for r in context_optimization_results if r["type"] == query_type) > 0 else 0,
                    "avg_time": sum(r["time"] for r in context_optimization_results if r["type"] == query_type) / len([r for r in context_optimization_results if r["type"] == query_type]) if [r for r in context_optimization_results if r["type"] == query_type] else 0
                }
            }
            for query_type in query_types
        },
        "overall": {
            "query_analysis": {
                "complexity_distribution": all_complexity_counts,
                "avg_time": avg_all_qa_time
            },
            "chunk_evaluation": {
                "needs_refinement_rate": all_needs_refinement_count / len(chunk_evaluation_results) if chunk_evaluation_results else 0,
                "avg_time": avg_all_ce_time
            },
            "query_refinement": {
                "avg_length_change_percent": avg_all_length_change_percent,
                "avg_time": avg_all_qr_time
            },
            "context_optimization": {
                "avg_chunk_reduction_percent": avg_all_chunk_reduction_percent,
                "avg_time": avg_all_co_time
            }
        },
        "type_performance_ranking": {
            "by_refinement_rate": [{"type": t, "rate": m["needs_refinement_rate"]} for t, m in refinement_sorted],
            "by_chunk_reduction": [{"type": t, "reduction": m["avg_chunk_reduction_percent"]} for t, m in chunk_reduction_sorted]
        }
    }
    
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    analysis_path = os.path.join(results_dir, "judge_edge_case_analysis.json")
    
    with open(analysis_path, "w") as f:
        json.dump(analysis, f, indent=2)
    
    logger.info(f"Edge case analysis saved to {os.path.abspath(analysis_path)}")
    
    return analysis

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge edge case tests...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create OllamaClient
        ollama_client = OllamaClient()
        
        # Create Retrieval Judge
        retrieval_judge = RetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
        
        # Test query analysis
        query_analysis_results = await test_query_analysis(retrieval_judge)
        
        # Test chunk evaluation
        chunk_evaluation_results = await test_chunk_evaluation(retrieval_judge)
        
        # Test query refinement
        query_refinement_results = await test_query_refinement(retrieval_judge)
        
        # Test context optimization
        context_optimization_results = await test_context_optimization(retrieval_judge)
        
        # Analyze results
        analysis = await analyze_results(
            query_analysis_results,
            chunk_evaluation_results,
            query_refinement_results,
            context_optimization_results
        )
        
        logger.info("Retrieval Judge edge case tests completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge edge case tests: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_performance_analysis.py
================
#!/usr/bin/env python3
"""
Test script to analyze why the judge-enhanced retrieval is faster than standard retrieval.
This script:
1. Instruments both retrieval methods with detailed timing
2. Analyzes the time spent in each component
3. Investigates caching effects and other optimizations
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_performance_analysis")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test queries
TEST_QUERIES = [
    "What is the architecture of Metis RAG?",
    "What are the components of the RAG engine?",
    "How does the vector store work?",
    "What is the role of the LLM integration component?",
    "How does the document processing pipeline work?"
]

class InstrumentedVectorStore(VectorStore):
    """Instrumented version of VectorStore that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.query_cache = {}
    
    async def search(self, query: str, top_k: int = 5, filter_criteria: Optional[Dict[str, Any]] = None):
        """Instrumented version of search"""
        # Check cache
        cache_key = f"{query}_{top_k}_{json.dumps(filter_criteria) if filter_criteria else 'none'}"
        if cache_key in self.query_cache:
            self.cache_hits += 1
            logger.info(f"Vector store cache hit for query: {query[:30]}...")
            return self.query_cache[cache_key]
        
        self.cache_misses += 1
        logger.info(f"Vector store cache miss for query: {query[:30]}...")
        
        # Measure search time
        start_time = time.time()
        result = await super().search(query, top_k, filter_criteria)
        elapsed = time.time() - start_time
        
        # Store timing
        if "search" not in self.timings:
            self.timings["search"] = []
        self.timings["search"].append(elapsed)
        
        # Cache result
        self.query_cache[cache_key] = result
        
        logger.info(f"Vector store search took {elapsed:.2f}s")
        return result

class InstrumentedOllamaClient(OllamaClient):
    """Instrumented version of OllamaClient that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.generate_cache = {}
        self.embedding_cache = {}
    
    async def generate(self, prompt: str, model: Optional[str] = None, system_prompt: Optional[str] = None, 
                      stream: bool = False, parameters: Optional[Dict[str, Any]] = None):
        """Instrumented version of generate"""
        # Check cache for non-streaming requests
        if not stream:
            cache_key = f"{prompt}_{model}_{system_prompt}_{json.dumps(parameters) if parameters else 'none'}"
            if cache_key in self.generate_cache:
                self.cache_hits += 1
                logger.info(f"LLM generate cache hit for prompt: {prompt[:30]}...")
                return self.generate_cache[cache_key]
            
            self.cache_misses += 1
            logger.info(f"LLM generate cache miss for prompt: {prompt[:30]}...")
        
        # Measure generate time
        start_time = time.time()
        result = await super().generate(prompt, model, system_prompt, stream, parameters)
        elapsed = time.time() - start_time
        
        # Store timing
        if "generate" not in self.timings:
            self.timings["generate"] = []
        self.timings["generate"].append(elapsed)
        
        # Cache result for non-streaming requests
        if not stream:
            self.generate_cache[cache_key] = result
        
        logger.info(f"LLM generate took {elapsed:.2f}s")
        return result
    
    async def create_embedding(self, text: str, model: Optional[str] = None):
        """Instrumented version of create_embedding"""
        # Check cache
        cache_key = f"{text}_{model}"
        if cache_key in self.embedding_cache:
            self.cache_hits += 1
            logger.info(f"Embedding cache hit for text: {text[:30]}...")
            return self.embedding_cache[cache_key]
        
        self.cache_misses += 1
        logger.info(f"Embedding cache miss for text: {text[:30]}...")
        
        # Measure embedding time
        start_time = time.time()
        result = await super().create_embedding(text, model)
        elapsed = time.time() - start_time
        
        # Store timing
        if "embedding" not in self.timings:
            self.timings["embedding"] = []
        self.timings["embedding"].append(elapsed)
        
        # Cache result
        self.embedding_cache[cache_key] = result
        
        logger.info(f"Embedding creation took {elapsed:.2f}s")
        return result

class InstrumentedRetrievalJudge(RetrievalJudge):
    """Instrumented version of RetrievalJudge that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """Instrumented version of analyze_query"""
        start_time = time.time()
        result = await super().analyze_query(query)
        elapsed = time.time() - start_time
        
        if "analyze_query" not in self.timings:
            self.timings["analyze_query"] = []
        self.timings["analyze_query"].append(elapsed)
        
        logger.info(f"analyze_query took {elapsed:.2f}s")
        return result
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Instrumented version of evaluate_chunks"""
        start_time = time.time()
        result = await super().evaluate_chunks(query, chunks)
        elapsed = time.time() - start_time
        
        if "evaluate_chunks" not in self.timings:
            self.timings["evaluate_chunks"] = []
        self.timings["evaluate_chunks"].append(elapsed)
        
        logger.info(f"evaluate_chunks took {elapsed:.2f}s")
        return result
    
    async def refine_query(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """Instrumented version of refine_query"""
        start_time = time.time()
        result = await super().refine_query(query, chunks)
        elapsed = time.time() - start_time
        
        if "refine_query" not in self.timings:
            self.timings["refine_query"] = []
        self.timings["refine_query"].append(elapsed)
        
        logger.info(f"refine_query took {elapsed:.2f}s")
        return result
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Instrumented version of optimize_context"""
        start_time = time.time()
        result = await super().optimize_context(query, chunks)
        elapsed = time.time() - start_time
        
        if "optimize_context" not in self.timings:
            self.timings["optimize_context"] = []
        self.timings["optimize_context"].append(elapsed)
        
        logger.info(f"optimize_context took {elapsed:.2f}s")
        return result

class InstrumentedRAGEngine(RAGEngine):
    """Instrumented version of RAGEngine that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def _enhanced_retrieval(self, *args, **kwargs):
        """Instrumented version of _enhanced_retrieval"""
        start_time = time.time()
        result = await super()._enhanced_retrieval(*args, **kwargs)
        elapsed = time.time() - start_time
        
        if "enhanced_retrieval" not in self.timings:
            self.timings["enhanced_retrieval"] = []
        self.timings["enhanced_retrieval"].append(elapsed)
        
        logger.info(f"enhanced_retrieval took {elapsed:.2f}s")
        return result

async def create_test_document():
    """Create a test document for RAG testing"""
    logger.info("Creating test document...")
    
    # Create directory for test document
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create document file
    doc_path = os.path.join(test_dir, "performance_test_document.md")
    with open(doc_path, "w") as f:
        f.write(TEST_DOCUMENT)
    
    logger.info(f"Created test document at {os.path.abspath(doc_path)}")
    return doc_path

async def process_document(doc_path):
    """Process the test document and add it to the vector store"""
    logger.info("Processing test document...")
    
    # Read file content
    with open(doc_path, "r") as f:
        doc_content = f.read()
    
    # Create Document object
    document = Document(
        id=str(uuid.uuid4()),
        filename="performance_test_document.md",
        content=doc_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    # Create a vector store
    test_persist_dir = os.path.join("tests", "retrieval_judge", "performance_test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = InstrumentedVectorStore(persist_directory=test_persist_dir)
    
    # Create chunks
    document.chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=doc_content,
            metadata={
                "index": 0,
                "source": doc_path,
                "document_id": document.id
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(document)
    
    logger.info(f"Added document {document.id} to vector store")
    
    return document, vector_store

async def run_performance_tests(vector_store):
    """Run performance tests for standard and judge-enhanced retrieval"""
    logger.info("Running performance tests...")
    
    # Create instrumented OllamaClient
    ollama_client = InstrumentedOllamaClient()
    
    # Create instrumented Retrieval Judge
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = InstrumentedRetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Create engines
    rag_engine_with_judge = InstrumentedRAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    rag_engine_standard = InstrumentedRAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    results = []
    
    # Run each test query twice - first with standard retrieval, then with judge
    for query in TEST_QUERIES:
        logger.info(f"\n=== Testing query: {query} ===")
        
        # Test with standard retrieval
        logger.info("Running with standard retrieval...")
        standard_start_time = time.time()
        standard_response = await rag_engine_standard.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        standard_total_time = time.time() - standard_start_time
        
        # Test with judge-enhanced retrieval
        logger.info("Running with judge-enhanced retrieval...")
        judge_start_time = time.time()
        judge_response = await rag_engine_with_judge.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        judge_total_time = time.time() - judge_start_time
        
        # Extract results
        standard_answer = standard_response.get("answer", "")
        standard_sources = standard_response.get("sources", [])
        
        judge_answer = judge_response.get("answer", "")
        judge_sources = judge_response.get("sources", [])
        
        # Log results
        logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_total_time:.2f}s")
        logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_total_time:.2f}s")
        
        # Collect component timings
        standard_component_timings = {}
        judge_component_timings = {}
        
        # Vector store timings
        if hasattr(vector_store, 'timings'):
            for component, times in vector_store.timings.items():
                standard_component_timings[f"vector_store_{component}"] = times[0] if len(times) > 0 else 0
                judge_component_timings[f"vector_store_{component}"] = times[1] if len(times) > 1 else 0
        
        # Ollama client timings
        if hasattr(ollama_client, 'timings'):
            for component, times in ollama_client.timings.items():
                # First half of times are from standard retrieval, second half from judge
                standard_times = [t for i, t in enumerate(times) if i < len(times)//2]
                judge_times = [t for i, t in enumerate(times) if i >= len(times)//2]
                
                standard_component_timings[f"ollama_{component}"] = sum(standard_times)
                judge_component_timings[f"ollama_{component}"] = sum(judge_times)
        
        # Retrieval judge timings
        if hasattr(retrieval_judge, 'timings'):
            for component, times in retrieval_judge.timings.items():
                judge_component_timings[f"judge_{component}"] = sum(times)
        
        # RAG engine timings
        if hasattr(rag_engine_with_judge, 'timings'):
            for component, times in rag_engine_with_judge.timings.items():
                judge_component_timings[f"rag_engine_{component}"] = sum(times)
        
        if hasattr(rag_engine_standard, 'timings'):
            for component, times in rag_engine_standard.timings.items():
                standard_component_timings[f"rag_engine_{component}"] = sum(times)
        
        # Cache statistics
        cache_stats = {
            "vector_store_cache_hits": vector_store.cache_hits,
            "vector_store_cache_misses": vector_store.cache_misses,
            "ollama_cache_hits": ollama_client.cache_hits,
            "ollama_cache_misses": ollama_client.cache_misses
        }
        
        # Store results
        results.append({
            "query": query,
            "standard": {
                "total_time": standard_total_time,
                "sources_count": len(standard_sources),
                "component_timings": standard_component_timings
            },
            "judge": {
                "total_time": judge_total_time,
                "sources_count": len(judge_sources),
                "component_timings": judge_component_timings
            },
            "cache_stats": cache_stats
        })
        
        # Reset timings for next run
        vector_store.timings = {}
        ollama_client.timings = {}
        retrieval_judge.timings = {}
        rag_engine_with_judge.timings = {}
        rag_engine_standard.timings = {}
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "performance_analysis_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Performance analysis results saved to {os.path.abspath(results_path)}")
    
    return results

async def analyze_performance_results(results):
    """Analyze the performance test results"""
    logger.info("\n=== PERFORMANCE ANALYSIS ===")
    
    # Calculate averages
    avg_standard_time = sum(r["standard"]["total_time"] for r in results) / len(results)
    avg_judge_time = sum(r["judge"]["total_time"] for r in results) / len(results)
    
    logger.info(f"Average standard retrieval time: {avg_standard_time:.2f}s")
    logger.info(f"Average judge-enhanced retrieval time: {avg_judge_time:.2f}s")
    logger.info(f"Time difference: {((avg_judge_time - avg_standard_time) / avg_standard_time * 100):.2f}%")
    
    # Analyze component timings
    standard_component_times = {}
    judge_component_times = {}
    
    for r in results:
        for component, time_value in r["standard"]["component_timings"].items():
            if component not in standard_component_times:
                standard_component_times[component] = []
            standard_component_times[component].append(time_value)
        
        for component, time_value in r["judge"]["component_timings"].items():
            if component not in judge_component_times:
                judge_component_times[component] = []
            judge_component_times[component].append(time_value)
    
    logger.info("\nStandard retrieval component timing averages:")
    for component, times in standard_component_times.items():
        avg_time = sum(times) / len(times)
        logger.info(f"  {component}: {avg_time:.2f}s")
    
    logger.info("\nJudge-enhanced retrieval component timing averages:")
    for component, times in judge_component_times.items():
        avg_time = sum(times) / len(times)
        logger.info(f"  {component}: {avg_time:.2f}s")
    
    # Analyze cache statistics
    total_vector_store_hits = sum(r["cache_stats"]["vector_store_cache_hits"] for r in results)
    total_vector_store_misses = sum(r["cache_stats"]["vector_store_cache_misses"] for r in results)
    total_ollama_hits = sum(r["cache_stats"]["ollama_cache_hits"] for r in results)
    total_ollama_misses = sum(r["cache_stats"]["ollama_cache_misses"] for r in results)
    
    logger.info("\nCache statistics:")
    logger.info(f"  Vector store cache hits: {total_vector_store_hits}")
    logger.info(f"  Vector store cache misses: {total_vector_store_misses}")
    logger.info(f"  Vector store cache hit rate: {total_vector_store_hits/(total_vector_store_hits+total_vector_store_misses)*100:.2f}%")
    logger.info(f"  Ollama cache hits: {total_ollama_hits}")
    logger.info(f"  Ollama cache misses: {total_ollama_misses}")
    logger.info(f"  Ollama cache hit rate: {total_ollama_hits/(total_ollama_hits+total_ollama_misses)*100:.2f}%")
    
    # Analyze potential reasons for performance difference
    logger.info("\nPotential reasons for performance difference:")
    
    # Check if judge retrieves fewer chunks
    avg_standard_sources = sum(r["standard"]["sources_count"] for r in results) / len(results)
    avg_judge_sources = sum(r["judge"]["sources_count"] for r in results) / len(results)
    
    if avg_judge_sources < avg_standard_sources:
        logger.info(f"  - Judge retrieves fewer sources ({avg_judge_sources:.2f} vs {avg_standard_sources:.2f})")
    
    # Check if judge has better cache utilization
    if total_ollama_hits > 0:
        logger.info(f"  - Judge benefits from LLM caching ({total_ollama_hits} cache hits)")
    
    if total_vector_store_hits > 0:
        logger.info(f"  - Judge benefits from vector store caching ({total_vector_store_hits} cache hits)")
    
    # Check if judge optimizes query parameters
    if "judge_analyze_query" in judge_component_times:
        logger.info(f"  - Judge optimizes query parameters (analyze_query: {sum(judge_component_times['judge_analyze_query'])/len(judge_component_times['judge_analyze_query']):.2f}s)")
    
    # Save analysis to file
    analysis = {
        "avg_standard_time": avg_standard_time,
        "avg_judge_time": avg_judge_time,
        "time_difference_percent": ((avg_judge_time - avg_standard_time) / avg_standard_time * 100),
        "standard_component_averages": {component: sum(times) / len(times) for component, times in standard_component_times.items()},
        "judge_component_averages": {component: sum(times) / len(times) for component, times in judge_component_times.items()},
        "cache_statistics": {
            "vector_store_cache_hits": total_vector_store_hits,
            "vector_store_cache_misses": total_vector_store_misses,
            "vector_store_cache_hit_rate": total_vector_store_hits/(total_vector_store_hits+total_vector_store_misses)*100 if (total_vector_store_hits+total_vector_store_misses) > 0 else 0,
            "ollama_cache_hits": total_ollama_hits,
            "ollama_cache_misses": total_ollama_misses,
            "ollama_cache_hit_rate": total_ollama_hits/(total_ollama_hits+total_ollama_misses)*100 if (total_ollama_hits+total_ollama_misses) > 0 else 0
        },
        "avg_standard_sources": avg_standard_sources,
        "avg_judge_sources": avg_judge_sources
    }
    
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    analysis_path = os.path.join(results_dir, "performance_analysis.json")
    
    with open(analysis_path, "w") as f:
        json.dump(analysis, f, indent=2)
    
    logger.info(f"Performance analysis saved to {os.path.abspath(analysis_path)}")
    
    return analysis

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge performance analysis...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test document
        doc_path = await create_test_document()
        
        # Process document
        document, vector_store = await process_document(doc_path)
        
        # Run performance tests
        results = await run_performance_tests(vector_store)
        
        # Analyze results
        analysis = await analyze_performance_results(results)
        
        logger.info("Retrieval Judge performance analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge performance analysis: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_retrieval_judge_comparison.py
================
#!/usr/bin/env python3
"""
Test script to compare standard retrieval vs. retrieval with the Retrieval Judge enabled.
This script:
1. Creates test documents and processes them
2. Runs test queries with both standard retrieval and judge-enhanced retrieval
3. Compares the results to evaluate the judge's effectiveness
4. Analyzes areas for improvement
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_retrieval_judge_comparison")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content - using the same content as test_rag_retrieval.py
MARKDOWN_CONTENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

PDF_CONTENT = """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
"""

# Additional test document with more technical content
TECHNICAL_SPECS_CONTENT = """# Product Specifications

## System Requirements

The Metis RAG system requires the following minimum specifications:
- CPU: 4 cores, 2.5GHz or higher
- RAM: 16GB minimum, 32GB recommended
- Storage: 100GB SSD
- Operating System: Ubuntu 22.04 LTS, Windows Server 2019, or macOS 12+
- Network: 100Mbps internet connection

## API Reference

### Authentication

All API requests require authentication using JWT tokens. To obtain a token:

```
POST /api/auth/token
{
  "username": "your_username",
  "password": "your_password"
}
```

The response will include an access token valid for 24 hours.

### Document Management

#### Upload Document

```
POST /api/documents/upload
Content-Type: multipart/form-data
Authorization: Bearer <token>

Form fields:
- file: The document file
- tags: Comma-separated tags (optional)
- folder: Target folder path (optional)
```

#### List Documents

```
GET /api/documents/list
Authorization: Bearer <token>
```

Optional query parameters:
- folder: Filter by folder
- tags: Filter by tags (comma-separated)
- page: Page number (default: 1)
- limit: Items per page (default: 20)

### Chat API

#### Create Chat Session

```
POST /api/chat/sessions
Authorization: Bearer <token>
{
  "title": "Optional chat title"
}
```

#### Send Message

```
POST /api/chat/messages
Authorization: Bearer <token>
{
  "session_id": "chat_session_id",
  "content": "Your message here",
  "use_rag": true
}
```

## Performance Benchmarks

The system has been benchmarked with the following results:
- Document processing: 5 pages/second
- Vector search latency: <50ms for 10k documents
- End-to-end query response time: <2 seconds
- Maximum documents: 100,000
- Maximum vector store size: 10GB
"""

# Test queries of varying complexity
TEST_QUERIES = [
    # Simple factual queries
    {
        "query": "What is the architecture of Metis RAG?",
        "complexity": "simple",
        "description": "Simple factual query about architecture"
    },
    {
        "query": "What was the revenue reported in Q1 2025?",
        "complexity": "simple",
        "description": "Simple factual query about revenue"
    },
    
    # Moderate complexity queries
    {
        "query": "What are the components of the RAG engine and how do they work together?",
        "complexity": "moderate",
        "description": "Moderate complexity query requiring synthesis of multiple components"
    },
    {
        "query": "Compare the financial performance metrics from the quarterly report.",
        "complexity": "moderate",
        "description": "Moderate complexity query requiring comparison and analysis"
    },
    
    # Complex analytical queries
    {
        "query": "How does the document processing pipeline handle different file types and what are the implications for retrieval quality?",
        "complexity": "complex",
        "description": "Complex query requiring deep technical understanding and inference"
    },
    {
        "query": "Based on the quarterly report, what strategic initiatives might have the highest ROI and why?",
        "complexity": "complex",
        "description": "Complex query requiring business analysis and inference"
    },
    
    # Ambiguous queries
    {
        "query": "What are the system requirements?",
        "complexity": "ambiguous",
        "description": "Ambiguous query that could refer to different aspects"
    },
    {
        "query": "How does the API work?",
        "complexity": "ambiguous",
        "description": "Ambiguous query with broad scope"
    },
    
    # Multi-part queries
    {
        "query": "What is the vector store responsible for and what are the minimum RAM requirements for the system?",
        "complexity": "multi-part",
        "description": "Multi-part query combining two different topics"
    },
    {
        "query": "Explain the authentication process for the API and list the strategic initiatives for Q2.",
        "complexity": "multi-part",
        "description": "Multi-part query requiring information from different documents"
    }
]

async def create_test_documents():
    """Create test documents for RAG testing"""
    logger.info("Creating test documents...")
    
    # Create directories for test documents
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create Markdown document
    markdown_path = os.path.join(test_dir, "technical_documentation.md")
    with open(markdown_path, "w") as f:
        f.write(MARKDOWN_CONTENT)
    
    # Create PDF-like text document
    pdf_path = os.path.join(test_dir, "quarterly_report.txt")
    with open(pdf_path, "w") as f:
        f.write(PDF_CONTENT)
    
    # Create technical specs document
    specs_path = os.path.join(test_dir, "product_specifications.md")
    with open(specs_path, "w") as f:
        f.write(TECHNICAL_SPECS_CONTENT)
    
    logger.info(f"Created test documents in {os.path.abspath(test_dir)}")
    return markdown_path, pdf_path, specs_path

async def process_documents(vector_store, markdown_path, pdf_path, specs_path):
    """Process the test documents and add them to the vector store"""
    logger.info("Processing test documents...")
    
    # Read file contents
    with open(markdown_path, "r") as f:
        markdown_content = f.read()
    
    with open(pdf_path, "r") as f:
        pdf_content = f.read()
    
    with open(specs_path, "r") as f:
        specs_content = f.read()
    
    # Create Document objects
    markdown_doc = Document(
        id=str(uuid.uuid4()),
        filename="technical_documentation.md",
        content=markdown_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    pdf_doc = Document(
        id=str(uuid.uuid4()),
        filename="quarterly_report.txt",
        content=pdf_content,
        tags=["business", "report", "quarterly"],
        folder="/test"
    )
    
    specs_doc = Document(
        id=str(uuid.uuid4()),
        filename="product_specifications.md",
        content=specs_content,
        tags=["technical", "specifications", "api"],
        folder="/test"
    )
    
    # Create a custom vector store to handle tags properly
    class CustomVectorStore(VectorStore):
        async def add_document(self, document: Document) -> None:
            """Override to fix tags handling"""
            try:
                logger.info(f"Adding document {document.id} to vector store")
                
                # Make sure we have an Ollama client
                if self.ollama_client is None:
                    self.ollama_client = OllamaClient()
                
                # Prepare chunks for batch processing
                chunks_to_embed = [chunk for chunk in document.chunks if not chunk.embedding]
                chunk_contents = [chunk.content for chunk in chunks_to_embed]
                
                # Create embeddings in batch if possible
                if chunk_contents:
                    try:
                        # Batch embedding
                        embeddings = await self._batch_create_embeddings(chunk_contents)
                        
                        # Assign embeddings to chunks
                        for i, chunk in enumerate(chunks_to_embed):
                            chunk.embedding = embeddings[i]
                    except Exception as batch_error:
                        logger.warning(f"Batch embedding failed: {str(batch_error)}. Falling back to sequential embedding.")
                        # Fall back to sequential embedding
                        for chunk in chunks_to_embed:
                            chunk.embedding = await self.ollama_client.create_embedding(
                                text=chunk.content,
                                model=self.embedding_model
                            )
                
                # Add chunks to the collection
                for chunk in document.chunks:
                    if not chunk.embedding:
                        logger.warning(f"Chunk {chunk.id} has no embedding, skipping")
                        continue
                        
                    # Convert tags to string to avoid ChromaDB error
                    tags_str = ",".join(document.tags) if document.tags else ""
                    
                    self.collection.add(
                        ids=[chunk.id],
                        embeddings=[chunk.embedding],
                        documents=[chunk.content],
                        metadatas=[{
                            "document_id": document.id,
                            "chunk_index": chunk.metadata.get("index", 0),
                            "filename": document.filename,
                            "tags": tags_str,  # Use string instead of list
                            "folder": document.folder,
                            **chunk.metadata
                        }]
                    )
                
                logger.info(f"Added {len(document.chunks)} chunks to vector store for document {document.id}")
            except Exception as e:
                logger.error(f"Error adding document {document.id} to vector store: {str(e)}")
                raise
    
    # Use our custom vector store with a unique persist directory for this test
    test_persist_dir = os.path.join("tests", "retrieval_judge", "test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = CustomVectorStore(persist_directory=test_persist_dir)
    
    # Create chunks for documents
    # For a more realistic test, we'll split the documents into smaller chunks
    
    # Helper function to create chunks
    def create_chunks(content, doc_id, source_path, chunk_size=500, overlap=100):
        chunks = []
        # Simple chunking by splitting the content
        words = content.split()
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            if len(chunk_words) < 50:  # Skip very small chunks
                continue
                
            chunk_content = " ".join(chunk_words)
            chunks.append(
                Chunk(
                    id=str(uuid.uuid4()),
                    content=chunk_content,
                    metadata={
                        "index": len(chunks),
                        "source": source_path,
                        "document_id": doc_id
                    }
                )
            )
        
        return chunks
    
    # Create chunks for each document
    markdown_doc.chunks = create_chunks(markdown_content, markdown_doc.id, markdown_path)
    pdf_doc.chunks = create_chunks(pdf_content, pdf_doc.id, pdf_path)
    specs_doc.chunks = create_chunks(specs_content, specs_doc.id, specs_path)
    
    # Add documents to vector store
    await vector_store.add_document(markdown_doc)
    await vector_store.add_document(pdf_doc)
    await vector_store.add_document(specs_doc)
    
    logger.info(f"Added documents to vector store: {markdown_doc.id}, {pdf_doc.id}, {specs_doc.id}")
    logger.info(f"Total chunks: {len(markdown_doc.chunks) + len(pdf_doc.chunks) + len(specs_doc.chunks)}")
    
    return markdown_doc, pdf_doc, specs_doc, vector_store

async def run_comparison_tests(vector_store):
    """Run comparison tests between standard retrieval and judge-enhanced retrieval"""
    logger.info("Running comparison tests...")
    
    # Create two RAG engines - one with Retrieval Judge and one without
    ollama_client = OllamaClient()
    
    # Create Retrieval Judge with the specified model
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = RetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Engine with Retrieval Judge
    rag_engine_with_judge = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    # Engine without Retrieval Judge
    rag_engine_standard = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    results = []
    
    # Run each test query with both engines
    for query_info in TEST_QUERIES:
        query = query_info["query"]
        complexity = query_info["complexity"]
        description = query_info["description"]
        
        logger.info(f"Testing query: {query}")
        logger.info(f"Complexity: {complexity}")
        
        # Test with standard retrieval
        logger.info("Running with standard retrieval...")
        start_time_standard = time.time()
        standard_response = await rag_engine_standard.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        standard_time = time.time() - start_time_standard
        
        # Test with judge-enhanced retrieval
        logger.info("Running with judge-enhanced retrieval...")
        start_time_judge = time.time()
        judge_response = await rag_engine_with_judge.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        judge_time = time.time() - start_time_judge
        
        # Extract results
        standard_answer = standard_response.get("answer", "")
        standard_sources = standard_response.get("sources", [])
        
        judge_answer = judge_response.get("answer", "")
        judge_sources = judge_response.get("sources", [])
        
        # Log basic results
        logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_time:.2f}s")
        logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_time:.2f}s")
        
        # Store results for analysis
        results.append({
            "query": query,
            "complexity": complexity,
            "description": description,
            "standard": {
                "answer": standard_answer,
                "sources": [
                    {
                        "document_id": s.document_id,
                        "chunk_id": s.chunk_id,
                        "relevance_score": s.relevance_score,
                        "excerpt": s.excerpt[:100] + "..." if len(s.excerpt) > 100 else s.excerpt
                    }
                    for s in standard_sources
                ] if standard_sources else [],
                "time": standard_time
            },
            "judge": {
                "answer": judge_answer,
                "sources": [
                    {
                        "document_id": s.document_id,
                        "chunk_id": s.chunk_id,
                        "relevance_score": s.relevance_score,
                        "excerpt": s.excerpt[:100] + "..." if len(s.excerpt) > 100 else s.excerpt
                    }
                    for s in judge_sources
                ] if judge_sources else [],
                "time": judge_time
            }
        })
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "retrieval_judge_comparison_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Test results saved to {os.path.abspath(results_path)}")
    return results

async def analyze_results(results):
    """Analyze the comparison test results"""
    logger.info("Analyzing comparison test results...")
    
    # Initialize metrics
    metrics = {
        "total_queries": len(results),
        "avg_standard_time": 0,
        "avg_judge_time": 0,
        "avg_standard_sources": 0,
        "avg_judge_sources": 0,
        "queries_with_more_judge_sources": 0,
        "queries_with_more_standard_sources": 0,
        "queries_with_equal_sources": 0,
        "by_complexity": {},
        "improvement_areas": []
    }
    
    # Calculate metrics
    total_standard_time = 0
    total_judge_time = 0
    total_standard_sources = 0
    total_judge_sources = 0
    
    # Initialize complexity metrics
    for complexity in ["simple", "moderate", "complex", "ambiguous", "multi-part"]:
        metrics["by_complexity"][complexity] = {
            "count": 0,
            "avg_standard_time": 0,
            "avg_judge_time": 0,
            "avg_standard_sources": 0,
            "avg_judge_sources": 0,
            "improvement_percentage": 0
        }
    
    # Process each result
    for result in results:
        complexity = result["complexity"]
        standard_sources = len(result["standard"]["sources"])
        judge_sources = len(result["judge"]["sources"])
        standard_time = result["standard"]["time"]
        judge_time = result["judge"]["time"]
        
        # Update totals
        total_standard_time += standard_time
        total_judge_time += judge_time
        total_standard_sources += standard_sources
        total_judge_sources += judge_sources
        
        # Update source comparison counts
        if judge_sources > standard_sources:
            metrics["queries_with_more_judge_sources"] += 1
        elif standard_sources > judge_sources:
            metrics["queries_with_more_standard_sources"] += 1
        else:
            metrics["queries_with_equal_sources"] += 1
        
        # Update complexity metrics
        if complexity in metrics["by_complexity"]:
            metrics["by_complexity"][complexity]["count"] += 1
            metrics["by_complexity"][complexity]["avg_standard_time"] += standard_time
            metrics["by_complexity"][complexity]["avg_judge_time"] += judge_time
            metrics["by_complexity"][complexity]["avg_standard_sources"] += standard_sources
            metrics["by_complexity"][complexity]["avg_judge_sources"] += judge_sources
    
    # Calculate averages
    metrics["avg_standard_time"] = total_standard_time / len(results)
    metrics["avg_judge_time"] = total_judge_time / len(results)
    metrics["avg_standard_sources"] = total_standard_sources / len(results)
    metrics["avg_judge_sources"] = total_judge_sources / len(results)
    
    # Calculate complexity averages and improvement percentages
    for complexity, data in metrics["by_complexity"].items():
        if data["count"] > 0:
            data["avg_standard_time"] /= data["count"]
            data["avg_judge_time"] /= data["count"]
            data["avg_standard_sources"] /= data["count"]
            data["avg_judge_sources"] /= data["count"]
            
            # Calculate improvement percentage in source relevance
            if data["avg_standard_sources"] > 0:
                data["improvement_percentage"] = ((data["avg_judge_sources"] - data["avg_standard_sources"]) / 
                                                data["avg_standard_sources"]) * 100
            else:
                data["improvement_percentage"] = 0
    
    # Identify areas for improvement
    # 1. Check for queries where judge performed worse
    for result in results:
        standard_sources = len(result["standard"]["sources"])
        judge_sources = len(result["judge"]["sources"])
        
        if standard_sources > judge_sources:
            metrics["improvement_areas"].append({
                "query": result["query"],
                "complexity": result["complexity"],
                "issue": "Judge retrieved fewer sources than standard retrieval",
                "standard_sources": standard_sources,
                "judge_sources": judge_sources
            })
        
        # 2. Check for excessive processing time
        if result["judge"]["time"] > result["standard"]["time"] * 2:
            metrics["improvement_areas"].append({
                "query": result["query"],
                "complexity": result["complexity"],
                "issue": "Judge processing time significantly higher",
                "standard_time": result["standard"]["time"],
                "judge_time": result["judge"]["time"]
            })
    
    # Log summary metrics
    logger.info(f"Total queries: {metrics['total_queries']}")
    logger.info(f"Average standard retrieval time: {metrics['avg_standard_time']:.2f}s")
    logger.info(f"Average judge-enhanced retrieval time: {metrics['avg_judge_time']:.2f}s")
    logger.info(f"Average standard sources: {metrics['avg_standard_sources']:.2f}")
    logger.info(f"Average judge-enhanced sources: {metrics['avg_judge_sources']:.2f}")
    logger.info(f"Queries with more judge sources: {metrics['queries_with_more_judge_sources']}")
    logger.info(f"Queries with more standard sources: {metrics['queries_with_more_standard_sources']}")
    logger.info(f"Queries with equal sources: {metrics['queries_with_equal_sources']}")
    
    # Log complexity metrics
    for complexity, data in metrics["by_complexity"].items():
        if data["count"] > 0:
            logger.info(f"\nComplexity: {complexity} ({data['count']} queries)")
            logger.info(f"  Avg standard time: {data['avg_standard_time']:.2f}s")
            logger.info(f"  Avg judge time: {data['avg_judge_time']:.2f}s")
            logger.info(f"  Avg standard sources: {data['avg_standard_sources']:.2f}")
            logger.info(f"  Avg judge sources: {data['avg_judge_sources']:.2f}")
            logger.info(f"  Improvement percentage: {data['improvement_percentage']:.2f}%")
    
    # Log improvement areas
    if metrics["improvement_areas"]:
        logger.info("\nAreas for improvement:")
        for area in metrics["improvement_areas"]:
            logger.info(f"  Query: {area['query']}")
            logger.info(f"  Issue: {area['issue']}")
    
    # Save metrics to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    metrics_path = os.path.join(results_dir, "retrieval_judge_metrics.json")
    
    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=2)
    
    logger.info(f"Analysis metrics saved to {os.path.abspath(metrics_path)}")
    return metrics

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge comparison test...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test documents
        markdown_path, pdf_path, specs_path = await create_test_documents()
        
        # Process documents
        markdown_doc, pdf_doc, specs_doc, vector_store = await process_documents(
            None, markdown_path, pdf_path, specs_path
        )
        
        # Run comparison tests
        results = await run_comparison_tests(vector_store)
        
        # Analyze results
        metrics = await analyze_results(results)
        
        # Print summary
        logger.info("\n=== RETRIEVAL JUDGE COMPARISON SUMMARY ===")
        logger.info(f"Total queries tested: {metrics['total_queries']}")
        
        # Calculate overall improvement
        source_improvement = ((metrics['avg_judge_sources'] - metrics['avg_standard_sources']) / 
                             metrics['avg_standard_sources'] * 100) if metrics['avg_standard_sources'] > 0 else 0
        
        time_difference = ((metrics['avg_judge_time'] - metrics['avg_standard_time']) / 
                          metrics['avg_standard_time'] * 100) if metrics['avg_standard_time'] > 0 else 0
        
        logger.info(f"Overall source relevance improvement: {source_improvement:.2f}%")
        logger.info(f"Processing time difference: {time_difference:.2f}%")
        
        # Effectiveness by query complexity
        logger.info("\nEffectiveness by query complexity:")
        for complexity, data in metrics["by_complexity"].items():
            if data["count"] > 0:
                logger.info(f"  {complexity.capitalize()}: {data['improvement_percentage']:.2f}% improvement")
        
        # Improvement areas summary
        if metrics["improvement_areas"]:
            logger.info(f"\nIdentified {len(metrics['improvement_areas'])} areas for improvement")
        else:
            logger.info("\nNo specific areas for improvement identified")
        
        logger.info("Retrieval Judge comparison test completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge comparison test: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_single_query.py
================
#!/usr/bin/env python3
"""
Simplified test script to demonstrate the Retrieval Judge with a single query.
This script:
1. Creates a test document
2. Processes the document
3. Runs a single query with both standard retrieval and judge-enhanced retrieval
4. Compares the results
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_single_query")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test query
TEST_QUERY = "What are the components of the RAG engine and how do they work together?"

async def create_test_document():
    """Create a test document for RAG testing"""
    logger.info("Creating test document...")
    
    # Create directory for test document
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create document file
    doc_path = os.path.join(test_dir, "test_document.md")
    with open(doc_path, "w") as f:
        f.write(TEST_DOCUMENT)
    
    logger.info(f"Created test document at {os.path.abspath(doc_path)}")
    return doc_path

async def process_document(doc_path):
    """Process the test document and add it to the vector store"""
    logger.info("Processing test document...")
    
    # Read file content
    with open(doc_path, "r") as f:
        doc_content = f.read()
    
    # Create Document object
    document = Document(
        id=str(uuid.uuid4()),
        filename="test_document.md",
        content=doc_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    # Create a vector store
    test_persist_dir = os.path.join("tests", "retrieval_judge", "test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = VectorStore(persist_directory=test_persist_dir)
    
    # Create chunks
    document.chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=doc_content,
            metadata={
                "index": 0,
                "source": doc_path,
                "document_id": document.id
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(document)
    
    logger.info(f"Added document {document.id} to vector store")
    
    return document, vector_store

async def run_comparison_test(vector_store):
    """Run a comparison test between standard retrieval and judge-enhanced retrieval"""
    logger.info("Running comparison test...")
    
    # Create OllamaClient
    ollama_client = OllamaClient()
    
    # Create Retrieval Judge with the specified model
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = RetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Engine with Retrieval Judge
    rag_engine_with_judge = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    # Engine without Retrieval Judge
    rag_engine_standard = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    # Test with standard retrieval
    logger.info("Running with standard retrieval...")
    start_time_standard = time.time()
    standard_response = await rag_engine_standard.query(
        query=TEST_QUERY,
        use_rag=True,
        top_k=5,
        stream=False
    )
    standard_time = time.time() - start_time_standard
    
    # Test with judge-enhanced retrieval
    logger.info("Running with judge-enhanced retrieval...")
    start_time_judge = time.time()
    judge_response = await rag_engine_with_judge.query(
        query=TEST_QUERY,
        use_rag=True,
        top_k=5,
        stream=False
    )
    judge_time = time.time() - start_time_judge
    
    # Extract results
    standard_answer = standard_response.get("answer", "")
    standard_sources = standard_response.get("sources", [])
    
    judge_answer = judge_response.get("answer", "")
    judge_sources = judge_response.get("sources", [])
    
    # Log results
    logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_time:.2f}s")
    logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_time:.2f}s")
    
    # Format results for saving
    result = {
        "query": TEST_QUERY,
        "standard": {
            "answer": standard_answer,
            "sources_count": len(standard_sources),
            "time": standard_time
        },
        "judge": {
            "answer": judge_answer,
            "sources_count": len(judge_sources),
            "time": judge_time
        }
    }
    
    # Save result to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    result_path = os.path.join(results_dir, "single_query_result.json")
    
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2)
    
    logger.info(f"Test result saved to {os.path.abspath(result_path)}")
    
    # Print comparison
    logger.info("\n=== RETRIEVAL JUDGE COMPARISON ===")
    logger.info(f"Query: {TEST_QUERY}")
    logger.info(f"Standard retrieval time: {standard_time:.2f}s")
    logger.info(f"Judge-enhanced retrieval time: {judge_time:.2f}s")
    logger.info(f"Time difference: {((judge_time - standard_time) / standard_time * 100):.2f}%")
    logger.info(f"Standard sources: {len(standard_sources)}")
    logger.info(f"Judge sources: {len(judge_sources)}")
    
    # Print answers
    logger.info("\nStandard Answer:")
    logger.info(standard_answer[:500] + "..." if len(standard_answer) > 500 else standard_answer)
    
    logger.info("\nJudge-Enhanced Answer:")
    logger.info(judge_answer[:500] + "..." if len(judge_answer) > 500 else judge_answer)
    
    return result

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge single query test...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test document
        doc_path = await create_test_document()
        
        # Process document
        document, vector_store = await process_document(doc_path)
        
        # Run comparison test
        result = await run_comparison_test(vector_store)
        
        logger.info("Retrieval Judge single query test completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge single query test: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/retrieval_judge/test_timing_analysis.py
================
#!/usr/bin/env python3
"""
Test script to analyze the timing differences between standard retrieval and judge-enhanced retrieval.
This script:
1. Creates a test document
2. Runs multiple queries with detailed timing measurements
3. Analyzes the timing differences between components
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_timing_analysis")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.agents.retrieval_judge import RetrievalJudge
from app.models.document import Document, Chunk

# Define the model to use for the Retrieval Judge
JUDGE_MODEL = "gemma3:4b"

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test queries
TEST_QUERIES = [
    "What is the architecture of Metis RAG?",
    "What are the components of the RAG engine?",
    "How does the vector store work?",
    "What is the role of the LLM integration component?",
    "How does the document processing pipeline work?"
]

class TimingTracker:
    """Helper class to track timing of operations"""
    def __init__(self):
        self.timings = {}
        self.current_operation = None
        self.start_time = None
    
    def start(self, operation):
        """Start timing an operation"""
        self.current_operation = operation
        self.start_time = time.time()
        logger.info(f"Starting operation: {operation}")
    
    def stop(self):
        """Stop timing the current operation"""
        if self.current_operation and self.start_time:
            elapsed = time.time() - self.start_time
            self.timings[self.current_operation] = elapsed
            logger.info(f"Completed operation: {self.current_operation} in {elapsed:.2f}s")
            self.current_operation = None
            self.start_time = None
    
    def get_summary(self):
        """Get a summary of all timings"""
        return self.timings

async def create_test_document():
    """Create a test document for RAG testing"""
    logger.info("Creating test document...")
    
    # Create directory for test document
    test_dir = os.path.join("tests", "retrieval_judge", "data")
    os.makedirs(test_dir, exist_ok=True)
    
    # Create document file
    doc_path = os.path.join(test_dir, "timing_test_document.md")
    with open(doc_path, "w") as f:
        f.write(TEST_DOCUMENT)
    
    logger.info(f"Created test document at {os.path.abspath(doc_path)}")
    return doc_path

async def process_document(doc_path):
    """Process the test document and add it to the vector store"""
    logger.info("Processing test document...")
    
    # Read file content
    with open(doc_path, "r") as f:
        doc_content = f.read()
    
    # Create Document object
    document = Document(
        id=str(uuid.uuid4()),
        filename="timing_test_document.md",
        content=doc_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    # Create a vector store
    test_persist_dir = os.path.join("tests", "retrieval_judge", "timing_test_chroma_db")
    os.makedirs(test_persist_dir, exist_ok=True)
    vector_store = VectorStore(persist_directory=test_persist_dir)
    
    # Create chunks
    document.chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=doc_content,
            metadata={
                "index": 0,
                "source": doc_path,
                "document_id": document.id
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(document)
    
    logger.info(f"Added document {document.id} to vector store")
    
    return document, vector_store

class InstrumentedRetrievalJudge(RetrievalJudge):
    """Instrumented version of RetrievalJudge that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """Instrumented version of analyze_query"""
        start_time = time.time()
        result = await super().analyze_query(query)
        elapsed = time.time() - start_time
        self.timings["analyze_query"] = elapsed
        logger.info(f"analyze_query took {elapsed:.2f}s")
        return result
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Instrumented version of evaluate_chunks"""
        start_time = time.time()
        result = await super().evaluate_chunks(query, chunks)
        elapsed = time.time() - start_time
        self.timings["evaluate_chunks"] = elapsed
        logger.info(f"evaluate_chunks took {elapsed:.2f}s")
        return result
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Instrumented version of optimize_context"""
        start_time = time.time()
        result = await super().optimize_context(query, chunks)
        elapsed = time.time() - start_time
        self.timings["optimize_context"] = elapsed
        logger.info(f"optimize_context took {elapsed:.2f}s")
        return result

class InstrumentedRAGEngine(RAGEngine):
    """Instrumented version of RAGEngine that tracks timing"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timings = {}
    
    async def _enhanced_retrieval(self, *args, **kwargs):
        """Instrumented version of _enhanced_retrieval"""
        start_time = time.time()
        result = await super()._enhanced_retrieval(*args, **kwargs)
        elapsed = time.time() - start_time
        self.timings["enhanced_retrieval"] = elapsed
        logger.info(f"enhanced_retrieval took {elapsed:.2f}s")
        return result

async def run_timing_tests(vector_store):
    """Run timing tests for standard and judge-enhanced retrieval"""
    logger.info("Running timing tests...")
    
    # Create OllamaClient
    ollama_client = OllamaClient()
    
    # Create instrumented Retrieval Judge
    logger.info(f"Creating Retrieval Judge with model: {JUDGE_MODEL}")
    retrieval_judge = InstrumentedRetrievalJudge(ollama_client=ollama_client, model=JUDGE_MODEL)
    
    # Create engines
    rag_engine_with_judge = InstrumentedRAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=retrieval_judge
    )
    
    rag_engine_standard = RAGEngine(
        vector_store=vector_store,
        ollama_client=ollama_client,
        retrieval_judge=None
    )
    
    results = []
    
    # Run each test query twice - first with standard retrieval, then with judge
    for query in TEST_QUERIES:
        logger.info(f"\n=== Testing query: {query} ===")
        
        # Test with standard retrieval
        logger.info("Running with standard retrieval...")
        standard_tracker = TimingTracker()
        
        standard_tracker.start("standard_total")
        standard_response = await rag_engine_standard.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        standard_tracker.stop()
        
        # Test with judge-enhanced retrieval
        logger.info("Running with judge-enhanced retrieval...")
        judge_tracker = TimingTracker()
        
        judge_tracker.start("judge_total")
        judge_response = await rag_engine_with_judge.query(
            query=query,
            use_rag=True,
            top_k=5,
            stream=False
        )
        judge_tracker.stop()
        
        # Extract results
        standard_answer = standard_response.get("answer", "")
        standard_sources = standard_response.get("sources", [])
        
        judge_answer = judge_response.get("answer", "")
        judge_sources = judge_response.get("sources", [])
        
        # Log results
        logger.info(f"Standard retrieval: {len(standard_sources)} sources, {standard_tracker.timings['standard_total']:.2f}s")
        logger.info(f"Judge-enhanced retrieval: {len(judge_sources)} sources, {judge_tracker.timings['judge_total']:.2f}s")
        
        # Collect judge component timings
        judge_component_timings = {}
        if hasattr(retrieval_judge, 'timings'):
            judge_component_timings = retrieval_judge.timings.copy()
        
        if hasattr(rag_engine_with_judge, 'timings'):
            judge_component_timings.update(rag_engine_with_judge.timings)
        
        # Clear timings for next run
        retrieval_judge.timings = {}
        rag_engine_with_judge.timings = {}
        
        # Store results
        results.append({
            "query": query,
            "standard": {
                "total_time": standard_tracker.timings['standard_total'],
                "sources_count": len(standard_sources)
            },
            "judge": {
                "total_time": judge_tracker.timings['judge_total'],
                "sources_count": len(judge_sources),
                "component_timings": judge_component_timings
            }
        })
    
    # Save results to file
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    os.makedirs(results_dir, exist_ok=True)
    results_path = os.path.join(results_dir, "timing_analysis_results.json")
    
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Timing analysis results saved to {os.path.abspath(results_path)}")
    
    return results

async def analyze_timing_results(results):
    """Analyze the timing test results"""
    logger.info("\n=== TIMING ANALYSIS ===")
    
    # Calculate averages
    avg_standard_time = sum(r["standard"]["total_time"] for r in results) / len(results)
    avg_judge_time = sum(r["judge"]["total_time"] for r in results) / len(results)
    
    logger.info(f"Average standard retrieval time: {avg_standard_time:.2f}s")
    logger.info(f"Average judge-enhanced retrieval time: {avg_judge_time:.2f}s")
    logger.info(f"Time difference: {((avg_judge_time - avg_standard_time) / avg_standard_time * 100):.2f}%")
    
    # Analyze component timings
    component_times = {}
    for r in results:
        for component, time_value in r["judge"].get("component_timings", {}).items():
            if component not in component_times:
                component_times[component] = []
            component_times[component].append(time_value)
    
    logger.info("\nJudge component timing averages:")
    for component, times in component_times.items():
        avg_time = sum(times) / len(times)
        logger.info(f"  {component}: {avg_time:.2f}s")
    
    # Analyze first run vs. subsequent runs
    first_standard_time = results[0]["standard"]["total_time"]
    first_judge_time = results[0]["judge"]["total_time"]
    
    rest_standard_time = sum(r["standard"]["total_time"] for r in results[1:]) / (len(results) - 1) if len(results) > 1 else 0
    rest_judge_time = sum(r["judge"]["total_time"] for r in results[1:]) / (len(results) - 1) if len(results) > 1 else 0
    
    logger.info("\nFirst run vs. subsequent runs:")
    logger.info(f"  First standard run: {first_standard_time:.2f}s")
    logger.info(f"  Average of subsequent standard runs: {rest_standard_time:.2f}s")
    logger.info(f"  First judge run: {first_judge_time:.2f}s")
    logger.info(f"  Average of subsequent judge runs: {rest_judge_time:.2f}s")
    
    # Check for caching effects
    logger.info("\nPossible explanations for timing differences:")
    
    if first_standard_time > rest_standard_time and first_judge_time > rest_judge_time:
        logger.info("  - Warm-up effect: Both methods show faster performance after the first run")
    
    if avg_judge_time < avg_standard_time:
        logger.info("  - Caching effect: Judge-enhanced retrieval might benefit from caching in the vector store")
        logger.info("  - Model loading: The LLM might be loaded into memory during the first run")
        logger.info("  - Query optimization: The judge might be selecting more efficient retrieval parameters")
    else:
        logger.info("  - Additional processing: Judge operations add overhead as expected")
    
    # Save analysis to file
    analysis = {
        "avg_standard_time": avg_standard_time,
        "avg_judge_time": avg_judge_time,
        "time_difference_percent": ((avg_judge_time - avg_standard_time) / avg_standard_time * 100),
        "component_averages": {component: sum(times) / len(times) for component, times in component_times.items()},
        "first_run": {
            "standard": first_standard_time,
            "judge": first_judge_time
        },
        "subsequent_runs": {
            "standard": rest_standard_time,
            "judge": rest_judge_time
        }
    }
    
    results_dir = os.path.join("tests", "retrieval_judge", "results")
    analysis_path = os.path.join(results_dir, "timing_analysis.json")
    
    with open(analysis_path, "w") as f:
        json.dump(analysis, f, indent=2)
    
    logger.info(f"Timing analysis saved to {os.path.abspath(analysis_path)}")
    
    return analysis

async def main():
    """Main test function"""
    logger.info("Starting Retrieval Judge timing analysis...")
    logger.info(f"Using model {JUDGE_MODEL} for the Retrieval Judge")
    
    try:
        # Create test document
        doc_path = await create_test_document()
        
        # Process document
        document, vector_store = await process_document(doc_path)
        
        # Run timing tests
        results = await run_timing_tests(vector_store)
        
        # Analyze results
        analysis = await analyze_timing_results(results)
        
        logger.info("Retrieval Judge timing analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Retrieval Judge timing analysis: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/unit/test_chunking_judge.py
================
"""
Unit tests for the Chunking Judge
"""
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
import json

from app.rag.agents.chunking_judge import ChunkingJudge
from app.models.document import Document

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "strategy": "markdown",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is a structured markdown document with clear headers."
        }
        """
    }
    return client

@pytest.fixture
def markdown_document():
    """Create a test markdown document"""
    return Document(
        id="test-id",
        filename="test.md",
        content="""# Header 1
This is content under header 1.

## Header 2
This is content under header 2.

### Header 3
This is content under header 3.
"""
    )

@pytest.fixture
def text_document():
    """Create a test text document"""
    return Document(
        id="test-id-2",
        filename="test.txt",
        content="""This is a plain text document.
It has multiple paragraphs but no clear structure.

This is the second paragraph.
It continues for a few lines.
"""
    )

@pytest.mark.asyncio
async def test_chunking_judge_markdown_analysis(mock_ollama_client, markdown_document):
    """Test that the Chunking Judge correctly analyzes a markdown document"""
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(markdown_document)
    
    # Verify result
    assert result["strategy"] == "markdown"
    assert result["parameters"]["chunk_size"] == 800
    assert result["parameters"]["chunk_overlap"] == 100
    assert "justification" in result
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "document analysis expert" in call_args["prompt"].lower()
    assert "test.md" in call_args["prompt"]
    assert "Header 1" in call_args["prompt"]
    assert "Header 2" in call_args["prompt"]

@pytest.mark.asyncio
async def test_chunking_judge_text_analysis(mock_ollama_client, text_document):
    """Test that the Chunking Judge correctly analyzes a text document"""
    # Override the mock response for text document
    mock_ollama_client.generate.return_value = {
        "response": """
        {
            "strategy": "recursive",
            "parameters": {
                "chunk_size": 1000,
                "chunk_overlap": 150
            },
            "justification": "This is a plain text document with paragraphs."
        }
        """
    }
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(text_document)
    
    # Verify result
    assert result["strategy"] == "recursive"
    assert result["parameters"]["chunk_size"] == 1000
    assert result["parameters"]["chunk_overlap"] == 150
    assert "justification" in result
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "document analysis expert" in call_args["prompt"].lower()
    assert "test.txt" in call_args["prompt"]
    assert "plain text document" in call_args["prompt"]

@pytest.mark.asyncio
async def test_chunking_judge_error_handling(mock_ollama_client, markdown_document):
    """Test that the Chunking Judge handles errors gracefully"""
    # Make the mock client return an invalid JSON response
    mock_ollama_client.generate.return_value = {
        "response": "This is not valid JSON"
    }
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(markdown_document)
    
    # Verify fallback to default values
    assert result["strategy"] == "recursive"
    assert result["parameters"]["chunk_size"] == 500
    assert result["parameters"]["chunk_overlap"] == 50
    assert "Failed to parse" in result["justification"]

@pytest.mark.asyncio
async def test_chunking_judge_invalid_strategy(mock_ollama_client, markdown_document):
    """Test that the Chunking Judge handles invalid strategies gracefully"""
    # Make the mock client return an invalid strategy
    mock_ollama_client.generate.return_value = {
        "response": """
        {
            "strategy": "invalid_strategy",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is an invalid strategy."
        }
        """
    }
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(markdown_document)
    
    # Verify fallback to recursive strategy
    assert result["strategy"] == "recursive"
    assert result["parameters"]["chunk_size"] == 800
    assert result["parameters"]["chunk_overlap"] == 100

@pytest.mark.asyncio
async def test_extract_representative_sample_markdown():
    """Test the _extract_representative_sample method with markdown content"""
    judge = ChunkingJudge(ollama_client=AsyncMock())
    
    # Create a long markdown document
    long_content = "# Header 1\n\n" + "Content under header 1.\n" * 100
    long_content += "\n## Header 2\n\n" + "Content under header 2.\n" * 100
    long_content += "\n### Header 3\n\n" + "Content under header 3.\n" * 100
    
    # Extract sample
    sample = judge._extract_representative_sample(long_content, "test.md", max_length=1000)
    
    # Verify sample contains headers and some content
    assert "# Header 1" in sample
    assert "## Header 2" in sample
    assert "### Header 3" in sample
    assert "Content under header" in sample
    assert len(sample) <= 1000 + 100  # Allow for some buffer

@pytest.mark.asyncio
async def test_extract_representative_sample_text():
    """Test the _extract_representative_sample method with text content"""
    judge = ChunkingJudge(ollama_client=AsyncMock())
    
    # Create a long text document
    long_content = "This is the beginning of the document.\n" * 50
    long_content += "This is the middle of the document.\n" * 50
    long_content += "This is the end of the document.\n" * 50
    
    # Extract sample
    sample = judge._extract_representative_sample(long_content, "test.txt", max_length=1000)
    
    # Verify sample contains beginning, middle, and end
    assert "beginning of the document" in sample
    assert "middle of the document" in sample
    assert "end of the document" in sample
    assert len(sample) <= 1000 + 100  # Allow for some buffer

================
File: tests/unit/test_rag_engine.py
================
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
import asyncio

from app.rag.rag_engine import RAGEngine
from app.models.chat import Citation

@pytest.fixture
def mock_vector_store():
    """
    Create a mock vector store
    """
    mock = AsyncMock()
    mock.search.return_value = [
        {
            "chunk_id": "chunk1",
            "content": "This is a test chunk",
            "metadata": {"document_id": "doc1"},
            "distance": 0.1
        },
        {
            "chunk_id": "chunk2",
            "content": "This is another test chunk",
            "metadata": {"document_id": "doc1"},
            "distance": 0.2
        }
    ]
    return mock

@pytest.fixture
def mock_ollama_client():
    """
    Create a mock Ollama client
    """
    mock = AsyncMock()
    mock.generate.return_value = {"response": "This is a test response"}
    return mock

@pytest.mark.asyncio
async def test_rag_engine_query_with_rag(mock_vector_store, mock_ollama_client):
    """
    Test RAG engine query with RAG enabled
    """
    # Arrange
    engine = RAGEngine(
        vector_store=mock_vector_store,
        ollama_client=mock_ollama_client
    )
    
    # Act
    result = await engine.query(
        query="test query",
        model="test-model",
        use_rag=True,
        stream=False
    )
    
    # Assert
    assert mock_vector_store.search.called
    assert mock_ollama_client.generate.called
    assert result["query"] == "test query"
    assert result["answer"] == "This is a test response"
    assert len(result["sources"]) == 2
    assert isinstance(result["sources"][0], Citation)

@pytest.mark.asyncio
async def test_rag_engine_query_without_rag(mock_vector_store, mock_ollama_client):
    """
    Test RAG engine query with RAG disabled
    """
    # Arrange
    engine = RAGEngine(
        vector_store=mock_vector_store,
        ollama_client=mock_ollama_client
    )
    
    # Act
    result = await engine.query(
        query="test query",
        model="test-model",
        use_rag=False,
        stream=False
    )
    
    # Assert
    assert not mock_vector_store.search.called
    assert mock_ollama_client.generate.called
    assert result["query"] == "test query"
    assert result["answer"] == "This is a test response"
    assert result["sources"] is None

================
File: tests/unit/test_retrieval_judge.py
================
"""
Unit tests for the Retrieval Judge
"""
import pytest
import json
from unittest.mock import AsyncMock, MagicMock, patch

from app.rag.agents.retrieval_judge import RetrievalJudge
from app.rag.ollama_client import OllamaClient


@pytest.fixture
def mock_ollama_client():
    """Create a mock OllamaClient"""
    client = AsyncMock(spec=OllamaClient)
    return client


@pytest.fixture
def retrieval_judge(mock_ollama_client):
    """Create a RetrievalJudge with a mock OllamaClient"""
    return RetrievalJudge(ollama_client=mock_ollama_client, model="test-model")


@pytest.fixture
def sample_chunks():
    """Create sample chunks for testing"""
    return [
        {
            "chunk_id": "chunk1",
            "content": "This is a sample chunk about artificial intelligence and machine learning.",
            "metadata": {
                "document_id": "doc1",
                "filename": "ai_basics.md",
                "tags": "ai,machine learning",
                "folder": "/tech"
            },
            "distance": 0.2
        },
        {
            "chunk_id": "chunk2",
            "content": "Neural networks are a subset of machine learning and are at the core of deep learning algorithms.",
            "metadata": {
                "document_id": "doc1",
                "filename": "ai_basics.md",
                "tags": "ai,machine learning,neural networks",
                "folder": "/tech"
            },
            "distance": 0.3
        },
        {
            "chunk_id": "chunk3",
            "content": "Python is a popular programming language for data science and machine learning.",
            "metadata": {
                "document_id": "doc2",
                "filename": "programming.md",
                "tags": "python,programming",
                "folder": "/tech/programming"
            },
            "distance": 0.5
        }
    ]


class TestRetrievalJudge:
    """Tests for the RetrievalJudge class"""

    @pytest.mark.asyncio
    async def test_analyze_query(self, retrieval_judge, mock_ollama_client):
        """Test analyze_query method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": json.dumps({
                "complexity": "moderate",
                "parameters": {
                    "k": 8,
                    "threshold": 0.5,
                    "reranking": True
                },
                "justification": "This is a moderately complex query that requires specific information about neural networks."
            })
        }

        # Call the method
        result = await retrieval_judge.analyze_query("How do neural networks work?")

        # Verify the result
        assert result["complexity"] == "moderate"
        assert result["parameters"]["k"] == 8
        assert result["parameters"]["threshold"] == 0.5
        assert result["parameters"]["reranking"] is True
        assert "justification" in result

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "analyze the query complexity" in prompt.lower()

    @pytest.mark.asyncio
    async def test_evaluate_chunks(self, retrieval_judge, mock_ollama_client, sample_chunks):
        """Test evaluate_chunks method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": json.dumps({
                "relevance_scores": {
                    "1": 0.9,
                    "2": 0.8,
                    "3": 0.4
                },
                "needs_refinement": False,
                "justification": "The first two chunks are highly relevant to neural networks."
            })
        }

        # Call the method
        result = await retrieval_judge.evaluate_chunks("How do neural networks work?", sample_chunks)

        # Verify the result
        assert "relevance_scores" in result
        assert "chunk1" in result["relevance_scores"]
        assert result["relevance_scores"]["chunk1"] == 0.9
        assert result["relevance_scores"]["chunk2"] == 0.8
        assert result["relevance_scores"]["chunk3"] == 0.4
        assert result["needs_refinement"] is False
        assert "justification" in result

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "evaluate the relevance" in prompt.lower()
        assert "ai_basics.md" in prompt  # Sample chunk metadata

    @pytest.mark.asyncio
    async def test_refine_query(self, retrieval_judge, mock_ollama_client, sample_chunks):
        """Test refine_query method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": "How do neural networks function in deep learning algorithms?"
        }

        # Call the method
        result = await retrieval_judge.refine_query("How do neural networks work?", sample_chunks)

        # Verify the result
        assert result == "How do neural networks function in deep learning algorithms?"

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "refine the user's query" in prompt.lower()

    @pytest.mark.asyncio
    async def test_optimize_context(self, retrieval_judge, mock_ollama_client, sample_chunks):
        """Test optimize_context method"""
        # Mock the LLM response
        mock_ollama_client.generate.return_value = {
            "response": json.dumps({
                "optimized_order": [2, 1],  # Chunk indices (1-based)
                "excluded_chunks": [3],  # Exclude chunk3
                "justification": "Ordered chunks for logical flow and excluded less relevant chunk."
            })
        }

        # Call the method
        result = await retrieval_judge.optimize_context("How do neural networks work?", sample_chunks)

        # Verify the result
        assert len(result) == 2
        assert result[0]["chunk_id"] == "chunk2"  # First in optimized order
        assert result[1]["chunk_id"] == "chunk1"  # Second in optimized order
        # chunk3 should be excluded

        # Verify the LLM was called with the correct prompt
        mock_ollama_client.generate.assert_called_once()
        prompt = mock_ollama_client.generate.call_args[1]["prompt"]
        assert "How do neural networks work?" in prompt
        assert "optimize the assembly" in prompt.lower()

    @pytest.mark.asyncio
    async def test_analyze_query_error_handling(self, retrieval_judge, mock_ollama_client):
        """Test error handling in analyze_query method"""
        # Mock the LLM response with invalid JSON
        mock_ollama_client.generate.return_value = {
            "response": "This is not valid JSON"
        }

        # Call the method
        result = await retrieval_judge.analyze_query("How do neural networks work?")

        # Verify default values are returned
        assert result["complexity"] == "moderate"
        assert result["parameters"]["k"] == 10
        assert result["parameters"]["threshold"] == 0.4
        assert result["parameters"]["reranking"] is True
        assert "Failed to parse" in result["justification"]

    @pytest.mark.asyncio
    async def test_extract_chunks_sample(self, retrieval_judge, sample_chunks):
        """Test _extract_chunks_sample method"""
        # Add more chunks to the sample
        extended_chunks = sample_chunks + [
            {
                "chunk_id": f"chunk{i}",
                "content": f"This is sample chunk {i} with some content that should be truncated if too long.",
                "metadata": {
                    "document_id": f"doc{i//2}",
                    "filename": f"file{i}.md",
                    "tags": "tag1,tag2",
                    "folder": "/folder"
                },
                "distance": 0.1 * i
            }
            for i in range(4, 10)
        ]

        # Call the method with max_chunks=3 and a small max_length
        result = retrieval_judge._extract_chunks_sample(extended_chunks, max_chunks=3, max_length=100)

        # Verify the result
        assert len(result) == 3  # Should only return 3 chunks
        assert all("chunk_id" in chunk for chunk in result)
        
        # Verify chunks are sorted by distance (ascending)
        distances = [chunk.get("distance", 1.0) for chunk in result]
        assert all(distances[i] <= distances[i+1] for i in range(len(distances)-1))
        
        # Verify content is truncated
        total_content_length = sum(len(chunk.get("content", "")) for chunk in result)
        assert total_content_length <= 100

================
File: tests/unit/test_semantic_chunker.py
================
"""
Unit tests for the Semantic Chunker
"""
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
import json

from app.rag.chunkers.semantic_chunker import SemanticChunker
from langchain.schema import Document as LangchainDocument

@pytest.fixture
def mock_ollama_client():
    """Create a mock Ollama client for testing"""
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        [500, 1000, 1500]
        """
    }
    return client

@pytest.fixture
def sample_text():
    """Create a sample text for testing"""
    return """
    This is the first section of the document. It contains information about the introduction.
    The introduction sets the stage for the rest of the document and provides context.
    
    This is the second section of the document. It contains information about the main topic.
    The main topic is discussed in detail with examples and explanations.
    
    This is the third section of the document. It contains information about the conclusion.
    The conclusion summarizes the main points and provides next steps.
    """

@pytest.mark.asyncio
async def test_semantic_chunker_split_text_async(mock_ollama_client, sample_text):
    """Test that the SemanticChunker correctly splits text asynchronously"""
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client)
    
    # Mock the _identify_semantic_boundaries method to return predictable results
    async def mock_identify_boundaries(text):
        # Return a simple chunk
        return [text]
    
    # Patch the _identify_semantic_boundaries method
    with patch.object(chunker, '_identify_semantic_boundaries', side_effect=mock_identify_boundaries):
        # Test async splitting
        chunks = await chunker.split_text_async(sample_text)
        
        # Verify result
        assert len(chunks) == 1

@pytest.mark.asyncio
async def test_semantic_chunker_caching(mock_ollama_client, sample_text):
    """Test that the SemanticChunker caches results"""
    # For this test, we need to use a sample text that's longer than chunk_size
    long_sample = sample_text * 10  # Make it long enough to trigger chunking
    
    # Create semantic chunker with mock client and caching enabled
    chunker = SemanticChunker(ollama_client=mock_ollama_client, cache_enabled=True, chunk_size=500, chunk_overlap=50)
    
    # Create a simple implementation for _semantic_chunking_async that we can track
    original_method = chunker._semantic_chunking_async
    call_count = 0
    
    async def tracked_method(text):
        nonlocal call_count
        call_count += 1
        # Just return a simple chunk for testing
        return ["Test chunk 1", "Test chunk 2"]
    
    # Replace the method
    chunker._semantic_chunking_async = tracked_method
    
    try:
        # First call should use our tracked method
        chunks1 = await chunker.split_text_async(long_sample)
        
        # Second call should use the cache
        chunks2 = await chunker.split_text_async(long_sample)
        
        # Verify results are the same
        assert chunks1 == chunks2
        
        # Verify the method was called only once
        assert call_count == 1
    finally:
        # Restore the original method
        chunker._semantic_chunking_async = original_method

@pytest.mark.asyncio
async def test_semantic_chunker_short_text(mock_ollama_client):
    """Test that the SemanticChunker handles short text correctly"""
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client, chunk_size=1000)
    
    # Short text that doesn't need chunking
    short_text = "This is a short text that doesn't need chunking."
    
    # Test async splitting
    chunks = await chunker.split_text_async(short_text)
    
    # Verify result
    assert len(chunks) == 1
    assert chunks[0] == short_text
    
    # Verify LLM was not called
    assert mock_ollama_client.generate.call_count == 0

@pytest.mark.asyncio
async def test_semantic_chunker_error_handling(mock_ollama_client, sample_text):
    """Test that the SemanticChunker handles errors gracefully"""
    # For this test, we need to use a sample text that's longer than chunk_size
    long_sample = sample_text * 10  # Make it long enough to trigger chunking
    
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client, chunk_size=500, chunk_overlap=50)
    
    # Save original methods
    original_semantic_chunking = chunker._semantic_chunking_async
    original_fallback = chunker._fallback_chunking
    
    # Create a method that directly calls _fallback_chunking to bypass the normal flow
    async def error_method(text):
        # This will force the use of fallback chunking
        return chunker._fallback_chunking(text)
    
    # Create a predictable fallback method
    def mock_fallback(text):
        return ["Fallback chunk 1", "Fallback chunk 2"]
    
    # Replace the methods
    chunker._semantic_chunking_async = error_method
    chunker._fallback_chunking = mock_fallback
    
    try:
        # Test async splitting with our modified methods
        chunks = await chunker.split_text_async(long_sample)
        
        # Verify fallback to simple chunking
        assert len(chunks) == 2
        assert chunks[0] == "Fallback chunk 1"
        assert chunks[1] == "Fallback chunk 2"
    finally:
        # Restore the original methods
        chunker._semantic_chunking_async = original_semantic_chunking
        chunker._fallback_chunking = original_fallback

@pytest.mark.asyncio
async def test_semantic_chunker_apply_overlap(mock_ollama_client):
    """Test that the SemanticChunker applies overlap correctly"""
    # Create semantic chunker with mock client and significant overlap
    chunker = SemanticChunker(ollama_client=mock_ollama_client, chunk_overlap=10)
    
    # Create test chunks with enough length for overlap
    original_chunks = [
        "This is the first chunk of text that is long enough for overlap.",
        "This is the second chunk of text that is also long enough.",
        "This is the third chunk of text with sufficient length."
    ]
    
    # Apply overlap
    overlapped_chunks = chunker._apply_overlap(original_chunks)
    
    # Verify result
    assert len(overlapped_chunks) == 3
    assert overlapped_chunks[0] == original_chunks[0]
    assert overlapped_chunks[1].startswith(original_chunks[0][-10:])
    assert overlapped_chunks[2].startswith(original_chunks[1][-10:])

@pytest.mark.asyncio
async def test_semantic_chunker_integration_with_langchain(mock_ollama_client, sample_text):
    """Test that the SemanticChunker works with Langchain documents"""
    # Create semantic chunker with mock client
    chunker = SemanticChunker(ollama_client=mock_ollama_client)
    
    # Create Langchain document
    doc = LangchainDocument(page_content=sample_text, metadata={"source": "test"})
    
    # Split document
    split_docs = chunker.split_documents([doc])
    
    # Verify result
    assert len(split_docs) > 0
    assert all(isinstance(d, LangchainDocument) for d in split_docs)
    assert all("source" in d.metadata for d in split_docs)

@pytest.mark.asyncio
async def test_semantic_chunker_long_text_processing(mock_ollama_client):
    """Test that the SemanticChunker handles long text correctly"""
    # Create a long text that exceeds the max_llm_context_length
    long_text = "This is section " + ". ".join([f"Part {i}" for i in range(1000)])
    
    # Create semantic chunker with mock client and small max context
    chunker = SemanticChunker(
        ollama_client=mock_ollama_client,
        max_llm_context_length=2000
    )
    
    # Mock the _process_long_text method to return predictable results
    async def mock_process_long_text(text):
        # Return a chunk for each 1000 characters
        chunks = []
        for i in range(0, len(text), 1000):
            end = min(i + 1000, len(text))
            chunks.append(text[i:end])
        return chunks
    
    # Patch the method
    with patch.object(chunker, '_process_long_text', side_effect=mock_process_long_text):
        # Test async splitting
        chunks = await chunker.split_text_async(long_text)
        
        # Verify result
        assert len(chunks) > 1
        assert len(chunks) == (len(long_text) + 999) // 1000  # Ceiling division

================
File: tests/conftest.py
================
import pytest
import sys
import os
from pathlib import Path

# Add the project root to sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

@pytest.fixture
def test_dir():
    """
    Return the directory containing test files
    """
    return Path(__file__).parent / "data"

@pytest.fixture
def temp_upload_dir(tmpdir):
    """
    Return a temporary directory for uploads
    """
    from app.core.config import UPLOAD_DIR
    original_upload_dir = UPLOAD_DIR
    
    # Temporarily override the upload directory
    import app.core.config
    app.core.config.UPLOAD_DIR = str(tmpdir)
    
    yield str(tmpdir)
    
    # Restore the original upload directory
    app.core.config.UPLOAD_DIR = original_upload_dir

================
File: tests/test_chunking_judge_phase1.py
================
"""
Test script for Phase 1 of the LLM-enhanced RAG system: Chunking Judge component.
This script tests the Chunking Judge's ability to analyze different document types
and recommend appropriate chunking strategies.
"""
import asyncio
import os
import sys
import json
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("chunking_judge_test")

# Add the app directory to the Python path if needed
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.document import Document
from app.rag.agents.chunking_judge import ChunkingJudge
from app.core.config import USE_CHUNKING_JUDGE

class MockOllamaClient:
    """Mock Ollama client for testing when the real server is not available"""
    async def generate(self, prompt, model, stream=False):
        """Mock generate method"""
        # Extract the filename from the prompt
        filename = None
        if "Document Filename:" in prompt:
            lines = prompt.split("\n")
            for line in lines:
                if line.startswith("Document Filename:"):
                    filename = line.split("Document Filename:")[1].strip()
                    break
        
        # Return a mock response based on the document type
        if filename and filename.lower().endswith(".md"):
            return {
                "response": """
                {
                    "strategy": "markdown",
                    "parameters": {
                        "chunk_size": 800,
                        "chunk_overlap": 100
                    },
                    "justification": "This is a structured markdown document with clear headers."
                }
                """
            }
        elif filename and filename.lower().endswith(".csv"):
            return {
                "response": """
                {
                    "strategy": "token",
                    "parameters": {
                        "chunk_size": 600,
                        "chunk_overlap": 75
                    },
                    "justification": "This is a CSV file with tabular data that benefits from token-based chunking."
                }
                """
            }
        else:
            return {
                "response": """
                {
                    "strategy": "recursive",
                    "parameters": {
                        "chunk_size": 1000,
                        "chunk_overlap": 150
                    },
                    "justification": "This is a plain text document with paragraphs that benefits from recursive chunking."
                }
                """
            }

async def test_chunking_judge():
    """Test the Chunking Judge with different document types"""
    logger.info("\n===== Testing Chunking Judge Component (Phase 1) =====\n")
    
    # Use mock client for testing
    logger.info("Using mock Ollama client for testing.")
    ollama_client = MockOllamaClient()
    
    # Create Chunking Judge
    chunking_judge = ChunkingJudge(ollama_client=ollama_client)
    
    # Test documents - using files from the project root
    test_files = [
        "test_document.txt",
        "technical_documentation.md",
        "test_data.csv"
    ]
    
    results = []
    
    for filename in test_files:
        if not os.path.exists(filename):
            logger.warning(f"Warning: {filename} not found, skipping...")
            continue
            
        logger.info(f"\n----- Testing with {filename} -----\n")
        
        # Read document content
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Create document object
        document = Document(
            id=f"test-{filename}",
            filename=filename,
            content=content
        )
        
        # Analyze document with Chunking Judge
        logger.info(f"Analyzing document: {filename}")
        analysis_result = await chunking_judge.analyze_document(document)
        
        # Print analysis result
        logger.info("\nChunking Judge Analysis Result:")
        logger.info(f"Strategy: {analysis_result['strategy']}")
        logger.info(f"Parameters: chunk_size={analysis_result['parameters']['chunk_size']}, " +
              f"chunk_overlap={analysis_result['parameters']['chunk_overlap']}")
        logger.info(f"Justification: {analysis_result['justification']}")
        
        # Store results
        result = {
            "filename": filename,
            "strategy": analysis_result["strategy"],
            "parameters": analysis_result["parameters"],
            "justification": analysis_result["justification"]
        }
        results.append(result)
        
        logger.info("\n" + "="*50)
    
    # Save results to a JSON file
    results_file = "chunking_judge_test_results.json"
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"\nTest results saved to {results_file}")
    
    return results

async def main():
    """Main function"""
    logger.info("Starting Chunking Judge Phase 1 test...")
    
    # Check if Chunking Judge is enabled
    if not USE_CHUNKING_JUDGE:
        logger.warning("Warning: Chunking Judge is disabled in configuration.")
        logger.warning("To enable it, set USE_CHUNKING_JUDGE=True in .env or app/core/config.py")
    
    results = await test_chunking_judge()
    
    # Print summary
    logger.info("\n===== Chunking Judge Test Summary =====")
    for result in results:
        logger.info(f"File: {result['filename']}")
        logger.info(f"Strategy: {result['strategy']}")
        logger.info(f"Parameters: chunk_size={result['parameters']['chunk_size']}, " +
              f"chunk_overlap={result['parameters']['chunk_overlap']}")
        logger.info("---")
    
    logger.info("\nChunking Judge Phase 1 test completed.")

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_chunking_judge_real.py
================
"""
Test script for Phase 1 of the LLM-enhanced RAG system: Chunking Judge component.
This script tests the Chunking Judge's ability to analyze different document types
and recommend appropriate chunking strategies using the real Ollama client.
"""
import asyncio
import os
import sys
import json
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("chunking_judge_test")

# Add the app directory to the Python path if needed
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.document import Document
from app.rag.agents.chunking_judge import ChunkingJudge
from app.rag.ollama_client import OllamaClient
from app.core.config import USE_CHUNKING_JUDGE, CHUNKING_JUDGE_MODEL

async def test_chunking_judge_with_real_ollama():
    """Test the Chunking Judge with different document types using the real Ollama client"""
    logger.info("\n===== Testing Chunking Judge Component with Real Ollama (Phase 1) =====\n")
    
    # Use real Ollama client
    logger.info(f"Using real Ollama client with model: {CHUNKING_JUDGE_MODEL}")
    ollama_client = OllamaClient()
    
    # Create Chunking Judge
    chunking_judge = ChunkingJudge(ollama_client=ollama_client)
    
    # Test documents - using files from the project root
    test_files = [
        "test_document.txt",
        "technical_documentation.md",
        "test_data.csv"
    ]
    
    results = []
    
    for filename in test_files:
        if not os.path.exists(filename):
            logger.warning(f"Warning: {filename} not found, skipping...")
            continue
            
        logger.info(f"\n----- Testing with {filename} -----\n")
        
        # Read document content
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Create document object
        document = Document(
            id=f"test-{filename}",
            filename=filename,
            content=content
        )
        
        # Analyze document with Chunking Judge
        logger.info(f"Analyzing document: {filename}")
        analysis_result = await chunking_judge.analyze_document(document)
        
        # Print analysis result
        logger.info("\nChunking Judge Analysis Result:")
        logger.info(f"Strategy: {analysis_result['strategy']}")
        logger.info(f"Parameters: chunk_size={analysis_result['parameters']['chunk_size']}, " +
              f"chunk_overlap={analysis_result['parameters']['chunk_overlap']}")
        logger.info(f"Justification: {analysis_result['justification']}")
        
        # Store results
        result = {
            "filename": filename,
            "strategy": analysis_result["strategy"],
            "parameters": analysis_result["parameters"],
            "justification": analysis_result["justification"]
        }
        results.append(result)
        
        logger.info("\n" + "="*50)
    
    # Save results to a JSON file
    results_file = "chunking_judge_real_results.json"
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"\nTest results saved to {results_file}")
    
    return results

async def main():
    """Main function"""
    logger.info("Starting Chunking Judge Phase 1 test with real Ollama...")
    
    # Check if Chunking Judge is enabled
    if not USE_CHUNKING_JUDGE:
        logger.warning("Warning: Chunking Judge is disabled in configuration.")
        logger.warning("To enable it, set USE_CHUNKING_JUDGE=True in .env or app/core/config.py")
    
    results = await test_chunking_judge_with_real_ollama()
    
    # Print summary
    logger.info("\n===== Chunking Judge Test Summary =====")
    for result in results:
        logger.info(f"File: {result['filename']}")
        logger.info(f"Strategy: {result['strategy']}")
        logger.info(f"Parameters: chunk_size={result['parameters']['chunk_size']}, " +
              f"chunk_overlap={result['parameters']['chunk_overlap']}")
        logger.info("---")
    
    logger.info("\nChunking Judge Phase 1 test with real Ollama completed.")

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_edge_cases.py
================
#!/usr/bin/env python3
"""
Edge case test suite for the Metis RAG system.
This test suite focuses on unusual inputs, error handling, and system resilience.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
import random
import string
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, MagicMock, AsyncMock
from io import BytesIO

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_edge_cases")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document, Chunk
from app.models.chat import ChatQuery
from app.main import app

# Test client
client = TestClient(app)

# Helper functions
def generate_random_string(length):
    """Generate a random string of specified length"""
    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))

def generate_binary_file(size_kb):
    """Generate a binary file of specified size in KB"""
    return os.urandom(size_kb * 1024)

@pytest.fixture
def test_dir():
    """Create a temporary directory for test files"""
    temp_dir = tempfile.mkdtemp(prefix="metis_rag_edge_")
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.mark.asyncio
async def test_empty_document():
    """Test handling of empty documents"""
    # Create an empty document
    empty_doc = Document(
        id=str(uuid.uuid4()),
        filename="empty.txt",
        content=""
    )
    
    # Process the document
    processor = DocumentProcessor()
    
    try:
        processed_doc = await processor.process_document(empty_doc)
        
        # Check if processing succeeded
        assert len(processed_doc.chunks) == 0, "Empty document should result in no chunks"
        logger.info("Empty document processed successfully with 0 chunks")
        
    except Exception as e:
        # Check if the error is handled gracefully
        logger.info(f"Empty document processing resulted in error: {str(e)}")
        assert False, f"Empty document should be handled gracefully, but got error: {str(e)}"

@pytest.mark.asyncio
async def test_very_long_document(test_dir):
    """Test handling of very long documents"""
    # Create a very long document (10 MB)
    file_path = os.path.join(test_dir, "very_long.txt")
    content = generate_random_string(10 * 1024 * 1024 // 10)  # 10 MB of text (approximated)
    
    with open(file_path, "w") as f:
        f.write(content)
    
    # Create document object
    long_doc = Document(
        id=str(uuid.uuid4()),
        filename="very_long.txt",
        content=content
    )
    
    # Process the document
    processor = DocumentProcessor()
    
    try:
        processed_doc = await processor.process_document(long_doc)
        
        # Check if processing succeeded
        assert len(processed_doc.chunks) > 0, "Long document should be chunked"
        logger.info(f"Very long document processed successfully with {len(processed_doc.chunks)} chunks")
        
    except Exception as e:
        # Check if the error is handled gracefully
        logger.info(f"Very long document processing resulted in error: {str(e)}")
        assert False, f"Very long document should be handled gracefully, but got error: {str(e)}"

@pytest.mark.asyncio
async def test_malformed_document(test_dir):
    """Test handling of malformed documents"""
    # Create a malformed document (binary data with text extension)
    file_path = os.path.join(test_dir, "malformed.txt")
    content = generate_binary_file(10)  # 10 KB of binary data
    
    with open(file_path, "wb") as f:
        f.write(content)
    
    # Try to upload the malformed document via API
    with open(file_path, "rb") as f:
        response = client.post(
            "/api/documents/upload",
            files={"file": ("malformed.txt", f, "text/plain")}
        )
    
    # Check if the API handles the malformed document gracefully
    logger.info(f"Malformed document upload response: {response.status_code}")
    logger.info(f"Response content: {response.content}")
    
    # The API should either accept the file (status 200) or reject it with a clear error
    if response.status_code == 200:
        # If accepted, try to process it
        upload_data = response.json()
        document_id = upload_data["document_id"]
        
        process_response = client.post(
            "/api/documents/process",
            json={"document_ids": [document_id]}
        )
        
        logger.info(f"Malformed document processing response: {process_response.status_code}")
        logger.info(f"Process response content: {process_response.content}")
        
        # Clean up
        client.delete(f"/api/documents/{document_id}")
    
    # We don't assert specific behavior here, just log what happens
    # The important thing is that the system doesn't crash

@pytest.mark.asyncio
async def test_very_long_query():
    """Test handling of very long queries"""
    # Create a very long query (10 KB)
    long_query = generate_random_string(10 * 1024)
    
    # Try to query the system
    response = client.post(
        "/api/chat/query",
        json={
            "message": long_query,
            "use_rag": True,
            "stream": False
        }
    )
    
    # Check if the API handles the long query gracefully
    logger.info(f"Very long query response: {response.status_code}")
    
    # The API should either process the query or reject it with a clear error
    assert response.status_code in [200, 400, 422], f"Unexpected status code: {response.status_code}"
    
    if response.status_code == 200:
        logger.info("Very long query was processed successfully")
    else:
        logger.info(f"Very long query was rejected with status {response.status_code}")
        logger.info(f"Response content: {response.content}")

@pytest.mark.asyncio
async def test_special_characters_query():
    """Test handling of queries with special characters"""
    # Create queries with special characters
    special_queries = [
        "What is RAG? <script>alert('XSS')</script>",
        "SELECT * FROM documents WHERE content LIKE '%secret%'",
        "curl -X POST http://example.com --data 'payload'",
        "Document\0with\0null\0bytes",
        "Multi\nline\nquery\nwith\nnewlines",
        "Query with emoji  and Unicode characters ",
        "Query with control characters \x01\x02\x03\x04\x05"
    ]
    
    results = []
    
    for query in special_queries:
        # Try to query the system
        response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        # Check if the API handles the special query gracefully
        logger.info(f"Special query '{query[:20]}...' response: {response.status_code}")
        
        # Store results
        results.append({
            "query": query,
            "status_code": response.status_code,
            "success": response.status_code == 200
        })
        
        # The API should either process the query or reject it with a clear error
        assert response.status_code in [200, 400, 422], f"Unexpected status code: {response.status_code}"
    
    # Save results to file
    results_path = "test_special_queries_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Special queries test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_network_failure():
    """Test handling of network failures when communicating with Ollama"""
    # Create a mock RAG engine with a failing Ollama client
    ollama_client = AsyncMock()
    ollama_client.generate.side_effect = Exception("Simulated network failure")
    
    rag_engine = RAGEngine(
        ollama_client=ollama_client,
        vector_store=AsyncMock()
    )
    
    # Try to query the RAG engine
    try:
        response = await rag_engine.query(
            query="Test query",
            use_rag=False,  # Disable RAG to focus on Ollama failure
            stream=False
        )
        
        # Check if the error is handled gracefully
        logger.info(f"Network failure response: {response}")
        assert "error" in response, "Network failure should result in an error response"
        
    except Exception as e:
        # If an exception is raised, the error wasn't handled gracefully
        logger.error(f"Network failure resulted in unhandled exception: {str(e)}")
        assert False, f"Network failure should be handled gracefully, but got exception: {str(e)}"

@pytest.mark.asyncio
async def test_invalid_model():
    """Test handling of invalid model name"""
    # Try to query with an invalid model name
    response = client.post(
        "/api/chat/query",
        json={
            "message": "Test query",
            "model": "non_existent_model_12345",
            "use_rag": False,
            "stream": False
        }
    )
    
    # Check if the API handles the invalid model gracefully
    logger.info(f"Invalid model response: {response.status_code}")
    logger.info(f"Response content: {response.content}")
    
    # The API should either fall back to a default model or return a clear error
    assert response.status_code in [200, 400, 422], f"Unexpected status code: {response.status_code}"

@pytest.mark.asyncio
async def test_concurrent_document_processing():
    """Test handling of concurrent document processing requests"""
    # Create multiple small documents
    document_count = 5
    document_ids = []
    
    for i in range(document_count):
        # Create a small document
        content = f"Test document {i}\n" * 100
        
        # Upload the document
        file_obj = BytesIO(content.encode())
        file_obj.name = f"concurrent_test_{i}.txt"
        
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (file_obj.name, file_obj, "text/plain")}
        )
        
        assert upload_response.status_code == 200
        upload_data = upload_response.json()
        document_ids.append(upload_data["document_id"])
    
    # Process documents concurrently
    async def process_document(doc_id):
        process_response = client.post(
            "/api/documents/process",
            json={"document_ids": [doc_id]}
        )
        return {
            "document_id": doc_id,
            "status_code": process_response.status_code,
            "success": process_response.status_code == 200
        }
    
    # Create tasks for concurrent processing
    tasks = [process_document(doc_id) for doc_id in document_ids]
    
    # Execute tasks concurrently
    results = await asyncio.gather(*tasks)
    
    # Check results
    success_count = sum(1 for r in results if r["success"])
    logger.info(f"Concurrent processing results: {success_count}/{len(results)} successful")
    
    # Clean up
    for doc_id in document_ids:
        client.delete(f"/api/documents/{doc_id}")
    
    # Save results to file
    results_path = "test_concurrent_processing_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Concurrent processing test results saved to {os.path.abspath(results_path)}")
    
    # Assert that at least some documents were processed successfully
    assert success_count > 0, "No documents were processed successfully"

@pytest.mark.asyncio
async def test_invalid_file_types():
    """Test handling of invalid file types"""
    invalid_files = [
        ("executable.exe", generate_binary_file(10), "application/octet-stream"),
        ("image.jpg", generate_binary_file(10), "image/jpeg"),
        ("archive.zip", generate_binary_file(10), "application/zip"),
        ("empty.txt", b"", "text/plain")
    ]
    
    results = []
    
    for filename, content, content_type in invalid_files:
        # Try to upload the invalid file
        file_obj = BytesIO(content)
        file_obj.name = filename
        
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (filename, file_obj, content_type)}
        )
        
        # Check if the API handles the invalid file gracefully
        logger.info(f"Invalid file '{filename}' upload response: {upload_response.status_code}")
        
        # Store results
        result = {
            "filename": filename,
            "content_type": content_type,
            "status_code": upload_response.status_code,
            "accepted": upload_response.status_code == 200
        }
        
        # If the file was accepted, try to process it
        if upload_response.status_code == 200:
            upload_data = upload_response.json()
            document_id = upload_data["document_id"]
            
            process_response = client.post(
                "/api/documents/process",
                json={"document_ids": [document_id]}
            )
            
            result["process_status_code"] = process_response.status_code
            result["process_success"] = process_response.status_code == 200
            
            # Clean up
            client.delete(f"/api/documents/{document_id}")
        
        results.append(result)
    
    # Save results to file
    results_path = "test_invalid_files_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Invalid file types test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_malformed_requests():
    """Test handling of malformed API requests"""
    malformed_requests = [
        # Missing required fields
        {"endpoint": "/api/chat/query", "data": {}, "method": "POST"},
        # Invalid data types
        {"endpoint": "/api/chat/query", "data": {"message": 123, "use_rag": "yes"}, "method": "POST"},
        # Extra fields
        {"endpoint": "/api/chat/query", "data": {"message": "Test", "invalid_field": "value"}, "method": "POST"},
        # Invalid JSON
        {"endpoint": "/api/documents/process", "data": "not_json", "method": "POST"},
        # Invalid HTTP method
        {"endpoint": "/api/documents/upload", "data": {}, "method": "PUT"}
    ]
    
    results = []
    
    for req in malformed_requests:
        # Try to make the malformed request
        try:
            if req["method"] == "POST":
                if req["data"] == "not_json":
                    # Send invalid JSON
                    response = client.post(
                        req["endpoint"],
                        data="not_json",
                        headers={"Content-Type": "application/json"}
                    )
                else:
                    # Send normal JSON
                    response = client.post(req["endpoint"], json=req["data"])
            elif req["method"] == "PUT":
                response = client.put(req["endpoint"], json=req["data"])
            else:
                response = client.get(req["endpoint"])
            
            # Check if the API handles the malformed request gracefully
            logger.info(f"Malformed request to {req['endpoint']} response: {response.status_code}")
            
            # Store results
            results.append({
                "endpoint": req["endpoint"],
                "method": req["method"],
                "data": str(req["data"]),
                "status_code": response.status_code,
                "is_error": response.status_code >= 400
            })
            
            # The API should return an error for malformed requests
            assert response.status_code >= 400, f"Malformed request should return error, got {response.status_code}"
            
        except Exception as e:
            # If an exception is raised, the error wasn't handled gracefully
            logger.error(f"Malformed request resulted in unhandled exception: {str(e)}")
            results.append({
                "endpoint": req["endpoint"],
                "method": req["method"],
                "data": str(req["data"]),
                "exception": str(e),
                "is_error": True
            })
    
    # Save results to file
    results_path = "test_malformed_requests_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Malformed requests test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_vector_store_resilience():
    """Test vector store resilience to invalid operations"""
    # Create a vector store
    vector_store = VectorStore()
    
    # Test operations that might cause issues
    edge_cases = [
        # Search with empty query
        {"operation": "search", "args": {"query": ""}},
        # Search with very long query
        {"operation": "search", "args": {"query": generate_random_string(10000)}},
        # Search with invalid filter
        {"operation": "search", "args": {"query": "test", "filter_criteria": {"invalid": "filter"}}},
        # Delete non-existent document
        {"operation": "delete", "args": {"document_id": "non_existent_id"}},
        # Update metadata for non-existent document
        {"operation": "update_metadata", "args": {"document_id": "non_existent_id", "metadata_update": {"tag": "value"}}}
    ]
    
    results = []
    
    for case in edge_cases:
        try:
            if case["operation"] == "search":
                await vector_store.search(**case["args"])
            elif case["operation"] == "delete":
                vector_store.delete_document(**case["args"])
            elif case["operation"] == "update_metadata":
                await vector_store.update_document_metadata(**case["args"])
            
            # If we get here, the operation didn't raise an exception
            logger.info(f"Vector store {case['operation']} succeeded with args: {case['args']}")
            results.append({
                "operation": case["operation"],
                "args": str(case["args"]),
                "success": True
            })
            
        except Exception as e:
            # The operation raised an exception
            logger.info(f"Vector store {case['operation']} failed with args: {case['args']}, error: {str(e)}")
            results.append({
                "operation": case["operation"],
                "args": str(case["args"]),
                "success": False,
                "error": str(e)
            })
    
    # Save results to file
    results_path = "test_vector_store_resilience_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Vector store resilience test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_generate_edge_case_report():
    """Generate a comprehensive edge case test report"""
    # Check if all result files exist
    result_files = [
        "test_special_queries_results.json",
        "test_concurrent_processing_results.json",
        "test_invalid_files_results.json",
        "test_malformed_requests_results.json",
        "test_vector_store_resilience_results.json"
    ]
    
    missing_files = [f for f in result_files if not os.path.exists(f)]
    
    if missing_files:
        logger.warning(f"Missing result files: {missing_files}")
        logger.warning("Run the individual edge case tests first")
        return
    
    # Load all results
    results = {}
    
    for file_path in result_files:
        with open(file_path, "r") as f:
            results[file_path] = json.load(f)
    
    # Generate report
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "special_queries": {
                "total": len(results["test_special_queries_results.json"]),
                "success": sum(1 for r in results["test_special_queries_results.json"] if r["success"])
            },
            "concurrent_processing": {
                "total": len(results["test_concurrent_processing_results.json"]),
                "success": sum(1 for r in results["test_concurrent_processing_results.json"] if r["success"])
            },
            "invalid_files": {
                "total": len(results["test_invalid_files_results.json"]),
                "accepted": sum(1 for r in results["test_invalid_files_results.json"] if r["accepted"]),
                "processed": sum(1 for r in results["test_invalid_files_results.json"] if r.get("process_success", False))
            },
            "malformed_requests": {
                "total": len(results["test_malformed_requests_results.json"]),
                "error_responses": sum(1 for r in results["test_malformed_requests_results.json"] if r["is_error"])
            },
            "vector_store_resilience": {
                "total": len(results["test_vector_store_resilience_results.json"]),
                "success": sum(1 for r in results["test_vector_store_resilience_results.json"] if r["success"])
            }
        },
        "detailed_results": results
    }
    
    # Save report
    report_path = "edge_case_test_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Edge case test report saved to {os.path.abspath(report_path)}")
    
    # Generate HTML report
    html_report = f"""<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Edge Case Test Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        .section {{ margin-bottom: 30px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .success {{ color: green; }}
        .failure {{ color: red; }}
    </style>
</head>
<body>
    <h1>Metis RAG Edge Case Test Report</h1>
    <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="section">
        <h2>Summary</h2>
        <table>
            <tr>
                <th>Test Category</th>
                <th>Total Tests</th>
                <th>Success Rate</th>
            </tr>
            <tr>
                <td>Special Character Queries</td>
                <td>{report["summary"]["special_queries"]["total"]}</td>
                <td>{report["summary"]["special_queries"]["success"]}/{report["summary"]["special_queries"]["total"]} ({report["summary"]["special_queries"]["success"]/report["summary"]["special_queries"]["total"]*100:.1f}%)</td>
            </tr>
            <tr>
                <td>Concurrent Document Processing</td>
                <td>{report["summary"]["concurrent_processing"]["total"]}</td>
                <td>{report["summary"]["concurrent_processing"]["success"]}/{report["summary"]["concurrent_processing"]["total"]} ({report["summary"]["concurrent_processing"]["success"]/report["summary"]["concurrent_processing"]["total"]*100:.1f}%)</td>
            </tr>
            <tr>
                <td>Invalid File Types</td>
                <td>{report["summary"]["invalid_files"]["total"]}</td>
                <td>Accepted: {report["summary"]["invalid_files"]["accepted"]}/{report["summary"]["invalid_files"]["total"]} ({report["summary"]["invalid_files"]["accepted"]/report["summary"]["invalid_files"]["total"]*100:.1f}%)<br>
                Processed: {report["summary"]["invalid_files"]["processed"]}/{report["summary"]["invalid_files"]["accepted"] if report["summary"]["invalid_files"]["accepted"] > 0 else 1} ({(report["summary"]["invalid_files"]["processed"]/report["summary"]["invalid_files"]["accepted"]*100 if report["summary"]["invalid_files"]["accepted"] > 0 else 0):.1f}%)</td>
            </tr>
            <tr>
                <td>Malformed API Requests</td>
                <td>{report["summary"]["malformed_requests"]["total"]}</td>
                <td>Error Responses: {report["summary"]["malformed_requests"]["error_responses"]}/{report["summary"]["malformed_requests"]["total"]} ({report["summary"]["malformed_requests"]["error_responses"]/report["summary"]["malformed_requests"]["total"]*100:.1f}%)</td>
            </tr>
            <tr>
                <td>Vector Store Resilience</td>
                <td>{report["summary"]["vector_store_resilience"]["total"]}</td>
                <td>{report["summary"]["vector_store_resilience"]["success"]}/{report["summary"]["vector_store_resilience"]["total"]} ({report["summary"]["vector_store_resilience"]["success"]/report["summary"]["vector_store_resilience"]["total"]*100:.1f}%)</td>
            </tr>
        </table>
    </div>
    
    <div class="section">
        <h2>Special Character Queries</h2>
        <table>
            <tr>
                <th>Query</th>
                <th>Status Code</th>
                <th>Result</th>
            </tr>
            {("".join([f"<tr><td>{r['query'][:50]}...</td><td>{r['status_code']}</td><td class=\"{'success' if r['success'] else 'failure'}\">{('Success' if r['success'] else 'Failure')}</td></tr>" for r in results["test_special_queries_results.json"]]))}
        </table>
    </div>
    
    <div class="section">
        <h2>Invalid File Types</h2>
        <table>
            <tr>
                <th>Filename</th>
                <th>Content Type</th>
                <th>Upload Status</th>
                <th>Processing Status</th>
            </tr>
            {("".join([f"<tr><td>{r['filename']}</td><td>{r['content_type']}</td><td class=\"{'success' if r['accepted'] else 'failure'}\">{r['status_code']} ({('Accepted' if r['accepted'] else 'Rejected')})</td><td class=\"{'success' if r.get('process_success', False) else 'failure'}\">{r.get('process_status_code', 'N/A')} ({('Success' if r.get('process_success', False) else 'N/A' if not r['accepted'] else 'Failure')})</td></tr>" for r in results["test_invalid_files_results.json"]]))}
        </table>
    </div>
    
    <div class="section">
        <h2>Malformed API Requests</h2>
        <table>
            <tr>
                <th>Endpoint</th>
                <th>Method</th>
                <th>Data</th>
                <th>Status Code</th>
                <th>Result</th>
            </tr>
            {("".join([f"<tr><td>{r['endpoint']}</td><td>{r['method']}</td><td>{r['data'][:30]}...</td><td>{r.get('status_code', 'Exception')}</td><td class=\"{'success' if r['is_error'] else 'failure'}\">{('Error Response' if r['is_error'] else 'Unexpected Success')}</td></tr>" for r in results["test_malformed_requests_results.json"]]))}
        </table>
    </div>
    
    <div class="section">
        <h2>Vector Store Resilience</h2>
        <table>
            <tr>
                <th>Operation</th>
                <th>Arguments</th>
                <th>Result</th>
                <th>Error</th>
            </tr>
            {("".join([f"<tr><td>{r['operation']}</td><td>{r['args'][:30]}...</td><td class=\"{'success' if r['success'] else 'failure'}\">{('Success' if r['success'] else 'Failure')}</td><td>{r.get('error', 'N/A')}</td></tr>" for r in results["test_vector_store_resilience_results.json"]]))}
        </table>
    </div>
</body>
</html>
"""
    
    html_report_path = "edge_case_test_report.html"
    with open(html_report_path, "w") as f:
        f.write(html_report)
    
    logger.info(f"HTML edge case report saved to {os.path.abspath(html_report_path)}")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_file_handling.py
================
#!/usr/bin/env python3
"""
Test suite for evaluating file handling capabilities in the Metis RAG system.
This test suite focuses on different file types, multiple file uploads, and large files.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
import random
import string

import pytest
from fastapi.testclient import TestClient
from io import BytesIO

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_file_handling")

# Import RAG components
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document
from app.main import app

# Test client
client = TestClient(app)

# Test file templates
TEST_FILE_TEMPLATES = {
    "txt": {
        "content": """This is a test text file for the Metis RAG system.
It contains multiple lines of text that will be processed and indexed.
The document processor should handle plain text files efficiently.
This file will be used to test the text file processing capabilities."""
    },
    "md": {
        "content": """# Test Markdown File

## Introduction
This is a test markdown file for the Metis RAG system.

## Features
- Supports headers
- Supports lists
- Supports **bold** and *italic* text

## Code Examples
```python
def test_function():
    return "This is a test"
```

## Conclusion
The document processor should handle markdown files correctly."""
    },
    "csv": {
        "content": """id,name,description,value
1,Item 1,This is the first item,100
2,Item 2,This is the second item,200
3,Item 3,This is the third item,300
4,Item 4,This is the fourth item,400
5,Item 5,This is the fifth item,500"""
    }
}

def generate_random_text(size_kb):
    """Generate random text of specified size in KB"""
    chars = string.ascii_letters + string.digits + string.punctuation + ' \n\t'
    # 1 KB is approximately 1000 characters
    return ''.join(random.choice(chars) for _ in range(size_kb * 1000))

@pytest.fixture
def test_files_dir():
    """Create a temporary directory for test files"""
    temp_dir = tempfile.mkdtemp(prefix="metis_rag_test_")
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.fixture
def create_test_files(test_files_dir):
    """Create test files of different types"""
    file_paths = {}
    
    # Create basic test files
    for ext, template in TEST_FILE_TEMPLATES.items():
        file_path = os.path.join(test_files_dir, f"test.{ext}")
        with open(file_path, "w") as f:
            f.write(template["content"])
        file_paths[ext] = file_path
    
    # Create a large text file (1 MB)
    large_file_path = os.path.join(test_files_dir, "large_file.txt")
    with open(large_file_path, "w") as f:
        f.write(generate_random_text(1024))  # 1024 KB = 1 MB
    file_paths["large_txt"] = large_file_path
    
    return file_paths

@pytest.fixture
def document_processor():
    """Create a document processor instance"""
    return DocumentProcessor()

@pytest.mark.asyncio
async def test_file_type_support(document_processor, create_test_files):
    """Test processing of different file types"""
    results = []
    
    for file_type, file_path in create_test_files.items():
        if file_type == "large_txt":
            continue  # Skip large file for this test
            
        logger.info(f"Testing file type: {file_type}")
        
        # Create a document object
        with open(file_path, "r") as f:
            content = f.read()
            
        document = Document(
            id=str(uuid.uuid4()),
            filename=os.path.basename(file_path),
            content=content
        )
        
        # Process the document
        try:
            processed_doc = await document_processor.process_document(document)
            
            # Check if chunks were created
            chunk_count = len(processed_doc.chunks)
            success = chunk_count > 0
            
            logger.info(f"Processed {file_type} file: {success}, {chunk_count} chunks created")
            
            # Store results
            results.append({
                "file_type": file_type,
                "success": success,
                "chunk_count": chunk_count,
                "avg_chunk_size": sum(len(chunk.content) for chunk in processed_doc.chunks) / chunk_count if chunk_count > 0 else 0
            })
            
            # Assertions
            assert success, f"Failed to process {file_type} file"
            assert chunk_count > 0, f"No chunks created for {file_type} file"
            
        except Exception as e:
            logger.error(f"Error processing {file_type} file: {str(e)}")
            results.append({
                "file_type": file_type,
                "success": False,
                "error": str(e)
            })
            assert False, f"Error processing {file_type} file: {str(e)}"
    
    # Save results to file
    results_path = "test_file_type_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"File type test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_chunking_strategies(document_processor, create_test_files):
    """Test different chunking strategies"""
    results = []
    
    chunking_strategies = ["recursive", "token", "markdown"]
    file_path = create_test_files["md"]  # Use markdown file for testing chunking strategies
    
    with open(file_path, "r") as f:
        content = f.read()
    
    for strategy in chunking_strategies:
        logger.info(f"Testing chunking strategy: {strategy}")
        
        # Create a custom document processor with the specified chunking strategy
        custom_processor = DocumentProcessor(chunking_strategy=strategy)
        
        # Create a document object
        document = Document(
            id=str(uuid.uuid4()),
            filename=os.path.basename(file_path),
            content=content
        )
        
        # Process the document
        try:
            processed_doc = await custom_processor.process_document(document)
            
            # Check if chunks were created
            chunk_count = len(processed_doc.chunks)
            success = chunk_count > 0
            
            logger.info(f"Processed with {strategy} strategy: {success}, {chunk_count} chunks created")
            
            # Store results
            results.append({
                "chunking_strategy": strategy,
                "success": success,
                "chunk_count": chunk_count,
                "avg_chunk_size": sum(len(chunk.content) for chunk in processed_doc.chunks) / chunk_count if chunk_count > 0 else 0
            })
            
            # Assertions
            assert success, f"Failed to process with {strategy} strategy"
            assert chunk_count > 0, f"No chunks created with {strategy} strategy"
            
        except Exception as e:
            logger.error(f"Error processing with {strategy} strategy: {str(e)}")
            results.append({
                "chunking_strategy": strategy,
                "success": False,
                "error": str(e)
            })
            assert False, f"Error processing with {strategy} strategy: {str(e)}"
    
    # Save results to file
    results_path = "test_chunking_strategy_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"Chunking strategy test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_large_file_handling(document_processor, create_test_files):
    """Test handling of large files"""
    results = []
    
    file_path = create_test_files["large_txt"]
    
    logger.info(f"Testing large file handling")
    
    # Create a document object
    with open(file_path, "r") as f:
        content = f.read()
        
    document = Document(
        id=str(uuid.uuid4()),
        filename=os.path.basename(file_path),
        content=content
    )
    
    # Process the document
    try:
        start_time = datetime.now()
        processed_doc = await document_processor.process_document(document)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Check if chunks were created
        chunk_count = len(processed_doc.chunks)
        success = chunk_count > 0
        
        file_size_kb = len(content) / 1000
        processing_speed = file_size_kb / processing_time if processing_time > 0 else 0
        
        logger.info(f"Processed large file: {success}, {chunk_count} chunks created")
        logger.info(f"Processing time: {processing_time:.2f} seconds")
        logger.info(f"Processing speed: {processing_speed:.2f} KB/s")
        
        # Store results
        results.append({
            "file_size_kb": file_size_kb,
            "success": success,
            "chunk_count": chunk_count,
            "processing_time_seconds": processing_time,
            "processing_speed_kb_per_second": processing_speed,
            "avg_chunk_size": sum(len(chunk.content) for chunk in processed_doc.chunks) / chunk_count if chunk_count > 0 else 0
        })
        
        # Assertions
        assert success, "Failed to process large file"
        assert chunk_count > 0, "No chunks created for large file"
        
    except Exception as e:
        logger.error(f"Error processing large file: {str(e)}")
        results.append({
            "file_size_kb": len(content) / 1000,
            "success": False,
            "error": str(e)
        })
        assert False, f"Error processing large file: {str(e)}"
    
    # Save results to file
    results_path = "test_large_file_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"Large file test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_api_file_upload(create_test_files):
    """Test file upload through API"""
    results = []
    
    for file_type, file_path in create_test_files.items():
        if file_type == "large_txt":
            continue  # Skip large file for API test to avoid timeouts
            
        logger.info(f"Testing API upload for file type: {file_type}")
        
        # Read file content
        with open(file_path, "rb") as f:
            file_content = f.read()
            
        # Create file-like object for upload
        file_obj = BytesIO(file_content)
        file_obj.name = os.path.basename(file_path)
        
        # Upload the file
        try:
            upload_response = client.post(
                "/api/documents/upload",
                files={"file": (file_obj.name, file_obj, f"text/{file_type}")}
            )
            
            # Check upload response
            assert upload_response.status_code == 200, f"Upload failed with status {upload_response.status_code}"
            upload_data = upload_response.json()
            assert upload_data["success"] is True, "Upload response indicates failure"
            assert "document_id" in upload_data, "No document_id in upload response"
            
            document_id = upload_data["document_id"]
            
            # Process the document
            process_response = client.post(
                "/api/documents/process",
                json={"document_ids": [document_id]}
            )
            
            # Check process response
            assert process_response.status_code == 200, f"Processing failed with status {process_response.status_code}"
            process_data = process_response.json()
            assert process_data["success"] is True, "Process response indicates failure"
            
            # Get document info
            info_response = client.get(f"/api/documents/{document_id}")
            
            # Check info response
            assert info_response.status_code == 200, f"Info request failed with status {info_response.status_code}"
            doc_info = info_response.json()
            
            # Store results
            results.append({
                "file_type": file_type,
                "document_id": document_id,
                "filename": file_obj.name,
                "success": True,
                "chunk_count": doc_info.get("chunk_count", 0) if isinstance(doc_info, dict) else 0
            })
            
            # Clean up - delete the document
            delete_response = client.delete(f"/api/documents/{document_id}")
            assert delete_response.status_code == 200, f"Delete failed with status {delete_response.status_code}"
            
        except Exception as e:
            logger.error(f"Error in API test for {file_type} file: {str(e)}")
            results.append({
                "file_type": file_type,
                "filename": os.path.basename(file_path),
                "success": False,
                "error": str(e)
            })
            assert False, f"Error in API test for {file_type} file: {str(e)}"
    
    # Save results to file
    results_path = "test_api_upload_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"API upload test results saved to {os.path.abspath(results_path)}")

@pytest.mark.asyncio
async def test_multiple_file_upload(create_test_files):
    """Test uploading and processing multiple files simultaneously"""
    results = []
    
    # Skip large file for this test
    file_paths = {k: v for k, v in create_test_files.items() if k != "large_txt"}
    
    logger.info(f"Testing multiple file upload with {len(file_paths)} files")
    
    # Upload all files
    document_ids = []
    
    for file_type, file_path in file_paths.items():
        # Read file content
        with open(file_path, "rb") as f:
            file_content = f.read()
            
        # Create file-like object for upload
        file_obj = BytesIO(file_content)
        file_obj.name = os.path.basename(file_path)
        
        # Upload the file
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (file_obj.name, file_obj, f"text/{file_type}")}
        )
        
        # Check upload response
        assert upload_response.status_code == 200, f"Upload failed with status {upload_response.status_code}"
        upload_data = upload_response.json()
        assert upload_data["success"] is True, "Upload response indicates failure"
        
        document_ids.append(upload_data["document_id"])
    
    # Process all documents at once
    try:
        start_time = datetime.now()
        
        process_response = client.post(
            "/api/documents/process",
            json={"document_ids": document_ids}
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Check process response
        assert process_response.status_code == 200, f"Processing failed with status {process_response.status_code}"
        process_data = process_response.json()
        assert process_data["success"] is True, "Process response indicates failure"
        
        # Get info for all documents
        for doc_id in document_ids:
            info_response = client.get(f"/api/documents/{doc_id}")
            assert info_response.status_code == 200, f"Info request failed for document {doc_id}"
        
        # Store results
        results.append({
            "file_count": len(document_ids),
            "document_ids": document_ids,
            "success": True,
            "processing_time_seconds": processing_time,
            "processing_speed_files_per_second": len(document_ids) / processing_time if processing_time > 0 else 0
        })
        
        # Clean up - delete all documents
        for doc_id in document_ids:
            delete_response = client.delete(f"/api/documents/{doc_id}")
            assert delete_response.status_code == 200, f"Delete failed for document {doc_id}"
            
    except Exception as e:
        logger.error(f"Error in multiple file upload test: {str(e)}")
        results.append({
            "file_count": len(document_ids),
            "success": False,
            "error": str(e)
        })
        assert False, f"Error in multiple file upload test: {str(e)}"
    
    # Save results to file
    results_path = "test_multiple_upload_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"Multiple file upload test results saved to {os.path.abspath(results_path)}")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_performance.py
================
#!/usr/bin/env python3
"""
Performance benchmarking test suite for the Metis RAG system.
This test suite measures response time, throughput, and resource utilization.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
import psutil
import tempfile
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
import statistics
import concurrent.futures
from io import BytesIO

import pytest
from fastapi.testclient import TestClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_performance")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document, Chunk
from app.models.chat import ChatQuery
from app.main import app

# Test client
client = TestClient(app)

# Test document content
TEST_DOCUMENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

# Test queries for benchmarking
BENCHMARK_QUERIES = [
    "What is the architecture of Metis RAG?",
    "How does the document processing pipeline work?",
    "What is the role of the vector store?",
    "How does the frontend interface with the API layer?",
    "What is the purpose of the LLM integration component?",
    "Explain the chunking strategies used in document processing.",
    "How does the system handle metadata filtering?",
    "What technologies are used in the frontend layer?",
    "How does the RAG engine retrieve relevant information?",
    "What is the overall purpose of the Metis RAG system?"
]

@pytest.fixture
def test_document_dir():
    """Create a temporary directory for test documents"""
    temp_dir = tempfile.mkdtemp(prefix="metis_rag_perf_")
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)

@pytest.fixture
def create_test_document(test_document_dir):
    """Create a test document for performance testing"""
    file_path = os.path.join(test_document_dir, "technical_doc.md")
    with open(file_path, "w") as f:
        f.write(TEST_DOCUMENT)
    return file_path

@pytest.fixture
async def setup_vector_store(create_test_document):
    """Set up vector store with test document"""
    # Use a separate directory for test ChromaDB
    test_chroma_dir = "test_perf_chroma"
    os.makedirs(test_chroma_dir, exist_ok=True)
    
    # Initialize vector store
    vector_store = VectorStore(persist_directory=test_chroma_dir)
    
    # Create Document object
    with open(create_test_document, "r") as f:
        content = f.read()
        
    doc = Document(
        id="test_doc",
        filename=os.path.basename(create_test_document),
        content=content,
        tags=["technical", "documentation"],
        folder="/test"
    )
    
    # Create a single chunk for simplicity
    doc.chunks = [
        Chunk(
            id="test_chunk_0",
            content=content,
            metadata={
                "index": 0,
                "source": os.path.basename(create_test_document)
            }
        )
    ]
    
    # Add document to vector store
    await vector_store.add_document(doc)
    
    return vector_store, doc

@pytest.fixture
async def setup_rag_engine(setup_vector_store):
    """Set up RAG engine with test vector store"""
    vector_store, document = await setup_vector_store
    rag_engine = RAGEngine(vector_store=vector_store)
    return rag_engine, document

def get_system_metrics():
    """Get current system resource utilization"""
    process = psutil.Process(os.getpid())
    return {
        "cpu_percent": process.cpu_percent(),
        "memory_percent": process.memory_percent(),
        "memory_mb": process.memory_info().rss / (1024 * 1024),  # Convert to MB
        "threads": process.num_threads(),
        "system_cpu_percent": psutil.cpu_percent(),
        "system_memory_percent": psutil.virtual_memory().percent
    }

@pytest.mark.asyncio
async def test_query_response_time(setup_rag_engine):
    """Benchmark query response time"""
    rag_engine, document = await setup_rag_engine
    
    results = []
    
    for query in BENCHMARK_QUERIES:
        # Warm-up query to initialize any lazy-loaded components
        await rag_engine.query(
            query="Warm-up query",
            use_rag=True,
            stream=False
        )
        
        # Measure response time
        start_time = time.time()
        
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            stream=False
        )
        
        end_time = time.time()
        response_time_ms = (end_time - start_time) * 1000
        
        # Get system metrics
        metrics = get_system_metrics()
        
        # Log results
        logger.info(f"Query: {query}")
        logger.info(f"Response time: {response_time_ms:.2f} ms")
        logger.info(f"CPU: {metrics['cpu_percent']:.1f}%, Memory: {metrics['memory_mb']:.1f} MB")
        
        # Store results
        results.append({
            "query": query,
            "response_time_ms": response_time_ms,
            "answer_length": len(response.get("answer", "")),
            "sources_count": len(response.get("sources", [])),
            **metrics
        })
    
    # Calculate statistics
    response_times = [r["response_time_ms"] for r in results]
    stats = {
        "min_response_time_ms": min(response_times),
        "max_response_time_ms": max(response_times),
        "avg_response_time_ms": statistics.mean(response_times),
        "median_response_time_ms": statistics.median(response_times),
        "stddev_response_time_ms": statistics.stdev(response_times) if len(response_times) > 1 else 0,
        "total_queries": len(response_times)
    }
    
    # Save results to file
    results_path = "test_response_time_results.json"
    with open(results_path, "w") as f:
        json.dump({
            "results": results,
            "statistics": stats
        }, f, indent=2)
    
    logger.info(f"Response time test results saved to {os.path.abspath(results_path)}")
    logger.info(f"Average response time: {stats['avg_response_time_ms']:.2f} ms")
    
    # Assert reasonable response time (adjust threshold as needed)
    assert stats["avg_response_time_ms"] < 5000, f"Average response time too high: {stats['avg_response_time_ms']:.2f} ms"

@pytest.mark.asyncio
async def test_throughput(setup_rag_engine):
    """Benchmark system throughput with concurrent queries"""
    rag_engine, document = await setup_rag_engine
    
    results = []
    concurrency_levels = [1, 2, 5, 10]  # Number of concurrent queries
    
    for concurrency in concurrency_levels:
        logger.info(f"Testing throughput with concurrency level: {concurrency}")
        
        # Create a list of queries (repeat the benchmark queries if needed)
        queries = BENCHMARK_QUERIES * (concurrency // len(BENCHMARK_QUERIES) + 1)
        queries = queries[:concurrency]  # Limit to the desired concurrency level
        
        # Function to execute a single query
        async def execute_query(query):
            start_time = time.time()
            
            response = await rag_engine.query(
                query=query,
                use_rag=True,
                stream=False
            )
            
            end_time = time.time()
            return {
                "query": query,
                "response_time_ms": (end_time - start_time) * 1000,
                "answer_length": len(response.get("answer", "")),
                "sources_count": len(response.get("sources", []))
            }
        
        # Execute queries concurrently
        start_time = time.time()
        
        # Create tasks for all queries
        tasks = [execute_query(query) for query in queries]
        
        # Wait for all tasks to complete
        query_results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time_seconds = end_time - start_time
        
        # Calculate throughput
        throughput_qps = concurrency / total_time_seconds if total_time_seconds > 0 else 0
        
        # Get system metrics
        metrics = get_system_metrics()
        
        # Calculate statistics
        response_times = [r["response_time_ms"] for r in query_results]
        stats = {
            "min_response_time_ms": min(response_times),
            "max_response_time_ms": max(response_times),
            "avg_response_time_ms": statistics.mean(response_times),
            "median_response_time_ms": statistics.median(response_times),
            "stddev_response_time_ms": statistics.stdev(response_times) if len(response_times) > 1 else 0
        }
        
        # Log results
        logger.info(f"Concurrency: {concurrency}, Throughput: {throughput_qps:.2f} queries/second")
        logger.info(f"Avg response time: {stats['avg_response_time_ms']:.2f} ms")
        logger.info(f"CPU: {metrics['cpu_percent']:.1f}%, Memory: {metrics['memory_mb']:.1f} MB")
        
        # Store results
        results.append({
            "concurrency": concurrency,
            "total_time_seconds": total_time_seconds,
            "throughput_queries_per_second": throughput_qps,
            "query_count": len(query_results),
            "response_time_stats": stats,
            "system_metrics": metrics,
            "query_results": query_results
        })
    
    # Save results to file
    results_path = "test_throughput_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Throughput test results saved to {os.path.abspath(results_path)}")
    
    # Assert reasonable throughput scaling
    throughputs = [r["throughput_queries_per_second"] for r in results]
    assert throughputs[-1] > 0, "Throughput should be greater than zero"
    
    # Ideally, throughput should scale with concurrency, but this depends on system resources
    # This is a simple check that throughput doesn't collapse under load
    assert throughputs[-1] >= throughputs[0] * 0.5, "Throughput collapsed under load"

@pytest.mark.asyncio
async def test_resource_utilization(setup_rag_engine):
    """Benchmark resource utilization during sustained load"""
    rag_engine, document = await setup_rag_engine
    
    results = []
    duration_seconds = 30  # Duration of the test
    query_interval_seconds = 1  # Time between queries
    
    logger.info(f"Testing resource utilization over {duration_seconds} seconds")
    
    # Function to execute queries at regular intervals
    async def query_task():
        start_time = time.time()
        query_count = 0
        
        while time.time() - start_time < duration_seconds:
            # Select a query
            query = BENCHMARK_QUERIES[query_count % len(BENCHMARK_QUERIES)]
            
            # Execute query
            await rag_engine.query(
                query=query,
                use_rag=True,
                stream=False
            )
            
            query_count += 1
            
            # Wait for the next interval
            await asyncio.sleep(query_interval_seconds)
            
        return query_count
    
    # Function to monitor system resources
    async def monitor_task():
        start_time = time.time()
        metrics_list = []
        
        while time.time() - start_time < duration_seconds:
            # Get system metrics
            metrics = get_system_metrics()
            metrics["timestamp"] = time.time() - start_time
            metrics_list.append(metrics)
            
            # Wait before next measurement
            await asyncio.sleep(1)
            
        return metrics_list
    
    # Run both tasks concurrently
    query_task_obj = asyncio.create_task(query_task())
    monitor_task_obj = asyncio.create_task(monitor_task())
    
    # Wait for both tasks to complete
    query_count = await query_task_obj
    metrics_list = await monitor_task_obj
    
    # Calculate statistics
    cpu_percentages = [m["cpu_percent"] for m in metrics_list]
    memory_mbs = [m["memory_mb"] for m in metrics_list]
    
    stats = {
        "avg_cpu_percent": statistics.mean(cpu_percentages),
        "max_cpu_percent": max(cpu_percentages),
        "avg_memory_mb": statistics.mean(memory_mbs),
        "max_memory_mb": max(memory_mbs),
        "query_count": query_count,
        "queries_per_second": query_count / duration_seconds
    }
    
    # Log results
    logger.info(f"Executed {query_count} queries over {duration_seconds} seconds")
    logger.info(f"Average CPU: {stats['avg_cpu_percent']:.1f}%, Max CPU: {stats['max_cpu_percent']:.1f}%")
    logger.info(f"Average Memory: {stats['avg_memory_mb']:.1f} MB, Max Memory: {stats['max_memory_mb']:.1f} MB")
    
    # Store results
    results = {
        "duration_seconds": duration_seconds,
        "query_interval_seconds": query_interval_seconds,
        "query_count": query_count,
        "queries_per_second": query_count / duration_seconds,
        "statistics": stats,
        "metrics_timeline": metrics_list
    }
    
    # Save results to file
    results_path = "test_resource_utilization_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Resource utilization test results saved to {os.path.abspath(results_path)}")
    
    # No specific assertions for resource utilization, as acceptable values depend on the system

@pytest.mark.asyncio
async def test_api_performance(create_test_document):
    """Benchmark API endpoint performance"""
    results = []
    
    # Test document upload and processing performance
    with open(create_test_document, "rb") as f:
        file_content = f.read()
    
    # Measure upload performance
    upload_times = []
    document_ids = []
    
    for i in range(5):  # Upload the same document 5 times
        file_obj = BytesIO(file_content)
        file_obj.name = f"perf_test_{i}.md"
        
        start_time = time.time()
        
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": (file_obj.name, file_obj, "text/markdown")}
        )
        
        end_time = time.time()
        upload_time_ms = (end_time - start_time) * 1000
        
        assert upload_response.status_code == 200
        upload_data = upload_response.json()
        document_ids.append(upload_data["document_id"])
        
        upload_times.append(upload_time_ms)
        
        logger.info(f"Upload {i+1}: {upload_time_ms:.2f} ms")
    
    # Measure processing performance
    start_time = time.time()
    
    process_response = client.post(
        "/api/documents/process",
        json={"document_ids": document_ids}
    )
    
    end_time = time.time()
    process_time_ms = (end_time - start_time) * 1000
    
    assert process_response.status_code == 200
    
    logger.info(f"Processing {len(document_ids)} documents: {process_time_ms:.2f} ms")
    
    # Measure query performance
    query_times = []
    
    for query in BENCHMARK_QUERIES[:5]:  # Use first 5 queries
        start_time = time.time()
        
        query_response = client.post(
            "/api/chat/query",
            json={
                "message": query,
                "use_rag": True,
                "stream": False
            }
        )
        
        end_time = time.time()
        query_time_ms = (end_time - start_time) * 1000
        
        assert query_response.status_code == 200
        
        query_times.append(query_time_ms)
        
        logger.info(f"Query: {query}")
        logger.info(f"Query time: {query_time_ms:.2f} ms")
    
    # Calculate statistics
    upload_stats = {
        "min_upload_time_ms": min(upload_times),
        "max_upload_time_ms": max(upload_times),
        "avg_upload_time_ms": statistics.mean(upload_times)
    }
    
    query_stats = {
        "min_query_time_ms": min(query_times),
        "max_query_time_ms": max(query_times),
        "avg_query_time_ms": statistics.mean(query_times)
    }
    
    # Store results
    results = {
        "upload_performance": {
            "document_count": len(document_ids),
            "upload_times_ms": upload_times,
            "statistics": upload_stats
        },
        "processing_performance": {
            "document_count": len(document_ids),
            "process_time_ms": process_time_ms,
            "process_time_per_document_ms": process_time_ms / len(document_ids)
        },
        "query_performance": {
            "query_count": len(query_times),
            "query_times_ms": query_times,
            "statistics": query_stats
        }
    }
    
    # Save results to file
    results_path = "test_api_performance_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"API performance test results saved to {os.path.abspath(results_path)}")
    
    # Clean up - delete all documents
    for doc_id in document_ids:
        client.delete(f"/api/documents/{doc_id}")

@pytest.mark.asyncio
async def test_generate_performance_report():
    """Generate a comprehensive performance report"""
    # Check if all result files exist
    result_files = [
        "test_response_time_results.json",
        "test_throughput_results.json",
        "test_resource_utilization_results.json",
        "test_api_performance_results.json"
    ]
    
    missing_files = [f for f in result_files if not os.path.exists(f)]
    
    if missing_files:
        logger.warning(f"Missing result files: {missing_files}")
        logger.warning("Run the individual performance tests first")
        return
    
    # Load all results
    results = {}
    
    for file_path in result_files:
        with open(file_path, "r") as f:
            results[file_path] = json.load(f)
    
    # Generate report
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "response_time": {
                "avg_ms": results["test_response_time_results.json"]["statistics"]["avg_response_time_ms"],
                "min_ms": results["test_response_time_results.json"]["statistics"]["min_response_time_ms"],
                "max_ms": results["test_response_time_results.json"]["statistics"]["max_response_time_ms"]
            },
            "throughput": {
                "max_qps": max(r["throughput_queries_per_second"] for r in results["test_throughput_results.json"]),
                "concurrency_levels": [r["concurrency"] for r in results["test_throughput_results.json"]]
            },
            "resource_utilization": {
                "avg_cpu_percent": results["test_resource_utilization_results.json"]["statistics"]["avg_cpu_percent"],
                "max_cpu_percent": results["test_resource_utilization_results.json"]["statistics"]["max_cpu_percent"],
                "avg_memory_mb": results["test_resource_utilization_results.json"]["statistics"]["avg_memory_mb"],
                "max_memory_mb": results["test_resource_utilization_results.json"]["statistics"]["max_memory_mb"]
            },
            "api_performance": {
                "avg_upload_time_ms": results["test_api_performance_results.json"]["upload_performance"]["statistics"]["avg_upload_time_ms"],
                "avg_query_time_ms": results["test_api_performance_results.json"]["query_performance"]["statistics"]["avg_query_time_ms"],
                "process_time_per_document_ms": results["test_api_performance_results.json"]["processing_performance"]["process_time_per_document_ms"]
            }
        },
        "detailed_results": results
    }
    
    # Save report
    report_path = "performance_benchmark_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Performance benchmark report saved to {os.path.abspath(report_path)}")
    
    # Generate HTML report
    html_report = f"""<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Performance Benchmark Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        .section {{ margin-bottom: 30px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .chart {{ width: 100%; height: 300px; margin-top: 20px; }}
    </style>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <h1>Metis RAG Performance Benchmark Report</h1>
    <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="section">
        <h2>Summary</h2>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Average Response Time</td>
                <td>{report["summary"]["response_time"]["avg_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Maximum Throughput</td>
                <td>{report["summary"]["throughput"]["max_qps"]:.2f} queries/second</td>
            </tr>
            <tr>
                <td>Average CPU Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_cpu_percent"]:.1f}%</td>
            </tr>
            <tr>
                <td>Average Memory Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_memory_mb"]:.1f} MB</td>
            </tr>
            <tr>
                <td>Average Upload Time</td>
                <td>{report["summary"]["api_performance"]["avg_upload_time_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Average Query Time (API)</td>
                <td>{report["summary"]["api_performance"]["avg_query_time_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Document Processing Time</td>
                <td>{report["summary"]["api_performance"]["process_time_per_document_ms"]:.2f} ms per document</td>
            </tr>
        </table>
    </div>
    
    <div class="section">
        <h2>Response Time</h2>
        <p>Statistics for query response time across {len(results["test_response_time_results.json"]["results"])} test queries.</p>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Minimum</td>
                <td>{report["summary"]["response_time"]["min_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Maximum</td>
                <td>{report["summary"]["response_time"]["max_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Average</td>
                <td>{report["summary"]["response_time"]["avg_ms"]:.2f} ms</td>
            </tr>
        </table>
        
        <div class="chart">
            <canvas id="responseTimeChart"></canvas>
        </div>
    </div>
    
    <div class="section">
        <h2>Throughput</h2>
        <p>Throughput measurements at different concurrency levels.</p>
        <table>
            <tr>
                <th>Concurrency</th>
                <th>Throughput (queries/second)</th>
                <th>Average Response Time (ms)</th>
            </tr>
            {"".join(f"<tr><td>{r['concurrency']}</td><td>{r['throughput_queries_per_second']:.2f}</td><td>{r['response_time_stats']['avg_response_time_ms']:.2f}</td></tr>" for r in results["test_throughput_results.json"])}
        </table>
        
        <div class="chart">
            <canvas id="throughputChart"></canvas>
        </div>
    </div>
    
    <div class="section">
        <h2>Resource Utilization</h2>
        <p>Resource utilization during sustained load over {results["test_resource_utilization_results.json"]["duration_seconds"]} seconds.</p>
        <table>
            <tr>
                <th>Metric</th>
                <th>Average</th>
                <th>Maximum</th>
            </tr>
            <tr>
                <td>CPU Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_cpu_percent"]:.1f}%</td>
                <td>{report["summary"]["resource_utilization"]["max_cpu_percent"]:.1f}%</td>
            </tr>
            <tr>
                <td>Memory Usage</td>
                <td>{report["summary"]["resource_utilization"]["avg_memory_mb"]:.1f} MB</td>
                <td>{report["summary"]["resource_utilization"]["max_memory_mb"]:.1f} MB</td>
            </tr>
        </table>
        
        <div class="chart">
            <canvas id="resourceChart"></canvas>
        </div>
    </div>
    
    <div class="section">
        <h2>API Performance</h2>
        <p>Performance measurements for API endpoints.</p>
        <h3>Upload Performance</h3>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Average Upload Time</td>
                <td>{report["summary"]["api_performance"]["avg_upload_time_ms"]:.2f} ms</td>
            </tr>
            <tr>
                <td>Document Processing Time</td>
                <td>{report["summary"]["api_performance"]["process_time_per_document_ms"]:.2f} ms per document</td>
            </tr>
        </table>
        
        <h3>Query Performance</h3>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Average Query Time</td>
                <td>{report["summary"]["api_performance"]["avg_query_time_ms"]:.2f} ms</td>
            </tr>
        </table>
    </div>
    
    <script>
        // Response Time Chart
        const responseTimeCtx = document.getElementById('responseTimeChart').getContext('2d');
        const responseTimeChart = new Chart(responseTimeCtx, {{
            type: 'bar',
            data: {{
                labels: {json.dumps([r["query"][:20] + "..." for r in results["test_response_time_results.json"]["results"]])},
                datasets: [{{
                    label: 'Response Time (ms)',
                    data: {json.dumps([r["response_time_ms"] for r in results["test_response_time_results.json"]["results"]])},
                    backgroundColor: 'rgba(54, 162, 235, 0.5)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1
                }}]
            }},
            options: {{
                scales: {{
                    y: {{
                        beginAtZero: true,
                        title: {{
                            display: true,
                            text: 'Response Time (ms)'
                        }}
                    }}
                }}
            }}
        }});
        
        // Throughput Chart
        const throughputCtx = document.getElementById('throughputChart').getContext('2d');
        const throughputChart = new Chart(throughputCtx, {{
            type: 'line',
            data: {{
                labels: {json.dumps([r["concurrency"] for r in results["test_throughput_results.json"]])},
                datasets: [
                    {{
                        label: 'Throughput (queries/second)',
                        data: {json.dumps([r["throughput_queries_per_second"] for r in results["test_throughput_results.json"]])},
                        backgroundColor: 'rgba(75, 192, 192, 0.5)',
                        borderColor: 'rgba(75, 192, 192, 1)',
                        borderWidth: 2,
                        yAxisID: 'y'
                    }},
                    {{
                        label: 'Avg Response Time (ms)',
                        data: {json.dumps([r["response_time_stats"]["avg_response_time_ms"] for r in results["test_throughput_results.json"]])},
                        backgroundColor: 'rgba(255, 99, 132, 0.5)',
                        borderColor: 'rgba(255, 99, 132, 1)',
                        borderWidth: 2,
                        yAxisID: 'y1'
                    }}
                ]
            }},
            options: {{
                scales: {{
                    y: {{
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {{
                            display: true,
                            text: 'Throughput (queries/second)'
                        }}
                    }},
                    y1: {{
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {{
                            display: true,
                            text: 'Response Time (ms)'
                        }},
                        grid: {{
                            drawOnChartArea: false
                        }}
                    }}
                }}
            }}
        }});
        
        // Resource Utilization Chart
        const resourceCtx = document.getElementById('resourceChart').getContext('2d');
        const resourceData = {json.dumps(results["test_resource_utilization_results.json"]["metrics_timeline"])};
        const resourceChart = new Chart(resourceCtx, {{
            type: 'line',
            data: {{
                labels: resourceData.map(m => m.timestamp.toFixed(1) + 's'),
                datasets: [
                    {{
                        label: 'CPU Usage (%)',
                        data: resourceData.map(m => m.cpu_percent),
                        backgroundColor: 'rgba(255, 99, 132, 0.5)',
                        borderColor: 'rgba(255, 99, 132, 1)',
                        borderWidth: 2,
                        yAxisID: 'y'
                    }},
                    {{
                        label: 'Memory Usage (MB)',
                        data: resourceData.map(m => m.memory_mb),
                        backgroundColor: 'rgba(54, 162, 235, 0.5)',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 2,
                        yAxisID: 'y1'
                    }}
                ]
            }},
            options: {{
                scales: {{
                    y: {{
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {{
                            display: true,
                            text: 'CPU Usage (%)'
                        }}
                    }},
                    y1: {{
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {{
                            display: true,
                            text: 'Memory Usage (MB)'
                        }},
                        grid: {{
                            drawOnChartArea: false
                        }}
                    }}
                }}
            }}
        }});
    </script>
</body>
</html>
"""
    
    html_report_path = "performance_benchmark_report.html"
    with open(html_report_path, "w") as f:
        f.write(html_report)
    
    logger.info(f"HTML performance report saved to {os.path.abspath(html_report_path)}")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: tests/test_rag_quality.py
================
#!/usr/bin/env python3
"""
Test suite for evaluating the quality of RAG responses in the Metis RAG system.
This test suite focuses on factual accuracy, relevance, and citation quality.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

import pytest
from fastapi.testclient import TestClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_rag_quality")

# Import RAG components
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.rag.document_processor import DocumentProcessor
from app.models.document import Document, Chunk
from app.models.chat import ChatQuery
from app.main import app

# Test client
client = TestClient(app)

# Test document content with known facts
TEST_DOCUMENTS = {
    "technical_doc": {
        "filename": "technical_documentation.md",
        "content": """# Metis RAG Technical Documentation

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
""",
        "tags": ["technical", "documentation", "architecture"],
        "folder": "/test"
    },
    "business_report": {
        "filename": "quarterly_report.txt",
        "content": """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
""",
        "tags": ["business", "report", "quarterly"],
        "folder": "/test"
    },
    "product_specs": {
        "filename": "product_specifications.csv",
        "content": """Product ID,Name,Category,Price,Features,Release Date
P001,MetisRAG Enterprise,Software,$4999,Advanced RAG capabilities|Multi-document retrieval|Enterprise security,2025-01-15
P002,MetisRAG Professional,Software,$1999,Standard RAG capabilities|Document management|API access,2025-01-15
P003,MetisRAG Basic,Software,$499,Basic RAG capabilities|Limited document storage|Web interface,2025-01-15
P004,MetisRAG API,Service,$0.10 per query,Pay-per-use|REST API|Developer documentation,2025-02-01
P005,MetisRAG Mobile,Mobile App,$9.99 per month,iOS and Android|Offline capabilities|Voice interface,2025-03-15
""",
        "tags": ["product", "specifications", "pricing"],
        "folder": "/test"
    }
}

# Test queries with expected facts to be present in responses
TEST_QUERIES = [
    {
        "query": "What is the architecture of Metis RAG?",
        "expected_facts": [
            "modular architecture",
            "Frontend Layer",
            "API Layer",
            "RAG Engine",
            "HTML, CSS, and JavaScript",
            "FastAPI"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What was the revenue reported in Q1 2025?",
        "expected_facts": [
            "$4.2M",
            "15% increase",
            "year-over-year",
            "net profit of $1.4M"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What are the components of the RAG engine?",
        "expected_facts": [
            "Document Processing",
            "Vector Store",
            "LLM Integration",
            "chunking",
            "embeddings",
            "Ollama"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What are the strategic initiatives for Q2?",
        "expected_facts": [
            "International Expansion",
            "European markets",
            "Enterprise Solution",
            "Strategic Partnerships",
            "Operational Efficiency"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What products are available and at what price points?",
        "expected_facts": [
            "MetisRAG Enterprise",
            "$4999",
            "MetisRAG Professional",
            "$1999",
            "MetisRAG Basic",
            "$499"
        ],
        "document_ids": ["product_specs"]
    },
    {
        "query": "What was the customer retention rate and NPS score?",
        "expected_facts": [
            "94%",
            "Net Promoter Score",
            "improved from 42 to 48"
        ],
        "document_ids": ["business_report"]
    }
]

# Test for multi-document queries
MULTI_DOC_QUERIES = [
    {
        "query": "Compare the MetisRAG Enterprise product with the RAG Engine architecture",
        "expected_facts": [
            "MetisRAG Enterprise",
            "$4999",
            "Advanced RAG capabilities",
            "RAG Engine",
            "Document Processing",
            "Vector Store"
        ],
        "document_ids": ["technical_doc", "product_specs"]
    },
    {
        "query": "What is the relationship between the Q1 financial performance and the product offerings?",
        "expected_facts": [
            "$4.2M in revenue",
            "MetisRAG Enterprise",
            "MetisRAG Professional",
            "MetisRAG Basic"
        ],
        "document_ids": ["business_report", "product_specs"]
    }
]

@pytest.fixture
def test_documents_dir():
    """Create a directory for test documents"""
    test_dir = "test_quality_docs"
    os.makedirs(test_dir, exist_ok=True)
    return test_dir

@pytest.fixture
def create_test_documents(test_documents_dir):
    """Create test documents with known facts"""
    document_paths = {}
    
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        file_path = os.path.join(test_documents_dir, doc_info["filename"])
        with open(file_path, "w") as f:
            f.write(doc_info["content"])
        document_paths[doc_id] = file_path
    
    return document_paths

@pytest.fixture
async def setup_vector_store(create_test_documents):
    """Set up vector store with test documents"""
    # Use a separate directory for test ChromaDB
    test_chroma_dir = "test_quality_chroma"
    os.makedirs(test_chroma_dir, exist_ok=True)
    
    # Initialize vector store
    vector_store = VectorStore(persist_directory=test_chroma_dir)
    
    # Create Document objects
    documents = {}
    for doc_id, doc_info in TEST_DOCUMENTS.items():
        doc = Document(
            id=doc_id,
            filename=doc_info["filename"],
            content=doc_info["content"],
            tags=doc_info["tags"],
            folder=doc_info["folder"]
        )
        
        # Create a single chunk for simplicity in testing
        doc.chunks = [
            Chunk(
                id=f"{doc_id}_chunk_0",
                content=doc_info["content"],
                metadata={
                    "index": 0,
                    "source": doc_info["filename"]
                }
            )
        ]
        
        # Add document to vector store
        await vector_store.add_document(doc)
        documents[doc_id] = doc
    
    return vector_store, documents

@pytest.fixture
async def setup_rag_engine(setup_vector_store):
    """Set up RAG engine with test vector store"""
    vector_store, documents = await setup_vector_store
    rag_engine = RAGEngine(vector_store=vector_store)
    return rag_engine, documents

@pytest.mark.asyncio
async def test_factual_accuracy(setup_rag_engine):
    """Test factual accuracy of RAG responses"""
    rag_engine, documents = await setup_rag_engine
    
    results = []
    for test_case in TEST_QUERIES:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Check if response contains expected facts
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        # Count how many expected facts are present in the answer
        fact_count = sum(1 for fact in expected_facts if fact.lower() in answer.lower())
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        # Check if sources are from the expected documents
        expected_doc_ids = test_case["document_ids"]
        source_doc_ids = [source.document_id for source in sources]
        correct_sources = all(doc_id in source_doc_ids for doc_id in expected_doc_ids)
        
        # Log results
        logger.info(f"Query: {query}")
        logger.info(f"Answer: {answer[:100]}...")
        logger.info(f"Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
        logger.info(f"Correct sources: {correct_sources}")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "correct_sources": correct_sources,
            "sources": [
                {
                    "document_id": s.document_id,
                    "relevance_score": s.relevance_score
                }
                for s in sources
            ] if sources else []
        })
    
    # Save results to file
    results_path = "test_quality_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    # Assert minimum factual accuracy
    for i, result in enumerate(results):
        assert result["fact_percentage"] >= 70, f"Query {i+1} has low factual accuracy: {result['fact_percentage']:.1f}%"
        assert result["correct_sources"], f"Query {i+1} has incorrect sources"

@pytest.mark.asyncio
async def test_multi_document_retrieval(setup_rag_engine):
    """Test retrieval across multiple documents"""
    rag_engine, documents = await setup_rag_engine
    
    results = []
    for test_case in MULTI_DOC_QUERIES:
        query = test_case["query"]
        expected_facts = test_case["expected_facts"]
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=5,  # Increase top_k for multi-document queries
            stream=False
        )
        
        # Check if response contains expected facts
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        # Count how many expected facts are present in the answer
        fact_count = sum(1 for fact in expected_facts if fact.lower() in answer.lower())
        fact_percentage = (fact_count / len(expected_facts)) * 100
        
        # Check if sources are from multiple documents
        source_doc_ids = set(source.document_id for source in sources)
        multi_doc_retrieval = len(source_doc_ids) > 1
        
        # Check if sources are from the expected documents
        expected_doc_ids = test_case["document_ids"]
        correct_sources = all(doc_id in source_doc_ids for doc_id in expected_doc_ids)
        
        # Log results
        logger.info(f"Multi-doc Query: {query}")
        logger.info(f"Answer: {answer[:100]}...")
        logger.info(f"Facts found: {fact_count}/{len(expected_facts)} ({fact_percentage:.1f}%)")
        logger.info(f"Multi-document retrieval: {multi_doc_retrieval}")
        logger.info(f"Correct sources: {correct_sources}")
        
        # Store results
        results.append({
            "query": query,
            "answer": answer,
            "expected_facts": expected_facts,
            "facts_found": fact_count,
            "fact_percentage": fact_percentage,
            "multi_doc_retrieval": multi_doc_retrieval,
            "correct_sources": correct_sources,
            "source_doc_ids": list(source_doc_ids)
        })
    
    # Save results to file
    results_path = "test_multi_doc_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    # Assert multi-document retrieval
    for i, result in enumerate(results):
        assert result["multi_doc_retrieval"], f"Query {i+1} failed to retrieve from multiple documents"
        assert result["fact_percentage"] >= 60, f"Query {i+1} has low factual accuracy: {result['fact_percentage']:.1f}%"

@pytest.mark.asyncio
async def test_citation_quality(setup_rag_engine):
    """Test quality of citations in RAG responses"""
    rag_engine, documents = await setup_rag_engine
    
    results = []
    for test_case in TEST_QUERIES:
        query = test_case["query"]
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Check citations in the answer
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        # Check if answer contains citation markers
        has_citation_markers = "[" in answer and "]" in answer
        
        # Check if citations in the answer correspond to sources
        citation_count = 0
        for i in range(1, 10):  # Check for citation markers [1] through [9]
            if f"[{i}]" in answer:
                citation_count += 1
        
        # Check if number of citations is reasonable
        reasonable_citation_count = 0 < citation_count <= len(sources) + 1  # Allow for one extra citation
        
        # Log results
        logger.info(f"Citation Query: {query}")
        logger.info(f"Has citation markers: {has_citation_markers}")
        logger.info(f"Citation count: {citation_count}")
        logger.info(f"Source count: {len(sources)}")
        
        # Store results
        results.append({
            "query": query,
            "has_citation_markers": has_citation_markers,
            "citation_count": citation_count,
            "source_count": len(sources),
            "reasonable_citation_count": reasonable_citation_count
        })
    
    # Save results to file
    results_path = "test_citation_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    # Assert citation quality
    for i, result in enumerate(results):
        assert result["has_citation_markers"], f"Query {i+1} response lacks citation markers"
        assert result["reasonable_citation_count"], f"Query {i+1} has unreasonable citation count"

@pytest.mark.asyncio
async def test_api_integration(create_test_documents):
    """Test integration with API endpoints"""
    # Test document upload and processing through API
    results = []
    
    for doc_id, doc_path in create_test_documents.items():
        doc_info = TEST_DOCUMENTS[doc_id]
        
        # Open the file for upload
        with open(doc_path, "rb") as f:
            # Upload the document
            upload_response = client.post(
                "/api/documents/upload",
                files={"file": (doc_info["filename"], f, "text/plain")}
            )
            
            # Check upload response
            assert upload_response.status_code == 200
            upload_data = upload_response.json()
            assert upload_data["success"] is True
            assert "document_id" in upload_data
            
            document_id = upload_data["document_id"]
            
            # Process the document
            process_response = client.post(
                "/api/documents/process",
                json={"document_ids": [document_id]}
            )
            
            # Check process response
            assert process_response.status_code == 200
            process_data = process_response.json()
            assert process_data["success"] is True
            
            # Test query with the uploaded document
            query = next((q["query"] for q in TEST_QUERIES if doc_id in q["document_ids"]), "What is this document about?")
            
            query_response = client.post(
                "/api/chat/query",
                json={
                    "message": query,
                    "use_rag": True,
                    "stream": False
                }
            )
            
            # Check query response
            assert query_response.status_code == 200
            query_data = query_response.json()
            assert "message" in query_data
            assert "conversation_id" in query_data
            
            # Store results
            results.append({
                "document_id": document_id,
                "filename": doc_info["filename"],
                "query": query,
                "response": query_data["message"]
            })
            
            # Clean up - delete the document
            delete_response = client.delete(f"/api/documents/{document_id}")
            assert delete_response.status_code == 200
            delete_data = delete_response.json()
            assert delete_data["success"] is True
    
    # Save results to file
    results_path = "test_api_integration_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])

================
File: venv_py310/bin/activate
================
# This file must be used with "source bin/activate" *from bash*
# you cannot run it directly

deactivate () {
    # reset old environment variables
    if [ -n "${_OLD_VIRTUAL_PATH:-}" ] ; then
        PATH="${_OLD_VIRTUAL_PATH:-}"
        export PATH
        unset _OLD_VIRTUAL_PATH
    fi
    if [ -n "${_OLD_VIRTUAL_PYTHONHOME:-}" ] ; then
        PYTHONHOME="${_OLD_VIRTUAL_PYTHONHOME:-}"
        export PYTHONHOME
        unset _OLD_VIRTUAL_PYTHONHOME
    fi

    # This should detect bash and zsh, which have a hash command that must
    # be called to get it to forget past commands.  Without forgetting
    # past commands the $PATH changes we made may not be respected
    if [ -n "${BASH:-}" -o -n "${ZSH_VERSION:-}" ] ; then
        hash -r 2> /dev/null
    fi

    if [ -n "${_OLD_VIRTUAL_PS1:-}" ] ; then
        PS1="${_OLD_VIRTUAL_PS1:-}"
        export PS1
        unset _OLD_VIRTUAL_PS1
    fi

    unset VIRTUAL_ENV
    unset VIRTUAL_ENV_PROMPT
    if [ ! "${1:-}" = "nondestructive" ] ; then
    # Self destruct!
        unset -f deactivate
    fi
}

# unset irrelevant variables
deactivate nondestructive

VIRTUAL_ENV=/Users/charleshoward/Metis_RAG/venv_py310
export VIRTUAL_ENV

_OLD_VIRTUAL_PATH="$PATH"
PATH="$VIRTUAL_ENV/"bin":$PATH"
export PATH

# unset PYTHONHOME if set
# this will fail if PYTHONHOME is set to the empty string (which is bad anyway)
# could use `if (set -u; : $PYTHONHOME) ;` in bash
if [ -n "${PYTHONHOME:-}" ] ; then
    _OLD_VIRTUAL_PYTHONHOME="${PYTHONHOME:-}"
    unset PYTHONHOME
fi

if [ -z "${VIRTUAL_ENV_DISABLE_PROMPT:-}" ] ; then
    _OLD_VIRTUAL_PS1="${PS1:-}"
    PS1='(venv_py310) '"${PS1:-}"
    export PS1
    VIRTUAL_ENV_PROMPT='(venv_py310) '
    export VIRTUAL_ENV_PROMPT
fi

# This should detect bash and zsh, which have a hash command that must
# be called to get it to forget past commands.  Without forgetting
# past commands the $PATH changes we made may not be respected
if [ -n "${BASH:-}" -o -n "${ZSH_VERSION:-}" ] ; then
    hash -r 2> /dev/null
fi

================
File: venv_py310/bin/activate.csh
================
# This file must be used with "source bin/activate.csh" *from csh*.
# You cannot run it directly.
# Created by Davide Di Blasi <davidedb@gmail.com>.
# Ported to Python 3.3 venv by Andrew Svetlov <andrew.svetlov@gmail.com>

alias deactivate 'test $?_OLD_VIRTUAL_PATH != 0 && setenv PATH "$_OLD_VIRTUAL_PATH" && unset _OLD_VIRTUAL_PATH; rehash; test $?_OLD_VIRTUAL_PROMPT != 0 && set prompt="$_OLD_VIRTUAL_PROMPT" && unset _OLD_VIRTUAL_PROMPT; unsetenv VIRTUAL_ENV; unsetenv VIRTUAL_ENV_PROMPT; test "\!:*" != "nondestructive" && unalias deactivate'

# Unset irrelevant variables.
deactivate nondestructive

setenv VIRTUAL_ENV /Users/charleshoward/Metis_RAG/venv_py310

set _OLD_VIRTUAL_PATH="$PATH"
setenv PATH "$VIRTUAL_ENV/"bin":$PATH"


set _OLD_VIRTUAL_PROMPT="$prompt"

if (! "$?VIRTUAL_ENV_DISABLE_PROMPT") then
    set prompt = '(venv_py310) '"$prompt"
    setenv VIRTUAL_ENV_PROMPT '(venv_py310) '
endif

alias pydoc python -m pydoc

rehash

================
File: venv_py310/bin/activate.fish
================
# This file must be used with "source <venv>/bin/activate.fish" *from fish*
# (https://fishshell.com/); you cannot run it directly.

function deactivate  -d "Exit virtual environment and return to normal shell environment"
    # reset old environment variables
    if test -n "$_OLD_VIRTUAL_PATH"
        set -gx PATH $_OLD_VIRTUAL_PATH
        set -e _OLD_VIRTUAL_PATH
    end
    if test -n "$_OLD_VIRTUAL_PYTHONHOME"
        set -gx PYTHONHOME $_OLD_VIRTUAL_PYTHONHOME
        set -e _OLD_VIRTUAL_PYTHONHOME
    end

    if test -n "$_OLD_FISH_PROMPT_OVERRIDE"
        set -e _OLD_FISH_PROMPT_OVERRIDE
        # prevents error when using nested fish instances (Issue #93858)
        if functions -q _old_fish_prompt
            functions -e fish_prompt
            functions -c _old_fish_prompt fish_prompt
            functions -e _old_fish_prompt
        end
    end

    set -e VIRTUAL_ENV
    set -e VIRTUAL_ENV_PROMPT
    if test "$argv[1]" != "nondestructive"
        # Self-destruct!
        functions -e deactivate
    end
end

# Unset irrelevant variables.
deactivate nondestructive

set -gx VIRTUAL_ENV /Users/charleshoward/Metis_RAG/venv_py310

set -gx _OLD_VIRTUAL_PATH $PATH
set -gx PATH "$VIRTUAL_ENV/"bin $PATH

# Unset PYTHONHOME if set.
if set -q PYTHONHOME
    set -gx _OLD_VIRTUAL_PYTHONHOME $PYTHONHOME
    set -e PYTHONHOME
end

if test -z "$VIRTUAL_ENV_DISABLE_PROMPT"
    # fish uses a function instead of an env var to generate the prompt.

    # Save the current fish_prompt function as the function _old_fish_prompt.
    functions -c fish_prompt _old_fish_prompt

    # With the original prompt function renamed, we can override with our own.
    function fish_prompt
        # Save the return status of the last command.
        set -l old_status $status

        # Output the venv prompt; color taken from the blue of the Python logo.
        printf "%s%s%s" (set_color 4B8BBE) '(venv_py310) ' (set_color normal)

        # Restore the return status of the previous command.
        echo "exit $old_status" | .
        # Output the original/"old" prompt.
        _old_fish_prompt
    end

    set -gx _OLD_FISH_PROMPT_OVERRIDE "$VIRTUAL_ENV"
    set -gx VIRTUAL_ENV_PROMPT '(venv_py310) '
end

================
File: venv_py310/bin/Activate.ps1
================
<#
.Synopsis
Activate a Python virtual environment for the current PowerShell session.

.Description
Pushes the python executable for a virtual environment to the front of the
$Env:PATH environment variable and sets the prompt to signify that you are
in a Python virtual environment. Makes use of the command line switches as
well as the `pyvenv.cfg` file values present in the virtual environment.

.Parameter VenvDir
Path to the directory that contains the virtual environment to activate. The
default value for this is the parent of the directory that the Activate.ps1
script is located within.

.Parameter Prompt
The prompt prefix to display when this virtual environment is activated. By
default, this prompt is the name of the virtual environment folder (VenvDir)
surrounded by parentheses and followed by a single space (ie. '(.venv) ').

.Example
Activate.ps1
Activates the Python virtual environment that contains the Activate.ps1 script.

.Example
Activate.ps1 -Verbose
Activates the Python virtual environment that contains the Activate.ps1 script,
and shows extra information about the activation as it executes.

.Example
Activate.ps1 -VenvDir C:\Users\MyUser\Common\.venv
Activates the Python virtual environment located in the specified location.

.Example
Activate.ps1 -Prompt "MyPython"
Activates the Python virtual environment that contains the Activate.ps1 script,
and prefixes the current prompt with the specified string (surrounded in
parentheses) while the virtual environment is active.

.Notes
On Windows, it may be required to enable this Activate.ps1 script by setting the
execution policy for the user. You can do this by issuing the following PowerShell
command:

PS C:\> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

For more information on Execution Policies: 
https://go.microsoft.com/fwlink/?LinkID=135170

#>
Param(
    [Parameter(Mandatory = $false)]
    [String]
    $VenvDir,
    [Parameter(Mandatory = $false)]
    [String]
    $Prompt
)

<# Function declarations --------------------------------------------------- #>

<#
.Synopsis
Remove all shell session elements added by the Activate script, including the
addition of the virtual environment's Python executable from the beginning of
the PATH variable.

.Parameter NonDestructive
If present, do not remove this function from the global namespace for the
session.

#>
function global:deactivate ([switch]$NonDestructive) {
    # Revert to original values

    # The prior prompt:
    if (Test-Path -Path Function:_OLD_VIRTUAL_PROMPT) {
        Copy-Item -Path Function:_OLD_VIRTUAL_PROMPT -Destination Function:prompt
        Remove-Item -Path Function:_OLD_VIRTUAL_PROMPT
    }

    # The prior PYTHONHOME:
    if (Test-Path -Path Env:_OLD_VIRTUAL_PYTHONHOME) {
        Copy-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME -Destination Env:PYTHONHOME
        Remove-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME
    }

    # The prior PATH:
    if (Test-Path -Path Env:_OLD_VIRTUAL_PATH) {
        Copy-Item -Path Env:_OLD_VIRTUAL_PATH -Destination Env:PATH
        Remove-Item -Path Env:_OLD_VIRTUAL_PATH
    }

    # Just remove the VIRTUAL_ENV altogether:
    if (Test-Path -Path Env:VIRTUAL_ENV) {
        Remove-Item -Path env:VIRTUAL_ENV
    }

    # Just remove VIRTUAL_ENV_PROMPT altogether.
    if (Test-Path -Path Env:VIRTUAL_ENV_PROMPT) {
        Remove-Item -Path env:VIRTUAL_ENV_PROMPT
    }

    # Just remove the _PYTHON_VENV_PROMPT_PREFIX altogether:
    if (Get-Variable -Name "_PYTHON_VENV_PROMPT_PREFIX" -ErrorAction SilentlyContinue) {
        Remove-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Scope Global -Force
    }

    # Leave deactivate function in the global namespace if requested:
    if (-not $NonDestructive) {
        Remove-Item -Path function:deactivate
    }
}

<#
.Description
Get-PyVenvConfig parses the values from the pyvenv.cfg file located in the
given folder, and returns them in a map.

For each line in the pyvenv.cfg file, if that line can be parsed into exactly
two strings separated by `=` (with any amount of whitespace surrounding the =)
then it is considered a `key = value` line. The left hand string is the key,
the right hand is the value.

If the value starts with a `'` or a `"` then the first and last character is
stripped from the value before being captured.

.Parameter ConfigDir
Path to the directory that contains the `pyvenv.cfg` file.
#>
function Get-PyVenvConfig(
    [String]
    $ConfigDir
) {
    Write-Verbose "Given ConfigDir=$ConfigDir, obtain values in pyvenv.cfg"

    # Ensure the file exists, and issue a warning if it doesn't (but still allow the function to continue).
    $pyvenvConfigPath = Join-Path -Resolve -Path $ConfigDir -ChildPath 'pyvenv.cfg' -ErrorAction Continue

    # An empty map will be returned if no config file is found.
    $pyvenvConfig = @{ }

    if ($pyvenvConfigPath) {

        Write-Verbose "File exists, parse `key = value` lines"
        $pyvenvConfigContent = Get-Content -Path $pyvenvConfigPath

        $pyvenvConfigContent | ForEach-Object {
            $keyval = $PSItem -split "\s*=\s*", 2
            if ($keyval[0] -and $keyval[1]) {
                $val = $keyval[1]

                # Remove extraneous quotations around a string value.
                if ("'""".Contains($val.Substring(0, 1))) {
                    $val = $val.Substring(1, $val.Length - 2)
                }

                $pyvenvConfig[$keyval[0]] = $val
                Write-Verbose "Adding Key: '$($keyval[0])'='$val'"
            }
        }
    }
    return $pyvenvConfig
}


<# Begin Activate script --------------------------------------------------- #>

# Determine the containing directory of this script
$VenvExecPath = Split-Path -Parent $MyInvocation.MyCommand.Definition
$VenvExecDir = Get-Item -Path $VenvExecPath

Write-Verbose "Activation script is located in path: '$VenvExecPath'"
Write-Verbose "VenvExecDir Fullname: '$($VenvExecDir.FullName)"
Write-Verbose "VenvExecDir Name: '$($VenvExecDir.Name)"

# Set values required in priority: CmdLine, ConfigFile, Default
# First, get the location of the virtual environment, it might not be
# VenvExecDir if specified on the command line.
if ($VenvDir) {
    Write-Verbose "VenvDir given as parameter, using '$VenvDir' to determine values"
}
else {
    Write-Verbose "VenvDir not given as a parameter, using parent directory name as VenvDir."
    $VenvDir = $VenvExecDir.Parent.FullName.TrimEnd("\\/")
    Write-Verbose "VenvDir=$VenvDir"
}

# Next, read the `pyvenv.cfg` file to determine any required value such
# as `prompt`.
$pyvenvCfg = Get-PyVenvConfig -ConfigDir $VenvDir

# Next, set the prompt from the command line, or the config file, or
# just use the name of the virtual environment folder.
if ($Prompt) {
    Write-Verbose "Prompt specified as argument, using '$Prompt'"
}
else {
    Write-Verbose "Prompt not specified as argument to script, checking pyvenv.cfg value"
    if ($pyvenvCfg -and $pyvenvCfg['prompt']) {
        Write-Verbose "  Setting based on value in pyvenv.cfg='$($pyvenvCfg['prompt'])'"
        $Prompt = $pyvenvCfg['prompt'];
    }
    else {
        Write-Verbose "  Setting prompt based on parent's directory's name. (Is the directory name passed to venv module when creating the virtual environment)"
        Write-Verbose "  Got leaf-name of $VenvDir='$(Split-Path -Path $venvDir -Leaf)'"
        $Prompt = Split-Path -Path $venvDir -Leaf
    }
}

Write-Verbose "Prompt = '$Prompt'"
Write-Verbose "VenvDir='$VenvDir'"

# Deactivate any currently active virtual environment, but leave the
# deactivate function in place.
deactivate -nondestructive

# Now set the environment variable VIRTUAL_ENV, used by many tools to determine
# that there is an activated venv.
$env:VIRTUAL_ENV = $VenvDir

if (-not $Env:VIRTUAL_ENV_DISABLE_PROMPT) {

    Write-Verbose "Setting prompt to '$Prompt'"

    # Set the prompt to include the env name
    # Make sure _OLD_VIRTUAL_PROMPT is global
    function global:_OLD_VIRTUAL_PROMPT { "" }
    Copy-Item -Path function:prompt -Destination function:_OLD_VIRTUAL_PROMPT
    New-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Description "Python virtual environment prompt prefix" -Scope Global -Option ReadOnly -Visibility Public -Value $Prompt

    function global:prompt {
        Write-Host -NoNewline -ForegroundColor Green "($_PYTHON_VENV_PROMPT_PREFIX) "
        _OLD_VIRTUAL_PROMPT
    }
    $env:VIRTUAL_ENV_PROMPT = $Prompt
}

# Clear PYTHONHOME
if (Test-Path -Path Env:PYTHONHOME) {
    Copy-Item -Path Env:PYTHONHOME -Destination Env:_OLD_VIRTUAL_PYTHONHOME
    Remove-Item -Path Env:PYTHONHOME
}

# Add the venv to the PATH
Copy-Item -Path Env:PATH -Destination Env:_OLD_VIRTUAL_PATH
$Env:PATH = "$VenvExecDir$([System.IO.Path]::PathSeparator)$Env:PATH"

================
File: venv_py310/bin/chardetect
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from chardet.cli.chardetect import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/chroma
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from chromadb.cli.cli import app
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(app())

================
File: venv_py310/bin/coloredlogs
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from coloredlogs.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/deep
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from deepdiff.commands import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli())

================
File: venv_py310/bin/distro
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from distro.distro import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/dotenv
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from dotenv.__main__ import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli())

================
File: venv_py310/bin/f2py
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from numpy.f2py.f2py2e import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/fastapi
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from fastapi.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/filetype
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from filetype.__main__ import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/httpx
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from httpx import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/huggingface-cli
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from huggingface_hub.commands.huggingface_cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/humanfriendly
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from humanfriendly.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/isympy
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from isympy import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/jsondiff
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-

from __future__ import print_function

import sys
import json
import jsonpatch
import argparse


parser = argparse.ArgumentParser(description='Diff two JSON files')
parser.add_argument('FILE1', type=argparse.FileType('r'))
parser.add_argument('FILE2', type=argparse.FileType('r'))
parser.add_argument('--indent', type=int, default=None,
                    help='Indent output by n spaces')
parser.add_argument('-u', '--preserve-unicode', action='store_true',
                    help='Output Unicode character as-is without using Code Point')
parser.add_argument('-v', '--version', action='version',
                    version='%(prog)s ' + jsonpatch.__version__)


def main():
    try:
        diff_files()
    except KeyboardInterrupt:
        sys.exit(1)


def diff_files():
    """ Diffs two JSON files and prints a patch """
    args = parser.parse_args()
    doc1 = json.load(args.FILE1)
    doc2 = json.load(args.FILE2)
    patch = jsonpatch.make_patch(doc1, doc2)
    if patch.patch:
        print(json.dumps(patch.patch, indent=args.indent, ensure_ascii=not(args.preserve_unicode)))
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: venv_py310/bin/jsonpatch
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-

import sys
import os.path
import json
import jsonpatch
import tempfile
import argparse


parser = argparse.ArgumentParser(
    description='Apply a JSON patch on a JSON file')
parser.add_argument('ORIGINAL', type=argparse.FileType('r'),
                    help='Original file')
parser.add_argument('PATCH', type=argparse.FileType('r'),
                    nargs='?', default=sys.stdin,
                    help='Patch file (read from stdin if omitted)')
parser.add_argument('--indent', type=int, default=None,
                    help='Indent output by n spaces')
parser.add_argument('-b', '--backup', action='store_true',
                    help='Back up ORIGINAL if modifying in-place')
parser.add_argument('-i', '--in-place', action='store_true',
                    help='Modify ORIGINAL in-place instead of to stdout')
parser.add_argument('-v', '--version', action='version',
                    version='%(prog)s ' + jsonpatch.__version__)
parser.add_argument('-u', '--preserve-unicode', action='store_true',
                    help='Output Unicode character as-is without using Code Point')

def main():
    try:
        patch_files()
    except KeyboardInterrupt:
        sys.exit(1)


def patch_files():
    """ Diffs two JSON files and prints a patch """
    args = parser.parse_args()
    doc = json.load(args.ORIGINAL)
    patch = json.load(args.PATCH)
    result = jsonpatch.apply_patch(doc, patch)

    if args.in_place:
        dirname = os.path.abspath(os.path.dirname(args.ORIGINAL.name))

        try:
            # Attempt to replace the file atomically.  We do this by
            # creating a temporary file in the same directory as the
            # original file so we can atomically move the new file over
            # the original later.  (This is done in the same directory
	    # because atomic renames do not work across mount points.)

            fd, pathname = tempfile.mkstemp(dir=dirname)
            fp = os.fdopen(fd, 'w')
            atomic = True

        except OSError:
            # We failed to create the temporary file for an atomic
            # replace, so fall back to non-atomic mode by backing up
            # the original (if desired) and writing a new file.

            if args.backup:
                os.rename(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')
            fp = open(args.ORIGINAL.name, 'w')
            atomic = False

    else:
        # Since we're not replacing the original file in-place, write
        # the modified JSON to stdout instead.

        fp = sys.stdout

    # By this point we have some sort of file object we can write the 
    # modified JSON to.
    
    json.dump(result, fp, indent=args.indent, ensure_ascii=not(args.preserve_unicode))
    fp.write('\n')

    if args.in_place:
        # Close the new file.  If we aren't replacing atomically, this
        # is our last step, since everything else is already in place.

        fp.close()

        if atomic:
            try:
                # Complete the atomic replace by linking the original
                # to a backup (if desired), fixing up the permissions
                # on the temporary file, and moving it into place.

                if args.backup:
                    os.link(args.ORIGINAL.name, args.ORIGINAL.name + '.orig')
                os.chmod(pathname, os.stat(args.ORIGINAL.name).st_mode)
                os.rename(pathname, args.ORIGINAL.name)

            except OSError:
                # In the event we could not actually do the atomic
                # replace, unlink the original to move it out of the
                # way and finally move the temporary file into place.
                
                os.unlink(args.ORIGINAL.name)
                os.rename(pathname, args.ORIGINAL.name)


if __name__ == "__main__":
    main()

================
File: venv_py310/bin/jsonpointer
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-


import argparse
import json
import sys

import jsonpointer

parser = argparse.ArgumentParser(
    description='Resolve a JSON pointer on JSON files')

# Accept pointer as argument or as file
ptr_group = parser.add_mutually_exclusive_group(required=True)

ptr_group.add_argument('-f', '--pointer-file', type=argparse.FileType('r'),
                       nargs='?',
                       help='File containing a JSON pointer expression')

ptr_group.add_argument('POINTER', type=str, nargs='?',
                       help='A JSON pointer expression')

parser.add_argument('FILE', type=argparse.FileType('r'), nargs='+',
                    help='Files for which the pointer should be resolved')
parser.add_argument('--indent', type=int, default=None,
                    help='Indent output by n spaces')
parser.add_argument('-v', '--version', action='version',
                    version='%(prog)s ' + jsonpointer.__version__)


def main():
    try:
        resolve_files()
    except KeyboardInterrupt:
        sys.exit(1)


def parse_pointer(args):
    if args.POINTER:
        ptr = args.POINTER
    elif args.pointer_file:
        ptr = args.pointer_file.read().strip()
    else:
        parser.print_usage()
        sys.exit(1)

    return ptr


def resolve_files():
    """ Resolve a JSON pointer on JSON files """
    args = parser.parse_args()

    ptr = parse_pointer(args)

    for f in args.FILE:
        doc = json.load(f)
        try:
            result = jsonpointer.resolve_pointer(doc, ptr)
            print(json.dumps(result, indent=args.indent))
        except jsonpointer.JsonPointerException as e:
            print('Could not resolve pointer: %s' % str(e), file=sys.stderr)


if __name__ == "__main__":
    main()

================
File: venv_py310/bin/langchain-server
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from langchain.server import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/langsmith
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from langsmith.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/markdown-it
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from markdown_it.cli.parse import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/nltk
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from nltk.cli import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli())

================
File: venv_py310/bin/normalizer
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from charset_normalizer import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli.cli_detect())

================
File: venv_py310/bin/onnxruntime_test
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from onnxruntime.tools.onnxruntime_test import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/oxmsg
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from oxmsg.cli import oxmsg
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(oxmsg())

================
File: venv_py310/bin/pip
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/pip3
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/pip3.10
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/py.test
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pytest import console_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(console_main())

================
File: venv_py310/bin/pygmentize
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pygments.cmdline import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/pytest
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from pytest import console_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(console_main())

================
File: venv_py310/bin/tqdm
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from tqdm.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/typer
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from typer.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/uvicorn
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from uvicorn.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/bin/watchfiles
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from watchfiles.cli import cli
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli())

================
File: venv_py310/bin/wheel
================
#!/Users/charleshoward/Metis_RAG/venv_py310/bin/python3.10
# -*- coding: utf-8 -*-
import re
import sys
from wheel.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())

================
File: venv_py310/share/man/man1/isympy.1
================
'\" -*- coding: us-ascii -*-
.if \n(.g .ds T< \\FC
.if \n(.g .ds T> \\F[\n[.fam]]
.de URL
\\$2 \(la\\$1\(ra\\$3
..
.if \n(.g .mso www.tmac
.TH isympy 1 2007-10-8 "" ""
.SH NAME
isympy \- interactive shell for SymPy
.SH SYNOPSIS
'nh
.fi
.ad l
\fBisympy\fR \kx
.if (\nx>(\n(.l/2)) .nr x (\n(.l/5)
'in \n(.iu+\nxu
[\fB-c\fR | \fB--console\fR] [\fB-p\fR ENCODING | \fB--pretty\fR ENCODING] [\fB-t\fR TYPE | \fB--types\fR TYPE] [\fB-o\fR ORDER | \fB--order\fR ORDER] [\fB-q\fR | \fB--quiet\fR] [\fB-d\fR | \fB--doctest\fR] [\fB-C\fR | \fB--no-cache\fR] [\fB-a\fR | \fB--auto\fR] [\fB-D\fR | \fB--debug\fR] [
-- | PYTHONOPTIONS]
'in \n(.iu-\nxu
.ad b
'hy
'nh
.fi
.ad l
\fBisympy\fR \kx
.if (\nx>(\n(.l/2)) .nr x (\n(.l/5)
'in \n(.iu+\nxu
[
{\fB-h\fR | \fB--help\fR}
|
{\fB-v\fR | \fB--version\fR}
]
'in \n(.iu-\nxu
.ad b
'hy
.SH DESCRIPTION
isympy is a Python shell for SymPy. It is just a normal python shell
(ipython shell if you have the ipython package installed) that executes
the following commands so that you don't have to:
.PP
.nf
\*(T<
>>> from __future__ import division
>>> from sympy import *
>>> x, y, z = symbols("x,y,z")
>>> k, m, n = symbols("k,m,n", integer=True)
    \*(T>
.fi
.PP
So starting isympy is equivalent to starting python (or ipython) and
executing the above commands by hand. It is intended for easy and quick
experimentation with SymPy. For more complicated programs, it is recommended
to write a script and import things explicitly (using the "from sympy
import sin, log, Symbol, ..." idiom).
.SH OPTIONS
.TP
\*(T<\fB\-c \fR\*(T>\fISHELL\fR, \*(T<\fB\-\-console=\fR\*(T>\fISHELL\fR
Use the specified shell (python or ipython) as
console backend instead of the default one (ipython
if present or python otherwise).

Example: isympy -c python

\fISHELL\fR could be either
\&'ipython' or 'python'
.TP
\*(T<\fB\-p \fR\*(T>\fIENCODING\fR, \*(T<\fB\-\-pretty=\fR\*(T>\fIENCODING\fR
Setup pretty printing in SymPy. By default, the most pretty, unicode
printing is enabled (if the terminal supports it). You can use less
pretty ASCII printing instead or no pretty printing at all.

Example: isympy -p no

\fIENCODING\fR must be one of 'unicode',
\&'ascii' or 'no'.
.TP
\*(T<\fB\-t \fR\*(T>\fITYPE\fR, \*(T<\fB\-\-types=\fR\*(T>\fITYPE\fR
Setup the ground types for the polys. By default, gmpy ground types
are used if gmpy2 or gmpy is installed, otherwise it falls back to python
ground types, which are a little bit slower. You can manually
choose python ground types even if gmpy is installed (e.g., for testing purposes).

Note that sympy ground types are not supported, and should be used
only for experimental purposes.

Note that the gmpy1 ground type is primarily intended for testing; it the
use of gmpy even if gmpy2 is available.

This is the same as setting the environment variable
SYMPY_GROUND_TYPES to the given ground type (e.g.,
SYMPY_GROUND_TYPES='gmpy')

The ground types can be determined interactively from the variable
sympy.polys.domains.GROUND_TYPES inside the isympy shell itself.

Example: isympy -t python

\fITYPE\fR must be one of 'gmpy',
\&'gmpy1' or 'python'.
.TP
\*(T<\fB\-o \fR\*(T>\fIORDER\fR, \*(T<\fB\-\-order=\fR\*(T>\fIORDER\fR
Setup the ordering of terms for printing. The default is lex, which
orders terms lexicographically (e.g., x**2 + x + 1). You can choose
other orderings, such as rev-lex, which will use reverse
lexicographic ordering (e.g., 1 + x + x**2).

Note that for very large expressions, ORDER='none' may speed up
printing considerably, with the tradeoff that the order of the terms
in the printed expression will have no canonical order

Example: isympy -o rev-lax

\fIORDER\fR must be one of 'lex', 'rev-lex', 'grlex',
\&'rev-grlex', 'grevlex', 'rev-grevlex', 'old', or 'none'.
.TP
\*(T<\fB\-q\fR\*(T>, \*(T<\fB\-\-quiet\fR\*(T>
Print only Python's and SymPy's versions to stdout at startup, and nothing else.
.TP
\*(T<\fB\-d\fR\*(T>, \*(T<\fB\-\-doctest\fR\*(T>
Use the same format that should be used for doctests. This is
equivalent to '\fIisympy -c python -p no\fR'.
.TP
\*(T<\fB\-C\fR\*(T>, \*(T<\fB\-\-no\-cache\fR\*(T>
Disable the caching mechanism. Disabling the cache may slow certain
operations down considerably. This is useful for testing the cache,
or for benchmarking, as the cache can result in deceptive benchmark timings.

This is the same as setting the environment variable SYMPY_USE_CACHE
to 'no'.
.TP
\*(T<\fB\-a\fR\*(T>, \*(T<\fB\-\-auto\fR\*(T>
Automatically create missing symbols. Normally, typing a name of a
Symbol that has not been instantiated first would raise NameError,
but with this option enabled, any undefined name will be
automatically created as a Symbol. This only works in IPython 0.11.

Note that this is intended only for interactive, calculator style
usage. In a script that uses SymPy, Symbols should be instantiated
at the top, so that it's clear what they are.

This will not override any names that are already defined, which
includes the single character letters represented by the mnemonic
QCOSINE (see the "Gotchas and Pitfalls" document in the
documentation). You can delete existing names by executing "del
name" in the shell itself. You can see if a name is defined by typing
"'name' in globals()".

The Symbols that are created using this have default assumptions.
If you want to place assumptions on symbols, you should create them
using symbols() or var().

Finally, this only works in the top level namespace. So, for
example, if you define a function in isympy with an undefined
Symbol, it will not work.
.TP
\*(T<\fB\-D\fR\*(T>, \*(T<\fB\-\-debug\fR\*(T>
Enable debugging output. This is the same as setting the
environment variable SYMPY_DEBUG to 'True'. The debug status is set
in the variable SYMPY_DEBUG within isympy.
.TP
-- \fIPYTHONOPTIONS\fR
These options will be passed on to \fIipython (1)\fR shell.
Only supported when ipython is being used (standard python shell not supported).

Two dashes (--) are required to separate \fIPYTHONOPTIONS\fR
from the other isympy options.

For example, to run iSymPy without startup banner and colors:

isympy -q -c ipython -- --colors=NoColor
.TP
\*(T<\fB\-h\fR\*(T>, \*(T<\fB\-\-help\fR\*(T>
Print help output and exit.
.TP
\*(T<\fB\-v\fR\*(T>, \*(T<\fB\-\-version\fR\*(T>
Print isympy version information and exit.
.SH FILES
.TP
\*(T<\fI${HOME}/.sympy\-history\fR\*(T>
Saves the history of commands when using the python
shell as backend.
.SH BUGS
The upstreams BTS can be found at \(lahttps://github.com/sympy/sympy/issues\(ra
Please report all bugs that you find in there, this will help improve
the overall quality of SymPy.
.SH "SEE ALSO"
\fBipython\fR(1), \fBpython\fR(1)

================
File: venv_py310/pyvenv.cfg
================
home = /opt/homebrew/opt/python@3.10/bin
include-system-site-packages = false
version = 3.10.16

================
File: .env.example
================
# API Settings
API_HOST=0.0.0.0
API_PORT=8000

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=gemma3:12b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# LLM Judge Settings
CHUNKING_JUDGE_MODEL=gemma3:12b
RETRIEVAL_JUDGE_MODEL=gemma3:12b
USE_CHUNKING_JUDGE=True
USE_RETRIEVAL_JUDGE=True

# LangGraph RAG Agent Settings
LANGGRAPH_RAG_MODEL=gemma3:12b
USE_LANGGRAPH_RAG=True

# Document Settings
UPLOAD_DIR=./uploads
CHROMA_DB_DIR=./chroma_db
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Security Settings
CORS_ORIGINS=*

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
.env

# IDE
.idea/
.vscode/
*.swp
*.swo

# Application specific
uploads/
chroma_db/
.env

# OS specific
.DS_Store
Thumbs.db

================
File: chunking_judge_real_results.json
================
[
  {
    "filename": "test_document.txt",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 1500,
      "chunk_overlap": 200
    },
    "justification": "The document is formatted with Markdown headers, indicating a clear hierarchical structure. Using the 'markdown' strategy will preserve this structure, ensuring that related sections and subsections are grouped together. A chunk size of 1500 characters allows for substantial content within each chunk while maintaining reasonable size. An overlap of 200 characters helps to avoid abrupt transitions between chunks and ensures context is carried over."
  },
  {
    "filename": "technical_documentation.md",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 1500,
      "chunk_overlap": 200
    },
    "justification": "The document is a markdown file with a well-defined structure using headers (e.g., Introduction, Architecture Overview, Deployment Options, Configuration, API Reference, Performance Considerations).  Using the 'markdown' strategy will allow us to split the document along these logical sections, preserving the context and relationships between different parts of the documentation. A chunk size of 1500 characters allows for a reasonable amount of content within each chunk while still allowing for meaningful sections. An overlap of 200 ensures that context isn't lost between chunks."
  },
  {
    "filename": "test_data.csv",
    "strategy": "recursive",
    "parameters": {
      "chunk_size": 1000,
      "chunk_overlap": 200
    },
    "justification": "The document is a CSV file, which doesn't inherently have a clear header structure suitable for markdown chunking. While it's structured data, the content within each cell (particularly the 'skills' column) can be quite variable in length.  Token-based chunking might be overly granular, as it could split skills lists mid-item. Recursive chunking provides a good balance. A chunk size of 1000 characters allows for capturing a reasonable amount of context from multiple rows, while an overlap of 200 helps maintain continuity between chunks, especially when a row's content spans across two chunks. This approach will allow the RAG system to understand the relationships between employees and their skills."
  }
]

================
File: chunking_judge_test_results.json
================
[
  {
    "filename": "test_document.txt",
    "strategy": "recursive",
    "parameters": {
      "chunk_size": 1000,
      "chunk_overlap": 150
    },
    "justification": "This is a plain text document with paragraphs that benefits from recursive chunking."
  },
  {
    "filename": "technical_documentation.md",
    "strategy": "markdown",
    "parameters": {
      "chunk_size": 800,
      "chunk_overlap": 100
    },
    "justification": "This is a structured markdown document with clear headers."
  },
  {
    "filename": "test_data.csv",
    "strategy": "token",
    "parameters": {
      "chunk_size": 600,
      "chunk_overlap": 75
    },
    "justification": "This is a CSV file with tabular data that benefits from token-based chunking."
  }
]

================
File: clear_cache.py
================
#!/usr/bin/env python3
"""
Script to clear the vector store cache to ensure the system uses updated settings.
"""

import asyncio
from app.rag.vector_store import VectorStore

async def clear_cache():
    """Clear the vector store cache"""
    print("Clearing vector store cache...")
    vector_store = VectorStore()
    vector_store.clear_cache()
    print("Vector store cache cleared successfully!")

if __name__ == "__main__":
    asyncio.run(clear_cache())

================
File: clear_database.py
================
#!/usr/bin/env python3
"""
Script to clear the Metis RAG database and start fresh.
This script will:
1. Delete the ChromaDB directory
2. Delete the uploads directory (optional)
"""

import os
import shutil
import argparse
from pathlib import Path

# Get the base directory
BASE_DIR = Path(__file__).resolve().parent

# Default paths
CHROMA_DB_DIR = BASE_DIR / "chroma_db"
UPLOAD_DIR = BASE_DIR / "uploads"

def clear_database(clear_uploads=False):
    """Clear the database and optionally the uploads directory"""
    print(f"Clearing ChromaDB directory: {CHROMA_DB_DIR}")
    
    if os.path.exists(CHROMA_DB_DIR):
        try:
            shutil.rmtree(CHROMA_DB_DIR)
            print(" ChromaDB directory deleted successfully")
        except Exception as e:
            print(f" Error deleting ChromaDB directory: {str(e)}")
            print("   Make sure the application is not running")
            return False
    else:
        print(" ChromaDB directory does not exist, nothing to delete")
    
    # Recreate the directory
    os.makedirs(CHROMA_DB_DIR, exist_ok=True)
    print(" Created fresh ChromaDB directory")
    
    if clear_uploads:
        print(f"Clearing uploads directory: {UPLOAD_DIR}")
        if os.path.exists(UPLOAD_DIR):
            try:
                # Delete all contents but keep the directory
                for item in os.listdir(UPLOAD_DIR):
                    item_path = os.path.join(UPLOAD_DIR, item)
                    if os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                    else:
                        os.remove(item_path)
                print(" Uploads directory cleared successfully")
            except Exception as e:
                print(f" Error clearing uploads directory: {str(e)}")
                return False
        else:
            print(" Uploads directory does not exist, nothing to delete")
            os.makedirs(UPLOAD_DIR, exist_ok=True)
            print(" Created fresh uploads directory")
    
    print("\n Database cleared successfully!")
    print("\nYou can now restart the application with a fresh database.")
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Clear the Metis RAG database and start fresh")
    parser.add_argument("--clear-uploads", action="store_true", help="Also clear the uploads directory")
    args = parser.parse_args()
    
    clear_database(args.clear_uploads)

================
File: demo_presentation.py
================
#!/usr/bin/env python3
"""
Metis RAG Demo Presentation Script

This script simulates a basic interaction with the Metis RAG system for demonstration purposes.
It shows the key steps in the RAG process, from query processing to response generation.
"""

import time
import json
import random
from datetime import datetime

# ANSI color codes for terminal output
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def print_header(text):
    """Print a formatted header."""
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'=' * 80}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{text.center(80)}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{'=' * 80}{Colors.ENDC}\n")

def print_step(step_num, step_name):
    """Print a formatted step header."""
    print(f"\n{Colors.BLUE}{Colors.BOLD}[Step {step_num}] {step_name}{Colors.ENDC}")
    print(f"{Colors.BLUE}{'-' * 50}{Colors.ENDC}\n")

def simulate_typing(text, delay=0.03):
    """Simulate typing by printing characters with a delay."""
    for char in text:
        print(char, end='', flush=True)
        time.sleep(delay)
    print()

def progress_bar(duration, description):
    """Display a progress bar for the given duration."""
    steps = 40
    for i in range(steps + 1):
        progress = i / steps
        bar = '' * i + '' * (steps - i)
        percentage = int(progress * 100)
        print(f"\r{Colors.CYAN}{description}: [{bar}] {percentage}%{Colors.ENDC}", end='')
        time.sleep(duration / steps)
    print()

def simulate_document_processing():
    """Simulate document processing."""
    print_step(1, "Document Processing")
    
    documents = [
        {"name": "quarterly_report.txt", "size": "245 KB", "pages": 12},
        {"name": "technical_documentation.md", "size": "189 KB", "pages": 8},
        {"name": "product_specifications.csv", "size": "78 KB", "rows": 156}
    ]
    
    print(f"{Colors.YELLOW}Available Documents:{Colors.ENDC}")
    for i, doc in enumerate(documents, 1):
        pages = doc.get('pages', doc.get('rows', 'N/A'))
        print(f"{i}. {doc['name']} ({doc['size']}, {pages} pages/rows)")
    
    print(f"\n{Colors.GREEN}Processing documents...{Colors.ENDC}")
    
    for doc in documents:
        print(f"\n{Colors.CYAN}Processing {doc['name']}...{Colors.ENDC}")
        
        # Determine chunking strategy based on file extension
        if doc['name'].endswith('.md'):
            strategy = "Markdown Header Chunking"
        elif doc['name'].endswith('.csv'):
            strategy = "Token-based Chunking"
        else:
            strategy = "Recursive Chunking"
        
        print(f"  Using {Colors.YELLOW}{strategy}{Colors.ENDC}")
        progress_bar(2, f"Extracting text from {doc['name']}")
        
        # Show chunking process
        chunk_count = random.randint(5, 20)
        print(f"  Created {Colors.GREEN}{chunk_count} chunks{Colors.ENDC}")
        progress_bar(1.5, "Generating embeddings")
        
        # Show vector storage
        print(f"  Stored in vector database with {Colors.GREEN}metadata{Colors.ENDC}")
    
    print(f"\n{Colors.GREEN}All documents processed successfully!{Colors.ENDC}")

def simulate_query_processing():
    """Simulate query processing."""
    print_step(2, "Query Processing")
    
    # User query
    query = "What are the key performance metrics for the new product line?"
    print(f"{Colors.YELLOW}User Query:{Colors.ENDC}")
    simulate_typing(f"{Colors.BOLD}{query}{Colors.ENDC}", 0.05)
    
    print(f"\n{Colors.CYAN}Processing query...{Colors.ENDC}")
    progress_bar(1, "Generating query embedding")
    
    # Vector search
    print(f"\n{Colors.CYAN}Searching vector database...{Colors.ENDC}")
    progress_bar(2, "Performing semantic search")
    
    # Retrieved chunks
    print(f"\n{Colors.GREEN}Retrieved relevant chunks:{Colors.ENDC}")
    chunks = [
        {"text": "The new product line demonstrated a 27% increase in efficiency metrics compared to previous generation...", "source": "quarterly_report.txt", "similarity": 0.89},
        {"text": "Key performance indicators include: processing speed (45 units/min), energy efficiency (0.8 kWh), and reliability score (98.7%)...", "source": "product_specifications.csv", "similarity": 0.87},
        {"text": "Performance testing revealed consistent results across all operational environments with metrics exceeding target thresholds...", "source": "technical_documentation.md", "similarity": 0.76}
    ]
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\n{Colors.YELLOW}Chunk {i} (Similarity: {chunk['similarity']:.2f}){Colors.ENDC}")
        print(f"Source: {chunk['source']}")
        print(f"Text: \"{chunk['text']}\"")
    
    # Context assembly
    print(f"\n{Colors.CYAN}Assembling context for LLM...{Colors.ENDC}")
    progress_bar(1.5, "Building prompt with retrieved context")

def simulate_response_generation():
    """Simulate response generation."""
    print_step(3, "Response Generation")
    
    # Prompt construction
    print(f"{Colors.CYAN}Constructing prompt with retrieved context...{Colors.ENDC}")
    progress_bar(1, "Optimizing prompt")
    
    # LLM generation
    print(f"\n{Colors.CYAN}Generating response with Ollama (llama3)...{Colors.ENDC}")
    
    response = """Based on the provided documents, the key performance metrics for the new product line are:

1. **Efficiency**: 27% increase compared to the previous generation (source: quarterly_report.txt)

2. **Processing Speed**: 45 units per minute, which exceeds the industry standard (source: product_specifications.csv)

3. **Energy Efficiency**: 0.8 kWh consumption rate, representing a 15% improvement (source: product_specifications.csv)

4. **Reliability Score**: 98.7%, which surpasses the target threshold of 95% (source: product_specifications.csv)

5. **Environmental Performance**: Consistent results across all operational environments, with all metrics exceeding target thresholds (source: technical_documentation.md)

These metrics indicate that the new product line is performing exceptionally well, particularly in terms of efficiency and reliability. The consistent performance across different operational environments also suggests robust design and implementation."""
    
    # Stream the response
    print(f"\n{Colors.GREEN}Response:{Colors.ENDC}")
    simulate_typing(response, 0.01)
    
    # Show citations
    print(f"\n{Colors.YELLOW}Sources:{Colors.ENDC}")
    print("1. quarterly_report.txt (Section: Q2 Performance Review)")
    print("2. product_specifications.csv (Rows: 45-48)")
    print("3. technical_documentation.md (Section: Performance Testing)")

def simulate_analytics():
    """Simulate analytics dashboard."""
    print_step(4, "Analytics")
    
    print(f"{Colors.CYAN}Generating analytics data...{Colors.ENDC}")
    
    # Query statistics
    print(f"\n{Colors.YELLOW}Query Statistics:{Colors.ENDC}")
    stats = {
        "total_queries": 1245,
        "avg_response_time": 9.8,
        "rag_usage_percentage": 78,
        "avg_tokens_per_response": 512,
        "top_document": "technical_documentation.md (used in 34% of responses)"
    }
    
    for key, value in stats.items():
        key_formatted = key.replace("_", " ").title()
        print(f"  {key_formatted}: {Colors.GREEN}{value}{Colors.ENDC}")
    
    # Performance metrics
    print(f"\n{Colors.YELLOW}Performance Metrics:{Colors.ENDC}")
    print(f"  Vector Search Time: {Colors.GREEN}0.45s (avg){Colors.ENDC}")
    print(f"  Context Assembly Time: {Colors.GREEN}0.12s (avg){Colors.ENDC}")
    print(f"  LLM Generation Time: {Colors.GREEN}9.2s (avg){Colors.ENDC}")
    print(f"  Total Response Time: {Colors.GREEN}9.8s (avg){Colors.ENDC}")
    
    # Document usage
    print(f"\n{Colors.YELLOW}Document Usage:{Colors.ENDC}")
    print(f"  Most Used Document: {Colors.GREEN}technical_documentation.md{Colors.ENDC}")
    print(f"  Most Relevant Section: {Colors.GREEN}Performance Testing{Colors.ENDC}")
    print(f"  Cache Hit Ratio: {Colors.GREEN}68%{Colors.ENDC}")

def main():
    """Run the demo presentation."""
    print_header("METIS RAG TECHNICAL DEMONSTRATION")
    
    print(f"{Colors.BOLD}Date:{Colors.ENDC} {datetime.now().strftime('%B %d, %Y')}")
    print(f"{Colors.BOLD}System:{Colors.ENDC} Metis RAG v1.2.0")
    print(f"{Colors.BOLD}Models:{Colors.ENDC} llama3 (LLM), nomic-embed-text (Embeddings)")
    
    input(f"\n{Colors.YELLOW}Press Enter to start the demonstration...{Colors.ENDC}")
    
    simulate_document_processing()
    input(f"\n{Colors.YELLOW}Press Enter to continue to query processing...{Colors.ENDC}")
    
    simulate_query_processing()
    input(f"\n{Colors.YELLOW}Press Enter to continue to response generation...{Colors.ENDC}")
    
    simulate_response_generation()
    input(f"\n{Colors.YELLOW}Press Enter to view analytics...{Colors.ENDC}")
    
    simulate_analytics()
    
    print_header("DEMONSTRATION COMPLETE")
    print(f"{Colors.GREEN}{Colors.BOLD}Thank you for attending the Metis RAG technical demonstration!{Colors.ENDC}")

if __name__ == "__main__":
    main()

================
File: demo_tests.py
================
#!/usr/bin/env python3
"""
Demonstration script for Metis RAG testing strategy.
This script runs a subset of tests and displays the results in a user-friendly way.
Ideal for presentations and demonstrations.
"""

import os
import sys
import json
import logging
import subprocess
import time
import argparse
from datetime import datetime
import webbrowser
import shutil
import threading
import random

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("demo_tests")

# Demo test cases
DEMO_TESTS = [
    {
        "name": "Factual Accuracy Test",
        "description": "Tests if RAG responses contain expected facts from source documents",
        "command": ["pytest", "-xvs", "tests/test_rag_quality.py::test_factual_accuracy", "-v"],
        "report_file": "test_quality_results.json",
        "expected_duration": 10
    },
    {
        "name": "Multi-Document Retrieval Test",
        "description": "Tests retrieval across multiple documents",
        "command": ["pytest", "-xvs", "tests/test_rag_quality.py::test_multi_document_retrieval", "-v"],
        "report_file": "test_multi_doc_results.json",
        "expected_duration": 8
    },
    {
        "name": "File Type Support Test",
        "description": "Tests processing of different file types",
        "command": ["pytest", "-xvs", "tests/test_file_handling.py::test_file_type_support", "-v"],
        "report_file": "test_file_type_results.json",
        "expected_duration": 12
    },
    {
        "name": "Query Response Time Test",
        "description": "Measures response time for different query types",
        "command": ["pytest", "-xvs", "tests/test_performance.py::test_query_response_time", "-v"],
        "report_file": "test_response_time_results.json",
        "expected_duration": 15
    },
    {
        "name": "Special Characters Query Test",
        "description": "Tests queries with special characters, SQL injection, XSS, etc.",
        "command": ["pytest", "-xvs", "tests/test_edge_cases.py::test_special_characters_query", "-v"],
        "report_file": "test_special_queries_results.json",
        "expected_duration": 10
    }
]

def print_header(text):
    """Print a header with decoration"""
    width = 80
    print("\n" + "=" * width)
    print(f"{text.center(width)}")
    print("=" * width + "\n")

def print_test_info(test):
    """Print test information"""
    print(f" Test: {test['name']}")
    print(f" Description: {test['description']}")
    print(f"  Expected Duration: {test['expected_duration']} seconds\n")

def print_progress_bar(iteration, total, prefix='', suffix='', length=50, fill=''):
    """Print a progress bar"""
    percent = ("{0:.1f}").format(100 * (iteration / float(total)))
    filled_length = int(length * iteration // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end='\r')
    if iteration == total:
        print()

def animate_progress(duration, stop_event):
    """Animate a progress bar for the given duration"""
    steps = 100
    for i in range(steps + 1):
        if stop_event.is_set():
            # Fill the progress bar completely when done
            print_progress_bar(steps, steps, prefix='Progress:', suffix='Complete', length=50)
            break
        print_progress_bar(i, steps, prefix='Progress:', suffix='Running...', length=50)
        time.sleep(duration / steps)

def run_test(test):
    """Run a test and display progress"""
    print_test_info(test)
    
    # Create a stop event for the animation thread
    stop_event = threading.Event()
    
    # Start the animation in a separate thread
    animation_thread = threading.Thread(target=animate_progress, args=(test["expected_duration"], stop_event))
    animation_thread.start()
    
    # Run the test
    start_time = time.time()
    result = subprocess.run(test["command"], capture_output=True, text=True)
    end_time = time.time()
    
    # Stop the animation
    stop_event.set()
    animation_thread.join()
    
    # Print the result
    actual_duration = end_time - start_time
    print(f"\n  Actual Duration: {actual_duration:.2f} seconds")
    
    if result.returncode == 0:
        print(" Test PASSED\n")
    else:
        print(" Test FAILED\n")
        print("Error details:")
        print(result.stderr)
    
    # Check if report file exists
    if os.path.exists(test["report_file"]):
        print(f" Report generated: {test['report_file']}")
        try:
            with open(test["report_file"], "r") as f:
                report_data = json.load(f)
                print(f" Report contains {len(report_data) if isinstance(report_data, list) else 'structured'} data points")
        except json.JSONDecodeError:
            print("  Report file is not valid JSON")
    else:
        print("  No report file generated")
    
    print("\n" + "-" * 80 + "\n")
    
    return {
        "name": test["name"],
        "success": result.returncode == 0,
        "duration_seconds": actual_duration,
        "report_file": test["report_file"] if os.path.exists(test["report_file"]) else None
    }

def generate_summary(results):
    """Generate a summary of test results"""
    success_count = sum(1 for r in results if r["success"])
    total_count = len(results)
    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
    total_duration = sum(r["duration_seconds"] for r in results)
    
    print_header("TEST SUMMARY")
    print(f"Total Tests: {total_count}")
    print(f"Passed: {success_count}")
    print(f"Failed: {total_count - success_count}")
    print(f"Success Rate: {success_rate:.1f}%")
    print(f"Total Duration: {total_duration:.2f} seconds")
    
    # Print individual test results
    print("\nTest Results:")
    for i, result in enumerate(results):
        status = " PASSED" if result["success"] else " FAILED"
        print(f"{i+1}. {result['name']}: {status} ({result['duration_seconds']:.2f}s)")
    
    # Print report files
    print("\nReport Files:")
    for result in results:
        if result["report_file"]:
            print(f"- {result['report_file']}")
    
    return {
        "success_count": success_count,
        "total_count": total_count,
        "success_rate": success_rate,
        "total_duration": total_duration
    }

def open_reports():
    """Open HTML reports in the browser"""
    report_files = [
        "performance_benchmark_report.html",
        "edge_case_test_report.html",
        "metis_rag_test_report.html"
    ]
    
    for file in report_files:
        if os.path.exists(file):
            print(f"Opening {file} in browser...")
            webbrowser.open(f"file://{os.path.abspath(file)}")
            time.sleep(1)  # Delay to prevent browser from being overwhelmed

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Run Metis RAG demo tests")
    parser.add_argument("--test", type=int, help="Run a specific test (1-5)")
    parser.add_argument("--open-reports", action="store_true", help="Open HTML reports in browser")
    parser.add_argument("--random", action="store_true", help="Run tests in random order")
    return parser.parse_args()

def main():
    """Main function"""
    args = parse_args()
    
    print_header("METIS RAG TESTING DEMONSTRATION")
    print("This demonstration will run a subset of tests to showcase the testing framework.")
    print("Each test will display progress and results, and generate a report file.")
    print("At the end, a summary of all test results will be displayed.")
    
    # Select tests to run
    tests_to_run = DEMO_TESTS
    if args.test:
        if 1 <= args.test <= len(DEMO_TESTS):
            tests_to_run = [DEMO_TESTS[args.test - 1]]
        else:
            print(f"Error: Test number must be between 1 and {len(DEMO_TESTS)}")
            return 1
    
    # Randomize test order if requested
    if args.random:
        random.shuffle(tests_to_run)
    
    # Run tests
    results = []
    for test in tests_to_run:
        result = run_test(test)
        results.append(result)
    
    # Generate summary
    summary = generate_summary(results)
    
    # Open reports if requested
    if args.open_reports:
        open_reports()
    
    return 0 if summary["success_count"] == summary["total_count"] else 1

if __name__ == "__main__":
    sys.exit(main())

================
File: docker-compose.yml
================
version: '3.8'

services:
  metis-rag:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./uploads:/app/uploads
      - ./chroma_db:/app/chroma_db
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  ollama_data:

================
File: Dockerfile
================
FROM python:3.10-slim

WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=utf-8

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    python3-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p uploads chroma_db

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

================
File: generate_pdf.py
================
from reportlab.lib.pagesizes import letter
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
import os

def generate_test_pdf(output_path="sample_report.pdf"):
    """Generate a sample PDF report for testing Metis RAG"""
    
    # Create the PDF document
    doc = SimpleDocTemplate(output_path, pagesize=letter)
    styles = getSampleStyleSheet()
    
    # Create custom styles
    title_style = styles["Heading1"]
    title_style.alignment = 1  # Center alignment
    
    heading2_style = styles["Heading2"]
    heading3_style = styles["Heading3"]
    
    normal_style = styles["Normal"]
    normal_style.spaceAfter = 12
    
    # Create the content elements
    elements = []
    
    # Title
    elements.append(Paragraph("Quarterly Business Report", title_style))
    elements.append(Spacer(1, 24))
    
    # Executive Summary
    elements.append(Paragraph("Executive Summary", heading2_style))
    elements.append(Paragraph(
        """This quarterly report provides an overview of business performance for Q1 2025. 
        Overall, the company has seen strong growth in key metrics including revenue, customer 
        acquisition, and product engagement. This document summarizes the performance across 
        departments and outlines strategic initiatives for the upcoming quarter.""",
        normal_style
    ))
    elements.append(Spacer(1, 12))
    
    # Financial Performance
    elements.append(Paragraph("Financial Performance", heading2_style))
    elements.append(Paragraph(
        """The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. 
        Gross margin improved to 72%, up from 68% in the previous quarter. Operating expenses were 
        kept under control at $2.8M, resulting in a net profit of $1.4M.""",
        normal_style
    ))
    
    # Create a table for financial data
    financial_data = [
        ['Metric', 'Q1 2025', 'Q4 2024', 'Q1 2024', 'YoY Change'],
        ['Revenue', '$4.2M', '$3.8M', '$3.65M', '+15%'],
        ['Gross Margin', '72%', '68%', '65%', '+7%'],
        ['Operating Expenses', '$2.8M', '$2.7M', '$2.5M', '+12%'],
        ['Net Profit', '$1.4M', '$1.1M', '$0.9M', '+56%'],
    ]
    
    table = Table(financial_data, colWidths=[120, 80, 80, 80, 80])
    table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.black),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('GRID', (0, 0), (-1, -1), 1, colors.black),
    ]))
    
    elements.append(table)
    elements.append(Spacer(1, 24))
    
    # Product Development
    elements.append(Paragraph("Product Development", heading2_style))
    elements.append(Paragraph(
        """The product team successfully launched 3 major features this quarter:""",
        normal_style
    ))
    
    # Feature list
    features = [
        "Advanced Analytics Dashboard: Providing deeper insights into user behavior",
        "Mobile Application Redesign: Improving user experience and engagement",
        "API Integration Platform: Enabling third-party developers to build on our platform"
    ]
    
    for feature in features:
        elements.append(Paragraph(f" {feature}", normal_style))
    
    elements.append(Spacer(1, 12))
    elements.append(Paragraph(
        """User engagement metrics show a 22% increase in daily active users following these releases. 
        The product roadmap for Q2 focuses on scalability improvements and enterprise features.""",
        normal_style
    ))
    elements.append(Spacer(1, 12))
    
    # Marketing and Sales
    elements.append(Paragraph("Marketing and Sales", heading2_style))
    elements.append(Paragraph(
        """The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from 
        the previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. 
        Customer acquisition cost (CAC) decreased by 15% to $350 per customer.""",
        normal_style
    ))
    
    # Customer Success
    elements.append(Paragraph("Customer Success", heading2_style))
    elements.append(Paragraph(
        """Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 
        42 to 48. The support team handled 3,200 tickets with an average response time of 2.5 hours 
        and a satisfaction rating of 4.8/5.""",
        normal_style
    ))
    
    # Strategic Initiatives for Q2
    elements.append(Paragraph("Strategic Initiatives for Q2", heading2_style))
    elements.append(Paragraph("The following initiatives are planned for Q2 2025:", normal_style))
    
    initiatives = [
        "International Expansion: Launch in European markets",
        "Enterprise Solution: Develop and release enterprise-grade features",
        "Strategic Partnerships: Form alliances with complementary service providers",
        "Operational Efficiency: Implement automation to reduce operational costs"
    ]
    
    for initiative in initiatives:
        elements.append(Paragraph(f" {initiative}", normal_style))
    
    # Build the PDF
    doc.build(elements)
    
    print(f"PDF generated successfully at {os.path.abspath(output_path)}")
    return os.path.abspath(output_path)

if __name__ == "__main__":
    generate_test_pdf()

================
File: generate_test_data.py
================
#!/usr/bin/env python3
"""
Test data generator for Metis RAG testing.
This script generates test documents with known facts for testing the RAG system.
"""

import os
import json
import argparse
import logging
import random
import string
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("generate_test_data")

# Test document templates
TEST_DOCUMENTS = {
    "technical_doc": {
        "filename": "technical_documentation.md",
        "content": """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
""",
        "tags": ["technical", "documentation", "architecture"],
        "folder": "/test"
    },
    "business_report": {
        "filename": "quarterly_report.txt",
        "content": """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
""",
        "tags": ["business", "report", "quarterly"],
        "folder": "/test"
    },
    "product_specs": {
        "filename": "product_specifications.csv",
        "content": """Product ID,Name,Category,Price,Features,Release Date
P001,MetisRAG Enterprise,Software,$4999,Advanced RAG capabilities|Multi-document retrieval|Enterprise security,2025-01-15
P002,MetisRAG Professional,Software,$1999,Standard RAG capabilities|Document management|API access,2025-01-15
P003,MetisRAG Basic,Software,$499,Basic RAG capabilities|Limited document storage|Web interface,2025-01-15
P004,MetisRAG API,Service,$0.10 per query,Pay-per-use|REST API|Developer documentation,2025-02-01
P005,MetisRAG Mobile,Mobile App,$9.99 per month,iOS and Android|Offline capabilities|Voice interface,2025-03-15
""",
        "tags": ["product", "specifications", "pricing"],
        "folder": "/test"
    },
    "user_guide": {
        "filename": "user_guide.md",
        "content": """# Metis RAG User Guide

## Getting Started

Welcome to Metis RAG, a powerful Retrieval-Augmented Generation system for enterprise knowledge management. This guide will help you get started with the system.

### Installation

To install Metis RAG, follow these steps:

1. Clone the repository: `git clone https://github.com/example/metis-rag.git`
2. Install dependencies: `pip install -r requirements.txt`
3. Configure environment variables: Copy `.env.example` to `.env` and update as needed
4. Start the application: `python -m app.main`

### First Steps

Once the application is running, you can access it at `http://localhost:8000`. The main interface provides the following sections:

- **Chat**: Interact with the RAG system through a chat interface
- **Documents**: Upload, manage, and organize your documents
- **System**: Configure system settings and monitor performance
- **Analytics**: View usage statistics and performance metrics

## Document Management

### Uploading Documents

To upload documents:

1. Navigate to the Documents page
2. Click the "Upload" button
3. Select one or more files from your computer
4. Click "Upload" to start the upload process

Supported file types include:
- PDF (.pdf)
- Text (.txt)
- Markdown (.md)
- CSV (.csv)

### Processing Documents

After uploading, documents need to be processed before they can be used for RAG:

1. Select the documents you want to process
2. Click the "Process" button
3. Wait for processing to complete

Processing includes:
- Text extraction
- Chunking
- Embedding generation
- Vector store indexing

### Organizing Documents

You can organize documents using folders and tags:

- **Folders**: Create a hierarchical structure for your documents
- **Tags**: Add labels to documents for flexible categorization

## Using the Chat Interface

### Basic Queries

To ask a question:

1. Type your question in the input field
2. Click "Send" or press Enter
3. View the response, including citations to source documents

### Advanced Options

The chat interface provides several advanced options:

- **RAG Toggle**: Enable or disable RAG for specific queries
- **Model Selection**: Choose different language models
- **Parameter Adjustment**: Fine-tune model parameters
- **Conversation History**: View and manage conversation history

## System Configuration

### Model Settings

You can configure the following model settings:

- **Default Model**: Set the default language model
- **Context Window**: Adjust the context window size
- **Temperature**: Control response randomness
- **Top-P**: Adjust nucleus sampling

### Vector Store Settings

Vector store settings include:

- **Embedding Model**: Choose the embedding model
- **Similarity Metric**: Select the similarity metric (cosine, dot product, etc.)
- **Top-K Results**: Set the number of results to retrieve

### Chunking Settings

Chunking settings include:

- **Chunking Strategy**: Choose between recursive, token, or markdown chunking
- **Chunk Size**: Set the chunk size in tokens or characters
- **Chunk Overlap**: Set the overlap between chunks

## Analytics

The analytics dashboard provides insights into:

- **Usage Metrics**: Queries per day, document uploads, etc.
- **Performance Metrics**: Response time, throughput, etc.
- **Document Metrics**: Document count, chunk count, etc.
- **Model Metrics**: Token usage, model performance, etc.

## Troubleshooting

### Common Issues

- **Slow Response Time**: Try reducing the number of retrieved chunks or using a smaller model
- **Poor Relevance**: Check if your documents are properly processed and indexed
- **Processing Errors**: Ensure your documents are in a supported format and not corrupted
- **Connection Issues**: Check if Ollama is running and accessible

### Getting Help

If you encounter issues not covered in this guide, please:

- Check the FAQ section
- Search the knowledge base
- Contact support at support@example.com
""",
        "tags": ["user guide", "documentation", "help"],
        "folder": "/test"
    },
    "api_documentation": {
        "filename": "api_documentation.md",
        "content": """# Metis RAG API Documentation

## Overview

The Metis RAG API provides programmatic access to the Retrieval-Augmented Generation system. This document describes the available endpoints, request/response formats, and authentication methods.

## Base URL

All API endpoints are relative to the base URL:

```
https://api.example.com/v1
```

## Authentication

The API uses API keys for authentication. To authenticate, include your API key in the `Authorization` header:

```
Authorization: Bearer YOUR_API_KEY
```

You can generate an API key in the System settings page.

## Endpoints

### Chat

#### Query

```
POST /chat/query
```

Submit a query to the RAG system.

**Request Body:**

```json
{
  "message": "What is the architecture of Metis RAG?",
  "conversation_id": "optional-conversation-id",
  "model": "optional-model-name",
  "use_rag": true,
  "stream": false,
  "model_parameters": {
    "temperature": 0.7,
    "top_p": 0.9
  },
  "metadata_filters": {
    "tags": ["technical", "documentation"]
  }
}
```

**Response:**

```json
{
  "message": "Metis RAG follows a modular architecture with the following components: Frontend Layer, API Layer, and RAG Engine. The RAG Engine consists of Document Processing, Vector Store, and LLM Integration. [1]",
  "conversation_id": "conversation-id",
  "citations": [
    {
      "document_id": "doc-id",
      "chunk_id": "chunk-id",
      "relevance_score": 0.92,
      "excerpt": "Metis RAG follows a modular architecture with the following components..."
    }
  ]
}
```

#### List Conversations

```
GET /chat/conversations
```

List all conversations.

**Response:**

```json
{
  "conversations": [
    {
      "id": "conversation-id",
      "created": "2025-01-15T12:34:56Z",
      "updated": "2025-01-15T13:45:67Z",
      "message_count": 10
    }
  ]
}
```

#### Get Conversation

```
GET /chat/conversations/{conversation_id}
```

Get a specific conversation.

**Response:**

```json
{
  "id": "conversation-id",
  "created": "2025-01-15T12:34:56Z",
  "updated": "2025-01-15T13:45:67Z",
  "messages": [
    {
      "content": "What is the architecture of Metis RAG?",
      "role": "user",
      "timestamp": "2025-01-15T12:34:56Z"
    },
    {
      "content": "Metis RAG follows a modular architecture with the following components...",
      "role": "assistant",
      "timestamp": "2025-01-15T12:35:00Z",
      "citations": [
        {
          "document_id": "doc-id",
          "chunk_id": "chunk-id",
          "relevance_score": 0.92,
          "excerpt": "Metis RAG follows a modular architecture with the following components..."
        }
      ]
    }
  ]
}
```

### Documents

#### Upload Document

```
POST /documents/upload
```

Upload a document.

**Request:**

Multipart form data with a `file` field.

**Response:**

```json
{
  "success": true,
  "document_id": "doc-id",
  "filename": "technical_documentation.md",
  "size": 12345
}
```

#### Process Documents

```
POST /documents/process
```

Process uploaded documents.

**Request Body:**

```json
{
  "document_ids": ["doc-id-1", "doc-id-2"],
  "force_reprocess": false,
  "chunking_strategy": "recursive",
  "chunk_size": 1000,
  "chunk_overlap": 200
}
```

**Response:**

```json
{
  "success": true,
  "processed_count": 2,
  "failed_count": 0,
  "failed_documents": []
}
```

#### List Documents

```
GET /documents/list
```

List all documents.

**Query Parameters:**

- `folder` (optional): Filter by folder
- `tags` (optional): Filter by tags (comma-separated)

**Response:**

```json
[
  {
    "id": "doc-id",
    "filename": "technical_documentation.md",
    "chunk_count": 10,
    "metadata": {
      "file_size": 12345,
      "file_type": "md",
      "created_at": 1642234567,
      "modified_at": 1642234567
    },
    "tags": ["technical", "documentation"],
    "folder": "/test",
    "uploaded": "2025-01-15T12:34:56Z"
  }
]
```

#### Get Document

```
GET /documents/{document_id}
```

Get a specific document.

**Response:**

```json
{
  "id": "doc-id",
  "filename": "technical_documentation.md",
  "content": "# Metis RAG Technical Documentation...",
  "chunk_count": 10,
  "metadata": {
    "file_size": 12345,
    "file_type": "md",
    "created_at": 1642234567,
    "modified_at": 1642234567
  },
  "tags": ["technical", "documentation"],
  "folder": "/test",
  "uploaded": "2025-01-15T12:34:56Z"
}
```

#### Delete Document

```
DELETE /documents/{document_id}
```

Delete a specific document.

**Response:**

```json
{
  "success": true
}
```

### System

#### Health Check

```
GET /system/health
```

Check system health.

**Response:**

```json
{
  "status": "ok",
  "version": "1.0.0",
  "components": {
    "api": "ok",
    "vector_store": "ok",
    "ollama": "ok"
  }
}
```

#### Models

```
GET /system/models
```

List available models.

**Response:**

```json
{
  "models": [
    {
      "name": "llama2",
      "description": "Llama 2 7B",
      "parameters": 7000000000,
      "context_window": 4096
    },
    {
      "name": "mistral",
      "description": "Mistral 7B",
      "parameters": 7000000000,
      "context_window": 8192
    }
  ]
}
```

### Analytics

#### Query Analytics

```
GET /analytics/queries
```

Get query analytics.

**Query Parameters:**

- `start_date` (optional): Start date (ISO format)
- `end_date` (optional): End date (ISO format)
- `model` (optional): Filter by model

**Response:**

```json
{
  "total_queries": 1234,
  "average_response_time_ms": 567,
  "queries_per_day": [
    {
      "date": "2025-01-15",
      "count": 123
    },
    {
      "date": "2025-01-16",
      "count": 456
    }
  ],
  "models": [
    {
      "name": "llama2",
      "count": 789
    },
    {
      "name": "mistral",
      "count": 345
    }
  ]
}
```

## Error Handling

The API uses standard HTTP status codes to indicate success or failure:

- 200: Success
- 400: Bad Request
- 401: Unauthorized
- 404: Not Found
- 500: Internal Server Error

Error responses include a JSON body with details:

```json
{
  "error": {
    "code": "invalid_request",
    "message": "Invalid request parameters",
    "details": {
      "message": "This field is required"
    }
  }
}
```

## Rate Limiting

The API is rate limited to 100 requests per minute per API key. If you exceed this limit, you'll receive a 429 Too Many Requests response.

## Pagination

List endpoints support pagination using the `offset` and `limit` query parameters:

```
GET /documents/list?offset=0&limit=10
```

Paginated responses include pagination metadata:

```json
{
  "data": [...],
  "pagination": {
    "total": 123,
    "offset": 0,
    "limit": 10,
    "next": "/documents/list?offset=10&limit=10",
    "previous": null
  }
}
```
""",
        "tags": ["api", "documentation", "reference"],
        "folder": "/test"
    }
}

# Test queries with expected facts
TEST_QUERIES = [
    {
        "query": "What is the architecture of Metis RAG?",
        "expected_facts": [
            "modular architecture",
            "Frontend Layer",
            "API Layer",
            "RAG Engine",
            "HTML, CSS, and JavaScript",
            "FastAPI"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What was the revenue reported in Q1 2025?",
        "expected_facts": [
            "$4.2M",
            "15% increase",
            "year-over-year",
            "net profit of $1.4M"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What are the components of the RAG engine?",
        "expected_facts": [
            "Document Processing",
            "Vector Store",
            "LLM Integration",
            "chunking",
            "embeddings",
            "Ollama"
        ],
        "document_ids": ["technical_doc"]
    },
    {
        "query": "What are the strategic initiatives for Q2?",
        "expected_facts": [
            "International Expansion",
            "European markets",
            "Enterprise Solution",
            "Strategic Partnerships",
            "Operational Efficiency"
        ],
        "document_ids": ["business_report"]
    },
    {
        "query": "What products are available and at what price points?",
        "expected_facts": [
            "MetisRAG Enterprise",
            "$4999",
            "MetisRAG Professional",
            "$1999",
            "MetisRAG Basic",
            "$499"
        ],
        "document_ids": ["product_specs"]
    },
    {
        "query": "How do I upload documents to the system?",
        "expected_facts": [
            "Navigate to the Documents page",
            "Click the \"Upload\" button",
            "Select one or more files",
            "Click \"Upload\" to start the upload process"
        ],
        "document_ids": ["user_guide"]
    },
    {
        "query": "What authentication method does the API use?",
        "expected_facts": [
            "API keys",
            "Authorization header",
            "Bearer YOUR_API_KEY"
        ],
        "document_ids": ["api_documentation"]
    }
]

def generate_random_string(length):
    """Generate a random string of specified length"""
    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))

def generate_test_documents(output_dir, count=None, specific_docs=None):
    """Generate test documents"""
    os.makedirs(output_dir, exist_ok=True)
    
    documents = []
    
    # Determine which documents to generate
    doc_ids = []
    if specific_docs:
        doc_ids = specific_docs
    elif count:
        doc_ids = random.sample(list(TEST_DOCUMENTS.keys()), min(count, len(TEST_DOCUMENTS)))
    else:
        doc_ids = list(TEST_DOCUMENTS.keys())
    
    # Generate documents
    for doc_id in doc_ids:
        doc_info = TEST_DOCUMENTS[doc_id]
        
        # Create document file
        file_path = os.path.join(output_dir, doc_info["filename"])
        with open(file_path, "w") as f:
            f.write(doc_info["content"])
        
        # Store document metadata
        documents.append({
            "id": doc_id,
            "filename": doc_info["filename"],
            "path": file_path,
            "tags": doc_info["tags"],
            "folder": doc_info["folder"]
        })
        
        logger.info(f"Generated document: {file_path}")
    
    # Generate metadata file
    metadata_path = os.path.join(output_dir, "metadata.json")
    with open(metadata_path, "w") as f:
        json.dump({
            "documents": documents,
            "generated_at": datetime.now().isoformat()
        }, f, indent=2)
    
    logger.info(f"Generated metadata: {metadata_path}")
    
    return documents

def generate_test_queries(output_dir, count=None, specific_queries=None):
    """Generate test queries"""
    # Determine which queries to generate
    queries = []
    if specific_queries:
        queries = [q for i, q in enumerate(TEST_QUERIES) if i in specific_queries]
    elif count:
        queries = random.sample(TEST_QUERIES, min(count, len(TEST_QUERIES)))
    else:
        queries = TEST_QUERIES
    
    # Generate queries file
    queries_path = os.path.join(output_dir, "test_queries.json")
    with open(queries_path, "w") as f:
        json.dump({
            "queries": queries,
            "generated_at": datetime.now().isoformat()
        }, f, indent=2)
    
    logger.info(f"Generated queries: {queries_path}")
    
    return queries

def generate_large_document(output_dir, size_kb):
    """Generate a large document of specified size in KB"""
    # Generate random content
    content = generate_random_string(size_kb * 1024)
    
    # Create document file
    file_path = os.path.join(output_dir, f"large_document_{size_kb}kb.txt")
    with open(file_path, "w") as f:
        f.write(content)
    
    logger.info(f"Generated large document: {file_path} ({size_kb} KB)")
    
    return file_path

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Generate test data for Metis RAG testing")
    parser.add_argument("--output-dir", type=str, default="test_data", help="Output directory for test data")
    parser.add_argument("--document-count", type=int, help="Number of documents to generate")
    parser.add_argument("--query-count", type=int, help="Number of queries to generate")
    parser.add_argument("--specific-docs", type=str, help="Specific documents to generate (comma-separated)")
    parser.add_argument("--specific-queries", type=str, help="Specific queries to generate (comma-separated)")
    parser.add_argument("--large-document", type=int, help="Generate a large document of specified size in KB")
    return parser.parse_args()

def main():
    """Main function"""
    args = parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Parse specific documents
    specific_docs = None
    if args.specific_docs:
        specific_docs = args.specific_docs.split(",")
    
    # Parse specific queries
    specific_queries = None
    if args.specific_queries:
        specific_queries = [int(i) for i in args.specific_queries.split(",")]
    
    # Generate test documents
    documents = generate_test_documents(args.output_dir, args.document_count, specific_docs)
    
    # Generate test queries
    queries = generate_test_queries(args.output_dir, args.query_count, specific_queries)
    
    # Generate large document if requested
    if args.large_document:
        large_doc_path = generate_large_document(args.output_dir, args.large_document)
    
    logger.info(f"Test data generation complete. Generated {len(documents)} documents and {len(queries)} queries.")

if __name__ == "__main__":
    main()

================
File: llm_enhanced_rag_implementation_plan_updated.md
================
# LLM-Enhanced RAG System Implementation Plan (Updated)

## Overview

This document outlines a comprehensive plan for implementing an enhanced Retrieval Augmented Generation (RAG) system using LLM-based "judges" to improve two critical aspects of the RAG pipeline:

1. **Dynamic Chunking Strategy Selection**: An LLM agent ("Chunking Judge") that analyzes documents and selects the most appropriate chunking strategy and parameters.

2. **Query Refinement and Retrieval Enhancement**: An LLM agent ("Retrieval Judge") that analyzes queries and retrieved chunks to improve retrieval quality through query refinement, relevance evaluation, and potential re-retrieval.

## Implementation Progress

### Completed:

- [x] Created git branch `llm-enhanced-rag` for the implementation
- [x] Updated configuration to use gemma3:12b as the default model
- [x] Added configuration variables for LLM judges
- [x] Created the agents directory structure
- [x] Implemented the Chunking Judge class with enhanced document sample extraction
- [x] Integrated Chunking Judge with DocumentProcessor
- [x] Added unit tests for the Chunking Judge
- [x] Added integration tests for the Chunking Judge
- [x] Updated documentation in README.md
- [x] Conducted testing of Chunking Judge with real Ollama client
- [x] Analyzed test results and verified Chunking Judge functionality
- [x] Implemented the Retrieval Judge class
- [x] Integrated Retrieval Judge with RAGEngine
- [x] Added unit tests for the Retrieval Judge
- [x] Added integration tests for the Retrieval Judge
- [x] Conducted performance analysis of Retrieval Judge vs. standard retrieval
- [x] Tested Retrieval Judge with edge cases (ambiguous, typo, domain-specific, multi-part, short, and long queries)
- [x] Analyzed test results and verified Retrieval Judge functionality
- [x] Updated technical documentation with Retrieval Judge capabilities and performance metrics
- [x] Updated implementation plan with test results and progress

### Pending:

- [x] Implement advanced semantic chunking
- [x] Integrate LangGraph for agentic RAG
- [ ] Create benchmarking scripts
- [ ] Conduct comprehensive performance evaluation with real-world queries

## Current System Analysis

The current RAG implementation has several strengths:

- Multiple chunking strategies (recursive, token-based, markdown) with file type-specific handling
- Relevance filtering with a threshold of 0.4 to filter out less relevant chunks
- Robust vector store using ChromaDB with caching for performance
- Well-structured document model with chunks, metadata, tags, and folder organization
- Resilient Ollama client with retry logic and error handling

However, the system currently uses static rules for chunking strategy selection and fixed parameters, which limits adaptability to diverse document types. The retrieval process is also relatively simple, without query refinement or iterative retrieval.

## Implementation Approach

We are taking a phased approach to implementation, starting with the Chunking Judge and then moving on to the Retrieval Judge. Both judges use the gemma3:12b model for its multimodal capabilities, with the system designed to allow changing the model in the future.

## Phase 1: Chunking Judge Implementation (COMPLETED)

### 1.1 Create Agent Architecture (COMPLETED)

```mermaid
flowchart TD
    A[Document Upload] --> B[Document Processor]
    B --> C{Chunking Judge}
    C -->|Analyze Document| D[Select Strategy]
    D --> E[Determine Parameters]
    E --> F[Apply Chunking]
    F --> G[Store Chunks]
```

### 1.2 Create the Chunking Judge Class (COMPLETED)

The Chunking Judge class has been implemented in `app/rag/agents/chunking_judge.py`. The implementation includes:

- Analysis of document structure, content type, and formatting
- Selection of the most appropriate chunking strategy (recursive, token, markdown)
- Recommendation of optimal chunk size and overlap parameters
- Enhanced document sample extraction that prioritizes headers, introduction, and conclusion
- Robust error handling and fallback mechanisms

### 1.3 Update Configuration (COMPLETED)

Configuration variables have been added to `app/core/config.py`:

```python
# LLM Judge settings
CHUNKING_JUDGE_MODEL = os.getenv("CHUNKING_JUDGE_MODEL", "gemma3:12b")
RETRIEVAL_JUDGE_MODEL = os.getenv("RETRIEVAL_JUDGE_MODEL", "gemma3:12b")
USE_CHUNKING_JUDGE = os.getenv("USE_CHUNKING_JUDGE", "True").lower() == "true"
USE_RETRIEVAL_JUDGE = os.getenv("USE_RETRIEVAL_JUDGE", "True").lower() == "true"
```

### 1.4 Integrate with DocumentProcessor (COMPLETED)

The DocumentProcessor has been updated to use the Chunking Judge when processing documents. The integration includes:

- Using the Chunking Judge if enabled via configuration
- Updating chunking strategy and parameters based on the judge's recommendation
- Storing the chunking analysis in document metadata
- Logging the judge's recommendations and justification

### 1.5 Testing (COMPLETED)

Unit and integration tests have been created for the Chunking Judge:

- Unit tests in `tests/unit/test_chunking_judge.py`
- Integration tests in `tests/integration/test_chunking_judge_integration.py`

### 1.6 Test Results with Real Ollama (COMPLETED)

The Chunking Judge was tested with the real Ollama client (gemma3:12b model) using three different document types:

#### Test Results Summary:

1. **Plain Text Document (test_document.txt)**
   - **Recommended Strategy**: markdown
   - **Parameters**: chunk_size=1500, chunk_overlap=200
   - **Justification**: "The document is formatted with Markdown headers, indicating a clear hierarchical structure. Using the 'markdown' strategy will preserve this structure, ensuring that related sections and subsections are grouped together. A chunk size of 1500 characters allows for substantial content within each chunk while maintaining reasonable size. An overlap of 200 characters helps to avoid abrupt transitions between chunks and ensures context is carried over."

2. **Markdown Document (technical_documentation.md)**
   - **Recommended Strategy**: markdown
   - **Parameters**: chunk_size=1500, chunk_overlap=200
   - **Justification**: "The document is a markdown file with a well-defined structure using headers (e.g., Introduction, Architecture Overview, Deployment Options, Configuration, API Reference, Performance Considerations). Using the 'markdown' strategy will allow us to split the document along these logical sections, preserving the context and relationships between different parts of the documentation. A chunk size of 1500 characters allows for a reasonable amount of content within each chunk while still allowing for meaningful sections. An overlap of 200 ensures that context isn't lost between chunks."

3. **CSV Document (test_data.csv)**
   - **Recommended Strategy**: recursive
   - **Parameters**: chunk_size=1000, chunk_overlap=200
   - **Justification**: "The document is a CSV file, which doesn't inherently have a clear header structure suitable for markdown chunking. While it's structured data, the content within each cell (particularly the 'skills' column) can be quite variable in length. Token-based chunking might be overly granular, as it could split skills lists mid-item. Recursive chunking provides a good balance. A chunk size of 1000 characters allows for capturing a reasonable amount of context from multiple rows, while an overlap of 200 helps maintain continuity between chunks, especially when a row's content spans across two chunks. This approach will allow the RAG system to understand the relationships between employees and their skills."

#### Analysis of Results:

The test results demonstrate that the Chunking Judge effectively:

1. **Recognizes Document Structure**: Correctly identifies document structures, even recognizing markdown-like formatting in the plain text file.

2. **Selects Appropriate Strategies**: Recommends chunking strategies that align with the document structure (markdown for header-based documents, recursive for tabular data).

3. **Optimizes Parameters**: Suggests appropriate chunk sizes and overlaps based on document characteristics.

4. **Provides Detailed Justifications**: Offers clear, detailed explanations that demonstrate understanding of document structure and the implications of different chunking strategies.

The real Ollama LLM provides more nuanced and detailed recommendations than the mock client used in initial testing, demonstrating the value of using a sophisticated language model for this task.

## Phase 2: Retrieval Judge Implementation (COMPLETED)

### 2.1 Create Agent Architecture

```mermaid
flowchart TD
    A[User Query] --> B[RAG Engine]
    B --> C[Initial Retrieval]
    C --> D{Retrieval Judge}
    D -->|Analyze Query & Chunks| E[Refine Query]
    D -->|Evaluate Relevance| F[Filter/Re-rank]
    D -->|Insufficient Results| G[Request More]
    E --> H[Additional Retrieval]
    F --> I[Final Context]
    G --> H
    H --> I
    I --> J[Generate Response]
```

### 2.2 Create the Retrieval Judge Class (COMPLETED)

The Retrieval Judge class has been implemented in `app/rag/agents/retrieval_judge.py`. The implementation includes:

- Query analysis to determine complexity, specificity, and intent
- Dynamic recommendation of retrieval parameters (k, threshold, reranking)
- Evaluation of retrieved chunks for relevance to the query
- Query refinement to improve retrieval precision
- Context optimization for better response generation
- Robust error handling and fallback mechanisms

The class provides the following key methods:

```python
class RetrievalJudge:
    """
    LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = RETRIEVAL_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Analyze a query and recommend retrieval parameters
        
        Returns:
            Dict with keys:
            - complexity: The assessed complexity of the query (simple, moderate, complex)
            - parameters: Dict of recommended retrieval parameters (k, threshold, etc.)
            - justification: Explanation of the recommendation
        """
    
    async def evaluate_chunks(self, query: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Evaluate retrieved chunks for relevance to the query
        
        Returns:
            Dict with keys:
            - relevance_scores: Dict mapping chunk IDs to relevance scores (0-1)
            - needs_refinement: Boolean indicating if query refinement is needed
            - justification: Explanation of the evaluation
        """
    
    async def refine_query(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """
        Refine a query based on retrieved chunks to improve retrieval precision
        
        Returns:
            Refined query string
        """
    
    async def optimize_context(self, query: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Optimize the assembly of chunks into a context for the LLM
        
        Returns:
            Reordered and filtered list of chunks optimized for context assembly
        """
```

Each method uses carefully crafted prompts to guide the LLM in performing its specific task, with robust parsing of the LLM responses and fallback mechanisms for error handling.

### 2.3 Integrate with RAGEngine (COMPLETED)

The Retrieval Judge has been integrated with the RAGEngine to enhance the retrieval process. The integration includes:

- Initialization of the RetrievalJudge in the RAGEngine constructor if enabled via configuration
- A new `_enhanced_retrieval` method that implements the enhanced retrieval process
- Updates to the main `query` method to use enhanced retrieval when the judge is enabled

The enhanced retrieval process includes:

1. Query analysis to determine complexity and optimal retrieval parameters
2. Initial retrieval with recommended parameters
3. Evaluation of retrieved chunks for relevance
4. Query refinement when initial results are insufficient
5. Additional retrieval with refined query when needed
6. Re-ranking and filtering of chunks based on relevance scores
7. Context optimization for better response generation

The implementation in `rag_engine.py` includes:

```python
# In rag_engine.py

async def _enhanced_retrieval(
    self,
    query: str,
    conversation_context: str = "",
    top_k: int = 10,
    metadata_filters: Optional[Dict[str, Any]] = None
) -> Tuple[str, List[Dict[str, Any]], List[str]]:
    """
    Enhanced retrieval using the Retrieval Judge
    """
    # Step 1: Analyze the query using the Retrieval Judge
    query_analysis = await self.retrieval_judge.analyze_query(query)
    
    # Extract recommended parameters
    recommended_k = query_analysis.get("parameters", {}).get("k", top_k)
    relevance_threshold = query_analysis.get("parameters", {}).get("threshold", 0.4)
    apply_reranking = query_analysis.get("parameters", {}).get("reranking", True)
    
    # Step 2: Initial retrieval with recommended parameters
    search_results = await self.vector_store.search(
        query=search_query,
        top_k=max(15, recommended_k + 5),  # Get a few extra for filtering
        filter_criteria=metadata_filters
    )
    
    # Step 3: Evaluate chunks with the Retrieval Judge
    evaluation = await self.retrieval_judge.evaluate_chunks(query, search_results)
    
    # Step 4: Refine query if needed and perform additional retrieval
    if evaluation.get("needs_refinement", False):
        refined_query = await self.retrieval_judge.refine_query(query, search_results)
        additional_results = await self.vector_store.search(
            query=refined_query,
            top_k=recommended_k,
            filter_criteria=metadata_filters
        )
        # Combine results, avoiding duplicates
        # ...
    
    # Step 5: Filter and re-rank chunks based on relevance scores
    # ...
    
    # Step 6: Optimize context assembly
    if len(relevant_results) > 3 and apply_reranking:
        optimized_results = await self.retrieval_judge.optimize_context(query, relevant_results)
        if optimized_results:
            relevant_results = optimized_results
    
    # Step 7: Format context with source information
    # ...
    
    return context, sources, document_ids
```

The integration is designed to be seamless, with the Retrieval Judge being enabled or disabled via configuration. This allows for flexibility in deployment, where the enhanced retrieval can be used for complex queries while falling back to standard retrieval for simpler queries or when performance is a concern.

## Phase 3: Advanced Semantic Chunking (COMPLETED)

Building on the success of the Chunking Judge, we have implemented advanced semantic chunking strategies:

### 3.1 Implement Semantic Chunking (COMPLETED)

Created a SemanticChunker class in `app/rag/chunkers/semantic_chunker.py` that uses the LLM to identify natural semantic boundaries in text. The implementation includes:

- Intelligent boundary detection based on topic transitions and subject matter shifts
- Handling of long documents by processing in sections
- Caching for performance optimization
- Fallback mechanisms for error handling
- Both synchronous and asynchronous interfaces

### 3.2 Integrate Semantic Chunking with DocumentProcessor (COMPLETED)

Updated the DocumentProcessor to support semantic chunking as a new chunking strategy. The integration includes:

- Adding semantic chunking as a strategy option in the DocumentProcessor
- Updating the Chunking Judge to recommend semantic chunking when appropriate
- Creating comprehensive unit and integration tests

## Phase 4: LangGraph Integration for Agentic RAG (FUTURE PHASE)

Building on the success of the individual judges, we will implement a more sophisticated agentic RAG system using LangGraph:

### 4.1 Install Dependencies

Add LangGraph to the project dependencies.

### 4.2 Create LangGraph RAG Agent

Implement a LangGraphRAGAgent class that uses a state machine to orchestrate the RAG process.

### 4.3 Integrate LangGraph Agent with API

Add a new endpoint to the API for the LangGraph RAG agent.

## Implementation Timeline

### Phase 1: Chunking Judge (COMPLETED - March 5, 2025)
- Implement Chunking Judge class and integration with DocumentProcessor
- Testing and refinement of Chunking Judge
- Analysis of test results with real documents

### Phase 2: Retrieval Judge (COMPLETED - March 18, 2025)
- Implemented Retrieval Judge class and integration with RAGEngine
- Added unit and integration tests for the Retrieval Judge
- Conducted performance analysis comparing judge-enhanced vs. standard retrieval
- Tested with edge cases (ambiguous, typo, domain-specific, multi-part, short, and long queries)
- Verified functionality with comprehensive test coverage

### Phase 3: Advanced Semantic Chunking (COMPLETED - March 18, 2025)
- Implemented SemanticChunker class in app/rag/chunkers/semantic_chunker.py
- Integrated with DocumentProcessor and Chunking Judge
- Added unit and integration tests for the SemanticChunker
- Updated technical documentation with semantic chunking capabilities

### Phase 4: LangGraph Integration (COMPLETED - March 18, 2025)
- Implemented LangGraphRAGAgent class with state machine for orchestrating the RAG process
- Integrated with existing components (Chunking Judge, Semantic Chunker, Retrieval Judge)
- Added API endpoint for the LangGraph RAG Agent
- Created integration tests for the LangGraph RAG Agent

## Potential Challenges and Mitigations

1. **Increased Latency**
   - **Challenge**: Adding LLM calls will increase response time (observed ~30 seconds for Chunking Judge analysis with gemma3:12b)
   - **Mitigation**: Implement caching for judge decisions (already showing 33.33% vector store cache hit rate and 16.67% LLM cache hit rate), make judges optional via configuration, optimize when to use judges, consider using smaller models for faster inference
   - **Update**: Our testing shows that while initial queries with the Retrieval Judge are slower, subsequent queries are significantly faster (89.26% improvement) due to caching effects

2. **Cost Considerations**
   - **Challenge**: Additional LLM calls increase computational costs
   - **Mitigation**: Make judges optional, implement usage tracking, optimize when to use judges, cache results for similar documents/queries
   - **Update**: The Retrieval Judge's context optimization reduces the number of chunks by 76.4% on average, which can reduce the token count for the final LLM call, potentially offsetting some of the additional costs

3. **Error Handling**
   - **Challenge**: LLM responses may not always be parseable (observed occasional timeout errors during testing)
   - **Mitigation**: Robust fallback mechanisms in the parsing functions, implement retry logic with exponential backoff (already implemented in OllamaClient)
   - **Update**: Our testing revealed occasional timeout errors with the Retrieval Judge, but the retry logic in OllamaClient successfully handled these cases

4. **Cold Start Problem**
   - **Challenge**: Judges may perform poorly on new document types
   - **Mitigation**: Implement a feedback loop to improve judge performance over time, store successful analyses for reference
   - **Update**: The Retrieval Judge showed strong performance across different query types, with domain-specific and long queries being handled particularly well

5. **Model Quality Variations**
   - **Challenge**: Different LLMs may provide varying quality of recommendations
   - **Mitigation**: Test multiple models and select the best performing one, allow configuration of model per judge
   - **Update**: The gemma3:4b model used for testing the Retrieval Judge provided high-quality recommendations, suggesting that even smaller models can be effective for this task

6. **Justification Quality**
   - **Challenge**: Ensuring justifications are detailed and helpful for debugging and understanding
   - **Mitigation**: Include specific prompting for detailed justifications, as demonstrated in the Chunking Judge test results
   - **Update**: The Retrieval Judge provided detailed justifications for its decisions, particularly in the chunk evaluation component, which helps in understanding and debugging the system

## Conclusion

The LLM-enhanced RAG system with "judges" for chunking and retrieval has significant potential to improve the adaptability and accuracy of the RAG pipeline. By dynamically selecting chunking strategies and refining queries, the system can better handle diverse document types and complex queries.

Both Phase 1 (Chunking Judge) and Phase 2 (Retrieval Judge) have been successfully implemented and tested, providing a solid foundation for the remaining phases.

### Phase 1 Results (Chunking Judge)

The Chunking Judge effectively:

1. Recognizes document structure and formatting, even identifying markdown-like elements in plain text files
2. Selects appropriate chunking strategies based on document characteristics
3. Recommends optimized parameters (chunk size and overlap) tailored to each document type
4. Provides detailed, insightful justifications for its recommendations

The real Ollama LLM (gemma3:12b) provided more nuanced and detailed recommendations than our initial mock implementation, demonstrating the value of using a sophisticated language model for this task. The Chunking Judge makes the system more adaptable to different document types without manual configuration, which improves retrieval quality and user experience.

### Phase 2 Results (Retrieval Judge)

The Retrieval Judge implementation enhances the RAG pipeline by:

1. Analyzing queries to determine complexity, specificity, and intent
2. Dynamically adjusting retrieval parameters based on query characteristics
3. Evaluating retrieved chunks for relevance with detailed scoring
4. Refining queries when initial results are insufficient
5. Optimizing the order and selection of chunks for context assembly

### Phase 3 Results (Semantic Chunker)

The Semantic Chunker implementation enhances the document processing pipeline by:

1. Identifying natural semantic boundaries in text based on topic transitions
2. Preserving semantic meaning and context in chunks
3. Creating more coherent, self-contained chunks than traditional methods
4. Respecting the logical flow of information in documents
5. Providing caching and fallback mechanisms for robust performance

### Phase 4 Results (LangGraph RAG Agent)

The LangGraph RAG Agent implementation provides a sophisticated orchestration layer for the RAG process:

1. Uses a state machine to coordinate the entire RAG workflow
2. Integrates all existing components (Chunking Judge, Semantic Chunker, Retrieval Judge)
3. Provides a clear, modular architecture with well-defined states and transitions
4. Enables more complex, multi-step reasoning during the RAG process
5. Supports both streaming and non-streaming responses
6. Includes robust error handling and fallback mechanisms
7. Exposes a dedicated API endpoint for direct access

The LangGraph integration represents a significant advancement in our RAG system's architecture, moving from a linear pipeline to a more flexible, state-based approach that can adapt to different query types and document characteristics.

Our comprehensive testing of the Retrieval Judge with edge cases revealed:

- **Query Analysis**: The judge classifies most queries as "moderate" complexity, with only very short queries as "simple" and long multi-part queries as "complex". It provides detailed justifications for parameter recommendations.

- **Query Refinement**: The judge dramatically expands and clarifies queries:
  - Short queries expanded by 887.5% on average
  - Ambiguous queries gain 650.6% more specificity
  - Typos are automatically corrected
  - Query refinement is very fast (2.08s average)

- **Chunk Evaluation**: The judge effectively evaluates chunk relevance:
  - 7/11 test queries identified as needing refinement
  - Domain-specific and long queries rarely need refinement (0%)
  - Ambiguous, multi-part, and short queries always need refinement (100%)

- **Context Optimization**: The judge dramatically reduces context size:
  - 76.4% average reduction in chunks (typically from 5 to 1)
  - Long queries maintain more chunks (5 to 3) for comprehensive answers

- **Performance Improvement**: The judge-enhanced retrieval is 89.26% faster than standard retrieval (18.41s vs 171.47s on average) due to effective caching of both vector store queries and LLM responses.

The integration with the RAGEngine provides a seamless experience, with the Retrieval Judge being enabled or disabled via configuration. This allows for flexibility in deployment, where the enhanced retrieval can be used for complex queries while falling back to standard retrieval for simpler queries or when performance is a concern.

### Next Steps

With all four phases (Chunking Judge, Retrieval Judge, Semantic Chunker, and LangGraph RAG Agent) successfully implemented and tested, the next steps will focus on:

1. Comprehensive benchmarking and performance evaluation with real-world queries
2. Implementing a feedback loop to continuously improve the system over time
3. Exploring additional optimizations and enhancements to the LangGraph state machine
4. Developing user interface components to visualize the agentic RAG process

Our testing has demonstrated that the LLM-enhanced RAG system with judges for chunking and retrieval, semantic chunking, and LangGraph orchestration significantly improves the adaptability, performance, and accuracy of the RAG pipeline:

1. The Chunking Judge intelligently selects the most appropriate chunking strategy and parameters
2. The Semantic Chunker creates more coherent, meaningful chunks that preserve context
3. The Retrieval Judge optimizes query processing and context assembly

Together, these components form a powerful, intelligent RAG system that:
- Adapts to different document types without manual configuration
- Preserves semantic meaning throughout the pipeline
- Transforms ambiguous queries into specific, detailed requests
- Evaluates and filters content for maximum relevance
- Orchestrates complex, multi-step reasoning processes
- Provides a flexible, state-based architecture for the RAG workflow
- Optimizes performance through effective caching

These enhancements have made the RAG system more effective for a wider range of use cases, particularly for complex, domain-specific queries. The LangGraph integration has successfully built on the foundation of the judges and semantic chunker to create a sophisticated, agentic RAG system that can adapt to different query types and document characteristics.

================
File: llm_enhanced_rag_implementation_plan.md
================
# LLM-Enhanced RAG System Implementation Plan

## Overview

This document outlines a comprehensive plan for implementing an enhanced Retrieval Augmented Generation (RAG) system using LLM-based "judges" to improve two critical aspects of the RAG pipeline:

1. **Dynamic Chunking Strategy Selection**: An LLM agent ("Chunking Judge") that analyzes documents and selects the most appropriate chunking strategy and parameters.

2. **Query Refinement and Retrieval Enhancement**: An LLM agent ("Retrieval Judge") that analyzes queries and retrieved chunks to improve retrieval quality through query refinement, relevance evaluation, and potential re-retrieval.

## Current System Analysis

The current RAG implementation has several strengths:

- Multiple chunking strategies (recursive, token-based, markdown) with file type-specific handling
- Relevance filtering with a threshold of 0.4 to filter out less relevant chunks
- Robust vector store using ChromaDB with caching for performance
- Well-structured document model with chunks, metadata, tags, and folder organization
- Resilient Ollama client with retry logic and error handling

However, the system currently uses static rules for chunking strategy selection and fixed parameters, which limits adaptability to diverse document types. The retrieval process is also relatively simple, without query refinement or iterative retrieval.

## Implementation Approach

We will take a phased approach to implementation, starting with the Chunking Judge and then moving on to the Retrieval Judge. Both judges will use the gemma3:12b model for its multimodal capabilities, with the system designed to allow changing the model in the future.

## Phase 1: Chunking Judge Implementation

### 1.1 Create Agent Architecture

```mermaid
flowchart TD
    A[Document Upload] --> B[Document Processor]
    B --> C{Chunking Judge}
    C -->|Analyze Document| D[Select Strategy]
    D --> E[Determine Parameters]
    E --> F[Apply Chunking]
    F --> G[Store Chunks]
```

### 1.2 Create the Chunking Judge Class

```python
# app/rag/agents/chunking_judge.py
import logging
import json
import re
from typing import Dict, Any, List, Optional
from app.models.document import Document
from app.rag.ollama_client import OllamaClient
from app.core.config import CHUNKING_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.chunking_judge")

class ChunkingJudge:
    """
    LLM-based agent that analyzes documents and recommends optimal chunking strategies
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = CHUNKING_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def analyze_document(self, document: Document) -> Dict[str, Any]:
        """
        Analyze a document and recommend the best chunking strategy and parameters
        
        Returns:
            Dict with keys:
            - strategy: The recommended chunking strategy
            - parameters: Dict of parameters for the chosen strategy
            - justification: Explanation of the recommendation
        """
        # Extract a sample of the document content (to avoid exceeding context window)
        content_sample = self._extract_representative_sample(document.content)
        
        # Create prompt for the LLM
        prompt = self._create_analysis_prompt(document.filename, content_sample)
        
        # Get recommendation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        recommendation = self._parse_recommendation(response.get("response", ""))
        
        logger.info(f"Chunking Judge recommended strategy '{recommendation['strategy']}' for document {document.filename}")
        
        return recommendation
    
    def _extract_representative_sample(self, content: str, max_length: int = 5000) -> str:
        """Extract a representative sample of the document content"""
        if len(content) <= max_length:
            return content
        
        # Take beginning, middle and end sections
        third = max_length // 3
        beginning = content[:third]
        middle_start = (len(content) - third) // 2
        middle = content[middle_start:middle_start + third]
        end = content[-third:]
        
        return f"{beginning}\n\n[...]\n\n{middle}\n\n[...]\n\n{end}"
    
    def _create_analysis_prompt(self, filename: str, content_sample: str) -> str:
        """Create a prompt for the LLM to analyze the document"""
        return f"""You are a document analysis expert. Your task is to analyze the following document and recommend the best chunking strategy for a RAG (Retrieval Augmented Generation) system.

Available Strategies:
- recursive: Splits text recursively by characters. Good for general text with natural separators.
- token: Splits text by tokens. Good for preserving semantic units in technical content.
- markdown: Splits markdown documents by headers. Good for structured documents with clear sections.

Document Filename: {filename}

Document Sample:
{content_sample}

Analyze the document structure, content type, and formatting. Consider:
1. Is this a structured document with clear sections or headers?
2. Does it contain code, tables, or other special formatting?
3. What's the typical paragraph and sentence length?
4. Are there natural breaks in the content?
5. Would semantic chunking be more appropriate than fixed-size chunking?

Output your recommendation in JSON format:
{{
    "strategy": "...",  // One of: recursive, token, markdown
    "parameters": {{
        "chunk_size": ...,  // Recommended chunk size (characters or tokens)
        "chunk_overlap": ...  // Recommended overlap size
    }},
    "justification": "..." // Explanation of your reasoning
}}
"""
    
    def _parse_recommendation(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM response to extract the recommendation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                recommendation = json.loads(json_str)
                
                # Validate the recommendation
                if "strategy" not in recommendation:
                    raise ValueError("Missing 'strategy' in recommendation")
                
                # Set defaults if missing
                if "parameters" not in recommendation:
                    recommendation["parameters"] = {}
                if "chunk_size" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_size"] = 500
                if "chunk_overlap" not in recommendation["parameters"]:
                    recommendation["parameters"]["chunk_overlap"] = 50
                
                return recommendation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing chunking recommendation: {str(e)}")
            # Return default recommendation
            return {
                "strategy": "recursive",
                "parameters": {
                    "chunk_size": 500,
                    "chunk_overlap": 50
                },
                "justification": "Failed to parse LLM recommendation, using default strategy."
            }
```

### 1.3 Update Configuration

```python
# app/core/config.py
# Add new configuration variables
CHUNKING_JUDGE_MODEL = os.getenv("CHUNKING_JUDGE_MODEL", DEFAULT_MODEL)
RETRIEVAL_JUDGE_MODEL = os.getenv("RETRIEVAL_JUDGE_MODEL", DEFAULT_MODEL)
USE_CHUNKING_JUDGE = os.getenv("USE_CHUNKING_JUDGE", "True").lower() == "true"
USE_RETRIEVAL_JUDGE = os.getenv("USE_RETRIEVAL_JUDGE", "True").lower() == "true"
```

### 1.4 Integrate with DocumentProcessor

```python
# app/rag/document_processor.py
# Update the process_document method

from app.rag.agents.chunking_judge import ChunkingJudge
from app.core.config import USE_CHUNKING_JUDGE

async def process_document(self, document: Document) -> Document:
    """
    Process a document by splitting it into chunks
    """
    try:
        logger.info(f"Processing document: {document.filename}")
        
        # Get the document path
        file_path = os.path.join(UPLOAD_DIR, document.id, document.filename)
        
        # Get file extension for specialized handling
        _, ext = os.path.splitext(file_path.lower())
        
        # Use Chunking Judge if enabled
        if USE_CHUNKING_JUDGE:
            chunking_judge = ChunkingJudge()
            analysis_result = await chunking_judge.analyze_document(document)
            
            # Update chunking strategy and parameters
            self.chunking_strategy = analysis_result["strategy"]
            if "chunk_size" in analysis_result["parameters"]:
                self.chunk_size = analysis_result["parameters"]["chunk_size"]
            if "chunk_overlap" in analysis_result["parameters"]:
                self.chunk_overlap = analysis_result["parameters"]["chunk_overlap"]
            
            # Store the chunking analysis in document metadata
            document.metadata["chunking_analysis"] = analysis_result
            
            logger.info(f"Using Chunking Judge recommendation: strategy={self.chunking_strategy}, " +
                       f"chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
        else:
            logger.info(f"Chunking Judge disabled, using default strategy: {self.chunking_strategy}")
        
        # Get appropriate text splitter for this file type and strategy
        self.text_splitter = self._get_text_splitter(ext)
        
        # Rest of the existing code...
```

### 1.5 Enhance Text Splitter Selection

```python
# app/rag/document_processor.py
# Update the _get_text_splitter method to support more advanced chunking strategies

def _get_text_splitter(self, file_ext=None):
    """Get the appropriate text splitter based on chunking strategy and file type"""
    logger.info(f"Using chunking strategy: {self.chunking_strategy} for file type: {file_ext}")
    
    # If we have a chunking analysis, log it
    if hasattr(self, 'document') and self.document and 'chunking_analysis' in self.document.metadata:
        logger.info(f"Chunking analysis: {self.document.metadata['chunking_analysis']['justification']}")
    
    # Text file handling - use paragraph-based splitting for more natural chunks
    if file_ext == ".txt":
        # Use a larger chunk size for text files to preserve more context
        larger_chunk_size = self.chunk_size * 3  # Increase from 500 to 1500
        logger.info(f"Using paragraph-based splitting for text file with increased chunk size {larger_chunk_size}")
        return RecursiveCharacterTextSplitter(
            chunk_size=larger_chunk_size,
            chunk_overlap=self.chunk_overlap * 2,  # Increase overlap as well
            separators=["\n\n", "\n", ".", " ", ""]
        )
    
    # PDF-specific handling
    if file_ext == ".pdf":
        logger.info(f"Using PDF-specific splitting with chunk size {self.chunk_size}")
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
    
    # Markdown-specific handling
    if file_ext == ".md" and self.chunking_strategy == "markdown":
        logger.info("Using header-based splitting for markdown")
        # First split by headers
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "header1"),
                ("##", "header2"),
                ("###", "header3"),
                ("####", "header4"),
            ]
        )
        # Then apply recursive splitting to each section
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
    
    # CSV-specific handling
    if file_ext == ".csv":
        logger.info(f"Using larger chunks for CSV with chunk size {self.chunk_size}")
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size * 2,  # Double chunk size for CSVs
            chunk_overlap=self.chunk_overlap
        )
    
    # Standard strategies
    if self.chunking_strategy == "recursive":
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
    elif self.chunking_strategy == "token":
        return TokenTextSplitter(
            chunk_size=self.chunk_size // 4,  # Adjust for tokens vs characters
            chunk_overlap=self.chunk_overlap // 4
        )
    elif self.chunking_strategy == "markdown":
        return MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "header1"),
                ("##", "header2"),
                ("###", "header3"),
                ("####", "header4"),
            ]
        )
    else:
        logger.warning(f"Unknown chunking strategy: {self.chunking_strategy}, falling back to recursive")
        return RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
```

## Phase 2: Retrieval Judge Implementation

### 2.1 Create Agent Architecture

```mermaid
flowchart TD
    A[User Query] --> B[RAG Engine]
    B --> C[Initial Retrieval]
    C --> D{Retrieval Judge}
    D -->|Analyze Query & Chunks| E[Refine Query]
    D -->|Evaluate Relevance| F[Filter/Re-rank]
    D -->|Insufficient Results| G[Request More]
    E --> H[Additional Retrieval]
    F --> I[Final Context]
    G --> H
    H --> I
    I --> J[Generate Response]
```

### 2.2 Create the Retrieval Judge Class

```python
# app/rag/agents/retrieval_judge.py
import logging
import json
import re
from typing import Dict, Any, List, Optional
from app.rag.ollama_client import OllamaClient
from app.core.config import RETRIEVAL_JUDGE_MODEL

logger = logging.getLogger("app.rag.agents.retrieval_judge")

class RetrievalJudge:
    """
    LLM-based agent that analyzes queries and retrieved chunks to improve retrieval quality
    """
    def __init__(self, ollama_client: Optional[OllamaClient] = None, model: str = RETRIEVAL_JUDGE_MODEL):
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
    
    async def evaluate_retrieval(
        self, 
        query: str, 
        chunks: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate the retrieved chunks for a query and recommend improvements
        
        Returns:
            Dict with keys:
            - refined_query: Reformulated query (or original if no change)
            - relevant_chunks: List of relevant chunk IDs
            - re_rank_chunks: List of chunk IDs in re-ranked order
            - request_more: Boolean indicating if more retrieval is needed
            - justification: Explanation of the evaluation
        """
        # Create prompt for the LLM
        prompt = self._create_evaluation_prompt(query, chunks, conversation_history)
        
        # Get evaluation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        evaluation = self._parse_evaluation(response.get("response", ""), chunks)
        
        logger.info(f"Retrieval Judge evaluated query: '{query}', refined to: '{evaluation['refined_query']}'")
        logger.info(f"Found {len(evaluation['relevant_chunks'])} relevant chunks out of {len(chunks)}")
        
        return evaluation
    
    def _create_evaluation_prompt(
        self, 
        query: str, 
        chunks: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """Create a prompt for the LLM to evaluate the retrieval"""
        # Format chunks for the prompt
        chunks_text = ""
        for i, chunk in enumerate(chunks):
            # Extract a preview of the chunk content
            content = chunk.get("content", "")
            content_preview = content[:300] + "..." if len(content) > 300 else content
            
            # Format chunk with metadata
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            
            chunks_text += f"Chunk {i+1} (ID: {chunk.get('chunk_id', 'unknown')}):\n"
            chunks_text += f"Source: {filename}\n"
            chunks_text += f"Content: {content_preview}\n\n"
        
        # Format conversation history if provided
        history_text = ""
        if conversation_history and len(conversation_history) > 0:
            history_text = "Previous conversation:\n"
            for msg in conversation_history:
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                history_text += f"{role.capitalize()}: {content}\n"
            history_text += "\n"
        
        return f"""You are a retrieval evaluation expert. Your task is to analyze a user query and a set of retrieved document chunks, and determine if the retrieval is sufficient and relevant.

User Query: {query}

{history_text}Retrieved Chunks:
{chunks_text}

Your task:
1. Evaluate the relevance of each chunk to the query
2. Determine if the retrieval is sufficient to answer the query
3. If needed, reformulate the query to improve retrieval
4. Identify which chunks are most relevant
5. Re-rank the chunks in order of relevance to the query

Output your evaluation in JSON format:
{{
    "refined_query": "...",  // Reformulated query (or original if no change needed)
    "relevant_chunks": [...],  // List of relevant chunk IDs (or indices)
    "re_rank_chunks": [...],  // List of chunk IDs in order of relevance (most to least)
    "request_more": true/false,  // Whether additional retrieval is needed
    "justification": "..."  // Explanation of your evaluation
}}
"""
    
    def _parse_evaluation(self, response_text: str, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Parse the LLM response to extract the evaluation"""
        try:
            # Extract JSON from the response
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                evaluation = json.loads(json_str)
                
                # Set defaults if missing
                if "refined_query" not in evaluation:
                    evaluation["refined_query"] = ""
                if "relevant_chunks" not in evaluation:
                    evaluation["relevant_chunks"] = []
                if "re_rank_chunks" not in evaluation:
                    evaluation["re_rank_chunks"] = []
                if "request_more" not in evaluation:
                    evaluation["request_more"] = False
                
                return evaluation
            else:
                raise ValueError("Could not find JSON in response")
        except Exception as e:
            logger.error(f"Error parsing retrieval evaluation: {str(e)}")
            # Return default evaluation
            return {
                "refined_query": "",
                "relevant_chunks": [chunk.get("chunk_id") for chunk in chunks],
                "re_rank_chunks": [],
                "request_more": False,
                "justification": "Failed to parse LLM evaluation, using all chunks."
            }
```

### 2.3 Integrate with RAGEngine

```python
# app/rag/rag_engine.py
# Update the query method

from app.rag.agents.retrieval_judge import RetrievalJudge
from app.core.config import USE_RETRIEVAL_JUDGE

async def query(
    self,
    query: str,
    model: str = DEFAULT_MODEL,
    use_rag: bool = True,
    top_k: int = 10,
    system_prompt: Optional[str] = None,
    stream: bool = False,
    model_parameters: Dict[str, Any] = None,
    conversation_history: Optional[List[Message]] = None,
    metadata_filters: Optional[Dict[str, Any]] = None,
    max_retrieval_iterations: int = 2  # New parameter
) -> Dict[str, Any]:
    """
    Query the RAG engine with optional conversation history and metadata filtering
    """
    start_time = time.time()
    document_ids = []
    
    try:
        logger.info(f"RAG query: {query[:50]}...")
        
        # Get context from vector store if RAG is enabled
        context = ""
        sources = []
        
        # Format conversation history if provided
        conversation_context = ""
        formatted_history = []
        
        if conversation_history and len(conversation_history) > 1:
            # Format the conversation history (existing code)
            # ...
            
            # Also prepare a version for the retrieval judge
            for msg in conversation_history[:-1]:  # Exclude current query
                formatted_history.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        if use_rag:
            # Check if there are any documents in the vector store
            stats = self.vector_store.get_stats()
            if stats["count"] == 0:
                # Handle empty vector store (existing code)
                # ...
            else:
                # Iterative retrieval with Retrieval Judge if enabled
                search_query = query
                iteration = 0
                all_results = []
                
                while iteration < max_retrieval_iterations:
                    # Log the search query for this iteration
                    logger.info(f"Retrieval iteration {iteration+1}: searching with query: {search_query[:100]}...")
                    
                    # Perform search with current query
                    search_results = await self.vector_store.search(
                        query=search_query,
                        top_k=15,  # Get more results for the judge to evaluate
                        filter_criteria=metadata_filters
                    )
                    
                    if not search_results:
                        logger.warning(f"No results found for query in iteration {iteration+1}")
                        break
                    
                    # Add results to our collection
                    all_results.extend(search_results)
                    
                    # Use Retrieval Judge if enabled and not the last iteration
                    if USE_RETRIEVAL_JUDGE and iteration < max_retrieval_iterations - 1:
                        retrieval_judge = RetrievalJudge()
                        evaluation = await retrieval_judge.evaluate_retrieval(
                            query=query,  # Original query
                            chunks=search_results,
                            conversation_history=formatted_history
                        )
                        
                        # Log the evaluation
                        logger.info(f"Retrieval Judge evaluation: {evaluation['justification']}")
                        
                        # Check if we need more retrieval
                        if evaluation["request_more"] and evaluation["refined_query"]:
                            # Use the refined query for the next iteration
                            search_query = evaluation["refined_query"]
                            iteration += 1
                            logger.info(f"Retrieval Judge requested more retrieval with refined query: {search_query}")
                        else:
                            # No more retrieval needed
                            break
                    else:
                        # No retrieval judge or last iteration
                        break
                
                # After retrieval iterations, process all results
                if all_results:
                    logger.info(f"Retrieved {len(all_results)} total chunks from vector store")
                    
                    # If we used the retrieval judge, filter and re-rank results
                    if USE_RETRIEVAL_JUDGE and 'evaluation' in locals():
                        # Filter to relevant chunks if specified
                        if evaluation["relevant_chunks"]:
                            relevant_ids = set(evaluation["relevant_chunks"])
                            all_results = [r for r in all_results if r.get("chunk_id") in relevant_ids]
                            logger.info(f"Filtered to {len(all_results)} relevant chunks based on Retrieval Judge")
                        
                        # Re-rank chunks if specified
                        if evaluation["re_rank_chunks"]:
                            # Create a mapping of chunk_id to result
                            result_map = {r.get("chunk_id"): r for r in all_results}
                            
                            # Re-order results based on the re-ranking
                            reranked_results = []
                            for chunk_id in evaluation["re_rank_chunks"]:
                                if chunk_id in result_map:
                                    reranked_results.append(result_map[chunk_id])
                            
                            # Add any remaining results not in the re-ranking
                            remaining = [r for r in all_results if r.get("chunk_id") not in evaluation["re_rank_chunks"]]
                            all_results = reranked_results + remaining
                            logger.info(f"Re-ranked chunks based on Retrieval Judge")
                    
                    # Format context with source information (existing code, but using all_results)
                    # ...
                
                # Rest of the existing code for context formatting
                # ...
        
        # Rest of the existing code for prompt construction and response generation
        # ...
```

## Phase 3: Advanced Semantic Chunking

Building on the success of the Chunking Judge, we can implement more advanced semantic chunking strategies:

### 3.1 Implement Semantic Chunking

```python
# app/rag/semantic_chunker.py
import logging
from typing import List, Dict, Any
from langchain.text_splitter import TextSplitter
from app.rag.ollama_client import OllamaClient
from app.models.document import Document

logger = logging.getLogger("app.rag.semantic_chunker")

class SemanticChunker(TextSplitter):
    """
    LLM-based semantic chunker that splits text based on semantic boundaries
    """
    def __init__(
        self,
        ollama_client: OllamaClient = None,
        model: str = "gemma3:12b",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        super().__init__()
        self.ollama_client = ollama_client or OllamaClient()
        self.model = model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    async def split_text(self, text: str) -> List[str]:
        """Split text into semantically meaningful chunks"""
        # For very short texts, just return as is
        if len(text) < self.chunk_size:
            return [text]
        
        # Create prompt for the LLM
        prompt = self._create_chunking_prompt(text)
        
        # Get chunking from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response to get chunks
        chunks = self._parse_chunks(response.get("response", ""), text)
        
        logger.info(f"Semantic chunker created {len(chunks)} chunks")
        
        return chunks
    
    def _create_chunking_prompt(self, text: str) -> str:
        """Create a prompt for the LLM to chunk the text"""
        # Use a sample if the text is too long
        if len(text) > 10000:
            text = text[:10000] + "...\n[Text truncated for prompt length]"
        
        return f"""You are a document chunking expert. Your task is to split the following text into semantically meaningful chunks.

Each chunk should:
1. Be a coherent, self-contained unit of information
2. Preserve the semantic meaning of the content
3. Be approximately {self.chunk_size} characters in length (but prioritize semantic coherence over exact size)
4. Not break in the middle of a sentence or logical unit

Text to chunk:
{text}

Output format:
CHUNK 1:
[Content of first chunk]
END CHUNK 1

CHUNK 2:
[Content of second chunk]
END CHUNK 2

... and so on.

Make sure each chunk is semantically meaningful and can stand on its own as much as possible.
"""
    
    def _parse_chunks(self, response_text: str, original_text: str) -> List[str]:
        """Parse the LLM response to extract chunks"""
        try:
            # Extract chunks using regex
            import re
            chunk_pattern = r"CHUNK \d+:\n(.*?)END CHUNK \d+"
            chunks = re.findall(chunk_pattern, response_text, re.DOTALL)
            
            # Clean up chunks
            chunks = [chunk.strip() for chunk in chunks]
            
            # If no chunks were found, fall back to a simple split
            if not chunks:
                logger.warning("No chunks found in LLM response, falling back to simple splitting")
                return self._fallback_split(original_text)
            
            return chunks
        except Exception as e:
            logger.error(f"Error parsing chunks: {str(e)}")
            return self._fallback_split(original_text)
    
    def _fallback_split(self, text: str) -> List[str]:
        """Fallback method to split text if LLM chunking fails"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
        return splitter.split_text(text)
```

### 3.2 Integrate Semantic Chunking with DocumentProcessor

```python
# app/rag/document_processor.py
# Update the _get_text_splitter method to support semantic chunking

from app.rag.semantic_chunker import SemanticChunker

def _get_text_splitter(self, file_ext=None):
    """Get the appropriate text splitter based on chunking strategy and file type"""
    logger.info(f"Using chunking strategy: {self.chunking_strategy} for file type: {file_ext}")
    
    # Add semantic chunking strategy
    if self.chunking_strategy == "semantic":
        logger.info(f"Using semantic chunking with chunk size {self.chunk_size}")
        return SemanticChunker(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
    
    # Rest of the existing code...
```

## Phase 4: LangGraph Integration for Agentic RAG

Building on the success of the individual judges, we can implement a more sophisticated agentic RAG system using LangGraph:

### 4.1 Install Dependencies

```bash
pip install langgraph
```

### 4.2 Create LangGraph RAG Agent

```python
# app/rag/agents/langgraph_rag_agent.py
import logging
from typing import Dict, Any, List, Optional, TypedDict, Annotated, Literal
from enum import Enum
import json

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from app.rag.ollama_client import OllamaClient
from app.rag.vector_store import VectorStore
from app.core.config import DEFAULT_MODEL

logger = logging.getLogger("app.rag.agents.langgraph_rag_agent")

# Define state types
class AgentState(TypedDict):
    query: str
    conversation_history: List[Dict[str, str]]
    retrieved_chunks: List[Dict[str, Any]]
    refined_query: Optional[str]
    relevant_chunks: List[Dict[str, Any]]
    final_context: str
    answer: Optional[str]
    sources: List[Dict[str, Any]]
    needs_more_retrieval: bool
    needs_query_refinement: bool

class RAGAgentAction(str, Enum):
    RETRIEVE = "retrieve"
    EVALUATE = "evaluate"
    REFINE_QUERY = "refine_query"
    GENERATE = "generate"
    END = "end"

class LangGraphRAGAgent:
    """
    Agentic RAG system using LangGraph
    """
    def __init__(
        self,
        ollama_client: Optional[OllamaClient] = None,
        vector_store: Optional[VectorStore] = None,
        model: str = DEFAULT_MODEL,
        top_k: int = 10
    ):
        self.ollama_client = ollama_client or OllamaClient()
        self.vector_store = vector_store or VectorStore()
        self.model = model
        self.top_k = top_k
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph for the RAG agent"""
        # Create the graph
        graph = StateGraph(AgentState)
        
        # Add nodes
        graph.add_node("retrieve", self._retrieve)
        graph.add_node("evaluate", self._evaluate_retrieval)
        graph.add_node("refine_query", self._refine_query)
        graph.add_node("generate", self._generate_response)
        
        # Add edges
        graph.add_edge("retrieve", "evaluate")
        graph.add_conditional_edges(
            "evaluate",
            self._route_after_evaluation,
            {
                RAGAgentAction.REFINE_QUERY: "refine_query",
                RAGAgentAction.GENERATE: "generate",
            }
        )
        graph.add_edge("refine_query", "retrieve")
        graph.add_edge("generate", END)
        
        # Set entry point
        graph.set_entry_point("retrieve")
        
        return graph.compile()
    
    async def _retrieve(self, state: AgentState) -> AgentState:
        """Retrieve chunks from the vector store"""
        query = state.get("refined_query") or state.get("query")
        logger.info(f"Retrieving chunks for query: {query[:100]}...")
        
        search_results = await self.vector_store.search(
            query=query,
            top_k=self.top_k
        )
        
        logger.info(f"Retrieved {len(search_results)} chunks")
        
        return {
            **state,
            "retrieved_chunks": search_results
        }
    
    async def _evaluate_retrieval(self, state: AgentState) -> AgentState:
        """Evaluate the retrieved chunks"""
        query = state.get("query")
        chunks = state.get("retrieved_chunks", [])
        
        if not chunks:
            logger.warning("No chunks retrieved to evaluate")
            return {
                **state,
                "needs_more_retrieval": False,
                "needs_query_refinement": False,
                "relevant_chunks": [],
                "final_context": "No relevant information found."
            }
        
        # Create prompt for evaluation
        prompt = f"""You are a retrieval evaluation expert. Analyze these retrieved chunks for the query: "{query}"

Retrieved Chunks:
{self._format_chunks_for_prompt(chunks)}

Evaluate:
1. Are these chunks relevant to the query?
2. Do they contain sufficient information to answer the query?
3. Is query refinement needed to get better results?

Output your evaluation in JSON format:
{{
    "needs_query_refinement": true/false,
    "needs_more_retrieval": true/false,
    "relevant_chunk_ids": [...],
    "justification": "..."
}}
"""
        
        # Get evaluation from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        evaluation = self._parse_json_response(response.get("response", ""))
        
        # Get relevant chunks
        relevant_chunk_ids = evaluation.get("relevant_chunk_ids", [])
        relevant_chunks = [c for c in chunks if c.get("chunk_id") in relevant_chunk_ids]
        
        # If no relevant chunks specified, use all chunks
        if not relevant_chunks and chunks:
            relevant_chunks = chunks
        
        # Format final context
        final_context = self._format_context(relevant_chunks)
        
        return {
            **state,
            "needs_more_retrieval": evaluation.get("needs_more_retrieval", False),
            "needs_query_refinement": evaluation.get("needs_query_refinement", False),
            "relevant_chunks": relevant_chunks,
            "final_context": final_context
        }
    
    def _route_after_evaluation(self, state: AgentState) -> RAGAgentAction:
        """Determine next action based on evaluation"""
        if state.get("needs_query_refinement", False):
            return RAGAgentAction.REFINE_QUERY
        else:
            return RAGAgentAction.GENERATE
    
    async def _refine_query(self, state: AgentState) -> AgentState:
        """Refine the query based on evaluation"""
        original_query = state.get("query", "")
        chunks = state.get("retrieved_chunks", [])
        
        # Create prompt for query refinement
        prompt = f"""You are a query refinement expert. The original query was: "{original_query}"

The retrieved chunks were:
{self._format_chunks_for_prompt(chunks)}

These chunks were not sufficient to answer the query. Please refine the query to get better results.
The refined query should be more specific and help retrieve more relevant information.

Output your refined query in JSON format:
{{
    "refined_query": "..."
}}
"""
        
        # Get refinement from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Parse the response
        refinement = self._parse_json_response(response.get("response", ""))
        refined_query = refinement.get("refined_query", original_query)
        
        logger.info(f"Refined query: '{refined_query}'")
        
        return {
            **state,
            "refined_query": refined_query
        }
    
    async def _generate_response(self, state: AgentState) -> AgentState:
        """Generate a response based on the retrieved context"""
        query = state.get("query", "")
        context = state.get("final_context", "")
        
        # Create prompt for response generation
        prompt = f"""Answer the following question based on the provided context:

Context:
{context}

Question: {query}

Provide a comprehensive answer based only on the information in the context. If the context doesn't contain enough information to answer the question, say so clearly.
"""
        
        # Get response from LLM
        response = await self.ollama_client.generate(
            prompt=prompt,
            model=self.model,
            stream=False
        )
        
        # Extract sources for citations
        sources = []
        for chunk in state.get("relevant_chunks", []):
            metadata = chunk.get("metadata", {})
            sources.append({
                "document_id": metadata.get("document_id", ""),
                "chunk_id": chunk.get("chunk_id", ""),
                "filename": metadata.get("filename", "Unknown"),
                "excerpt": chunk.get("content", "")[:200] + "..." if len(chunk.get("content", "")) > 200 else chunk.get("content", "")
            })
        
        return {
            **state,
            "answer": response.get("response", ""),
            "sources": sources
        }
    
    def _format_chunks_for_prompt(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks for inclusion in a prompt"""
        formatted_chunks = []
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            content_preview = content[:300] + "..." if len(content) > 300 else content
            
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            
            formatted_chunks.append(
                f"Chunk {i+1} (ID: {chunk.get('chunk_id', 'unknown')}):\n"
                f"Source: {filename}\n"
                f"Content: {content_preview}"
            )
        
        return "\n\n".join(formatted_chunks)
    
    def _format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks into a context for the LLM"""
        context_pieces = []
        for i, chunk in enumerate(chunks):
            content = chunk.get("content", "")
            metadata = chunk.get("metadata", {})
            filename = metadata.get("filename", "Unknown")
            
            context_pieces.append(
                f"[{i+1}] Source: {filename}\n\n{content}"
            )
        
        return "\n\n".join(context_pieces)
    
    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """Parse JSON from LLM response"""
        try:
            import re
            json_match = re.search(r'({[\s\S]*})', response_text)
            if json_match:
                json_str = json_match.group(1)
                return json.loads(json_str)
            else:
                logger.warning("Could not find JSON in response")
                return {}
        except Exception as e:
            logger.error(f"Error parsing JSON response: {str(e)}")
            return {}
    
    async def query(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Process a query through the agentic RAG system
        """
        # Initialize state
        initial_state = {
            "query": query,
            "conversation_history": conversation_history or [],
            "retrieved_chunks": [],
            "refined_query": None,
            "relevant_chunks": [],
            "final_context": "",
            "answer": None,
            "sources": [],
            "needs_more_retrieval": False,
            "needs_query_refinement": False
        }
        
        # Run the graph
        result = await self.graph.ainvoke(initial_state)
        
        # Return the result
        return {
            "query": query,
            "answer": result.get("answer", ""),
            "sources": result.get("sources", [])
        }
```

### 4.3 Integrate LangGraph Agent with API

```python
# app/api/chat.py
# Add a new endpoint for the LangGraph RAG agent

from app.rag.agents.langgraph_rag_agent import LangGraphRAGAgent

@router.post("/chat/langgraph_rag", response_model=ChatResponse)
async def langgraph_rag_chat(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    rag_agent: LangGraphRAGAgent = Depends(get_langgraph_rag_agent)
):
    """
    Chat with the LangGraph RAG agent
    """
    try:
        # Format conversation history
        formatted_history = []
        if request.conversation_history:
            for msg in request.conversation_history:
                formatted_history.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # Process query through the agent
        result = await rag_agent.query(
            query=request.query,
            conversation_history=formatted_history
        )
        
        # Format response
        response = ChatResponse(
            query=request.query,
            answer=result.get("answer", ""),
            sources=[Citation(**source) for source in result.get("sources", [])]
        )
        
        # Record analytics in the background
        background_tasks.add_task(
            record_chat_analytics,
            query=request.query,
            model=request.model,
            use_rag=True,
            document_ids=[s.document_id for s in response.sources]
        )
        
        return response
    except Exception as e:
        logger.error(f"Error in LangGraph RAG chat: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
```

## Testing and Evaluation

### Unit Tests for Chunking Judge

```python
# tests/unit/test_chunking_judge.py
import pytest
from unittest.mock import AsyncMock, patch
from app.rag.agents.chunking_judge import ChunkingJudge
from app.models.document import Document

@pytest.fixture
def mock_ollama_client():
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "strategy": "markdown",
            "parameters": {
                "chunk_size": 800,
                "chunk_overlap": 100
            },
            "justification": "This is a structured markdown document with clear headers."
        }
        """
    }
    return client

@pytest.mark.asyncio
async def test_chunking_judge_analysis(mock_ollama_client):
    # Create a test document
    doc = Document(
        id="test-id",
        filename="test.md",
        content="# Header 1\nContent\n## Header 2\nMore content"
    )
    
    # Create chunking judge with mock client
    judge = ChunkingJudge(ollama_client=mock_ollama_client)
    
    # Test analysis
    result = await judge.analyze_document(doc)
    
    # Verify result
    assert result["strategy"] == "markdown"
    assert result["parameters"]["chunk_size"] == 800
    assert result["parameters"]["chunk_overlap"] == 100
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "document analysis expert" in call_args["prompt"].lower()
    assert "test.md" in call_args["prompt"]
```

### Unit Tests for Retrieval Judge

```python
# tests/unit/test_retrieval_judge.py
import pytest
from unittest.mock import AsyncMock, patch
from app.rag.agents.retrieval_judge import RetrievalJudge

@pytest.fixture
def mock_ollama_client():
    client = AsyncMock()
    client.generate.return_value = {
        "response": """
        {
            "refined_query": "advanced machine learning techniques for time series data",
            "relevant_chunks": ["chunk1", "chunk3"],
            "re_rank_chunks": ["chunk3", "chunk1"],
            "request_more": true,
            "justification": "The query needs to be more specific about time series data."
        }
        """
    }
    return client

@pytest.fixture
def test_chunks():
    return [
        {
            "chunk_id": "chunk1",
            "content": "Machine learning is a field of study...",
            "metadata": {"filename": "ml.txt"}
        },
        {
            "chunk_id": "chunk2",
            "content": "Data visualization techniques include...",
            "metadata": {"filename": "viz.txt"}
        },
        {
            "chunk_id": "chunk3",
            "content": "Time series analysis requires specialized algorithms...",
            "metadata": {"filename": "timeseries.txt"}
        }
    ]

@pytest.mark.asyncio
async def test_retrieval_judge_evaluation(mock_ollama_client, test_chunks):
    # Create retrieval judge with mock client
    judge = RetrievalJudge(ollama_client=mock_ollama_client)
    
    # Test evaluation
    result = await judge.evaluate_retrieval(
        query="machine learning for time series",
        chunks=test_chunks
    )
    
    # Verify result
    assert "advanced machine learning techniques" in result["refined_query"]
    assert "chunk1" in result["relevant_chunks"]
    assert "chunk3" in result["relevant_chunks"]
    assert result["re_rank_chunks"][0] == "chunk3"  # Re-ranked with chunk3 first
    assert result["request_more"] is True
    
    # Verify prompt creation
    call_args = mock_ollama_client.generate.call_args[1]
    assert "retrieval evaluation expert" in call_args["prompt"].lower()
    assert "machine learning for time series" in call_args["prompt"]
```

### Performance Benchmarking

```python
# scripts/benchmark_judges.py
import asyncio
import time
import json
import argparse
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore

async def run_benchmark(queries, use_judges=True):
    """Run benchmark tests with and without judges"""
    vector_store = VectorStore()
    rag_engine = RAGEngine(vector_store=vector_store)
    
    results = []
    
    for query in queries:
        start_time = time.time()
        
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            max_retrieval_iterations=2 if use_judges else 1
        )
        
        elapsed_time = time.time() - start_time
        
        results.append({
            "query": query,
            "time_seconds": elapsed_time,
            "sources_count": len(response.get("sources", [])),
            "answer_length": len(response.get("answer", ""))
        })
    
    return results

async def main():
    parser = argparse.ArgumentParser(description="Benchmark RAG with and without judges")
    parser.add_argument("--queries_file", type=str, default="benchmark_queries.json", help="JSON file with test queries")
    parser.add_argument("--output_file", type=str, default="benchmark_results.json", help="Output file for results")
    args = parser.parse_args()
    
    # Load test queries
    with open(args.queries_file, "r") as f:
        queries = json.load(f)
    
    # Run benchmarks
    print(f"Running benchmark with {len(queries)} queries...")
    
    print("Testing with judges enabled...")
    with_judges = await run_benchmark(queries, use_judges=True)
    
    print("Testing with judges disabled...")
    without_judges = await run_benchmark(queries, use_judges=False)
    
    # Compute averages
    avg_time_with = sum(r["time_seconds"] for r in with_judges) / len(with_judges)
    avg_time_without = sum(r["time_seconds"] for r in without_judges) / len(without_judges)
    
    avg_sources_with = sum(r["sources_count"] for r in with_judges) / len(with_judges)
    avg_sources_without = sum(r["sources_count"] for r in without_judges) / len(without_judges)
    
    # Save results
    results = {
        "with_judges": with_judges,
        "without_judges": without_judges,
        "summary": {
            "avg_time_with_judges": avg_time_with,
            "avg_time_without_judges": avg_time_without,
            "time_difference_percent": (avg_time_with - avg_time_without) / avg_time_without * 100,
            "avg_sources_with_judges": avg_sources_with,
            "avg_sources_without_judges": avg_sources_without
        }
    }
    
    with open(args.output_file, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"Benchmark complete. Results saved to {args.output_file}")
    print(f"Average time with judges: {avg_time_with:.2f}s")
    print(f"Average time without judges: {avg_time_without:.2f}s")
    print(f"Time difference: {(avg_time_with - avg_time_without) / avg_time_without * 100:.2f}%")

if __name__ == "__main__":
    asyncio.run(main())
```

## Implementation Timeline

### Phase 1: Chunking Judge (Weeks 1-2)
- Week 1: Implement Chunking Judge class and integration with DocumentProcessor
- Week 2: Testing and refinement of Chunking Judge

### Phase 2: Retrieval Judge (Weeks 3-4)
- Week 3: Implement Retrieval Judge class and integration with RAGEngine
- Week 4: Testing and refinement of Retrieval Judge

### Phase 3: Advanced Semantic Chunking (Weeks 5-6)
- Week 5: Implement SemanticChunker class
- Week 6: Integration and testing of semantic chunking

### Phase 4: LangGraph Integration (Weeks 7-8)
- Week 7: Implement LangGraphRAGAgent
- Week 8: Integration with API and testing

## Potential Challenges and Mitigations

1. **Increased Latency**
   - **Challenge**: Adding LLM calls will increase response time
   - **Mitigation**: Implement caching for judge decisions, make judges optional via configuration, optimize when to use judges

2. **Cost Considerations**
   - **Challenge**: Additional LLM calls increase computational costs
   - **Mitigation**: Make judges optional, implement usage tracking, optimize when to use judges

3. **Error Handling**
   - **Challenge**: LLM responses may not always be parseable
   - **Mitigation**: Robust fallback mechanisms in the parsing functions

4. **Cold Start Problem**
   - **Challenge**: Judges may perform poorly on new document types
   - **Mitigation**: Implement a feedback loop to improve judge performance over time

## Conclusion

The proposed LLM-enhanced RAG system with "judges" for chunking and retrieval has significant potential to improve the adaptability and accuracy of the RAG pipeline. By dynamically selecting chunking strategies and refining queries, the system can better handle diverse document types and complex queries.

The phased implementation approach allows for incremental testing and refinement, with each phase building on the success of the previous one. The use of gemma3:12b as the model for both judges leverages its multimodal capabilities, with the system designed to allow changing the model in the future.

This implementation plan draws on best practices and techniques from LangChain, LangGraph, and recent research in semantic chunking and query refinement. The result will be a more intelligent and adaptable RAG system that can provide more accurate and relevant responses to user queries.

================
File: mem0_integration_plan.md
================
# Mem0 Integration Plan for Metis_RAG

## Overview
This document outlines the plan for integrating mem0, a memory layer for AI applications, into the Metis_RAG system. The integration will enhance Metis_RAG with persistent memory capabilities, enabling more personalized and context-aware responses.

## Architecture Diagram
```mermaid
graph TD
    A[Metis_RAG] --> B[mem0 Integration]
    B --> C1[User Memory]
    B --> C2[Session Memory]
    B --> C3[Document Memory]
    C1 --> D1[User Preferences]
    C1 --> D2[Conversation History]
    C2 --> D3[Current Session Context]
    C3 --> D4[Document Interactions]
    D1 --> E1[Personalized Responses]
    D2 --> E2[Contextual Continuity]
    D3 --> E3[Real-time Adaptation]
    D4 --> E4[Improved Document Relevance]
```

## Integration Steps

### 1. Memory Layer Addition
- Add mem0 as a memory layer between the RAG system and the user interface
- Implement three memory types:
  * User Memory: Store long-term user preferences and conversation history
  * Session Memory: Maintain context for the current conversation
  * Document Memory: Track user interactions with specific documents

### 2. API Integration
- Add mem0 API calls in the FastAPI backend
- Modify the chat endpoint to:
  * Retrieve relevant memories before generating responses
  * Store new memories after each interaction

### 3. Document Processing Enhancement
- Integrate mem0 with LangChain document processing
- Store document-specific memories (e.g., frequently referenced sections)

### 4. User Interface Updates
- Add memory management controls
- Show memory-related information in the chat interface

### 5. Testing and Validation
- Add tests for memory integration
- Verify memory persistence across sessions
- Ensure proper memory retrieval and usage

## Implementation Timeline
1. Week 1: Memory layer setup and API integration
2. Week 2: Document processing enhancement
3. Week 3: User interface updates
4. Week 4: Testing and validation

## Dependencies
- mem0 Python package
- Additional memory storage capacity
- Testing infrastructure for memory-related features

## Risks and Mitigation
- **Risk**: Increased latency due to memory operations
  - **Mitigation**: Implement caching and optimize memory queries
- **Risk**: Memory storage growth
  - **Mitigation**: Implement memory pruning and archiving strategies

================
File: Metis_RAG_Implementation_Steps.md
================
# Metis RAG Implementation Steps

This document outlines the implementation progress for the Metis RAG project.

## Completed Features

### UI Integration
-  Updated base template with Metis_Chat UI
-  Implemented light/dark mode support
-  Enhanced chat interface with model controls
-  Added conversation management features
-  Implemented token tracking

### Document Management
-  Integrated document management sidebar
-  Improved document upload and processing UI
-  Added document tagging and organization
-  Implemented folder hierarchy for documents
-  Added filtering by tags and folders

### RAG Engine
-  Enhanced Ollama client with retry mechanisms
-  Implemented conversation context in RAG
-  Added metadata filtering for document retrieval
-  Improved source citation display
-  Enhanced logging throughout the retrieval process
-  Improved prompt engineering for better context utilization
-  Fixed metadata handling for tags and other list attributes

### Advanced Features
-  Implemented multiple chunking strategies
  - Recursive chunking (default)
  - Token-based chunking
  - Markdown header chunking
-  Added analytics and monitoring dashboard
-  Optimized performance for large document collections
-  Enhanced mobile experience

## Advanced Chunking Strategies

The document processor now supports multiple chunking strategies:

1. **Recursive Chunking**
   - Default strategy that recursively splits text by characters
   - Configurable chunk size and overlap

2. **Token-based Chunking**
   - Splits text based on token count rather than characters
   - Better for maintaining semantic meaning

3. **Markdown Header Chunking**
   - Splits markdown documents based on headers
   - Preserves document structure

## Analytics Dashboard

The analytics system tracks system usage and performance:

1. **Query Analytics**
   - Records query details (text, model, RAG usage)
   - Tracks response times and token counts
   - Monitors document usage

2. **System Statistics**
   - Tracks vector store performance
   - Monitors cache hit ratios
   - Provides overall system health metrics

## Performance Optimizations

Several optimizations improve performance with large document collections:

1. **Vector Store Caching**
   - In-memory cache for search results
   - Configurable TTL and size limits
   - Cache statistics tracking

2. **Metadata Filtering**
   - Pre-filtering before vector similarity calculation
   - Support for complex filter criteria
   - Optimized tag and folder filtering

## Mobile Experience

The mobile interface includes touch-friendly features:

1. **Responsive Design**
   - Optimized controls for touch
   - Improved layout for small screens
   - Better readability on mobile devices

2. **Touch Gestures**
   - Pull-to-refresh for document lists
   - Swipe actions for document management
   - Optimized for touch interactions

## Testing Framework

A comprehensive testing framework has been implemented to verify RAG functionality:

1. **RAG Retrieval Testing**
   - Automated test script for document processing
   - Test cases for different query types
   - Verification of source citations and relevance

2. **Document Processing Testing**
   - Tests for various document formats (PDF, TXT, CSV, MD)
   - Verification of chunking strategies
   - Metadata extraction validation

3. **Performance Metrics**
   - Response time tracking
   - Relevance scoring
   - Success rate calculation

================
File: Metis_RAG_Improvement_Plan.md
================
# Metis RAG Improvement Plan

## Overview

This document outlines the plan to integrate the best features of Metis_Chat and Metis_RAG into a unified application, combining Metis_Chat's superior UI with Metis_RAG's document retrieval capabilities.

## Current State Analysis

### Metis_Chat Strengths
- Modern, responsive UI with light/dark mode
- Robust conversation management
- Streaming responses with token tracking

### Metis_RAG Strengths
- Document upload and processing
- Vector storage for semantic search
- Retrieval-augmented generation

## Implementation Roadmap

### Phase 1: Foundation Integration 
1.  Merge UI frameworks
2.  Integrate RAG engine
3.  Implement document management
4.  Create unified configuration

### Phase 2: Feature Enhancement 
1.  Implement RAG toggle
2.  Add document upload capabilities
3.  Enhance conversation management
4.  Implement source citation

### Phase 3: Advanced Features 
1.  Add document organization and tagging
2.  Implement advanced chunking strategies
3.  Add analytics dashboard
4.  Implement user feedback collection

### Phase 4: Optimization and Polish 
1.  Optimize performance for large collections
2.  Enhance mobile experience
3.  Add keyboard shortcuts
4.  Implement advanced visualization

## Recent Enhancements

1. **Enhanced RAG Engine**
   - Improved logging throughout the retrieval process
   - Enhanced prompt engineering for better context utilization
   - Fixed metadata handling for tags and other list attributes
   - Added comprehensive testing framework

2. **Document Processing Improvements**
   - Fixed issues with processing Markdown and PDF files
   - Improved error handling with fallback mechanisms
   - Enhanced chunking strategies for different document types

3. **Testing Framework**
   - Created test script for RAG retrieval verification
   - Implemented test cases for different query types
   - Added analysis of retrieval success rate

## Current Issues and Improvements (March 2025)

Based on recent testing, we've identified several issues that need to be addressed:

### 1. Streaming Text Formatting Issues
- [ ] Fix token-by-token streaming that causes incorrect word breaks (e.g., "St abil ization")
- [ ] Modify token processing to accumulate tokens until complete words are formed
- [ ] Update frontend JavaScript to properly handle streamed tokens
- [ ] Implement buffer that only renders complete words
- [ ] Add specific tests for streaming functionality

### 2. Multi-Document Synthesis
- [ ] Improve synthesis of information across multiple documents
- [ ] Update prompt templates in `app/rag/rag_engine.py` with better synthesis instructions
- [ ] Implement cross-document reference analysis
- [ ] Create metadata that links related concepts across documents
- [ ] Refine relevance scoring algorithm

### 3. File Handling Problems
- [ ] Fix directory permissions in `app/utils/file_utils.py`
- [ ] Implement robust path handling with absolute paths
- [ ] Add path validation and normalization
- [ ] Improve error handling for file operations
- [ ] Update test fixtures to properly set up and tear down test directories

### 4. Edge Case Handling
- [ ] Implement comprehensive input sanitization
- [ ] Add global error handlers for different exception types
- [ ] Create standardized error responses
- [ ] Expand test suite with more edge cases

### 5. Response Time Optimization
- [ ] Reduce average response time (currently ~9.8s)
- [ ] Implement caching layer for frequently accessed documents and queries
- [ ] Optimize vector store implementation
- [ ] Process multiple chunks in parallel where possible
- [ ] Review prompt design for efficiency

## Implementation Timeline

### Phase 1 (Immediate - 1 week)
- [ ] Fix streaming text formatting issues
- [ ] Address file handling problems
- [ ] Implement basic input sanitization

### Phase 2 (2-3 weeks)
- [ ] Enhance prompt engineering for better multi-document synthesis
- [ ] Implement caching for performance improvement
- [ ] Expand edge case testing

### Phase 3 (4-6 weeks)
- [ ] Implement cross-document reference analysis
- [ ] Optimize vector search algorithms
- [ ] Add parallel processing capabilities

## Future Enhancements

1. **Multi-modal RAG Support**
   - Support for image and audio content
   - Cross-modal retrieval

2. **Advanced LLM Integration**
   - Support for more LLM providers
   - Model fine-tuning capabilities

3. **Enterprise Features**
   - User authentication and access control
   - Document sharing and collaboration

4. **Integration Capabilities**
   - API for external application integration
   - Plugin system for extending functionality

5. **UI Enhancements**
   - Visual indicators for source documents in responses
   - Debug mode to show retrieval details
   - Improved relevance threshold controls

## Technical Considerations

### Performance Optimization
- Caching for frequently accessed documents
- Optimized vector search with detailed logging
- Background processing for document ingestion
- Improved metadata handling for better filtering

### Security Considerations
- Input validation
- Document access controls
- Secure API endpoints
- Proper error handling and logging

### Testing and Quality Assurance
- Comprehensive test suite for RAG functionality
- Automated testing for document processing
- Performance benchmarking
- Continuous integration

### Monitoring and Evaluation
- [ ] Implement comprehensive logging throughout the application
- [ ] Track performance metrics and error rates
- [ ] Develop monitoring dashboard for system performance
- [ ] Include metrics for response time, accuracy, and error rates
- [ ] Schedule automated test runs
- [ ] Compare results over time to track improvements

## Success Metrics

### Current Metrics
- Response time under 2 seconds for RAG queries (currently ~9.8s)
- Support for up to 10,000 documents
- Intuitive document management
- Seamless switching between modes
- 100% success rate in RAG retrieval tests
- Proper source citations in responses
- Support for all document types (PDF, TXT, CSV, MD)

### New Metrics to Track
- [ ] Zero instances of word-breaking in streaming responses
- [ ] 90%+ factual accuracy in multi-document synthesis
- [ ] Zero file handling errors in test suite
- [ ] 100% success rate in edge case tests
- [ ] Reduced average response time to under 3 seconds
- [ ] Comprehensive logging coverage across all components

================
File: Metis_RAG_Setup_Plan.md
================
# Metis RAG Setup Plan

This document outlines a detailed plan to get the Metis RAG application running with Python 3.10, which should resolve the pydantic-core wheel building issue you're encountering with Python 3.13.

## Overview

```mermaid
graph TD
    A[Install Python 3.10] --> B[Create New Virtual Environment]
    B --> C[Install Requirements]
    C --> D[Configure Environment Variables]
    D --> E[Ensure Ollama is Running]
    E --> F[Run the Application]
    F --> G[Test the Application]
```

## Detailed Steps

### 1. Install Python 3.10

First, we need to install Python 3.10 on your system:

#### For macOS (using Homebrew):
```bash
brew install python@3.10
```

#### For Linux (Ubuntu/Debian):
```bash
sudo apt update
sudo apt install python3.10 python3.10-venv python3.10-dev
```

#### For Windows:
Download the Python 3.10 installer from the official Python website and run it.

### 2. Create a New Virtual Environment with Python 3.10

Once Python 3.10 is installed, exit your current virtual environment and create a new one:

```bash
# Exit current virtual environment
deactivate

# Create a new virtual environment with Python 3.10
python3.10 -m venv venv_py310

# Activate the new virtual environment
source venv_py310/bin/activate  # On macOS/Linux
# or
# venv_py310\Scripts\activate  # On Windows
```

### 3. Install Requirements

With the Python 3.10 virtual environment activated, install the requirements:

```bash
# Install wheel and setuptools first
pip install --upgrade pip wheel setuptools

# Install the requirements
pip install -r requirements.txt
```

### 4. Configure Environment Variables

Create a `.env` file based on the `.env.example`:

```bash
cp .env.example .env
```

Edit the `.env` file if needed, particularly the Ollama settings:

```
# API Settings
API_HOST=0.0.0.0
API_PORT=8000

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=llama3
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# Document Settings
UPLOAD_DIR=./uploads
CHROMA_DB_DIR=./chroma_db
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Security Settings
CORS_ORIGINS=*
```

### 5. Ensure Ollama is Running

Make sure Ollama is running and has the required models:

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If needed, pull the required models
ollama pull llama3
ollama pull nomic-embed-text
```

### 6. Run the Application

With everything set up, run the application:

```bash
# Create necessary directories if they don't exist
mkdir -p uploads chroma_db

# Run the application
uvicorn app.main:app --reload
```

### 7. Test the Application

Once the application is running, you can access it at:
- Main interface: http://localhost:8000
- API documentation: http://localhost:8000/docs

You can also run the RAG retrieval test script to verify that the document processing and retrieval are working correctly:

```bash
# Run the RAG retrieval test script
python test_rag_retrieval.py
```

This script creates test documents, processes them, and tests the RAG retrieval with specific queries to verify that the system correctly retrieves and uses information from the documents.

## Troubleshooting Common Issues

1. **If you still encounter issues with pydantic-core**:
   Try installing it separately with:
   ```bash
   pip install pydantic-core
   ```

2. **If you encounter issues with langchain or langchain-community**:
   Try installing specific versions:
   ```bash
   pip install langchain==0.0.335 langchain-community==0.0.13
   ```

3. **If Ollama connection fails**:
   Ensure Ollama is running and accessible at http://localhost:11434

4. **If directories are not accessible**:
   Check permissions for the uploads and chroma_db directories

5. **If you encounter issues with specific dependencies**:
   You might need to install system-level dependencies. For example, on Ubuntu:
   ```bash
   sudo apt-get install build-essential python3-dev
   ```

6. **If you encounter issues with document processing**:
   Ensure you have the necessary dependencies for document processing:
   ```bash
   pip install pypdf>=3.15.1 unstructured>=0.10.16
   ```
   
7. **If you encounter issues with RAG retrieval**:
   Run the test script to diagnose issues:
   ```bash
   python test_rag_retrieval.py
   ```
   Check the logs for detailed information about the retrieval process.

## Alternative Approach: Using Docker

If you continue to face issues with the Python setup, you can alternatively use Docker:

```bash
# Create a .env file
cp .env.example .env

# Build and start the containers
docker-compose up -d

# Access the application at http://localhost:8000
```

This approach isolates all dependencies within containers and should work regardless of your local Python setup.

================
File: Metis_RAG_Technical_Demo.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metis RAG Technical Demonstration</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        h2 {
            color: #2980b9;
            border-left: 4px solid #3498db;
            padding-left: 10px;
            margin-top: 30px;
        }
        ul {
            padding-left: 20px;
        }
        li {
            margin-bottom: 8px;
        }
        .feature {
            font-weight: bold;
            color: #2c3e50;
        }
        .section {
            background-color: white;
            border-radius: 5px;
            padding: 15px 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .highlight {
            background-color: #f1f8ff;
            border-left: 4px solid #3498db;
            padding: 10px 15px;
            margin: 15px 0;
        }
        ol {
            padding-left: 20px;
        }
        ol li {
            margin-bottom: 10px;
        }
        .strategy {
            margin-bottom: 20px;
        }
        .strategy-name {
            font-weight: bold;
            color: #2980b9;
        }
        .future-feature {
            background-color: #e8f4fc;
            border-radius: 5px;
            padding: 10px 15px;
            margin-bottom: 15px;
        }
        .future-feature h3 {
            color: #2c3e50;
            margin-top: 0;
        }
    </style>
</head>
<body>
    <h1>Metis RAG Technical Demonstration</h1>
    
    <div class="section">
        <h2>What is Metis RAG?</h2>
        <p>
            Metis RAG is an advanced Retrieval-Augmented Generation platform that combines conversational AI with document-based knowledge retrieval. It enables users to chat with large language models while providing relevant context from uploaded documents, enhancing response accuracy and factual grounding.
        </p>
    </div>

    <div class="section">
        <h2>Key Features</h2>
        <ul>
            <li><span class="feature">Document Intelligence:</span> Supports multiple file formats (PDF, TXT, CSV, MD) with advanced processing capabilities</li>
            <li><span class="feature">Advanced RAG Engine:</span> Implements multiple chunking strategies (recursive, token-based, markdown header) for optimal document representation</li>
            <li><span class="feature">Vector Database Integration:</span> Uses ChromaDB with optimized caching for efficient similarity search</li>
            <li><span class="feature">LLM Integration:</span> Connects to Ollama for local LLM inference with enhanced prompt engineering</li>
            <li><span class="feature">Document Management:</span> Provides tagging, organization, and folder hierarchy for documents</li>
            <li><span class="feature">Analytics Dashboard:</span> Tracks system usage, performance metrics, and document utilization</li>
            <li><span class="feature">Responsive UI:</span> Features modern interface with light/dark mode and mobile optimization</li>
        </ul>
    </div>

    <div class="section">
        <h2>Technical Architecture</h2>
        <ul>
            <li><span class="feature">Frontend:</span> HTML/CSS/JavaScript with responsive design and streaming response handling</li>
            <li><span class="feature">Backend:</span> FastAPI (Python) with modular component design</li>
            <li><span class="feature">RAG Engine:</span> Custom implementation with document processor, vector store, and LLM integration</li>
            <li><span class="feature">Vector Database:</span> ChromaDB with performance optimizations and caching</li>
            <li><span class="feature">Testing Framework:</span> Comprehensive test suite for quality, performance, and edge cases</li>
        </ul>
    </div>

    <div class="section">
        <h2>Advanced Chunking Strategies</h2>
        
        <div class="strategy">
            <p><span class="strategy-name">1. Recursive Chunking</span> (Default)</p>
            <ul>
                <li>Recursively splits text by characters with configurable chunk size and overlap</li>
                <li>Maintains semantic coherence by respecting natural text boundaries</li>
                <li>Optimal for general-purpose documents with varied content</li>
                <li>Configurable parameters: chunk size (default: 500), chunk overlap (default: 50)</li>
            </ul>
        </div>
        
        <div class="strategy">
            <p><span class="strategy-name">2. Token-based Chunking</span></p>
            <ul>
                <li>Splits text based on token count rather than character count</li>
                <li>Preserves semantic meaning by respecting token boundaries</li>
                <li>Better suited for technical content where token-level precision matters</li>
                <li>Avoids mid-word splits that can occur with character-based chunking</li>
                <li>Optimizes for LLM context window utilization</li>
            </ul>
        </div>
        
        <div class="strategy">
            <p><span class="strategy-name">3. Markdown Header Chunking</span></p>
            <ul>
                <li>Intelligently splits markdown documents based on header structure</li>
                <li>Preserves document hierarchy and organizational structure</li>
                <li>Creates chunks that align with logical document sections</li>
                <li>Maintains header context for improved retrieval relevance</li>
                <li>Particularly effective for technical documentation and knowledge bases</li>
            </ul>
        </div>
        
        <div class="highlight">
            The chunking strategy selection is automatically determined based on document type but can be manually overridden. This multi-strategy approach significantly improves retrieval accuracy by ensuring that document chunks maintain semantic coherence and structural integrity.
        </div>
    </div>

    <div class="section">
        <h2>Basic Operation</h2>
        
        <ol>
            <li>
                <strong>Document Upload & Processing:</strong>
                <ul>
                    <li>Upload documents through the document management interface</li>
                    <li>Documents are processed through configurable chunking strategies</li>
                    <li>Text is embedded and stored in the vector database with metadata</li>
                </ul>
            </li>
            
            <li>
                <strong>Chat Interaction:</strong>
                <ul>
                    <li>User sends a query through the chat interface</li>
                    <li>System retrieves relevant document chunks based on semantic similarity</li>
                    <li>Retrieved context is combined with the query in a prompt to the LLM</li>
                    <li>Response is generated with citations to source documents</li>
                </ul>
            </li>
            
            <li>
                <strong>System Management:</strong>
                <ul>
                    <li>Monitor system performance through the analytics dashboard</li>
                    <li>Configure models and system parameters</li>
                    <li>View document usage statistics and query patterns</li>
                </ul>
            </li>
        </ol>
    </div>

    <div class="section">
        <h2>Performance & Testing</h2>
        <ul>
            <li>Comprehensive testing framework with quality, performance, and edge case tests</li>
            <li>Current response time averaging ~9.8s (optimization target: <3s)</li>
            <li>Support for up to 10,000 documents with efficient retrieval</li>
            <li>Automated test suite for RAG functionality verification</li>
            <li>Performance benchmarking for response time, throughput, and resource utilization</li>
        </ul>
    </div>

    <div class="section">
        <h2>Future Advances: Agentic RAG</h2>
        
        <div class="future-feature">
            <h3>1. Autonomous Information Gathering</h3>
            <ul>
                <li>Self-directed exploration of document corpus beyond initial retrieval</li>
                <li>Iterative search refinement based on information gaps</li>
                <li>Multi-hop reasoning across document boundaries</li>
                <li>Autonomous query decomposition and recomposition</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>2. Tool Integration</h3>
            <ul>
                <li>Integration with external tools and APIs for data enrichment</li>
                <li>Ability to execute code for data analysis within responses</li>
                <li>Database querying capabilities for structured data integration</li>
                <li>Web search integration for supplementing internal knowledge</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>3. Adaptive Retrieval</h3>
            <ul>
                <li>Dynamic adjustment of retrieval parameters based on query complexity</li>
                <li>Automatic chunking strategy selection based on document content</li>
                <li>Self-tuning relevance thresholds based on feedback</li>
                <li>Context-aware retrieval that considers conversation history</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>4. Reasoning and Synthesis</h3>
            <ul>
                <li>Enhanced multi-document synthesis with cross-reference analysis</li>
                <li>Contradiction detection and resolution across documents</li>
                <li>Temporal awareness for handling time-sensitive information</li>
                <li>Uncertainty quantification in responses</li>
            </ul>
        </div>
        
        <div class="future-feature">
            <h3>5. Feedback Loop Integration</h3>
            <ul>
                <li>Learning from user interactions to improve retrieval</li>
                <li>Automated fine-tuning of embeddings based on usage patterns</li>
                <li>Continuous improvement of prompt templates</li>
                <li>Self-evaluation of response quality</li>
            </ul>
        </div>
        
        <div class="highlight">
            These Agentic RAG capabilities will transform Metis RAG from a passive retrieval system to an active knowledge assistant that can autonomously navigate complex information landscapes, reason across documents, and continuously improve its performance.
        </div>
    </div>

    <div class="section">
        <h2>Demonstration Points</h2>
        <ol>
            <li><strong>Document Upload:</strong> Show support for multiple file types and processing options</li>
            <li><strong>RAG in Action:</strong> Demonstrate how responses incorporate document knowledge</li>
            <li><strong>Source Citations:</strong> Highlight how the system cites sources for factual claims</li>
            <li><strong>Multiple Document Synthesis:</strong> Show how the system combines information across documents</li>
            <li><strong>Analytics:</strong> Display system performance metrics and document utilization</li>
            <li><strong>Testing:</strong> Run the test script to demonstrate RAG retrieval verification</li>
        </ol>
        
        <div class="highlight">
            Metis RAG represents a significant advancement in combining local LLM capabilities with enterprise knowledge management, providing accurate, contextual responses grounded in organizational documents.
        </div>
    </div>
</body>
</html>

================
File: Metis_RAG_Technical_Demo.md
================
# Metis RAG Technical Demonstration

## What is Metis RAG?

Metis RAG is an advanced Retrieval-Augmented Generation platform that combines conversational AI with document-based knowledge retrieval. It enables users to chat with large language models while providing relevant context from uploaded documents, enhancing response accuracy and factual grounding.

## Key Features

- **Document Intelligence**: Supports multiple file formats (PDF, TXT, CSV, MD) with advanced processing capabilities
- **Advanced RAG Engine**: Implements multiple chunking strategies (recursive, token-based, markdown header) for optimal document representation
- **Vector Database Integration**: Uses ChromaDB with optimized caching for efficient similarity search
- **LLM Integration**: Connects to Ollama for local LLM inference with enhanced prompt engineering
- **Document Management**: Provides tagging, organization, and folder hierarchy for documents
- **Analytics Dashboard**: Tracks system usage, performance metrics, and document utilization
- **Responsive UI**: Features modern interface with light/dark mode and mobile optimization

## Technical Architecture

- **Frontend**: HTML/CSS/JavaScript with responsive design and streaming response handling
- **Backend**: FastAPI (Python) with modular component design
- **RAG Engine**: Custom implementation with document processor, vector store, and LLM integration
- **Vector Database**: ChromaDB with performance optimizations and caching
- **Testing Framework**: Comprehensive test suite for quality, performance, and edge cases

## Advanced Chunking Strategies

Metis RAG implements three sophisticated chunking strategies to optimize document representation for different content types:

1. **Recursive Chunking** (Default)
   - Recursively splits text by characters with configurable chunk size and overlap
   - Maintains semantic coherence by respecting natural text boundaries
   - Optimal for general-purpose documents with varied content
   - Configurable parameters: chunk size (default: 500), chunk overlap (default: 50)

2. **Token-based Chunking**
   - Splits text based on token count rather than character count
   - Preserves semantic meaning by respecting token boundaries
   - Better suited for technical content where token-level precision matters
   - Avoids mid-word splits that can occur with character-based chunking
   - Optimizes for LLM context window utilization

3. **Markdown Header Chunking**
   - Intelligently splits markdown documents based on header structure
   - Preserves document hierarchy and organizational structure
   - Creates chunks that align with logical document sections
   - Maintains header context for improved retrieval relevance
   - Particularly effective for technical documentation and knowledge bases

The chunking strategy selection is automatically determined based on document type but can be manually overridden. This multi-strategy approach significantly improves retrieval accuracy by ensuring that document chunks maintain semantic coherence and structural integrity.

## Basic Operation

1. **Document Upload & Processing**:
   - Upload documents through the document management interface
   - Documents are processed through configurable chunking strategies
   - Text is embedded and stored in the vector database with metadata

2. **Chat Interaction**:
   - User sends a query through the chat interface
   - System retrieves relevant document chunks based on semantic similarity
   - Retrieved context is combined with the query in a prompt to the LLM
   - Response is generated with citations to source documents

3. **System Management**:
   - Monitor system performance through the analytics dashboard
   - Configure models and system parameters
   - View document usage statistics and query patterns

## Performance & Testing

- Comprehensive testing framework with quality, performance, and edge case tests
- Current response time averaging ~9.8s (optimization target: <3s)
- Support for up to 10,000 documents with efficient retrieval
- Automated test suite for RAG functionality verification
- Performance benchmarking for response time, throughput, and resource utilization

## Future Advances: Agentic RAG

The roadmap for Metis RAG includes significant advancements in Agentic RAG capabilities:

1. **Autonomous Information Gathering**
   - Self-directed exploration of document corpus beyond initial retrieval
   - Iterative search refinement based on information gaps
   - Multi-hop reasoning across document boundaries
   - Autonomous query decomposition and recomposition

2. **Tool Integration**
   - Integration with external tools and APIs for data enrichment
   - Ability to execute code for data analysis within responses
   - Database querying capabilities for structured data integration
   - Web search integration for supplementing internal knowledge

3. **Adaptive Retrieval**
   - Dynamic adjustment of retrieval parameters based on query complexity
   - Automatic chunking strategy selection based on document content
   - Self-tuning relevance thresholds based on feedback
   - Context-aware retrieval that considers conversation history

4. **Reasoning and Synthesis**
   - Enhanced multi-document synthesis with cross-reference analysis
   - Contradiction detection and resolution across documents
   - Temporal awareness for handling time-sensitive information
   - Uncertainty quantification in responses

5. **Feedback Loop Integration**
   - Learning from user interactions to improve retrieval
   - Automated fine-tuning of embeddings based on usage patterns
   - Continuous improvement of prompt templates
   - Self-evaluation of response quality

These Agentic RAG capabilities will transform Metis RAG from a passive retrieval system to an active knowledge assistant that can autonomously navigate complex information landscapes, reason across documents, and continuously improve its performance.

## Demonstration Points

1. **Document Upload**: Show support for multiple file types and processing options
2. **RAG in Action**: Demonstrate how responses incorporate document knowledge
3. **Source Citations**: Highlight how the system cites sources for factual claims
4. **Multiple Document Synthesis**: Show how the system combines information across documents
5. **Analytics**: Display system performance metrics and document utilization
6. **Testing**: Run the test script to demonstrate RAG retrieval verification

Metis RAG represents a significant advancement in combining local LLM capabilities with enterprise knowledge management, providing accurate, contextual responses grounded in organizational documents.

================
File: metis_rag_test_report.html
================
<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Test Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1, h2, h3 { color: #333; }
        .section { margin-bottom: 30px; }
        .summary { display: flex; justify-content: space-between; margin-bottom: 20px; }
        .summary-box { background-color: #f5f5f5; padding: 15px; border-radius: 5px; width: 30%; text-align: center; }
        .success { color: green; }
        .failure { color: red; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        tr:nth-child(even) { background-color: #f9f9f9; }
        .details { background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-top: 10px; }
        .details pre { white-space: pre-wrap; overflow-x: auto; }
        .toggle-btn { background-color: #4CAF50; color: white; padding: 5px 10px; border: none; border-radius: 3px; cursor: pointer; }
        .toggle-btn:hover { background-color: #45a049; }
    </style>
    <script>
        function toggleDetails(id) {
            var details = document.getElementById(id);
            if (details.style.display === "none") {
                details.style.display = "block";
            } else {
                details.style.display = "none";
            }
        }
    </script>
</head>
<body>
    <h1>Metis RAG Test Report</h1>
    <p>Generated on: 2025-03-07 10:58:40</p>
    
    <div class="summary">
        <div class="summary-box">
            <h3>Test Suites</h3>
            <p class="failure">0/1 (0.0%)</p>
        </div>
        <div class="summary-box">
            <h3>Total Duration</h3>
            <p>0.18 seconds</p>
        </div>
        <div class="summary-box">
            <h3>Timestamp</h3>
            <p>2025-03-07 10:58:40</p>
        </div>
    </div>
    
    <div class="section">
        <h2>Test Suite Results</h2>
        <table>
            <tr>
                <th>Test Suite</th>
                <th>Description</th>
                <th>Status</th>
                <th>Duration</th>
                <th>Details</th>
            </tr>
            
            <tr>
                <td>RAG Quality Tests</td>
                <td>Tests for factual accuracy, relevance, and citation quality</td>
                <td class="failure">Failure</td>
                <td>0.18 seconds</td>
                <td><button class="toggle-btn" onclick="toggleDetails('details-0')">Toggle Details</button></td>
            </tr>
            <tr>
                <td colspan="5">
                    <div id="details-0" class="details" style="display: none;">
                        <h4>Output:</h4>
                        <pre>============================= test session starts ==============================
platform darwin -- Python 3.10.16, pytest-8.3.5, pluggy-1.5.0 -- /Users/charleshoward/Metis_RAG/venv_py310/bin/python
cachedir: .pytest_cache
rootdir: /Users/charleshoward/Metis_RAG
configfile: pyproject.toml
plugins: asyncio-0.25.3, anyio-3.7.1
asyncio: mode=strict, asyncio_default_fixture_loop_scope=None
collecting ... collected 0 items

============================ no tests ran in 0.00s =============================
</pre>
                        <h4>Error Output:</h4>
                        <pre>/Users/charleshoward/Metis_RAG/venv_py310/lib/python3.10/site-packages/pytest_asyncio/plugin.py:207: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
ERROR: file or directory not found: tests.test_rag_quality

</pre>
                        <h4>Reports:</h4>
                        <ul>
                            <li><a href="test_quality_results.json">test_quality_results.json</a></li><li><a href="test_multi_doc_results.json">test_multi_doc_results.json</a></li><li><a href="test_citation_results.json">test_citation_results.json</a></li>
                        </ul>
                    </div>
                </td>
            </tr>
            
        </table>
    </div>
    
    <div class="section">
        <h2>Individual Test Reports</h2>
        <p>The following reports were generated by the test suites:</p>
        <ul>
            <li><a href="test_quality_results.json">test_quality_results.json</a></li><li><a href="test_multi_doc_results.json">test_multi_doc_results.json</a></li><li><a href="test_citation_results.json">test_citation_results.json</a></li>
        </ul>
    </div>
    
    <div class="section">
        <h2>Specific Report Links</h2>
        <h3>Performance Report</h3>
        <p><a href="performance_benchmark_report.html">Performance Benchmark Report</a></p>
        
        <h3>Edge Case Report</h3>
        <p><a href="edge_case_test_report.html">Edge Case Test Report</a></p>
    </div>
</body>
</html>

================
File: metis_rag_test_report.json
================
{
  "timestamp": "2025-03-07T10:58:40.732785",
  "test_suites": [
    {
      "name": "RAG Quality Tests",
      "description": "Tests for factual accuracy, relevance, and citation quality",
      "result": {
        "name": "RAG Quality Tests",
        "success": false,
        "duration_seconds": 0.18236899375915527,
        "stdout": "============================= test session starts ==============================\nplatform darwin -- Python 3.10.16, pytest-8.3.5, pluggy-1.5.0 -- /Users/charleshoward/Metis_RAG/venv_py310/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/charleshoward/Metis_RAG\nconfigfile: pyproject.toml\nplugins: asyncio-0.25.3, anyio-3.7.1\nasyncio: mode=strict, asyncio_default_fixture_loop_scope=None\ncollecting ... collected 0 items\n\n============================ no tests ran in 0.00s =============================\n",
        "stderr": "/Users/charleshoward/Metis_RAG/venv_py310/lib/python3.10/site-packages/pytest_asyncio/plugin.py:207: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\nERROR: file or directory not found: tests.test_rag_quality\n\n",
        "return_code": 4
      },
      "reports": {
        "test_quality_results.json": [
          {
            "query": "What is the architecture of Metis RAG?",
            "answer": "Based on the provided context, I can provide a detailed overview of the architecture of Metis RAG.\n\nAccording to [1] Source: technical_documentation.md, Metis RAG follows a modular architecture with three main components:\n\n* Frontend Layer: Built with HTML, CSS, and JavaScript, this layer provides an intuitive interface for document management, chat interactions, system configuration, and analytics and monitoring.\n* API Layer: Implemented using FastAPI, this layer provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval.\n* RAG Engine: The core engine consists of three sub-components:\n\t+ Document Processing: Handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction.\n\t+ Vector Store: Responsible for storing document embeddings, efficient similarity search, and metadata filtering.\n\t+ LLM Integration: Connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization.\n\nThese components work together to provide advanced RAG capabilities, such as multi-document retrieval, enterprise security, and other features mentioned in the product specifications (Source: [2] product_specifications.csv).\n\nPlease note that this answer is based primarily on the provided context, specifically [1] technical_documentation.md. If you have any further questions or concerns, feel free to ask!",
            "expected_facts": [
              "modular architecture",
              "Frontend Layer",
              "API Layer",
              "RAG Engine",
              "HTML, CSS, and JavaScript",
              "FastAPI"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "technical_doc",
                "relevance_score": 0.6709409430541496
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4804580121776464
              },
              {
                "document_id": "business_report",
                "relevance_score": 0.2760642660368716
              }
            ]
          },
          {
            "query": "What was the revenue reported in Q1 2025?",
            "answer": "Based on the provided context, specifically [1] Source: quarterly_report.txt, I can find the answer to your question.\n\nAccording to the \"Financial Performance\" section of the report, the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year.\n\nSource: [1]\n\nTherefore, the reported revenue for Q1 2025 was $4.2 million.",
            "expected_facts": [
              "$4.2M",
              "15% increase",
              "year-over-year",
              "net profit of $1.4M"
            ],
            "facts_found": 3,
            "fact_percentage": 75.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "business_report",
                "relevance_score": 0.6630528548563427
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4826635659687333
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.418785820604604
              }
            ]
          },
          {
            "query": "What are the components of the RAG engine?",
            "answer": "Based on the provided context, I found the relevant information about the RAG engine's components.\n\nAccording to [1] Source: technical_documentation.md, Tags: technical,documentation,architecture, Folder: /test:\n\n\"The core RAG engine consists of:\n\n#### Document Processing\n\nThe document processing pipeline handles:\n- File validation and parsing\n- Text extraction\n- Chunking with configurable strategies\n- Metadata extraction\n\n#### Vector Store\n\nThe vector store is responsible for:\n- Storing document embeddings\n- Efficient similarity search\n- Metadata filtering\n\n#### LLM Integration\n\nThe LLM integration component:\n- Connects to Ollama for local LLM inference\n- Manages prompt templates\n- Handles context window optimization\"\n\nFrom this, we can conclude that the RAG engine has three main components: Document Processing, Vector Store, and LLM Integration.\n\nI did not find any additional information about the RAG engine's components in the other provided contexts ([2] Source: product_specifications.csv, Tags: product,specifications,pricing, Folder: /test) or [3] Source: quarterly_report.txt, Tags: business,report,quarterly, Folder: /test).\n\nTherefore, my answer is based primarily on the information found in [1].",
            "expected_facts": [
              "Document Processing",
              "Vector Store",
              "LLM Integration",
              "chunking",
              "embeddings",
              "Ollama"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "technical_doc",
                "relevance_score": 0.6109136250495526
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4640470886109769
              },
              {
                "document_id": "business_report",
                "relevance_score": 0.3472817564888965
              }
            ]
          },
          {
            "query": "What are the strategic initiatives for Q2?",
            "answer": "I'll do my best to provide an accurate and helpful response based on the provided context.\n\nAccording to the Quarterly Business Report [1], the strategic initiatives planned for Q2 2025 are:\n\n* International Expansion: Launch in European markets\n* Enterprise Solution: Develop and release enterprise-grade features\n* Strategic Partnerships: Form alliances with complementary service providers\n* Operational Efficiency: Implement automation to reduce operational costs\n\nThese initiatives aim to build upon the company's strong growth in key metrics, including revenue, customer acquisition, and product engagement.\n\nPlease note that I've based my response primarily on the provided context. If more information is needed or clarification is required, please let me know!",
            "expected_facts": [
              "International Expansion",
              "European markets",
              "Enterprise Solution",
              "Strategic Partnerships",
              "Operational Efficiency"
            ],
            "facts_found": 5,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "business_report",
                "relevance_score": 0.5109074115398808
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.38224892211823436
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.3782355963681827
              }
            ]
          },
          {
            "query": "What products are available and at what price points?",
            "answer": "I've reviewed the provided context and will do my best to provide an accurate answer based on the available information.\n\nAccording to the product specifications [1], Metis RAG offers several products with different price points:\n\n* MetisRAG Enterprise: $4999\n* MetisRAG Professional: $1999\n* MetisRAG Basic: $499\n* MetisRAG API: $0.10 per query (pay-per-use)\n* MetisRAG Mobile: $9.99 per month\n\nThe context does not contain any information about other products or pricing tiers beyond these five options.\n\nPlease note that this answer is based solely on the provided context and may not include all possible products or pricing information. If you have further questions, feel free to ask!",
            "expected_facts": [
              "MetisRAG Enterprise",
              "$4999",
              "MetisRAG Professional",
              "$1999",
              "MetisRAG Basic",
              "$499"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "product_specs",
                "relevance_score": 0.511074574924676
              },
              {
                "document_id": "business_report",
                "relevance_score": 0.45753814380850444
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.43169304353423765
              }
            ]
          },
          {
            "query": "What was the customer retention rate and NPS score?",
            "answer": "Based on the provided context [1], I found the customer retention rate and NPS score.\n\nAccording to the \"Customer Success\" section of the Quarterly Business Report [1], the customer retention rate remained strong at 94%. Additionally, the Net Promoter Score (NPS) improved from 42 to 48.\n\nSources: [1]",
            "expected_facts": [
              "94%",
              "Net Promoter Score",
              "improved from 42 to 48"
            ],
            "facts_found": 3,
            "fact_percentage": 100.0,
            "correct_sources": true,
            "sources": [
              {
                "document_id": "business_report",
                "relevance_score": 0.5571846440343674
              },
              {
                "document_id": "product_specs",
                "relevance_score": 0.4651842907204182
              },
              {
                "document_id": "technical_doc",
                "relevance_score": 0.40303926901945974
              }
            ]
          }
        ],
        "test_multi_doc_results.json": [
          {
            "query": "Compare the MetisRAG Enterprise product with the RAG Engine architecture",
            "answer": "I'll do my best to provide an accurate and informative response based on the provided context.\n\nTo compare the MetisRAG Enterprise product with the RAG Engine architecture, let's start by looking at the product specifications [2]. The MetisRAG Enterprise product is categorized as software, priced at $4999, and offers advanced RAG capabilities, multi-document retrieval, and enterprise security. This suggests that the product is a comprehensive solution that leverages the RAG Engine architecture.\n\nThe RAG Engine architecture [1] consists of several components, including the frontend layer, API layer, and RAG engine. The RAG engine itself includes document processing, vector store, and LLM integration. It appears that the RAG Engine architecture provides the underlying technology for the MetisRAG Enterprise product.\n\nOne key difference between the product and the architecture is the scope of features offered. The product specifications list advanced capabilities like multi-document retrieval and enterprise security, which are not explicitly mentioned in the RAG Engine architecture documentation. This suggests that the product may offer additional features beyond what's described in the architecture.\n\nHowever, it's important to note that the context doesn't provide direct information on how the MetisRAG Enterprise product specifically utilizes the RAG Engine architecture. For instance, we don't know which specific components of the architecture are used or how they contribute to the product's features and capabilities.\n\nTo better understand the relationship between the product and the architecture, I would recommend consulting additional resources or documentation that provides more detailed information on how the MetisRAG Enterprise product is built upon the RAG Engine architecture.",
            "expected_facts": [
              "MetisRAG Enterprise",
              "$4999",
              "Advanced RAG capabilities",
              "RAG Engine",
              "Document Processing",
              "Vector Store"
            ],
            "facts_found": 6,
            "fact_percentage": 100.0,
            "multi_doc_retrieval": true,
            "correct_sources": true,
            "source_doc_ids": [
              "technical_doc",
              "product_specs",
              "business_report"
            ]
          },
          {
            "query": "What is the relationship between the Q1 financial performance and the product offerings?",
            "answer": "Based on the provided context, it appears that there is a relationship between the Q1 financial performance and the product offerings.\n\nAccording to [1], the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year. This strong financial performance can be attributed to the successful launch of three major features (Advanced Analytics Dashboard, Mobile Application Redesign, and API Integration Platform) by the product team.\n\nThe user engagement metrics show a 22% increase in daily active users following these releases, which likely contributed to the revenue growth. Additionally, the marketing team executed campaigns that generated 2,500 new leads, resulting in 300 new customers with a sales conversion rate of 12%. This suggests that the company's product offerings were effective in driving customer acquisition and revenue.\n\nFurthermore, the Customer Success section indicates that customer retention rate remained strong at 94%, which also likely had a positive impact on the financial performance. The overall improvement in key metrics, including revenue, customer acquisition, and product engagement, demonstrates a direct correlation between the Q1 financial performance and the company's product offerings.\n\nIn summary, based on the context, it can be inferred that the successful launch of new features and the resulting increase in user engagement and customer acquisition contributed to the strong financial performance in Q1 2025.",
            "expected_facts": [
              "$4.2M in revenue",
              "MetisRAG Enterprise",
              "MetisRAG Professional",
              "MetisRAG Basic"
            ],
            "facts_found": 1,
            "fact_percentage": 25.0,
            "multi_doc_retrieval": true,
            "correct_sources": true,
            "source_doc_ids": [
              "technical_doc",
              "product_specs",
              "business_report"
            ]
          }
        ],
        "test_citation_results.json": [
          {
            "query": "What is the architecture of Metis RAG?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What was the revenue reported in Q1 2025?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What are the components of the RAG engine?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What are the strategic initiatives for Q2?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What products are available and at what price points?",
            "has_citation_markers": true,
            "citation_count": 2,
            "source_count": 3,
            "reasonable_citation_count": true
          },
          {
            "query": "What was the customer retention rate and NPS score?",
            "has_citation_markers": true,
            "citation_count": 1,
            "source_count": 3,
            "reasonable_citation_count": true
          }
        ]
      }
    }
  ]
}

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "metis_rag"
version = "0.1.0"
description = "Metis RAG - Retrieval Augmented Generation with Ollama"
readme = "README.md"
authors = [
    {name = "Your Name", email = "your.email@example.com"},
]
license = {text = "MIT"}
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
requires-python = ">=3.10"
dependencies = [
    "fastapi>=0.104.1",
    "uvicorn>=0.23.2",
    "python-multipart>=0.0.6",
    "httpx>=0.25.0",
    "pydantic>=2.4.2",
    "langchain>=0.0.335",
    "langchain-community>=0.0.13",
    "langgraph>=0.0.20",
    "chromadb>=0.4.18",
    "python-dotenv>=1.0.0",
    "jinja2>=3.1.2",
    "sse-starlette>=1.6.5",
    "psutil>=5.9.5",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.1.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
    "mypy>=1.0.0",
]

[tool.setuptools]
packages = ["app"]

[tool.pytest]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"
pythonpath = ["."]

[tool.black]
line-length = 88
target-version = ["py310"]
include = '\.pyi?$'

[tool.isort]
profile = "black"
line_length = 88

[tool.mypy]
python_version = "3.10"
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
strict_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_return_any = true
warn_unreachable = true

================
File: README.md
================
# Metis RAG

Metis RAG is an application that combines conversational AI with Retrieval Augmented Generation (RAG) capabilities. It allows users to chat with large language models while providing relevant context from uploaded documents.

## Features

- Chat with large language models through Ollama
- Upload and process documents (PDF, TXT, CSV, MD)
- Retrieval Augmented Generation for contextual responses
- LLM-enhanced document processing with intelligent chunking strategies
- Advanced query refinement and retrieval enhancement
- Document management with tagging and organization
- System monitoring and analytics
- Responsive UI with light/dark mode
- Enhanced logging and debugging capabilities

## LLM-Enhanced RAG System

Metis RAG includes an advanced LLM-enhanced system that improves two critical aspects of the RAG pipeline:

### Dynamic Chunking Strategy Selection

The system uses a "Chunking Judge" (an LLM agent) to analyze documents and select the most appropriate chunking strategy and parameters:

- Analyzes document structure, content type, and formatting
- Dynamically selects between recursive, token-based, or markdown chunking strategies
- Recommends optimal chunk size and overlap parameters
- Preserves semantic meaning and document structure
- Adapts to different document types without manual configuration

### Query Refinement and Retrieval Enhancement

The system uses a "Retrieval Judge" (an LLM agent) to improve retrieval quality:

- Analyzes queries and retrieved chunks
- Refines queries to improve retrieval precision
- Evaluates relevance of retrieved chunks
- Re-ranks chunks based on relevance to the query
- Requests additional retrieval when necessary

These enhancements make the RAG system more adaptable to different document types and query styles, improving the accuracy and relevance of responses.
## Architecture

Metis RAG is built with the following technologies:

- **Backend**: FastAPI, Python
- **Vector Database**: ChromaDB with optimized caching
- **LLM Integration**: Ollama API with enhanced prompt engineering
- **Document Processing**: LangChain with LLM-enhanced dynamic chunking strategies
- **Deployment**: Docker
- **Testing**: Comprehensive test suite for RAG functionality
- **Deployment**: Docker

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Ollama (optional, can run in Docker)

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/metis-rag.git
   cd metis-rag
   ```

2. Create a `.env` file based on `.env.example`:
   ```bash
   cp .env.example .env
   ```

   Key configuration options include:
   - `DEFAULT_MODEL`: The LLM model to use for RAG responses (default: gemma3:12b)
   - `CHUNKING_JUDGE_MODEL`: The model to use for document analysis (default: gemma3:12b)
   - `USE_CHUNKING_JUDGE`: Enable/disable the Chunking Judge (default: True)
   - `USE_RETRIEVAL_JUDGE`: Enable/disable the Retrieval Judge (default: True)

3. Build and start the application with Docker Compose:
   ```bash
   docker-compose up -d
   ```

4. Access the application at [http://localhost:8000](http://localhost:8000)

### Running Without Docker

1. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the application:
   ```bash
   uvicorn app.main:app --reload
   ```

## Usage

### Chat Interface

The main chat interface allows you to:
- Send messages to the model
- Toggle RAG functionality
- View citations from source documents
- Control conversation history

### Document Management

The document management page allows you to:
- Upload documents
- Process documents for RAG
- View document information
- Delete documents

### System Management

The system page allows you to:
- View system statistics
- Manage models
- Check system health

## Development

### Project Structure

```
metis_rag/
 app/
    api/         # API endpoints
    core/        # Core configuration
    rag/         # RAG engine
       agents/  # LLM-based agents for RAG enhancement
    models/      # Data models
    static/      # Static files
    templates/   # HTML templates
    utils/       # Utility functions
 tests/           # Tests
    unit/        # Unit tests
    integration/ # Integration tests
 uploads/         # Uploaded documents
 chroma_db/       # ChromaDB data
 docker-compose.yml
```

### Running Tests

Run tests with pytest:

```bash
pytest
```

For testing the RAG retrieval functionality specifically:

```bash
python test_rag_retrieval.py
```

This test script creates test documents, processes them, and tests the RAG retrieval with specific queries to verify that the system correctly retrieves and uses information from the documents.

## License

[MIT License](LICENSE)

## Acknowledgements

This project builds upon:
- Metis_Chat
- rag_ollama_mvp

================
File: reprocess_documents.py
================
#!/usr/bin/env python3
"""
Script to reprocess all documents in the Metis RAG system with updated chunking settings.
This is useful after changing chunking parameters to ensure all documents use the new settings.
"""

import os
import asyncio
import logging
from pathlib import Path

from app.models.document import Document
from app.rag.document_processor import DocumentProcessor
from app.rag.vector_store import VectorStore
from app.core.config import UPLOAD_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("reprocess_documents")

async def reprocess_documents():
    """Reprocess all documents with updated chunking settings"""
    logger.info("Starting document reprocessing")
    
    # Initialize document processor with updated settings
    processor = DocumentProcessor(
        chunking_strategy="recursive"  # This will use the updated chunk size for text files
    )
    
    # Initialize vector store
    vector_store = VectorStore()
    
    # Get all document directories in the uploads folder
    upload_path = Path(UPLOAD_DIR)
    if not upload_path.exists():
        logger.error(f"Uploads directory {UPLOAD_DIR} does not exist")
        return
    
    document_dirs = [d for d in upload_path.iterdir() if d.is_dir()]
    logger.info(f"Found {len(document_dirs)} document directories")
    
    # Process each document
    for doc_dir in document_dirs:
        try:
            # Get the document ID from the directory name
            document_id = doc_dir.name
            
            # Find the document file(s) in the directory
            files = list(doc_dir.iterdir())
            if not files:
                logger.warning(f"No files found in document directory {document_id}")
                continue
            
            # Assume the first file is the document (in a real system, you might need to be more sophisticated)
            document_file = files[0]
            
            logger.info(f"Reprocessing document: {document_file.name} (ID: {document_id})")
            
            # Read the file content
            try:
                with open(document_file, 'rb') as f:
                    content = f.read().decode('utf-8', errors='ignore')
            except Exception as e:
                logger.error(f"Error reading file {document_file}: {str(e)}")
                continue
                
            # Create a Document object
            document = Document(
                id=document_id,
                filename=document_file.name,
                content=content,  # Add the content field
                size=document_file.stat().st_size,
                folder="/",  # Default folder
                tags=[]  # No tags by default
            )
            
            # Delete existing document from vector store
            try:
                vector_store.delete_document(document_id)
                logger.info(f"Deleted existing document {document_id} from vector store")
            except Exception as e:
                logger.warning(f"Error deleting document {document_id} from vector store: {str(e)}")
            
            # Process the document with new chunking settings
            processed_document = await processor.process_document(document)
            
            # Add the document to the vector store
            await vector_store.add_document(processed_document)
            
            logger.info(f"Successfully reprocessed document {document_id} with {len(processed_document.chunks)} chunks")
        
        except Exception as e:
            logger.error(f"Error reprocessing document in directory {doc_dir}: {str(e)}")
    
    logger.info("Document reprocessing complete")

if __name__ == "__main__":
    asyncio.run(reprocess_documents())

================
File: requirements.txt
================
# Core dependencies
fastapi>=0.95.1,<0.97.0
uvicorn>=0.22.0,<0.23.0
python-multipart>=0.0.6
httpx>=0.24.0,<0.25.0
# Pydantic v1 (no Rust required)
pydantic>=1.10.8,<2.0.0
# LangChain and LangGraph
langchain>=0.0.235,<0.0.300
langgraph>=0.0.20
# ChromaDB and dependencies
chromadb>=0.4.13,<0.4.15
# Document processing dependencies
pypdf>=3.15.1
unstructured>=0.10.16
# Other dependencies
python-dotenv>=1.0.0
jinja2>=3.1.2
sse-starlette>=1.6.5
psutil>=5.9.5

================
File: run_tests.py
================
#!/usr/bin/env python3
"""
Test runner for Metis RAG testing strategy.
This script executes all test suites and generates a comprehensive report.
"""

import os
import sys
import json
import logging
import subprocess
import time
import argparse
from datetime import datetime
import webbrowser
import shutil

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_runner")

# Test suites
TEST_SUITES = [
    {
        "name": "RAG Quality Tests",
        "module": "tests.test_rag_quality",
        "description": "Tests for factual accuracy, relevance, and citation quality",
        "report_files": [
            "test_quality_results.json",
            "test_multi_doc_results.json",
            "test_citation_results.json",
            "test_api_integration_results.json"
        ]
    },
    {
        "name": "File Handling Tests",
        "module": "tests.test_file_handling",
        "description": "Tests for different file types, multiple file uploads, and large files",
        "report_files": [
            "test_file_type_results.json",
            "test_chunking_strategy_results.json",
            "test_large_file_results.json",
            "test_api_upload_results.json",
            "test_multiple_upload_results.json"
        ]
    },
    {
        "name": "Performance Tests",
        "module": "tests.test_performance",
        "description": "Tests for response time, throughput, and resource utilization",
        "report_files": [
            "test_response_time_results.json",
            "test_throughput_results.json",
            "test_resource_utilization_results.json",
            "test_api_performance_results.json",
            "performance_benchmark_report.json",
            "performance_benchmark_report.html"
        ]
    },
    {
        "name": "Edge Case Tests",
        "module": "tests.test_edge_cases",
        "description": "Tests for unusual inputs, error handling, and system resilience",
        "report_files": [
            "test_special_queries_results.json",
            "test_concurrent_processing_results.json",
            "test_invalid_files_results.json",
            "test_malformed_requests_results.json",
            "test_vector_store_resilience_results.json",
            "edge_case_test_report.json",
            "edge_case_test_report.html"
        ]
    }
]

def run_test_suite(suite, args):
    """Run a test suite using pytest"""
    logger.info(f"Running {suite['name']}...")
    
    # Build pytest command
    cmd = [sys.executable, "-m", "pytest", "-xvs"]
    
    # Add specific test options
    if args.failfast:
        cmd.append("-xvs")
    
    # Add the module to test
    cmd.append(suite["module"])
    
    # Run the command
    start_time = time.time()
    result = subprocess.run(cmd, capture_output=True, text=True)
    end_time = time.time()
    
    # Log the result
    if result.returncode == 0:
        logger.info(f"{suite['name']} completed successfully in {end_time - start_time:.2f} seconds")
    else:
        logger.error(f"{suite['name']} failed with return code {result.returncode}")
        logger.error(f"STDOUT: {result.stdout}")
        logger.error(f"STDERR: {result.stderr}")
    
    return {
        "name": suite["name"],
        "success": result.returncode == 0,
        "duration_seconds": end_time - start_time,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "return_code": result.returncode
    }

def collect_reports(suite):
    """Collect report files for a test suite"""
    reports = {}
    
    for report_file in suite["report_files"]:
        if os.path.exists(report_file):
            try:
                with open(report_file, "r") as f:
                    reports[report_file] = json.load(f)
            except json.JSONDecodeError:
                logger.warning(f"Could not parse {report_file} as JSON")
                with open(report_file, "r") as f:
                    reports[report_file] = f.read()
        else:
            logger.warning(f"Report file {report_file} not found")
    
    return reports

def generate_master_report(results, args):
    """Generate a master report from all test results"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "test_suites": results
    }
    
    # Save report
    report_path = "metis_rag_test_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Master report saved to {os.path.abspath(report_path)}")
    
    # Generate HTML report
    html_report = generate_html_report(report)
    html_report_path = "metis_rag_test_report.html"
    
    with open(html_report_path, "w") as f:
        f.write(html_report)
    
    logger.info(f"HTML report saved to {os.path.abspath(html_report_path)}")
    
    # Open the report in a browser if requested
    if args.open_report:
        webbrowser.open(f"file://{os.path.abspath(html_report_path)}")
    
    return report_path, html_report_path

def generate_html_report(report):
    """Generate an HTML report from the test results"""
    # Count successful test suites
    success_count = sum(1 for suite in report["test_suites"] if suite["result"]["success"])
    total_count = len(report["test_suites"])
    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
    
    # Calculate total duration
    total_duration = sum(suite["result"]["duration_seconds"] for suite in report["test_suites"])
    
    # Generate HTML
    html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Metis RAG Test Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        .section {{ margin-bottom: 30px; }}
        .summary {{ display: flex; justify-content: space-between; margin-bottom: 20px; }}
        .summary-box {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; width: 30%; text-align: center; }}
        .success {{ color: green; }}
        .failure {{ color: red; }}
        table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .details {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-top: 10px; }}
        .details pre {{ white-space: pre-wrap; overflow-x: auto; }}
        .toggle-btn {{ background-color: #4CAF50; color: white; padding: 5px 10px; border: none; border-radius: 3px; cursor: pointer; }}
        .toggle-btn:hover {{ background-color: #45a049; }}
    </style>
    <script>
        function toggleDetails(id) {{
            var details = document.getElementById(id);
            if (details.style.display === "none") {{
                details.style.display = "block";
            }} else {{
                details.style.display = "none";
            }}
        }}
    </script>
</head>
<body>
    <h1>Metis RAG Test Report</h1>
    <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="summary">
        <div class="summary-box">
            <h3>Test Suites</h3>
            <p class="{('success' if success_rate == 100 else 'failure')}">{success_count}/{total_count} ({success_rate:.1f}%)</p>
        </div>
        <div class="summary-box">
            <h3>Total Duration</h3>
            <p>{total_duration:.2f} seconds</p>
        </div>
        <div class="summary-box">
            <h3>Timestamp</h3>
            <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
    
    <div class="section">
        <h2>Test Suite Results</h2>
        <table>
            <tr>
                <th>Test Suite</th>
                <th>Description</th>
                <th>Status</th>
                <th>Duration</th>
                <th>Details</th>
            </tr>
            {"".join([f'''
            <tr>
                <td>{suite["name"]}</td>
                <td>{suite["description"]}</td>
                <td class="{'success' if suite['result']['success'] else 'failure'}">{('Success' if suite['result']['success'] else 'Failure')}</td>
                <td>{suite["result"]["duration_seconds"]:.2f} seconds</td>
                <td><button class="toggle-btn" onclick="toggleDetails('details-{i}')">Toggle Details</button></td>
            </tr>
            <tr>
                <td colspan="5">
                    <div id="details-{i}" class="details" style="display: none;">
                        <h4>Output:</h4>
                        <pre>{suite["result"]["stdout"]}</pre>
                        <h4>Error Output:</h4>
                        <pre>{suite["result"]["stderr"]}</pre>
                        <h4>Reports:</h4>
                        <ul>
                            {"".join([f'<li><a href="{report_file}">{report_file}</a></li>' for report_file in suite["reports"].keys()])}
                        </ul>
                    </div>
                </td>
            </tr>
            ''' for i, suite in enumerate(report["test_suites"])])}
        </table>
    </div>
    
    <div class="section">
        <h2>Individual Test Reports</h2>
        <p>The following reports were generated by the test suites:</p>
        <ul>
            {"".join([f'<li><a href="{report_file}">{report_file}</a></li>' for suite in report["test_suites"] for report_file in suite["reports"].keys()])}
        </ul>
    </div>
    
    <div class="section">
        <h2>Specific Report Links</h2>
        <h3>Performance Report</h3>
        <p><a href="performance_benchmark_report.html">Performance Benchmark Report</a></p>
        
        <h3>Edge Case Report</h3>
        <p><a href="edge_case_test_report.html">Edge Case Test Report</a></p>
    </div>
</body>
</html>
"""
    
    return html

def create_reports_directory():
    """Create a directory for test reports"""
    reports_dir = "test_reports"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    reports_dir = f"{reports_dir}_{timestamp}"
    
    os.makedirs(reports_dir, exist_ok=True)
    return reports_dir

def copy_reports_to_directory(reports_dir):
    """Copy all report files to the reports directory"""
    report_files = []
    for suite in TEST_SUITES:
        report_files.extend(suite["report_files"])
    
    report_files.extend(["metis_rag_test_report.json", "metis_rag_test_report.html"])
    
    for file in report_files:
        if os.path.exists(file):
            shutil.copy(file, os.path.join(reports_dir, file))
            logger.info(f"Copied {file} to {reports_dir}")

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Run Metis RAG tests")
    parser.add_argument("--suite", type=str, help="Run a specific test suite (rag_quality, file_handling, performance, edge_cases)")
    parser.add_argument("--failfast", action="store_true", help="Stop on first failure")
    parser.add_argument("--open-report", action="store_true", help="Open the HTML report in a browser")
    parser.add_argument("--save-reports", action="store_true", help="Save reports to a timestamped directory")
    return parser.parse_args()

def main():
    """Main function"""
    args = parse_args()
    
    # Run tests
    results = []
    
    # Filter test suites if a specific one is requested
    suites_to_run = TEST_SUITES
    if args.suite:
        suite_map = {
            "rag_quality": "RAG Quality Tests",
            "file_handling": "File Handling Tests",
            "performance": "Performance Tests",
            "edge_cases": "Edge Case Tests"
        }
        if args.suite in suite_map:
            suite_name = suite_map[args.suite]
            suites_to_run = [suite for suite in TEST_SUITES if suite["name"] == suite_name]
            if not suites_to_run:
                logger.error(f"Test suite {args.suite} not found")
                return 1
        else:
            logger.error(f"Unknown test suite: {args.suite}")
            logger.info(f"Available suites: {', '.join(suite_map.keys())}")
            return 1
    
    # Run each test suite
    for suite in suites_to_run:
        # Run the test suite
        result = run_test_suite(suite, args)
        
        # Collect reports
        reports = collect_reports(suite)
        
        # Store results
        results.append({
            "name": suite["name"],
            "description": suite["description"],
            "result": result,
            "reports": reports
        })
        
        # Stop on first failure if requested
        if args.failfast and not result["success"]:
            logger.error(f"Stopping due to failure in {suite['name']}")
            break
    
    # Generate master report
    report_path, html_report_path = generate_master_report(results, args)
    
    # Save reports to a directory if requested
    if args.save_reports:
        reports_dir = create_reports_directory()
        copy_reports_to_directory(reports_dir)
        logger.info(f"Reports saved to {reports_dir}")
    
    # Return success if all test suites passed
    return 0 if all(suite["result"]["success"] for suite in results) else 1

if __name__ == "__main__":
    sys.exit(main())

================
File: technical_documentation.md
================
# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies:
  - Recursive: Splits text recursively by characters, good for general text
  - Token: Splits text by token count rather than character count, useful for LLMs with token limits
  - Markdown: Splits markdown documents by headers, good for structured documents
  - Semantic: Uses LLM to identify natural semantic boundaries, best for preserving meaning
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization

#### Chunking Judge

The Chunking Judge is an LLM-based agent that enhances the document processing pipeline by:
- Analyzing document structure, content type, and formatting
- Selecting the most appropriate chunking strategy (recursive, markdown, or semantic)
- Recommending optimal chunk size and overlap parameters
- Providing detailed justification for its recommendations

Our testing shows the Chunking Judge effectively:
- Recognizes document structures, even identifying markdown-like elements in plain text
- Selects appropriate strategies that align with document structure
- Optimizes parameters based on document characteristics
- Adapts to different document types without manual configuration

#### Semantic Chunker

The Semantic Chunker is an advanced chunking strategy that:
- Uses LLM to identify natural semantic boundaries in text
- Preserves semantic meaning and context in chunks
- Creates more coherent, self-contained chunks than traditional methods
- Respects the logical flow of information in documents

Key features include:
- Intelligent boundary detection based on topic transitions and subject matter shifts
- Handling of long documents by processing in sections
- Caching for performance optimization
- Fallback mechanisms for error handling

#### Retrieval Judge

The Retrieval Judge is an LLM-based agent that enhances the RAG retrieval process by:
- Analyzing queries to determine optimal retrieval parameters
- Evaluating retrieved chunks for relevance
- Refining queries when needed to improve retrieval precision
- Optimizing context assembly for better response generation

Our testing shows the Retrieval Judge significantly improves retrieval quality:
- 89.26% faster retrieval times through effective caching
- Transforms ambiguous queries into specific, detailed requests
- Reduces context size by 76.4% on average while maintaining relevance
- Performs best with domain-specific and complex queries

## Deployment Options

Metis RAG can be deployed in several ways:

1. **Local Development**
   - Run directly with Python and uvicorn
   - Suitable for development and testing

2. **Docker Deployment**
   - Use the provided Dockerfile and docker-compose.yml
   - Containerized deployment for easier management

3. **Production Deployment**
   - Use a reverse proxy like Nginx
   - Configure proper authentication
   - Set up monitoring and logging

## Configuration

Metis RAG is configured through environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| OLLAMA_BASE_URL | URL for Ollama API | http://localhost:11434 |
| DEFAULT_MODEL | Default LLM model | llama3 |
| DEFAULT_EMBEDDING_MODEL | Model for embeddings | nomic-embed-text |
| UPLOAD_DIR | Directory for uploaded files | ./uploads |
| CHROMA_DB_DIR | Directory for vector DB | ./chroma_db |
| CHUNK_SIZE | Default chunk size | 500 |
| CHUNK_OVERLAP | Default chunk overlap | 50 |

## API Reference

### Document API

- `POST /api/documents/upload` - Upload a document
- `GET /api/documents/list` - List all documents
- `GET /api/documents/{document_id}` - Get document details
- `DELETE /api/documents/{document_id}` - Delete a document
- `POST /api/documents/process` - Process documents

### Chat API

- `POST /api/chat/message` - Send a chat message
- `GET /api/chat/history` - Get chat history
- `DELETE /api/chat/history` - Clear chat history

## Performance Considerations

For optimal performance:
- Use appropriate chunking strategies for different document types:
  - Semantic chunking for complex documents where preserving meaning is critical
  - Markdown chunking for structured documents with clear headers
  - Recursive chunking for general text with natural separators
- Configure chunk size based on your specific use case
- Consider hardware requirements for embedding generation
- Monitor vector store size and performance
- Enable the Chunking Judge to automatically select optimal chunking strategies
- Enable the Retrieval Judge for complex queries requiring precision
- Leverage caching for improved performance (33.33% vector store cache hit rate)
- Consider using a smaller model for the Judges in latency-sensitive applications

### Retrieval Judge Performance

Our testing shows that the Retrieval Judge significantly improves retrieval performance:
- 89.26% faster retrieval times compared to standard retrieval (18.41s vs 171.47s on average)
- Effective caching of both vector store queries and LLM responses
- Query analysis takes 9.03s on average
- Query refinement is very fast (2.08s on average)
- Context optimization reduces context size by 76.4% on average

### Semantic Chunking Performance

The Semantic Chunker provides several advantages over traditional chunking methods:
- Creates more coherent, self-contained chunks that maintain semantic integrity
- Reduces the need for large chunk overlaps since boundaries are semantically meaningful
- Improves retrieval precision by ensuring chunks contain complete concepts
- Works particularly well with complex, technical, or narrative content
- Caching mechanism minimizes the performance impact of LLM-based chunking

================
File: test_citation_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What was the revenue reported in Q1 2025?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What are the components of the RAG engine?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What are the strategic initiatives for Q2?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What products are available and at what price points?",
    "has_citation_markers": true,
    "citation_count": 2,
    "source_count": 3,
    "reasonable_citation_count": true
  },
  {
    "query": "What was the customer retention rate and NPS score?",
    "has_citation_markers": true,
    "citation_count": 1,
    "source_count": 3,
    "reasonable_citation_count": true
  }
]

================
File: test_data.csv
================
id,name,department,role,years_experience,skills
1,John Smith,Engineering,Software Engineer,5,"Python, JavaScript, Docker"
2,Emily Johnson,Data Science,Data Scientist,3,"Python, R, Machine Learning, SQL"
3,Michael Brown,Engineering,DevOps Engineer,7,"Kubernetes, AWS, Terraform, Linux"
4,Sarah Davis,Product,Product Manager,4,"Agile, User Research, Roadmapping"
5,David Wilson,Engineering,Frontend Developer,2,"React, TypeScript, CSS"
6,Jennifer Miller,Data Science,ML Engineer,6,"TensorFlow, PyTorch, Python, MLOps"
7,Robert Taylor,Engineering,Backend Developer,8,"Java, Spring, Microservices, Kafka"
8,Lisa Anderson,Product,UX Designer,5,"Figma, User Testing, Wireframing"
9,James Thomas,Engineering,Security Engineer,4,"Penetration Testing, OWASP, Cryptography"
10,Patricia Martinez,Data Science,Data Analyst,2,"SQL, Tableau, Excel, Python"
11,Christopher Harris,Engineering,QA Engineer,3,"Selenium, Jest, Cypress, Test Planning"
12,Elizabeth Clark,Product,Technical Writer,6,"Documentation, API Reference, Tutorials"
13,Daniel Lewis,Engineering,Mobile Developer,4,"Swift, Kotlin, React Native"
14,Jessica Lee,Data Science,Data Engineer,5,"Spark, Hadoop, Airflow, SQL"
15,Matthew Walker,Engineering,Site Reliability Engineer,7,"Prometheus, Grafana, AWS, GCP"

================
File: test_document.txt
================
# Metis RAG Test Document

This is a sample text file to test the document processing capabilities of Metis RAG.

## Key Features of RAG Systems

Retrieval-Augmented Generation (RAG) combines the power of large language models with the ability to retrieve information from external knowledge sources.

Key benefits include:
- Reduced hallucinations
- Access to domain-specific knowledge
- Up-to-date information
- Improved factual accuracy

## Test Sections

This document contains multiple sections to test the chunking capabilities of the system.

### Section 1: Technical Overview

RAG systems work by first retrieving relevant documents from a knowledge base, then providing these documents as context to a language model to generate a response.

The retrieval component typically uses vector embeddings to find semantically similar content.

### Section 2: Implementation Details

A typical RAG implementation includes:
1. Document processing pipeline
2. Vector database for storing embeddings
3. Retrieval mechanism
4. Language model integration
5. Response generation

### Section 3: Evaluation Metrics

Common evaluation metrics for RAG systems include:
- Relevance of retrieved documents
- Factual accuracy of generated responses
- Response coherence and fluency
- Latency and throughput

================
File: test_models.html
================
<!DOCTYPE html>
<html>
<head>
    <title>Test Model Selection</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
        }
        .form-group {
            margin-bottom: 20px;
        }
        label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        select {
            width: 100%;
            padding: 8px;
            border: 1px solid #ccc;
            border-radius: 4px;
        }
        button {
            padding: 8px 16px;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        pre {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Test Model Selection</h1>
        
        <div class="form-group">
            <label for="model">Select Model:</label>
            <select id="model" name="model">
                <option value="llama3" selected>Llama 3</option>
                <!-- Other models will be loaded dynamically -->
            </select>
        </div>
        
        <button id="load-models">Load Models</button>
        <button id="check-models">Check API Response</button>
        
        <h2>API Response:</h2>
        <pre id="api-response">Click "Check API Response" to see the raw API response</pre>
        
        <h2>Selected Model:</h2>
        <pre id="selected-model">No model selected</pre>
        
        <script>
            document.addEventListener('DOMContentLoaded', function() {
                const modelSelect = document.getElementById('model');
                const loadModelsBtn = document.getElementById('load-models');
                const checkModelsBtn = document.getElementById('check-models');
                const apiResponseEl = document.getElementById('api-response');
                const selectedModelEl = document.getElementById('selected-model');
                
                // Load models when the page loads
                loadModels();
                
                // Load models when the button is clicked
                loadModelsBtn.addEventListener('click', loadModels);
                
                // Check API response
                checkModelsBtn.addEventListener('click', checkApiResponse);
                
                // Update selected model when changed
                modelSelect.addEventListener('change', function() {
                    selectedModelEl.textContent = `Selected model: ${modelSelect.value}`;
                });
                
                function loadModels() {
                    fetch('/api/system/models')
                        .then(response => response.json())
                        .then(models => {
                            console.log('Models:', models);
                            
                            // Clear the dropdown
                            modelSelect.innerHTML = '';
                            
                            // Add each model to the dropdown
                            models.forEach(model => {
                                const option = document.createElement('option');
                                option.value = model.name;
                                option.textContent = model.name;
                                modelSelect.appendChild(option);
                                console.log('Added model:', model.name);
                            });
                            
                            // Update selected model
                            selectedModelEl.textContent = `Selected model: ${modelSelect.value}`;
                        })
                        .catch(error => {
                            console.error('Error loading models:', error);
                            apiResponseEl.textContent = `Error: ${error.message}`;
                        });
                }
                
                function checkApiResponse() {
                    fetch('/api/system/models')
                        .then(response => response.json())
                        .then(data => {
                            apiResponseEl.textContent = JSON.stringify(data, null, 2);
                        })
                        .catch(error => {
                            apiResponseEl.textContent = `Error: ${error.message}`;
                        });
                }
            });
        </script>
    </div>
</body>
</html>

================
File: test_multi_doc_results.json
================
[
  {
    "query": "Compare the MetisRAG Enterprise product with the RAG Engine architecture",
    "answer": "I'll do my best to provide an accurate and informative response based on the provided context.\n\nTo compare the MetisRAG Enterprise product with the RAG Engine architecture, let's start by looking at the product specifications [2]. The MetisRAG Enterprise product is categorized as software, priced at $4999, and offers advanced RAG capabilities, multi-document retrieval, and enterprise security. This suggests that the product is a comprehensive solution that leverages the RAG Engine architecture.\n\nThe RAG Engine architecture [1] consists of several components, including the frontend layer, API layer, and RAG engine. The RAG engine itself includes document processing, vector store, and LLM integration. It appears that the RAG Engine architecture provides the underlying technology for the MetisRAG Enterprise product.\n\nOne key difference between the product and the architecture is the scope of features offered. The product specifications list advanced capabilities like multi-document retrieval and enterprise security, which are not explicitly mentioned in the RAG Engine architecture documentation. This suggests that the product may offer additional features beyond what's described in the architecture.\n\nHowever, it's important to note that the context doesn't provide direct information on how the MetisRAG Enterprise product specifically utilizes the RAG Engine architecture. For instance, we don't know which specific components of the architecture are used or how they contribute to the product's features and capabilities.\n\nTo better understand the relationship between the product and the architecture, I would recommend consulting additional resources or documentation that provides more detailed information on how the MetisRAG Enterprise product is built upon the RAG Engine architecture.",
    "expected_facts": [
      "MetisRAG Enterprise",
      "$4999",
      "Advanced RAG capabilities",
      "RAG Engine",
      "Document Processing",
      "Vector Store"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "multi_doc_retrieval": true,
    "correct_sources": true,
    "source_doc_ids": [
      "technical_doc",
      "product_specs",
      "business_report"
    ]
  },
  {
    "query": "What is the relationship between the Q1 financial performance and the product offerings?",
    "answer": "Based on the provided context, it appears that there is a relationship between the Q1 financial performance and the product offerings.\n\nAccording to [1], the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year. This strong financial performance can be attributed to the successful launch of three major features (Advanced Analytics Dashboard, Mobile Application Redesign, and API Integration Platform) by the product team.\n\nThe user engagement metrics show a 22% increase in daily active users following these releases, which likely contributed to the revenue growth. Additionally, the marketing team executed campaigns that generated 2,500 new leads, resulting in 300 new customers with a sales conversion rate of 12%. This suggests that the company's product offerings were effective in driving customer acquisition and revenue.\n\nFurthermore, the Customer Success section indicates that customer retention rate remained strong at 94%, which also likely had a positive impact on the financial performance. The overall improvement in key metrics, including revenue, customer acquisition, and product engagement, demonstrates a direct correlation between the Q1 financial performance and the company's product offerings.\n\nIn summary, based on the context, it can be inferred that the successful launch of new features and the resulting increase in user engagement and customer acquisition contributed to the strong financial performance in Q1 2025.",
    "expected_facts": [
      "$4.2M in revenue",
      "MetisRAG Enterprise",
      "MetisRAG Professional",
      "MetisRAG Basic"
    ],
    "facts_found": 1,
    "fact_percentage": 25.0,
    "multi_doc_retrieval": true,
    "correct_sources": true,
    "source_doc_ids": [
      "technical_doc",
      "product_specs",
      "business_report"
    ]
  }
]

================
File: test_quality_results.json
================
[
  {
    "query": "What is the architecture of Metis RAG?",
    "answer": "Based on the provided context, I can provide a detailed overview of the architecture of Metis RAG.\n\nAccording to [1] Source: technical_documentation.md, Metis RAG follows a modular architecture with three main components:\n\n* Frontend Layer: Built with HTML, CSS, and JavaScript, this layer provides an intuitive interface for document management, chat interactions, system configuration, and analytics and monitoring.\n* API Layer: Implemented using FastAPI, this layer provides endpoints for document upload and management, chat interactions, system configuration, and analytics data retrieval.\n* RAG Engine: The core engine consists of three sub-components:\n\t+ Document Processing: Handles file validation and parsing, text extraction, chunking with configurable strategies, and metadata extraction.\n\t+ Vector Store: Responsible for storing document embeddings, efficient similarity search, and metadata filtering.\n\t+ LLM Integration: Connects to Ollama for local LLM inference, manages prompt templates, and handles context window optimization.\n\nThese components work together to provide advanced RAG capabilities, such as multi-document retrieval, enterprise security, and other features mentioned in the product specifications (Source: [2] product_specifications.csv).\n\nPlease note that this answer is based primarily on the provided context, specifically [1] technical_documentation.md. If you have any further questions or concerns, feel free to ask!",
    "expected_facts": [
      "modular architecture",
      "Frontend Layer",
      "API Layer",
      "RAG Engine",
      "HTML, CSS, and JavaScript",
      "FastAPI"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "technical_doc",
        "relevance_score": 0.6709409430541496
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4804580121776464
      },
      {
        "document_id": "business_report",
        "relevance_score": 0.2760642660368716
      }
    ]
  },
  {
    "query": "What was the revenue reported in Q1 2025?",
    "answer": "Based on the provided context, specifically [1] Source: quarterly_report.txt, I can find the answer to your question.\n\nAccording to the \"Financial Performance\" section of the report, the company achieved $4.2M in revenue for Q1 2025, representing a 15% increase year-over-year.\n\nSource: [1]\n\nTherefore, the reported revenue for Q1 2025 was $4.2 million.",
    "expected_facts": [
      "$4.2M",
      "15% increase",
      "year-over-year",
      "net profit of $1.4M"
    ],
    "facts_found": 3,
    "fact_percentage": 75.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "business_report",
        "relevance_score": 0.6630528548563427
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4826635659687333
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.418785820604604
      }
    ]
  },
  {
    "query": "What are the components of the RAG engine?",
    "answer": "Based on the provided context, I found the relevant information about the RAG engine's components.\n\nAccording to [1] Source: technical_documentation.md, Tags: technical,documentation,architecture, Folder: /test:\n\n\"The core RAG engine consists of:\n\n#### Document Processing\n\nThe document processing pipeline handles:\n- File validation and parsing\n- Text extraction\n- Chunking with configurable strategies\n- Metadata extraction\n\n#### Vector Store\n\nThe vector store is responsible for:\n- Storing document embeddings\n- Efficient similarity search\n- Metadata filtering\n\n#### LLM Integration\n\nThe LLM integration component:\n- Connects to Ollama for local LLM inference\n- Manages prompt templates\n- Handles context window optimization\"\n\nFrom this, we can conclude that the RAG engine has three main components: Document Processing, Vector Store, and LLM Integration.\n\nI did not find any additional information about the RAG engine's components in the other provided contexts ([2] Source: product_specifications.csv, Tags: product,specifications,pricing, Folder: /test) or [3] Source: quarterly_report.txt, Tags: business,report,quarterly, Folder: /test).\n\nTherefore, my answer is based primarily on the information found in [1].",
    "expected_facts": [
      "Document Processing",
      "Vector Store",
      "LLM Integration",
      "chunking",
      "embeddings",
      "Ollama"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "technical_doc",
        "relevance_score": 0.6109136250495526
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4640470886109769
      },
      {
        "document_id": "business_report",
        "relevance_score": 0.3472817564888965
      }
    ]
  },
  {
    "query": "What are the strategic initiatives for Q2?",
    "answer": "I'll do my best to provide an accurate and helpful response based on the provided context.\n\nAccording to the Quarterly Business Report [1], the strategic initiatives planned for Q2 2025 are:\n\n* International Expansion: Launch in European markets\n* Enterprise Solution: Develop and release enterprise-grade features\n* Strategic Partnerships: Form alliances with complementary service providers\n* Operational Efficiency: Implement automation to reduce operational costs\n\nThese initiatives aim to build upon the company's strong growth in key metrics, including revenue, customer acquisition, and product engagement.\n\nPlease note that I've based my response primarily on the provided context. If more information is needed or clarification is required, please let me know!",
    "expected_facts": [
      "International Expansion",
      "European markets",
      "Enterprise Solution",
      "Strategic Partnerships",
      "Operational Efficiency"
    ],
    "facts_found": 5,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "business_report",
        "relevance_score": 0.5109074115398808
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.38224892211823436
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.3782355963681827
      }
    ]
  },
  {
    "query": "What products are available and at what price points?",
    "answer": "I've reviewed the provided context and will do my best to provide an accurate answer based on the available information.\n\nAccording to the product specifications [1], Metis RAG offers several products with different price points:\n\n* MetisRAG Enterprise: $4999\n* MetisRAG Professional: $1999\n* MetisRAG Basic: $499\n* MetisRAG API: $0.10 per query (pay-per-use)\n* MetisRAG Mobile: $9.99 per month\n\nThe context does not contain any information about other products or pricing tiers beyond these five options.\n\nPlease note that this answer is based solely on the provided context and may not include all possible products or pricing information. If you have further questions, feel free to ask!",
    "expected_facts": [
      "MetisRAG Enterprise",
      "$4999",
      "MetisRAG Professional",
      "$1999",
      "MetisRAG Basic",
      "$499"
    ],
    "facts_found": 6,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "product_specs",
        "relevance_score": 0.511074574924676
      },
      {
        "document_id": "business_report",
        "relevance_score": 0.45753814380850444
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.43169304353423765
      }
    ]
  },
  {
    "query": "What was the customer retention rate and NPS score?",
    "answer": "Based on the provided context [1], I found the customer retention rate and NPS score.\n\nAccording to the \"Customer Success\" section of the Quarterly Business Report [1], the customer retention rate remained strong at 94%. Additionally, the Net Promoter Score (NPS) improved from 42 to 48.\n\nSources: [1]",
    "expected_facts": [
      "94%",
      "Net Promoter Score",
      "improved from 42 to 48"
    ],
    "facts_found": 3,
    "fact_percentage": 100.0,
    "correct_sources": true,
    "sources": [
      {
        "document_id": "business_report",
        "relevance_score": 0.5571846440343674
      },
      {
        "document_id": "product_specs",
        "relevance_score": 0.4651842907204182
      },
      {
        "document_id": "technical_doc",
        "relevance_score": 0.40303926901945974
      }
    ]
  }
]

================
File: test_rag_retrieval.py
================
#!/usr/bin/env python3
"""
Test script for Metis RAG retrieval functionality.
This script creates test documents, uploads and processes them,
and then tests the RAG retrieval with specific queries.
"""

import os
import sys
import json
import asyncio
import logging
import uuid
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("test_rag_retrieval")

# Import RAG components
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.rag.rag_engine import RAGEngine
from app.rag.vector_store import VectorStore
from app.rag.ollama_client import OllamaClient
from app.models.document import Document, Chunk

# Test document content
MARKDOWN_CONTENT = """# Metis RAG Technical Documentation

## Introduction

This document provides technical documentation for the Metis RAG system, a Retrieval-Augmented Generation platform designed for enterprise knowledge management.

## Architecture Overview

Metis RAG follows a modular architecture with the following components:

### Frontend Layer

The frontend is built with HTML, CSS, and JavaScript, providing an intuitive interface for:
- Document management
- Chat interactions
- System configuration
- Analytics and monitoring

### API Layer

The API layer is implemented using FastAPI and provides endpoints for:
- Document upload and management
- Chat interactions
- System configuration
- Analytics data retrieval

### RAG Engine

The core RAG engine consists of:

#### Document Processing

The document processing pipeline handles:
- File validation and parsing
- Text extraction
- Chunking with configurable strategies
- Metadata extraction

#### Vector Store

The vector store is responsible for:
- Storing document embeddings
- Efficient similarity search
- Metadata filtering

#### LLM Integration

The LLM integration component:
- Connects to Ollama for local LLM inference
- Manages prompt templates
- Handles context window optimization
"""

PDF_CONTENT = """Quarterly Business Report
Executive Summary
This quarterly report provides an overview of business performance for Q1 2025. Overall, the company
has seen strong growth in key metrics including revenue, customer acquisition, and product
engagement. This document summarizes the performance across departments and outlines strategic
initiatives for the upcoming quarter.

Financial Performance
The company achieved $4.2M in revenue for Q1, representing a 15% increase year-over-year. Gross
margin improved to 72%, up from 68% in the previous quarter. Operating expenses were kept under
control at $2.8M, resulting in a net profit of $1.4M.

Product Development
The product team successfully launched 3 major features this quarter:
- Advanced Analytics Dashboard: Providing deeper insights into user behavior
- Mobile Application Redesign: Improving user experience and engagement
- API Integration Platform: Enabling third-party developers to build on our platform

User engagement metrics show a 22% increase in daily active users following these releases. The
product roadmap for Q2 focuses on scalability improvements and enterprise features.

Marketing and Sales
The marketing team executed campaigns that generated 2,500 new leads, a 30% increase from the
previous quarter. Sales conversion rate improved to 12%, resulting in 300 new customers. Customer
acquisition cost (CAC) decreased by 15% to $350 per customer.

Customer Success
Customer retention rate remained strong at 94%. Net Promoter Score (NPS) improved from 42 to 48.
The support team handled 3,200 tickets with an average response time of 2.5 hours and a satisfaction
rating of 4.8/5.

Strategic Initiatives for Q2
The following initiatives are planned for Q2 2025:
- International Expansion: Launch in European markets
- Enterprise Solution: Develop and release enterprise-grade features
- Strategic Partnerships: Form alliances with complementary service providers
- Operational Efficiency: Implement automation to reduce operational costs
"""

async def create_test_documents():
    """Create test documents for RAG testing"""
    logger.info("Creating test documents...")
    
    # Create directories for test documents
    os.makedirs("test_docs", exist_ok=True)
    
    # Create Markdown document
    markdown_path = os.path.join("test_docs", "technical_documentation.md")
    with open(markdown_path, "w") as f:
        f.write(MARKDOWN_CONTENT)
    
    # Create PDF-like text document (since we can't easily create a real PDF programmatically)
    pdf_path = os.path.join("test_docs", "quarterly_report.txt")
    with open(pdf_path, "w") as f:
        f.write(PDF_CONTENT)
    
    logger.info(f"Created test documents in {os.path.abspath('test_docs')}")
    return markdown_path, pdf_path

async def process_documents(vector_store, markdown_path, pdf_path):
    """Process the test documents and add them to the vector store"""
    logger.info("Processing test documents...")
    
    # Read file contents first
    with open(markdown_path, "r") as f:
        markdown_content = f.read()
    
    with open(pdf_path, "r") as f:
        pdf_content = f.read()
    
    # Create Document objects
    markdown_doc = Document(
        id=str(uuid.uuid4()),
        filename="technical_documentation.md",
        content=markdown_content,
        tags=["technical", "documentation", "architecture"],
        folder="/test"
    )
    
    pdf_doc = Document(
        id=str(uuid.uuid4()),
        filename="quarterly_report.txt",
        content=pdf_content,
        tags=["business", "report", "quarterly"],
        folder="/test"
    )
    
    # Fix for ChromaDB metadata issue - convert tags to string
    # We'll modify the add_document method in vector_store.py to handle this properly
    class CustomVectorStore(VectorStore):
        async def add_document(self, document: Document) -> None:
            """Override to fix tags handling"""
            try:
                logger.info(f"Adding document {document.id} to vector store")
                
                # Make sure we have an Ollama client
                if self.ollama_client is None:
                    self.ollama_client = OllamaClient()
                
                # Prepare chunks for batch processing
                chunks_to_embed = [chunk for chunk in document.chunks if not chunk.embedding]
                chunk_contents = [chunk.content for chunk in chunks_to_embed]
                
                # Create embeddings in batch if possible
                if chunk_contents:
                    try:
                        # Batch embedding
                        embeddings = await self._batch_create_embeddings(chunk_contents)
                        
                        # Assign embeddings to chunks
                        for i, chunk in enumerate(chunks_to_embed):
                            chunk.embedding = embeddings[i]
                    except Exception as batch_error:
                        logger.warning(f"Batch embedding failed: {str(batch_error)}. Falling back to sequential embedding.")
                        # Fall back to sequential embedding
                        for chunk in chunks_to_embed:
                            chunk.embedding = await self.ollama_client.create_embedding(
                                text=chunk.content,
                                model=self.embedding_model
                            )
                
                # Add chunks to the collection
                for chunk in document.chunks:
                    if not chunk.embedding:
                        logger.warning(f"Chunk {chunk.id} has no embedding, skipping")
                        continue
                        
                    # Convert tags to string to avoid ChromaDB error
                    tags_str = ",".join(document.tags) if document.tags else ""
                    
                    self.collection.add(
                        ids=[chunk.id],
                        embeddings=[chunk.embedding],
                        documents=[chunk.content],
                        metadatas=[{
                            "document_id": document.id,
                            "chunk_index": chunk.metadata.get("index", 0),
                            "filename": document.filename,
                            "tags_str": tags_str,  # Use string instead of list
                            "folder": document.folder,
                            **chunk.metadata
                        }]
                    )
                
                logger.info(f"Added {len(document.chunks)} chunks to vector store for document {document.id}")
            except Exception as e:
                logger.error(f"Error adding document {document.id} to vector store: {str(e)}")
                raise
    
    # Use our custom vector store
    vector_store = CustomVectorStore()
    
    # Create chunks for Markdown document
    markdown_chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=markdown_content,
            metadata={
                "index": 0,
                "source": markdown_path
            }
        )
    ]
    
    # Create chunks for PDF document
    pdf_chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            content=pdf_content,
            metadata={
                "index": 0,
                "source": pdf_path
            }
        )
    ]
    
    # Assign chunks to documents
    markdown_doc.chunks = markdown_chunks
    pdf_doc.chunks = pdf_chunks
    
    # Add documents to vector store
    await vector_store.add_document(markdown_doc)
    await vector_store.add_document(pdf_doc)
    
    logger.info(f"Added documents to vector store: {markdown_doc.id}, {pdf_doc.id}")
    return markdown_doc, pdf_doc, vector_store

async def test_queries(rag_engine):
    """Test RAG retrieval with specific queries"""
    logger.info("Testing RAG retrieval with specific queries...")
    
    test_queries = [
        "What is the architecture of Metis RAG?",
        "What was the revenue reported in Q1 2025?",
        "What are the components of the RAG engine?",
        "What are the strategic initiatives for Q2?",
        "How does the vector store work?",
        "What was the customer retention rate?",
    ]
    
    results = []
    for query in test_queries:
        logger.info(f"Testing query: {query}")
        
        # Execute query with RAG
        response = await rag_engine.query(
            query=query,
            use_rag=True,
            top_k=3,
            stream=False
        )
        
        # Log results
        answer = response.get("answer", "")
        sources = response.get("sources", [])
        
        logger.info(f"Query: {query}")
        logger.info(f"Answer: {answer[:100]}...")
        logger.info(f"Sources: {len(sources)} chunks retrieved")
        
        # Store results for analysis
        results.append({
            "query": query,
            "answer": answer,
            "sources": [
                {
                    "document_id": s.document_id,
                    "chunk_id": s.chunk_id,
                    "relevance_score": s.relevance_score,
                    "excerpt": s.excerpt[:100] + "..." if len(s.excerpt) > 100 else s.excerpt
                }
                for s in sources
            ] if sources else []
        })
    
    # Save results to file
    results_path = os.path.join("test_docs", "rag_test_results.json")
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Test results saved to {os.path.abspath(results_path)}")
    return results

async def analyze_results(results):
    """Analyze the test results"""
    logger.info("Analyzing test results...")
    
    # Check if each query has sources
    queries_with_sources = sum(1 for r in results if r["sources"])
    logger.info(f"{queries_with_sources}/{len(results)} queries returned sources")
    
    # Check if answers contain source references
    answers_with_references = sum(1 for r in results if "[" in r["answer"] and "]" in r["answer"])
    logger.info(f"{answers_with_references}/{len(results)} answers contain source references")
    
    # Check for specific content in answers
    architecture_query = next((r for r in results if "architecture" in r["query"].lower()), None)
    revenue_query = next((r for r in results if "revenue" in r["query"].lower()), None)
    
    if architecture_query:
        has_architecture_info = any(keyword in architecture_query["answer"].lower() 
                                   for keyword in ["frontend", "api", "rag engine", "modular"])
        logger.info(f"Architecture query contains relevant information: {has_architecture_info}")
    
    if revenue_query:
        has_revenue_info = any(keyword in revenue_query["answer"].lower() 
                              for keyword in ["4.2", "million", "15%", "increase"])
        logger.info(f"Revenue query contains relevant information: {has_revenue_info}")
    
    # Overall assessment
    success_rate = (queries_with_sources / len(results)) * 100
    logger.info(f"Overall RAG retrieval success rate: {success_rate:.1f}%")
    
    return {
        "queries_with_sources": queries_with_sources,
        "total_queries": len(results),
        "answers_with_references": answers_with_references,
        "success_rate": success_rate
    }

async def main():
    """Main test function"""
    logger.info("Starting RAG retrieval test...")
    
    try:
        # Create test documents
        markdown_path, pdf_path = await create_test_documents()
        
        # Process documents - this now creates and returns our custom vector store
        markdown_doc, pdf_doc, vector_store = await process_documents(None, markdown_path, pdf_path)
        
        # Initialize RAG engine with our custom vector store
        rag_engine = RAGEngine(vector_store=vector_store)
        
        # Test queries
        results = await test_queries(rag_engine)
        
        # Analyze results
        analysis = await analyze_results(results)
        
        logger.info("RAG retrieval test completed successfully")
        logger.info(f"Success rate: {analysis['success_rate']:.1f}%")
        
    except Exception as e:
        logger.error(f"Error during RAG retrieval test: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: test_response_time_results.json
================
{
  "results": [
    {
      "query": "What is the architecture of Metis RAG?",
      "response_time_ms": 14820.42121887207,
      "answer_length": 1476,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0057449340820312,
      "memory_mb": 164.78125,
      "threads": 23,
      "system_cpu_percent": 11.5,
      "system_memory_percent": 81.5
    },
    {
      "query": "How does the document processing pipeline work?",
      "response_time_ms": 12753.172874450684,
      "answer_length": 1386,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0125160217285156,
      "memory_mb": 165.890625,
      "threads": 23,
      "system_cpu_percent": 9.7,
      "system_memory_percent": 81.5
    },
    {
      "query": "What is the role of the vector store?",
      "response_time_ms": 4216.8519496917725,
      "answer_length": 346,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0172843933105469,
      "memory_mb": 166.671875,
      "threads": 23,
      "system_cpu_percent": 9.4,
      "system_memory_percent": 81.6
    },
    {
      "query": "How does the frontend interface with the API layer?",
      "response_time_ms": 14805.053234100342,
      "answer_length": 1595,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0178565979003906,
      "memory_mb": 166.765625,
      "threads": 23,
      "system_cpu_percent": 8.4,
      "system_memory_percent": 80.6
    },
    {
      "query": "What is the purpose of the LLM integration component?",
      "response_time_ms": 10764.720916748047,
      "answer_length": 1051,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0232925415039062,
      "memory_mb": 167.65625,
      "threads": 23,
      "system_cpu_percent": 8.6,
      "system_memory_percent": 81.0
    },
    {
      "query": "Explain the chunking strategies used in document processing.",
      "response_time_ms": 12380.050897598267,
      "answer_length": 1321,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0245323181152344,
      "memory_mb": 167.859375,
      "threads": 23,
      "system_cpu_percent": 8.7,
      "system_memory_percent": 81.4
    },
    {
      "query": "How does the system handle metadata filtering?",
      "response_time_ms": 7647.094964981079,
      "answer_length": 786,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0296821594238281,
      "memory_mb": 168.703125,
      "threads": 23,
      "system_cpu_percent": 8.6,
      "system_memory_percent": 81.4
    },
    {
      "query": "What technologies are used in the frontend layer?",
      "response_time_ms": 4877.293109893799,
      "answer_length": 427,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0300636291503906,
      "memory_mb": 168.765625,
      "threads": 23,
      "system_cpu_percent": 9.6,
      "system_memory_percent": 81.0
    },
    {
      "query": "How does the RAG engine retrieve relevant information?",
      "response_time_ms": 6343.724012374878,
      "answer_length": 612,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0326385498046875,
      "memory_mb": 169.1875,
      "threads": 23,
      "system_cpu_percent": 13.0,
      "system_memory_percent": 81.2
    },
    {
      "query": "What is the overall purpose of the Metis RAG system?",
      "response_time_ms": 9755.89895248413,
      "answer_length": 1028,
      "sources_count": 1,
      "cpu_percent": 0.0,
      "memory_percent": 1.0327339172363281,
      "memory_mb": 169.203125,
      "threads": 23,
      "system_cpu_percent": 8.9,
      "system_memory_percent": 81.5
    }
  ],
  "statistics": {
    "min_response_time_ms": 4216.8519496917725,
    "max_response_time_ms": 14820.42121887207,
    "avg_response_time_ms": 9836.428213119507,
    "median_response_time_ms": 10260.309934616089,
    "stddev_response_time_ms": 3923.403709708135,
    "total_queries": 10
  }
}

================
File: TESTING.md
================
# Metis RAG Testing Strategy

This document outlines the comprehensive testing strategy for the Metis RAG application, covering automated unit tests, integration tests, performance benchmarks, UI testing, and more.

## Table of Contents

1. [Testing Objectives](#testing-objectives)
2. [Testing Levels](#testing-levels)
3. [Test Implementation](#test-implementation)
4. [Running Tests](#running-tests)
5. [Test Reports](#test-reports)
6. [CI/CD Integration](#cicd-integration)

## Testing Objectives

The primary objectives of this testing strategy are to:

1. Ensure the factual accuracy and quality of RAG responses
2. Verify proper handling of various file types and multiple file uploads
3. Test core RAG components (retrieval, augmentation, generation)
4. Validate end-to-end functionality across system boundaries
5. Measure and optimize performance
6. Identify and address edge cases and potential failure points
7. Establish regression testing to prevent feature regressions
8. Generate detailed test reports for demonstration purposes

## Testing Levels

### Unit Testing

Unit tests focus on testing individual components in isolation:

- **RAG Engine Testing**: Tests query function, context retrieval, response generation, conversation history handling, and citation generation.
- **Vector Store Testing**: Tests document addition and retrieval, embedding creation and storage, search functionality, metadata filtering, and caching mechanisms.
- **Document Processor Testing**: Tests different chunking strategies, handling of various file types, metadata extraction, and error handling.
- **Ollama Client Testing**: Tests model generation, embedding creation, streaming vs. non-streaming responses, and error handling.

### Integration Testing

Integration tests focus on testing component interactions and API endpoints:

- **API Endpoint Testing**: Tests all API endpoints for correct responses, error handling, request validation, and authentication.
- **Component Interaction Testing**: Tests RAG Engine with Vector Store, Document Processor with Vector Store, RAG Engine with Ollama Client, and end-to-end document processing and retrieval flow.

### System Testing

System tests focus on testing end-to-end workflows and performance:

- **End-to-End Workflow Testing**: Tests complete document upload, processing, and retrieval workflow, chat conversation with context from multiple documents, and analytics functionality.
- **Performance Testing**: Measures response time, throughput, and resource utilization under various loads.
- **Stress Testing**: Tests with high concurrency, large document sets, and complex queries.

### Edge Case Testing

Edge case tests focus on testing unusual inputs, error handling, and system resilience:

- **Input Validation**: Tests with malformed queries, invalid documents, and extremely long or short inputs.
- **Error Handling**: Tests network failures, service unavailability, and database connection issues.
- **Resource Constraints**: Tests under memory limitations, limited disk space, and CPU constraints.

## Test Implementation

The test implementation is organized into several test suites:

### RAG Quality Tests (`tests/test_rag_quality.py`)

Tests for factual accuracy, relevance, and citation quality:

- **Factual Accuracy Testing**: Tests if responses contain expected facts from the source documents.
- **Multi-Document Retrieval**: Tests retrieval across multiple documents.
- **Citation Quality**: Tests if responses properly cite sources.
- **API Integration**: Tests the complete flow through the API endpoints.

### File Handling Tests (`tests/test_file_handling.py`)

Tests for different file types, multiple file uploads, and large files:

- **File Type Support**: Tests processing of different file types (PDF, TXT, CSV, MD).
- **Chunking Strategies**: Tests different chunking strategies (recursive, token, markdown).
- **Large File Handling**: Tests processing of very large files.
- **Multiple File Upload**: Tests concurrent processing of multiple documents.

### Performance Tests (`tests/test_performance.py`)

Tests for response time, throughput, and resource utilization:

- **Query Response Time**: Measures response time for different query types.
- **Throughput**: Measures throughput under various concurrency levels.
- **Resource Utilization**: Monitors CPU and memory usage during sustained load.
- **API Performance**: Measures performance of API endpoints.

### Edge Case Tests (`tests/test_edge_cases.py`)

Tests for unusual inputs, error handling, and system resilience:

- **Special Character Queries**: Tests queries with special characters, SQL injection, XSS, etc.
- **Malformed Documents**: Tests handling of malformed or corrupted documents.
- **Network Failures**: Tests handling of network failures when communicating with Ollama.
- **Invalid Model Names**: Tests handling of invalid model names.
- **Concurrent Processing**: Tests handling of concurrent document processing requests.
- **Vector Store Resilience**: Tests vector store resilience to invalid operations.

## Running Tests

### Prerequisites

- Python 3.10 or higher
- All dependencies installed (`pip install -r requirements.txt`)
- Ollama running locally (for tests that interact with the LLM)

### Running All Tests

To run all tests, use the `run_tests.py` script:

```bash
python run_tests.py
```

This will run all test suites and generate a comprehensive report.

### Running Specific Test Suites

To run a specific test suite, use the `--suite` option:

```bash
python run_tests.py --suite rag_quality
python run_tests.py --suite file_handling
python run_tests.py --suite performance
python run_tests.py --suite edge_cases
```

### Additional Options

- `--failfast`: Stop on first failure
- `--open-report`: Open the HTML report in a browser
- `--save-reports`: Save reports to a timestamped directory

### Running Individual Tests

You can also run individual test files directly using pytest:

```bash
pytest -xvs tests/test_rag_quality.py
pytest -xvs tests/test_file_handling.py
pytest -xvs tests/test_performance.py
pytest -xvs tests/test_edge_cases.py
```

## Test Reports

The test runner generates several reports:

- **Master Report**: `metis_rag_test_report.json` and `metis_rag_test_report.html`
- **RAG Quality Reports**: `test_quality_results.json`, `test_multi_doc_results.json`, etc.
- **File Handling Reports**: `test_file_type_results.json`, `test_chunking_strategy_results.json`, etc.
- **Performance Reports**: `performance_benchmark_report.json`, `performance_benchmark_report.html`, etc.
- **Edge Case Reports**: `edge_case_test_report.json`, `edge_case_test_report.html`, etc.

These reports provide detailed information about test results, including:

- Test success/failure status
- Performance metrics
- Factual accuracy metrics
- Edge case handling metrics
- Detailed logs and error messages

## CI/CD Integration

The testing framework is designed to be integrated into a CI/CD pipeline. Here's a recommended approach:

1. **Continuous Integration**: Run unit and integration tests on every commit.
2. **Nightly Builds**: Run performance and edge case tests nightly.
3. **Release Validation**: Run all tests before releasing a new version.

### GitHub Actions Example

```yaml
name: Metis RAG Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run tests
      run: |
        python run_tests.py
    - name: Upload test reports
      uses: actions/upload-artifact@v2
      with:
        name: test-reports
        path: |
          metis_rag_test_report.html
          performance_benchmark_report.html
          edge_case_test_report.html
```

## Extending the Test Suite

To add new tests:

1. Create a new test file in the `tests` directory.
2. Add the test file to the appropriate test suite in `run_tests.py`.
3. Implement test functions using the pytest framework.
4. Generate test reports in JSON format for integration with the reporting system.

## Best Practices

- **Isolation**: Ensure tests are isolated and don't depend on each other.
- **Cleanup**: Clean up any resources created during tests.
- **Mocking**: Use mocks for external dependencies when appropriate.
- **Assertions**: Use clear assertions that provide helpful error messages.
- **Documentation**: Document test functions with clear docstrings.
- **Reporting**: Generate detailed reports for analysis and visualization.



================================================================
End of Codebase
================================================================
